{
    "docs": [
        {
            "location": "/", 
            "text": "OSG Site Administrator Documentation\n\n\n\n\nNote\n\n\nThe documentation contained here is currently a work in progress as we migrate our TWiki documentation to GitHub. Please let us know of any issues that you encounter by \nopening a ticket\n or e-mailing \ngoc@opensciencegrid.org\n. Please identify errors, inconsistencies, or incorrect information in the documentation, and also suggest extra content that you feel would be helpful to include in the documentation.\n\n\n\n\nWelcome to the home page of the Open Science Grid (OSG) Site Administrator documentation! If you're a new OSG site adminstrator, start with the \nInstallation Overview\n. If you are not a site adminstrator...\n\n\n\n\nIf you are a researcher interested in using OSG resources, you can find user documentation \nhere\n. \n\n\nIf you'd like to learn more about the OSG and our mission, visit our website \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#osg-site-administrator-documentation", 
            "text": "Note  The documentation contained here is currently a work in progress as we migrate our TWiki documentation to GitHub. Please let us know of any issues that you encounter by  opening a ticket  or e-mailing  goc@opensciencegrid.org . Please identify errors, inconsistencies, or incorrect information in the documentation, and also suggest extra content that you feel would be helpful to include in the documentation.   Welcome to the home page of the Open Science Grid (OSG) Site Administrator documentation! If you're a new OSG site adminstrator, start with the  Installation Overview . If you are not a site adminstrator...   If you are a researcher interested in using OSG resources, you can find user documentation  here .   If you'd like to learn more about the OSG and our mission, visit our website  here .", 
            "title": "OSG Site Administrator Documentation"
        }, 
        {
            "location": "/install-overview/", 
            "text": "OSG Site Installation Overview\n\n\nThis document outlines the overall installation process for an OSG site and provides many links into detailed installation, configuration, troubleshooting, and similar pages. If you do not see software-related technical documentation listed here, try the search bar to the left or contacting us at \ngoc@opensciencegrid.org\n.\n\n\nPlan the Site\n\n\nIf you have not done so already, \nplan the overall architecture of your OSG site\n. It is recommended that your plan be sufficiently detailed to include the OSG hosts that are needed and the main software components for each host. Be sure to consider \nthe operating systems that OSG supports\n. A simple way to organize this information is in a table; for example, a basic site might include:\n\n\n\n\n\n\n\n\nPurpose\n\n\nHost\n\n\nMajor Software\n\n\n\n\n\n\n\n\n\n\nCompute Element (CE)\n\n\nosg-ce.example.edu\n\n\nOSG CE, HTCondor Central Manager, etc. (\nosg-ce-condor\n)\n\n\n\n\n\n\nWorker Nodes\n\n\nwNNN.cluster.example.edu\n\n\nOSG worker node client (\nosg-wn-client\n)\n\n\n\n\n\n\n\n\nPrepare the Batch System\n\n\nThe assumption is that you have an existing batch system at your site. Currently, we support \nHTCondor\n, \nLSF\n, \nPBS\n and \nTORQUE\n, \nSGE\n, and \nSlurm\n batch systems.\n\n\nFor smaller sites (less than 50 worker nodes), the most common way to add a site to OSG is to install the OSG Compute Element (CE) on the central host of your batch system.  At such a site - especially if you have minimal time to maintain a CE - you may want to contact goc@opensciencegrid.org to ask about using an OSG-hosted CE instead of running your own.  Before proceeding with an install, be sure that you can submit and successfully run a job from your OSG CE host into your batch system.\n\n\nAdd OSG Software\n\n\nIf necessary, provision all OSG hosts that are in your site plan and that do not exist yet.\n\n\n\n\nNote\n\n\nFor sites with more than a trivial number of hosts, it is recommended to use some sort of configuration management tool to install, configure, and maintain your site. While beyond the scope of OSG\u2019s documentation to explain how to select and use such a system, some popular configuration management tools are \nPuppet\n, \nChef\n, \nAnsible\n, and \nCFEngine\n.\n\n\n\n\nGeneral Installation Instructions\n\n\n\n\nSecurity information for OSG signed RPMs\n\n\nUsing Yum and RPM\n\n\nInstall the OSG repositories\n\n\nOSG Software release series\n - look here to upgrade from OSG 3.1 to OSG 3.2 or from OSG 3.2 to OSG 3.3\n\n\nInstallation best practices\n\n\nFirewalls the complete guide\n\n\n\n\nInstalling and Managing Certificates for Site Security\n\n\n\n\nInstalling the grid certificate authorities (CAs)\n\n\nHow do I get PKI host and service X.509 certificates?\n\n\nAutomatically updating the grid certificate authorities (CAs)\n\n\nSHA-2 certificates and minimum required OSG software versions\n\n\nOSG PKI command line client reference\n\n\n\n\nAdding OSG Software to Worker Nodes\n\n\n\n\nWorker Node (WN) Client Overview\n\n\nInstall the WN client software on every worker node \u2013 pick a method:\n\n\nUsing RPMs\n \u2013 useful when managing your worker nodes with a tool (e.g., Puppet, Chef)\n\n\nUsing a tarball\n \u2013 useful for installation onto a shared filesystem (does not require root access)\n\n\nUsing OASIS\n \u2013 useful when \nCVMFS\n is already mounted on your worker nodes\n\n\n\n\n\n\n(optional) \nInstall the CernVM-FS client\n to make it easy for user jobs to use needed software from OSG's OASIS repositories\n\n\n(optional) \nInstall singularity on the OSG worker node\n, to allow pilot jobs to isolate user jobs.\n\n\n\n\nInstalling and Configuring the Compute Element\n\n\n\n\nPreparing to install the compute element\n\n\nInstall the compute element (HTCondor-CE and other software):\n\n\nOverview and architecture\n\n\nInstall HTCondor-CE\n\n\nConfigure the HTCondor-CE job router\n, including common recipes\n\n\nTroubleshooting HTCondor-CE installations\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\n\n\nTroubleshooting osg-configure\n\n\n\n\nInstalling and Configuring Other Nodes\n\n\nAll of these node types and their services are optional, although OSG requires the Frontier Squid caching service if you have installed \nCVMFS\n on your worker nodes.\n\n\n\n\nInstall Frontier Squid, the HTTP caching proxy service\n\n\nRSV monitoring to monitor and report to OSG on the health of your site\n\n\nRSV Overview\n\n\nInstall RSV\n\n\nTroubleshooting RSV\n\n\n\n\n\n\nInstall the GlideinWMS VO Frontend\n if your want your users\u2019 jobs to run on the OSG\n\n\nInstall the RSV GlideinWMS Tester\n if you want to test your front-end's ability to submit jobs to sites in the OSG\n\n\n\n\n\n\nStorage element (pick one):\n\n\nGridFTP\n\n\nInstall standalone OSG GridFTP\n: GridFTP server\n\n\n(optional) \nInstall load-balanced OSG GridFTP\n: when a single GridFTP server isn't enough\n\n\n\n\n\n\nBeStMan\n\n\nBeStMan Overview\n: Bestman-related information, planning, and guides\n\n\nInstall Bestman SE\n: BeStMan2 SRM server + GridFTP server\n\n\nInstall Bestman Gateway Hadoop\n: BeStMan2 SRM server + GridFTP server + Hadoop\n\n\n\n\n\n\nHadoop Distributed File System (HDFS)\n\n\nHadoop Overview\n: HDFS information, planning, and guides\n\n\n\n\n\n\nXRootD\n\n\nXRootd Overview\n: XRootD information, planning, and guides\n\n\nInstall Xrootd Server\n: XRootD redirector installation\n\n\nInstall BeStMan-Gateway XRootD\n: BeStMan2 SRM server + GridFTP server + XRootD fuse\n\n\n\n\n\n\n\n\n\n\n\n\nTest OSG Software\n\n\nAt very least, it is vital to test \nmanual\n submission of jobs from inside and outside of your site through your CE to your batch system. If this process does not work manually, it will probably not work for the glideinWMS pilot factory either.\n\n\n\n\nTest job submission into an HTCondor-CE\n\n\nOSG Troubleshooting guide\n\n\nValidating Supported VOs\n\n\n\n\nStart GlideinWMS Pilot Submissions\n\n\nTo begin running \nGlideinWMS\n pilot jobs at your site, e-mail \n and tell them that you want to start accepting Glideins. Please provide them with the following information:\n\n\n\n\nThe type of CE (HTCondor-CE or the now-unsupported GRAM-CE)\n\n\nThe fully qualified hostname of the CE\n\n\nResource/WLCG name\n\n\nOS major version of your worker nodes\u00a0\u2014 EL\u00a06, EL\u00a07, or a mix of both?\n\n\nDo you accept multicore jobs?\n\n\nMaximum job walltime\n\n\nMaximum job memory usage\n\n\n\n\nOnce the factory team has enough information, they will start submitting pilots from the test factory to your CE. Initially, this will be one pilot at a time but once the factory verifies that pilot jobs are running successfully, that number will be ramped up to 10, then 100.\n\n\nVerify Reporting and Monitoring\n\n\nTo verify that your site is correctly reporting to the OSG, check \nOSG's Accounting Portal\n for records of your site reports (select your site from the drop-down box). If you have enabled the OSG VO, you can also check \nhttp://osg-flock.grid.iu.edu/monitoring/condor/sites/all_1day.html\n.\n\n\nScale Up Site to Full Production\n\n\nAfter successfully running all the pilot jobs that are submitted by the test factory and verifying your site reports, your site will be deemed production ready. No action is required on your end, factory operations will start submitting pilot jobs from the production factory.", 
            "title": "Installation Overview"
        }, 
        {
            "location": "/install-overview/#osg-site-installation-overview", 
            "text": "This document outlines the overall installation process for an OSG site and provides many links into detailed installation, configuration, troubleshooting, and similar pages. If you do not see software-related technical documentation listed here, try the search bar to the left or contacting us at  goc@opensciencegrid.org .", 
            "title": "OSG Site Installation Overview"
        }, 
        {
            "location": "/install-overview/#plan-the-site", 
            "text": "If you have not done so already,  plan the overall architecture of your OSG site . It is recommended that your plan be sufficiently detailed to include the OSG hosts that are needed and the main software components for each host. Be sure to consider  the operating systems that OSG supports . A simple way to organize this information is in a table; for example, a basic site might include:     Purpose  Host  Major Software      Compute Element (CE)  osg-ce.example.edu  OSG CE, HTCondor Central Manager, etc. ( osg-ce-condor )    Worker Nodes  wNNN.cluster.example.edu  OSG worker node client ( osg-wn-client )", 
            "title": "Plan the Site"
        }, 
        {
            "location": "/install-overview/#prepare-the-batch-system", 
            "text": "The assumption is that you have an existing batch system at your site. Currently, we support  HTCondor ,  LSF ,  PBS  and  TORQUE ,  SGE , and  Slurm  batch systems.  For smaller sites (less than 50 worker nodes), the most common way to add a site to OSG is to install the OSG Compute Element (CE) on the central host of your batch system.  At such a site - especially if you have minimal time to maintain a CE - you may want to contact goc@opensciencegrid.org to ask about using an OSG-hosted CE instead of running your own.  Before proceeding with an install, be sure that you can submit and successfully run a job from your OSG CE host into your batch system.", 
            "title": "Prepare the Batch System"
        }, 
        {
            "location": "/install-overview/#add-osg-software", 
            "text": "If necessary, provision all OSG hosts that are in your site plan and that do not exist yet.   Note  For sites with more than a trivial number of hosts, it is recommended to use some sort of configuration management tool to install, configure, and maintain your site. While beyond the scope of OSG\u2019s documentation to explain how to select and use such a system, some popular configuration management tools are  Puppet ,  Chef ,  Ansible , and  CFEngine .", 
            "title": "Add OSG Software"
        }, 
        {
            "location": "/install-overview/#general-installation-instructions", 
            "text": "Security information for OSG signed RPMs  Using Yum and RPM  Install the OSG repositories  OSG Software release series  - look here to upgrade from OSG 3.1 to OSG 3.2 or from OSG 3.2 to OSG 3.3  Installation best practices  Firewalls the complete guide", 
            "title": "General Installation Instructions"
        }, 
        {
            "location": "/install-overview/#installing-and-managing-certificates-for-site-security", 
            "text": "Installing the grid certificate authorities (CAs)  How do I get PKI host and service X.509 certificates?  Automatically updating the grid certificate authorities (CAs)  SHA-2 certificates and minimum required OSG software versions  OSG PKI command line client reference", 
            "title": "Installing and Managing Certificates for Site Security"
        }, 
        {
            "location": "/install-overview/#adding-osg-software-to-worker-nodes", 
            "text": "Worker Node (WN) Client Overview  Install the WN client software on every worker node \u2013 pick a method:  Using RPMs  \u2013 useful when managing your worker nodes with a tool (e.g., Puppet, Chef)  Using a tarball  \u2013 useful for installation onto a shared filesystem (does not require root access)  Using OASIS  \u2013 useful when  CVMFS  is already mounted on your worker nodes    (optional)  Install the CernVM-FS client  to make it easy for user jobs to use needed software from OSG's OASIS repositories  (optional)  Install singularity on the OSG worker node , to allow pilot jobs to isolate user jobs.", 
            "title": "Adding OSG Software to Worker Nodes"
        }, 
        {
            "location": "/install-overview/#installing-and-configuring-the-compute-element", 
            "text": "Preparing to install the compute element  Install the compute element (HTCondor-CE and other software):  Overview and architecture  Install HTCondor-CE  Configure the HTCondor-CE job router , including common recipes  Troubleshooting HTCondor-CE installations  Submitting jobs to HTCondor-CE    Troubleshooting osg-configure", 
            "title": "Installing and Configuring the Compute Element"
        }, 
        {
            "location": "/install-overview/#installing-and-configuring-other-nodes", 
            "text": "All of these node types and their services are optional, although OSG requires the Frontier Squid caching service if you have installed  CVMFS  on your worker nodes.   Install Frontier Squid, the HTTP caching proxy service  RSV monitoring to monitor and report to OSG on the health of your site  RSV Overview  Install RSV  Troubleshooting RSV    Install the GlideinWMS VO Frontend  if your want your users\u2019 jobs to run on the OSG  Install the RSV GlideinWMS Tester  if you want to test your front-end's ability to submit jobs to sites in the OSG    Storage element (pick one):  GridFTP  Install standalone OSG GridFTP : GridFTP server  (optional)  Install load-balanced OSG GridFTP : when a single GridFTP server isn't enough    BeStMan  BeStMan Overview : Bestman-related information, planning, and guides  Install Bestman SE : BeStMan2 SRM server + GridFTP server  Install Bestman Gateway Hadoop : BeStMan2 SRM server + GridFTP server + Hadoop    Hadoop Distributed File System (HDFS)  Hadoop Overview : HDFS information, planning, and guides    XRootD  XRootd Overview : XRootD information, planning, and guides  Install Xrootd Server : XRootD redirector installation  Install BeStMan-Gateway XRootD : BeStMan2 SRM server + GridFTP server + XRootD fuse", 
            "title": "Installing and Configuring Other Nodes"
        }, 
        {
            "location": "/install-overview/#test-osg-software", 
            "text": "At very least, it is vital to test  manual  submission of jobs from inside and outside of your site through your CE to your batch system. If this process does not work manually, it will probably not work for the glideinWMS pilot factory either.   Test job submission into an HTCondor-CE  OSG Troubleshooting guide  Validating Supported VOs", 
            "title": "Test OSG Software"
        }, 
        {
            "location": "/install-overview/#start-glideinwms-pilot-submissions", 
            "text": "To begin running  GlideinWMS  pilot jobs at your site, e-mail   and tell them that you want to start accepting Glideins. Please provide them with the following information:   The type of CE (HTCondor-CE or the now-unsupported GRAM-CE)  The fully qualified hostname of the CE  Resource/WLCG name  OS major version of your worker nodes\u00a0\u2014 EL\u00a06, EL\u00a07, or a mix of both?  Do you accept multicore jobs?  Maximum job walltime  Maximum job memory usage   Once the factory team has enough information, they will start submitting pilots from the test factory to your CE. Initially, this will be one pilot at a time but once the factory verifies that pilot jobs are running successfully, that number will be ramped up to 10, then 100.", 
            "title": "Start GlideinWMS Pilot Submissions"
        }, 
        {
            "location": "/install-overview/#verify-reporting-and-monitoring", 
            "text": "To verify that your site is correctly reporting to the OSG, check  OSG's Accounting Portal  for records of your site reports (select your site from the drop-down box). If you have enabled the OSG VO, you can also check  http://osg-flock.grid.iu.edu/monitoring/condor/sites/all_1day.html .", 
            "title": "Verify Reporting and Monitoring"
        }, 
        {
            "location": "/install-overview/#scale-up-site-to-full-production", 
            "text": "After successfully running all the pilot jobs that are submitted by the test factory and verifying your site reports, your site will be deemed production ready. No action is required on your end, factory operations will start submitting pilot jobs from the production factory.", 
            "title": "Scale Up Site to Full Production"
        }, 
        {
            "location": "/common/install-best-practices/", 
            "text": "Installation Best Practices\n\n\nAbout This Document\n\n\nThis document covers best practices to be used in installing software packages or security certificates on large numbers of hosts. It assumes you understand \nYUM/RPM Basics\n.\n\n\nInstall yum-priorities\n\n\nWe use \nyum\n priorities to ensure that we get the right packages from the OSG repositories. Please install it:\n\n\nroot@host #\n yum install yum-priorities\n\n\n\n\n\nThere is \ngood documentation\n in the wild about yum priorities, but we'll summarize the essential bits for you.\n\n\nThe basic idea of yum priorities is that if a package is found in two repositories, priorities will influence how yum chooses which package to install. We want the OSG software repository to be chosen instead of the EPEL repository so software, such as Globus, comes from our repository instead of the EPEL repository. (This is important because you will have installation errors if you get the EPEL Globus.)\n\n\nYum priorities is a \nyum plugin\n. Once enabled, you can set the priority of each repository (located in /etc/yum.repos.d) The lower the numerical priority, the better the priority. The default priority for a repository when it's not specified is 99. We set the priority for the OSG repository to be 98, as you can see here:\n\n\nroot@host # cat /etc/yum.repos.d/osg.repo\n[osg]\nname=OSG Software for Enterprise Linux 5 - $basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg-release/$basearch\nfailovermethod=priority\npriority=\n98\n\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\nYou can adjust these priorities if you have special needs at your site.\n\n\nAutomatic Updates\n\n\nWe strongly recommend against automatic updates for production services. You want to only change software versions during a controlled downtime (or, at least, while a human is watching); we strive to thoroughly test software updates, but cannot guarantee new version of software will not be problematic for your site.\n\n\nFor testbeds, automatic updates are suggested.\n\n\nTroubleshooting\n\n\nSometimes when installing, you will get an error like this:\n\n\nhttp://ftp1.scientificlinux.org/linux/scientific/54/x86_64/updates/security/repodata/filelists.sqlite.bz2: \n[\nErrno -1\n]\n Metadata file does not match checksum\n\n\n\n\n\n\nThis often indicates that you have out of date information, cached by \nyum\n. The following command will clear the out of date information, and you can try again:\n\n\nroot@host #\n yum --enablerepo\n=\n*\n clean all\n\n\n\n\n\nConsiderations for Large Sites\n\n\nWhile \nyum\n is a wonderful tool for installing software on a single server, it's a poor tool to install the same version of the software on many hosts. We \nstrongly\n recommend a cluster management tool; as far as we know, all cluster management tools provide a mechanism to create a local yum repository and have all your worker nodes use that.\n\n\nIf you have more than 20 worker nodes, we have the following advice:\n\n\n\n\nDo \nnot\n use one of the OSG repositories directly for worker node installations; build a local mirror instead. (See below).\n\n\nDistribute CRLs to the worker nodes using an HTTP proxy.\n\n\n\n\nBoth items are covered in this document.\n\n\nIn the future, we will be providing a mechanism for distributing CAs and CRLs via a shared file system; this is not quite finished. If you choose to do this, remember that your security infrastructure will only be as safe and reliable as the shared filesystem!\n\n\nRepository Mirrors\n\n\nA local yum mirror allows you to reduce the amount of external bandwidth used when updating or installing packages.\n\n\nAdd the following to a file in \n/etc/cron.d\n:\n\n\nRANDOM\n * * * * root rsync -aH rsync://repo.grid.iu.edu/osg/ /var/www/html/osg/\n\n\n\n\n\n\nOr, to mirror only a single repository:\n\n\nRANDOM\n * * * * root rsync -aH rsync://repo.grid.iu.edu/osg/\nOSG_RELEASE\n/el6/development /var/www/html/osg/\nOSG_RELEASE\n/el6\n\n\n\n\n\n\nReplace \nRANDOM\n with a number between 0 and 59.\n\n\nReplace \nOSG_RELEASE\n with the OSG release you want to use (e.g. '3.3', or '3.4').\n\n\nOn your worker node, you can replace the \nbaseurl\n line of \n/etc/yum.repos.d/osg.repo\n with the appropriate URL for your mirror.\n\n\nIf you are interested in having your mirror be part of the OSG's default set of mirrors, \nplease file a GOC ticket\n.\n\n\nCA Certificate Installation Considerations\n\n\nCAs are distributed in two ways:\n\n\n\n\nAs an RPM that contains the set of CAs. There are several such RPMs corresponding to different sets of CAs.\n\n\nThrough direct downloads from GOC with the \nosg-update-certs\n tool (provided by the \nosg-ca-scripts\n RPM).\n\n\n\n\nAs long as you use one of these two mechanisms, the OSG software will install successfully.\n\n\nCertificate Revocation List (CRL) Installation/Update\n\n\nCRLs are not distributed via \nyum\n. Instead, we provide the \nfetch-crl\n \ntool\n that downloads CRLs to the CA directory.", 
            "title": "Install Best Practices"
        }, 
        {
            "location": "/common/install-best-practices/#installation-best-practices", 
            "text": "", 
            "title": "Installation Best Practices"
        }, 
        {
            "location": "/common/install-best-practices/#about-this-document", 
            "text": "This document covers best practices to be used in installing software packages or security certificates on large numbers of hosts. It assumes you understand  YUM/RPM Basics .", 
            "title": "About This Document"
        }, 
        {
            "location": "/common/install-best-practices/#install-yum-priorities", 
            "text": "We use  yum  priorities to ensure that we get the right packages from the OSG repositories. Please install it:  root@host #  yum install yum-priorities  There is  good documentation  in the wild about yum priorities, but we'll summarize the essential bits for you.  The basic idea of yum priorities is that if a package is found in two repositories, priorities will influence how yum chooses which package to install. We want the OSG software repository to be chosen instead of the EPEL repository so software, such as Globus, comes from our repository instead of the EPEL repository. (This is important because you will have installation errors if you get the EPEL Globus.)  Yum priorities is a  yum plugin . Once enabled, you can set the priority of each repository (located in /etc/yum.repos.d) The lower the numerical priority, the better the priority. The default priority for a repository when it's not specified is 99. We set the priority for the OSG repository to be 98, as you can see here:  root@host # cat /etc/yum.repos.d/osg.repo\n[osg]\nname=OSG Software for Enterprise Linux 5 - $basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg-release/$basearch\nfailovermethod=priority\npriority= 98 \nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  You can adjust these priorities if you have special needs at your site.", 
            "title": "Install yum-priorities"
        }, 
        {
            "location": "/common/install-best-practices/#automatic-updates", 
            "text": "We strongly recommend against automatic updates for production services. You want to only change software versions during a controlled downtime (or, at least, while a human is watching); we strive to thoroughly test software updates, but cannot guarantee new version of software will not be problematic for your site.  For testbeds, automatic updates are suggested.", 
            "title": "Automatic Updates"
        }, 
        {
            "location": "/common/install-best-practices/#troubleshooting", 
            "text": "Sometimes when installing, you will get an error like this:  http://ftp1.scientificlinux.org/linux/scientific/54/x86_64/updates/security/repodata/filelists.sqlite.bz2:  [ Errno -1 ]  Metadata file does not match checksum   This often indicates that you have out of date information, cached by  yum . The following command will clear the out of date information, and you can try again:  root@host #  yum --enablerepo = *  clean all", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/common/install-best-practices/#considerations-for-large-sites", 
            "text": "While  yum  is a wonderful tool for installing software on a single server, it's a poor tool to install the same version of the software on many hosts. We  strongly  recommend a cluster management tool; as far as we know, all cluster management tools provide a mechanism to create a local yum repository and have all your worker nodes use that.  If you have more than 20 worker nodes, we have the following advice:   Do  not  use one of the OSG repositories directly for worker node installations; build a local mirror instead. (See below).  Distribute CRLs to the worker nodes using an HTTP proxy.   Both items are covered in this document.  In the future, we will be providing a mechanism for distributing CAs and CRLs via a shared file system; this is not quite finished. If you choose to do this, remember that your security infrastructure will only be as safe and reliable as the shared filesystem!", 
            "title": "Considerations for Large Sites"
        }, 
        {
            "location": "/common/install-best-practices/#repository-mirrors", 
            "text": "A local yum mirror allows you to reduce the amount of external bandwidth used when updating or installing packages.  Add the following to a file in  /etc/cron.d :  RANDOM  * * * * root rsync -aH rsync://repo.grid.iu.edu/osg/ /var/www/html/osg/   Or, to mirror only a single repository:  RANDOM  * * * * root rsync -aH rsync://repo.grid.iu.edu/osg/ OSG_RELEASE /el6/development /var/www/html/osg/ OSG_RELEASE /el6   Replace  RANDOM  with a number between 0 and 59.  Replace  OSG_RELEASE  with the OSG release you want to use (e.g. '3.3', or '3.4').  On your worker node, you can replace the  baseurl  line of  /etc/yum.repos.d/osg.repo  with the appropriate URL for your mirror.  If you are interested in having your mirror be part of the OSG's default set of mirrors,  please file a GOC ticket .", 
            "title": "Repository Mirrors"
        }, 
        {
            "location": "/common/install-best-practices/#ca-certificate-installation-considerations", 
            "text": "CAs are distributed in two ways:   As an RPM that contains the set of CAs. There are several such RPMs corresponding to different sets of CAs.  Through direct downloads from GOC with the  osg-update-certs  tool (provided by the  osg-ca-scripts  RPM).   As long as you use one of these two mechanisms, the OSG software will install successfully.", 
            "title": "CA Certificate Installation Considerations"
        }, 
        {
            "location": "/common/install-best-practices/#certificate-revocation-list-crl-installationupdate", 
            "text": "CRLs are not distributed via  yum . Instead, we provide the  fetch-crl   tool  that downloads CRLs to the CA directory.", 
            "title": "Certificate Revocation List (CRL) Installation/Update"
        }, 
        {
            "location": "/common/ca/", 
            "text": "Installing Certificate Authorities Certificates and related RPMs\n\n\nThis document provides you with details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation list (CRL).\n\n\nWhen installing software with RPMs, you need to decide how you want to install the Certificate Authority (CA) certificates. You might ask \"why do I care? Can\u2019t you just give them to me?\" We can, but you have a few things to consider:\n\n\n\n\nWhat set of CA certificates do you want? How much control do you want over the set of CA certificates? (Some sites might not want to install specific CAs for policy or security reasons.)\n\n\nHow do you want to update them?\n\n\nDo you want to centrally manage the CA certificates or install them on each computer at your site?\n\n\n\n\nYou have four options for installing CA certificates:\n\n\n\n\nInstall an RPM for a specific set of CA certificates.\n\n\nInstall \nosg-update-certs\n, a program that lets you install/update a predefined set of CA certificates, then adjust the set by adding or deleting specific CAs.\n\n\nInstall an RPM that installs \nno\n CAs. This is useful when you want your RPM installations to succeed (because our RPMs require CA certificates, and this RPM satisfies that dependency) but you want to manage them with your own technique.\n\n\nMake no choice, let \nyum\n decide for you.\n\n\n\n\nAdditionally this page also provides instruction on installation of a tool (fetch-crl) to ensure your site has up-to-date certificate revocation list (CRL) from the CA.\n\n\nPrior to following the instructions on this page, you must enable our \nyum repositories\n\n\nInstall CA certificates: Options\n\n\nPlease choose one of the four options to install the CA certificates.\n\n\nOption 1: Install an RPM for a specific set of CA certificates\n\n\nIf you want to install an RPM for one of our predefined CA certificates, you have two choices to make:\n\n\nWhich set of CAs?\n\n\n\n\n(\nrecommended\n) The OSG CA certificates. This is similar to the IGTF set, but may have a small number of additions or deletions. (See \nhere\n for details)\n\n\nThe default \nIGTF\n CA certificates.\n\n\n\n\nDepending on your choice, you select one of two RPMs:\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n --\n\n\nRPM name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\nOSG\n\n\nOpenSSL-both\n\n\nosg-ca-certs\n\n\nyum install osg-ca-certs\n\n\n\n\n\n\nIGTF\n\n\nOpenSSL-both\n\n\nigtf-ca-certs\n\n\nyum install igtf-ca-certs\n\n\n\n\n\n\n\n\nHow do I keep CAs updated?\n\n\nPlease follow the \nupdate instructions\n to make sure that the CAs are kept updated.\n\n\nOption 2: Install osg-update-certs\n\n\nInstall this with:\n\n\nroot@host # yum install osg-ca-scripts\n\n\n\n\n\nYou have the same choices for CA certificates as above. In order to choose, you will run \nosg-ca-manage\n, which will install the CA certificates. Then (if desired) you need to enable periodic updating of the CA certificates.\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n\n\nCA certs name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\nOSG\n\n\nOpenSSL-both\n\n\nosg\n\n\n/usr/sbin/osg-ca-manage setupCA --location root --url osg\n\n\n\n\n\n\nIGTF\n\n\nOpenSSL-both\n\n\nigtf\n\n\n/usr/sbin/osg-ca-manage setupCA --location root --url igtf\n\n\n\n\n\n\n\n\nHere is an example:\n\n\nroot@host # /usr/sbin/osg-ca-manage setupCA --location root --url osg\nSetting up CA Certificates for OSG installation\nCA Certificates will be installed into /etc/grid-security/certificates\nosg-update-certs\n  Log file: /var/log/osg-update-certs.log\n  Updates from: http://software.grid.iu.edu/pacman/cadist/ca-certs-version-new\n\nWill update CA certificates from version unknown to version 1.21NEW.\nUpdate successful.\n\nSetup completed successfully.\n\n\n\n\n\nInitially the CA certificates will not be updated. You can tell by looking at:\n\n\nroot@host # /sbin/service osg-update-certs-cron  status\nPeriodic osg-update-certs is disabled.\n\n\n\n\n\nYou can enable the \ncron\n job that updates the CA certs with:\n\n\nroot@host # /sbin/service osg-update-certs-cron  start\nEnabling periodic osg-update-certs:                        [  \nOK\n  ]\n\n\n\n\n\nA complete set of options available though \nosg-ca-manage\n command, including your interface to adding and removing CAs, could be found at \nosg-ca-manage documentation\n\n\nOption 3: Install an RPM that installs no CAs\n\n\nInstall this with:\n\n\nyum install empty-ca-certs \u2013-enablerepo=osg-empty\n\n\n\n\n\n\n\nWarning\n\n\nIf you choose this option, you are responsible for installing the CA certificates yourself. You must install them in \n/etc/grid-security/certificates\n, or make a symlink from that location to the directory that contains the CA certificates.\n\n\n\n\nOption 4: Make no choice, let yum decide for you\n\n\nIf you use \nyum\n to install software that requires CA certificates but you haven\u2019t made one of these choices, yum will choose a default. Right now, it is Option #1 from above (\nInstall an RPM for a specific set of CA certificates\n), and the osg-ca-certs RPM is chosen.\n\n\nInstall other CAs\n\n\nIn addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is, \nosg-ca-certs\n and the like, but not \nosg-ca-scripts\n.) They are in addition to the above RPMs, so do not only install these extra CAs.\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n\n\nRPM name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\ncilogon-basic \n cilogon-openid\n\n\nOpenSSL-both\n\n\ncilogon-ca-certs\n\n\nyum install cilogon-ca-certs\n\n\n\n\n\n\n\n\nManaging Certificate Revocation Lists\n\n\nIn addition to CA certificates, you normally need to have updated Certificate Revocation Lists (CRLs) which are are lists of certificates that have been revoked for any reason. Software in the OSG Software Stack use these to ensure that you are talking to valid clients or servers. We use a tool named \nfetch-crl\n that periodically updates the CRLs. Fetch CRL is a utility that updates Certificate Authority (CA) Certificate Revocation Lists (CRLs). These are lists of certificates that were granted by the CA, but have since been revoked. It is good practice to regularly update the CRL list for each CA to ensure that you do not authenticate any certificate that has been revoked.\n\n\nfetch-crl\n is installed as two different system services. The fetch-crl-boot service runs only\nat boot time. The \nfetch-crl-cron\n service runs \nfetch-crl\n every 6 hours (with a random sleep\ntime included) by default. Both services are disabled by default. At the very minimum, the\n\nfetch-crl-cron\n service needs to be enabled otherwise services will begin to fail as the\nexisting CRLs expire.\n\n\nInstall \nfetch-crl\n\n\nNormally \nfetch-crl\n is installed when you install the rest of the software and you do not need\nto specifically install it. If you do wish to install it, you can install it as:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n\nroot\n@\nhost\n # \nyum\n \ninstall\n \nfetch\n-\ncrl3\n\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n\nroot\n@\nhost\n # \nyum\n \ninstall\n \nfetch\n-\ncrl\n\n\n\n\n\n\nEnable and Start \nfetch-crl\n\n\nTo enable fetch-crl (fetch Certificate Revocation Lists) services by default on the node:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl3\n-\nboot\n \non\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl3\n-\ncron\n \non\n\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl\n-\nboot\n \non\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl\n-\ncron\n \non\n\n\n\n\n\n\nTo start fetch-crl:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl3\n-\nboot\n \nstart\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl3\n-\ncron\n \nstart\n\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl\n-\nboot\n \nstart\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl\n-\ncron\n \nstart\n\n\n\n\n\n\n\n\nNote\n\n\nWhile it is necessary to start \nfetch-crl-cron\n in order to have it active, \nfetch-crl-boot\n is started automatically at boot time if enabled. The start command will run \nfetch-crl-boot\n at the moment when it is invoked and it may take some time to complete.\n\n\n\n\nConfigure \nfetch-crl\n\n\nTo modify the times that fetch-crl-cron runs, edit \n/etc/cron.d/fetch-crl\n (or \n/etc/cron.d/fetch-crl3\n depending on the version you have).\n\n\nBy default, \nfetch-crl\n connects directly to the remote CA; this is\ninefficient and potentially harmful if done simultaneously by many nodes\n(e.g. all the worker nodes of a big cluster). We recommend you provide a\nHTTP proxy (such as \nsquid\n) the worker nodes can utilize; OSG provides\n\npackaging of squid\n.\n\n\nTo configure fetch-crl to use an HTTP proxy server:\n\n\n\n\n\n\nIf using \nfetch-crl\n version 2 (the \nfetch-crl\n package on RHEL5 only), then create the file \n/etc/sysconfig/fetch-crl\n and add the following line:\n\n\nexport http_proxy=\nhttp://your.squid.fqdn:port\n\n\nAdjust the URL appropriately for your proxy server.\n\n\n\n\n\n\nIf using \nfetch-crl\n version 3 on RHEL5 via the \nfetch-crl3\n package\n    or on RHEL6/RHEL7 via the \nfetch-crl\n package, then create or edit the\n    file \n/etc/fetch-crl3.conf\n (RHEL5) or \n/etc/fetch-crl.conf\n\n    (RHEL6/RHEL7) and add the following line:\n\n\nhttp_proxy=\nhttp://your.squid.fqdn:port\n\n\nAgain, adjust the URL appropriately for your proxy server.\n\n\n\n\n\n\nNote that the \nnosymlinks\n option in the configuration files refers\nto ignoring links within the certificates directory (e.g. two different\nnames for the same file). It is perfectly fine if the path of the CA\ncertificates directory itself (\ninfodir\n) is a link to a directory.\n\n\nAny modifications to the configuration file will be preserved during an RPM update.\n\n\nCurrent versions of \nfetch-crl\n and \nfetch-crl3\n produce more output.\nIt is possible to send the output to syslog instead of the default email system. To do so:\n\n\n\n\n\n\nChange the configuration file to enable syslog:\n\n\nlogmode = syslog\n  syslogfacility = daemon\\\n/pre\\\n\n\n\n\n\n\nMake sure the file \n/var/log/daemon\n exists, e.g. touching the file\n\n\n\n\nChange \n/etc/logrotate.d\n files to rotate it\n\n\n\n\nStart/Stop fetch-crl: A quick guide\n\n\nYou need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n\nroot\n@\nhost\n # \n/\nusr\n/\nsbin\n/\nfetch\n-\ncrl3\n # \nThis\n \nfetches\n \nthe\n \nCRLs\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl3\n-\nboot\n \nstart\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl3\n-\ncron\n \nstart\n\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7\n\n\nroot\n@\nhost\n # \n/\nusr\n/\nsbin\n/\nfetch\n-\ncrl\n # \nThis\n \nfetches\n \nthe\n \nCRLs\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl\n-\nboot\n \nstart\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl\n-\ncron\n \nstart\n\n\n\n\n\n\nTo enable the \nfetch-crl\n service to keep the CRLs up to date after reboots:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl3\n-\nboot\n \non\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl3\n-\ncron\n \non\n\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl\n-\nboot\n \non\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl\n-\ncron\n \non\n\n\n\n\n\n\nTo stop \nfetch-crl\n:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl3\n-\nboot\n \nstop\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl3\n-\ncron\n \nstop\n\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl\n-\nboot\n \nstop\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nservice\n \nfetch\n-\ncrl\n-\ncron\n \nstop\n\n\n\n\n\n\nTo disable the fetch-crl service:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl3\n-\nboot\n \noff\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl3\n-\ncron\n \noff\n\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl\n-\nboot\n \noff\n\n\nroot\n@\nhost\n # \n/\nsbin\n/\nchkconfig\n \nfetch\n-\ncrl\n-\ncron\n \noff\n\n\n\n\n\n\nUpdating CAs/CRLs\n\n\nWhy maintain up-to-date Trusted CA /CRL information\n\n\nThe Trusted Certificate Authority (CA) certificates, and their associated Certificate Revocation Lists (CRLs), are used for every transaction on a resource that establishes an authenticated network connection based on end user\u2019s certificate. In order for the authentication to succeed, the user\u2019s certificate must have been issued by one of the CAs in the Trusted CA directory, and the user\u2019s certificate must not be listed in the CRL for that CA. CRLs can be thought of as a black list of certificates. CAs are the trust authorities, similar to DMV that issues you the driving license. (Another way of thinking CRLs is the do-not-fly lists at the airports. if your certificate shows up in CRLs, you are not allowed access.) This is handled at the certificate validation stage even before the authorization check (which will provide the mapping of an authenticated user to a local account UID/GID). So you do not need to do worry about it; the grid software will do this for you. However, you should make sure that your site has the most up-to-date list of Trusted CAs. There are multiple trust authorities in OSG (think of it as a different DMV for each state). If you do not have an up-to-date list of CAs it is possible that some of your users transactions at your site will start to fail. A current CRL list for each CA is also necessary, since without one transactions for users of that CA will fail.\n\n\nHow to ensure you are get up-to-date CA/CRL information\n\n\n\n\nIf you installed CAs using rpm packages (\nosg-ca-certs\n,\nigtf-ca-certs\n) (Options 1, 4), you will need to install the software described in \nthe CA update document\n, and enable \nosg-ca-certs-updater\n service to keep the CAs automatically updated. If you do not install the updater, you will have to regularly run yum update to keep the CAs updated.\n\n\nIf you use Option 2 (i.e. \nosg-update-certs\n) then make sure that you have the corresponding service enabled.\n\n\n\n\nroot@host # /sbin/service osg-update-certs-cron  status\n   Periodic osg-update-certs is enabled.\n\n\n\n\nEnsure that fetch-crl cron is enabled\\\n\n\n\n\nroot@host # /sbin/service fetch-crl-cron  status\n  Periodic fetch-crl is enabled.\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nConfiguration files:\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nAll CA Packages\n\n\nCA File Location\n\n\n/etc/grid-security/certificates\n\n\n\n\n\n\n\n\nAll CA Packages\n\n\nIndex files\n\n\n/etc/grid-security/certificates/INDEX.html\n or \n/etc/grid-security/certificates/INDEX.txt\n\n\nLatest version also available at \nhttp://repo.grid.iu.edu/pacman/cadist/\n\n\n\n\n\n\nAll CA Packages\n\n\nChange Log\n\n\n/etc/grid-security/certificates/CHANGES\n\n\nLatest version also available at \nhttp://repo.grid.iu.edu/pacman/cadist/CHANGES\n\n\n\n\n\n\nosg-ca-certs or igtf-ca-certs\n\n\ncontain only CA files\n\n\n\n\n\n\n\n\n\n\nosg-ca-scripts\n\n\nConfiguration File for osg-update-certs\n\n\n/etc/osg/osg-update-certs.conf\n\n\nThis file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.\n\n\n\n\n\n\nfetch-crl-2.x\n\n\nConfiguration file\n\n\n/etc/fetch-crl.conf\n\n\n\n\n\n\n\n\nfetch-crl-3.x\n\n\nConfiguration file\n\n\n/etc/fetch-crl3.conf\n\n\n\n\n\n\n\n\n\n\nThe index and change log files contain a summary of all the CA distributed and their version.\n\n\nLogs files:\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\nosg-ca-scripts\n\n\nLog file of osg-update-certs\n\n\n/var/log/osg-update-certs.log\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of osg-update-certs\n\n\n/var/log/osg-ca-certs-status.system.out\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of osg-ca-manage\n\n\n/var/log/osg-ca-manage.system.out\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of initial CA setup\n\n\n/var/log/osg-setup-ca-certificates.system.out\n\n\n\n\n\n\n\n\nTests\n\n\nTo test the host certificate of a server \nopenssl s_client\n can be used. Here is an example with the gatekeeper:\n\n\nuser@host $ openssl s_client -showcerts -cert /etc/grid-security/hostcert.pem -key /etc/grid-security/hostkey.pem -CApath /etc/grid-security/certificates/ -debug -connect osg-gk.mwt2.org:2119\n\n\n\n\n\nFrequently Asked Questions\n\n\nLocation of Certificates?\n\n\n /etc/grid-security/certificates\n\n\n\n\n\nWhat is the version of OSG CA package I have installed and what are its contents?\n\n\nThe version of the CA package ca be found at \n/etc/grid-security/certificates/INDEX.html\n or \n/etc/grid-security/certificates/INDEX.txt\n. The changes file can be found at \n/etc/grid-security/certificates/CHANGES\n.\n\n\nContents of OSG CA package?\n\n\nThe OSG CA Distribution contains:\n\n\n\n\nIGTF Distribution of Authority Root Certificates\n (CAs accredited by the \nInternational Grid Trust Federation\n)\n\n\nPurdue TeraGrid CA\n\n\n\n\nDetails of CAs in OSG distribution can be found \nhere\n. For additional details what is in the current release, see the \ndistribution site\n and \nchange log\n.\n\n\nHow can I add or remove a particular CA file?\n\n\nAdd and remove of CA files are supported only if you CA files are being installed using \nosg-update-certs\n, which is included in the \nosg-ca-scripts\n package (option 2), for all other options no support for adding and removing a particular CA file is provided by OSG. The preferred approach to add or remove a CA is to use \nosg-ca-manage\n. For adding a new CA \nosg-ca-manage add [--dir \nlocal_dir\n] --hash \nCA_hash\n may be used, while a CA is removed using \nosg-ca-manage remove --hash \nCA_hash\n.\n\n\nAre there any log files or configuration files associated with CA certificate package?\n\n\nIf CA files are installed using \nosg-ca-certs\n or \nigtf-ca-certs\n rpms (i.e. options 1, 4) no log or configuration files are present.\n\n\nLog and configuration files are however present for \nosg-ca-scripts\n rpm package (option 2).\n\n\nConfig files: \n/etc/osg/osg-update-certs.conf\n Log files: \n/var/log/osg-update-certs.log\n, \n/var/log/osg-ca-certs-status.system.out\n, \n/var/log/osg-ca-manage.system.out\n, \n/var/log/osg-setup-ca-certificates.system.out\n\n\nAre CA packages automatically updated?\n\n\nIf CA files are installed using \nosg-ca-certs\n or \nigtf-ca-certs\n rpms (i.e. options 1, 4), you will need to install the software described in \nOSG CA certs updater\n, and enable osg-ca-certs-updater service to keep the CAs automatically updated.\n\n\nIf CA files are being installed using \nosg-ca-scripts\n rpm package (option 2), CA files are kept up-to-date as long as \nosg-update-certs-cron\n service the package provides has been started.\n\n\nHow do I manually update my CA package?\n\n\nFor Option 1: run one of the following \nyum update osg-ca-certs\n or \nyum update igtf-ca-certs\n depending on the rpm package you installed.\n\n\nFor Option 4: run \nyum update osg-ca-certs\n\n\nFor Option 2: You do not need to do a manual update, make sure \nosg-update-certs-cron\n is enabled using\n\n\nroot@host # /sbin/service osg-update-certs-cron  status\n\n\n\n\n\nIf the service is disabled, enable it using\n\n\nroot@host # /sbin/service osg-update-certs-cron  start\n\n\n\n\n\nIf for some extraordinary reason you need to manually update the CA you could run \nosg-ca-manage [--force] refreshCA\n.\n\n\nWhere are the configuration files for fetch-crl?\n\n\n/etc/fetch-crl.conf\n or \n/etc/fetch-crl3.conf\n for fetch-crl 2.x or 3.x respectively\n\n\nReferences\n\n\nSome guides on x509 certificates:\n\n\n\n\nUseful commands: \nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html\n\n\nInstall GSI authentication on a server: \nhttp://security.ncsa.illinois.edu/research/wssec/gsihttps/\n\n\nCertificates how-to: \nhttp://www.nordugrid.org/documents/certificate_howto.html\n\n\n\n\nSome examples about verifying the certificates:\n\n\n\n\nhttp://gagravarr.org/writing/openssl-certs/others.shtml\n\n\nhttp://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/\n\n\nhttp://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html\n\n\n\n\nRelated software:\n\n\n\n\nDescription, manual and examples of \nosg-ca-manage\n\n\nosg-ca-certs-updater", 
            "title": "CA Certificates"
        }, 
        {
            "location": "/common/ca/#installing-certificate-authorities-certificates-and-related-rpms", 
            "text": "This document provides you with details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation list (CRL).  When installing software with RPMs, you need to decide how you want to install the Certificate Authority (CA) certificates. You might ask \"why do I care? Can\u2019t you just give them to me?\" We can, but you have a few things to consider:   What set of CA certificates do you want? How much control do you want over the set of CA certificates? (Some sites might not want to install specific CAs for policy or security reasons.)  How do you want to update them?  Do you want to centrally manage the CA certificates or install them on each computer at your site?   You have four options for installing CA certificates:   Install an RPM for a specific set of CA certificates.  Install  osg-update-certs , a program that lets you install/update a predefined set of CA certificates, then adjust the set by adding or deleting specific CAs.  Install an RPM that installs  no  CAs. This is useful when you want your RPM installations to succeed (because our RPMs require CA certificates, and this RPM satisfies that dependency) but you want to manage them with your own technique.  Make no choice, let  yum  decide for you.   Additionally this page also provides instruction on installation of a tool (fetch-crl) to ensure your site has up-to-date certificate revocation list (CRL) from the CA.  Prior to following the instructions on this page, you must enable our  yum repositories", 
            "title": "Installing Certificate Authorities Certificates and related RPMs"
        }, 
        {
            "location": "/common/ca/#install-ca-certificates-options", 
            "text": "Please choose one of the four options to install the CA certificates.", 
            "title": "Install CA certificates: Options"
        }, 
        {
            "location": "/common/ca/#option-1-install-an-rpm-for-a-specific-set-of-ca-certificates", 
            "text": "If you want to install an RPM for one of our predefined CA certificates, you have two choices to make:", 
            "title": "Option 1: Install an RPM for a specific set of CA certificates"
        }, 
        {
            "location": "/common/ca/#which-set-of-cas", 
            "text": "( recommended ) The OSG CA certificates. This is similar to the IGTF set, but may have a small number of additions or deletions. (See  here  for details)  The default  IGTF  CA certificates.   Depending on your choice, you select one of two RPMs:     Set of CAs  Format  --  RPM name  Installation command (as root)      OSG  OpenSSL-both  osg-ca-certs  yum install osg-ca-certs    IGTF  OpenSSL-both  igtf-ca-certs  yum install igtf-ca-certs", 
            "title": "Which set of CAs?"
        }, 
        {
            "location": "/common/ca/#how-do-i-keep-cas-updated", 
            "text": "Please follow the  update instructions  to make sure that the CAs are kept updated.", 
            "title": "How do I keep CAs updated?"
        }, 
        {
            "location": "/common/ca/#option-2-install-osg-update-certs", 
            "text": "Install this with:  root@host # yum install osg-ca-scripts  You have the same choices for CA certificates as above. In order to choose, you will run  osg-ca-manage , which will install the CA certificates. Then (if desired) you need to enable periodic updating of the CA certificates.     Set of CAs  Format  CA certs name  Installation command (as root)      OSG  OpenSSL-both  osg  /usr/sbin/osg-ca-manage setupCA --location root --url osg    IGTF  OpenSSL-both  igtf  /usr/sbin/osg-ca-manage setupCA --location root --url igtf     Here is an example:  root@host # /usr/sbin/osg-ca-manage setupCA --location root --url osg\nSetting up CA Certificates for OSG installation\nCA Certificates will be installed into /etc/grid-security/certificates\nosg-update-certs\n  Log file: /var/log/osg-update-certs.log\n  Updates from: http://software.grid.iu.edu/pacman/cadist/ca-certs-version-new\n\nWill update CA certificates from version unknown to version 1.21NEW.\nUpdate successful.\n\nSetup completed successfully.  Initially the CA certificates will not be updated. You can tell by looking at:  root@host # /sbin/service osg-update-certs-cron  status\nPeriodic osg-update-certs is disabled.  You can enable the  cron  job that updates the CA certs with:  root@host # /sbin/service osg-update-certs-cron  start\nEnabling periodic osg-update-certs:                        [   OK   ]  A complete set of options available though  osg-ca-manage  command, including your interface to adding and removing CAs, could be found at  osg-ca-manage documentation", 
            "title": "Option 2: Install osg-update-certs"
        }, 
        {
            "location": "/common/ca/#option-3-install-an-rpm-that-installs-no-cas", 
            "text": "Install this with:  yum install empty-ca-certs \u2013-enablerepo=osg-empty   Warning  If you choose this option, you are responsible for installing the CA certificates yourself. You must install them in  /etc/grid-security/certificates , or make a symlink from that location to the directory that contains the CA certificates.", 
            "title": "Option 3: Install an RPM that installs no CAs"
        }, 
        {
            "location": "/common/ca/#option-4-make-no-choice-let-yum-decide-for-you", 
            "text": "If you use  yum  to install software that requires CA certificates but you haven\u2019t made one of these choices, yum will choose a default. Right now, it is Option #1 from above ( Install an RPM for a specific set of CA certificates ), and the osg-ca-certs RPM is chosen.", 
            "title": "Option 4: Make no choice, let yum decide for you"
        }, 
        {
            "location": "/common/ca/#install-other-cas", 
            "text": "In addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is,  osg-ca-certs  and the like, but not  osg-ca-scripts .) They are in addition to the above RPMs, so do not only install these extra CAs.     Set of CAs  Format  RPM name  Installation command (as root)      cilogon-basic   cilogon-openid  OpenSSL-both  cilogon-ca-certs  yum install cilogon-ca-certs", 
            "title": "Install other CAs"
        }, 
        {
            "location": "/common/ca/#managing-certificate-revocation-lists", 
            "text": "In addition to CA certificates, you normally need to have updated Certificate Revocation Lists (CRLs) which are are lists of certificates that have been revoked for any reason. Software in the OSG Software Stack use these to ensure that you are talking to valid clients or servers. We use a tool named  fetch-crl  that periodically updates the CRLs. Fetch CRL is a utility that updates Certificate Authority (CA) Certificate Revocation Lists (CRLs). These are lists of certificates that were granted by the CA, but have since been revoked. It is good practice to regularly update the CRL list for each CA to ensure that you do not authenticate any certificate that has been revoked.  fetch-crl  is installed as two different system services. The fetch-crl-boot service runs only\nat boot time. The  fetch-crl-cron  service runs  fetch-crl  every 6 hours (with a random sleep\ntime included) by default. Both services are disabled by default. At the very minimum, the fetch-crl-cron  service needs to be enabled otherwise services will begin to fail as the\nexisting CRLs expire.", 
            "title": "Managing Certificate Revocation Lists"
        }, 
        {
            "location": "/common/ca/#install-fetch-crl", 
            "text": "Normally  fetch-crl  is installed when you install the rest of the software and you do not need\nto specifically install it. If you do wish to install it, you can install it as:  # For RHEL 5, CentOS 5, and SL5   root @ host  #  yum   install   fetch - crl3  # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7   root @ host  #  yum   install   fetch - crl", 
            "title": "Install fetch-crl"
        }, 
        {
            "location": "/common/ca/#enable-and-start-fetch-crl", 
            "text": "To enable fetch-crl (fetch Certificate Revocation Lists) services by default on the node:  # For RHEL 5, CentOS 5, and SL5   root @ host  #  / sbin / chkconfig   fetch - crl3 - boot   on  root @ host  #  / sbin / chkconfig   fetch - crl3 - cron   on  # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7   root @ host  #  / sbin / chkconfig   fetch - crl - boot   on  root @ host  #  / sbin / chkconfig   fetch - crl - cron   on   To start fetch-crl:  # For RHEL 5, CentOS 5, and SL5   root @ host  #  / sbin / service   fetch - crl3 - boot   start  root @ host  #  / sbin / service   fetch - crl3 - cron   start  # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7   root @ host  #  / sbin / service   fetch - crl - boot   start  root @ host  #  / sbin / service   fetch - crl - cron   start    Note  While it is necessary to start  fetch-crl-cron  in order to have it active,  fetch-crl-boot  is started automatically at boot time if enabled. The start command will run  fetch-crl-boot  at the moment when it is invoked and it may take some time to complete.", 
            "title": "Enable and Start fetch-crl"
        }, 
        {
            "location": "/common/ca/#configure-fetch-crl", 
            "text": "To modify the times that fetch-crl-cron runs, edit  /etc/cron.d/fetch-crl  (or  /etc/cron.d/fetch-crl3  depending on the version you have).  By default,  fetch-crl  connects directly to the remote CA; this is\ninefficient and potentially harmful if done simultaneously by many nodes\n(e.g. all the worker nodes of a big cluster). We recommend you provide a\nHTTP proxy (such as  squid ) the worker nodes can utilize; OSG provides packaging of squid .  To configure fetch-crl to use an HTTP proxy server:    If using  fetch-crl  version 2 (the  fetch-crl  package on RHEL5 only), then create the file  /etc/sysconfig/fetch-crl  and add the following line:  export http_proxy= http://your.squid.fqdn:port  Adjust the URL appropriately for your proxy server.    If using  fetch-crl  version 3 on RHEL5 via the  fetch-crl3  package\n    or on RHEL6/RHEL7 via the  fetch-crl  package, then create or edit the\n    file  /etc/fetch-crl3.conf  (RHEL5) or  /etc/fetch-crl.conf \n    (RHEL6/RHEL7) and add the following line:  http_proxy= http://your.squid.fqdn:port  Again, adjust the URL appropriately for your proxy server.    Note that the  nosymlinks  option in the configuration files refers\nto ignoring links within the certificates directory (e.g. two different\nnames for the same file). It is perfectly fine if the path of the CA\ncertificates directory itself ( infodir ) is a link to a directory.  Any modifications to the configuration file will be preserved during an RPM update.  Current versions of  fetch-crl  and  fetch-crl3  produce more output.\nIt is possible to send the output to syslog instead of the default email system. To do so:    Change the configuration file to enable syslog:  logmode = syslog\n  syslogfacility = daemon\\ /pre\\    Make sure the file  /var/log/daemon  exists, e.g. touching the file   Change  /etc/logrotate.d  files to rotate it", 
            "title": "Configure fetch-crl"
        }, 
        {
            "location": "/common/ca/#startstop-fetch-crl-a-quick-guide", 
            "text": "You need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:  # For RHEL 5, CentOS 5, and SL5   root @ host  #  / usr / sbin / fetch - crl3  #  This   fetches   the   CRLs  root @ host  #  / sbin / service   fetch - crl3 - boot   start  root @ host  #  / sbin / service   fetch - crl3 - cron   start  # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7  root @ host  #  / usr / sbin / fetch - crl  #  This   fetches   the   CRLs  root @ host  #  / sbin / service   fetch - crl - boot   start  root @ host  #  / sbin / service   fetch - crl - cron   start   To enable the  fetch-crl  service to keep the CRLs up to date after reboots:  # For RHEL 5, CentOS 5, and SL5   root @ host  #  / sbin / chkconfig   fetch - crl3 - boot   on  root @ host  #  / sbin / chkconfig   fetch - crl3 - cron   on  # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7   root @ host  #  / sbin / chkconfig   fetch - crl - boot   on  root @ host  #  / sbin / chkconfig   fetch - crl - cron   on   To stop  fetch-crl :  # For RHEL 5, CentOS 5, and SL5   root @ host  #  / sbin / service   fetch - crl3 - boot   stop  root @ host  #  / sbin / service   fetch - crl3 - cron   stop  # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7   root @ host  #  / sbin / service   fetch - crl - boot   stop  root @ host  #  / sbin / service   fetch - crl - cron   stop   To disable the fetch-crl service:  # For RHEL 5, CentOS 5, and SL5   root @ host  #  / sbin / chkconfig   fetch - crl3 - boot   off  root @ host  #  / sbin / chkconfig   fetch - crl3 - cron   off  # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7   root @ host  #  / sbin / chkconfig   fetch - crl - boot   off  root @ host  #  / sbin / chkconfig   fetch - crl - cron   off", 
            "title": "Start/Stop fetch-crl: A quick guide"
        }, 
        {
            "location": "/common/ca/#updating-cascrls", 
            "text": "", 
            "title": "Updating CAs/CRLs"
        }, 
        {
            "location": "/common/ca/#why-maintain-up-to-date-trusted-ca-crl-information", 
            "text": "The Trusted Certificate Authority (CA) certificates, and their associated Certificate Revocation Lists (CRLs), are used for every transaction on a resource that establishes an authenticated network connection based on end user\u2019s certificate. In order for the authentication to succeed, the user\u2019s certificate must have been issued by one of the CAs in the Trusted CA directory, and the user\u2019s certificate must not be listed in the CRL for that CA. CRLs can be thought of as a black list of certificates. CAs are the trust authorities, similar to DMV that issues you the driving license. (Another way of thinking CRLs is the do-not-fly lists at the airports. if your certificate shows up in CRLs, you are not allowed access.) This is handled at the certificate validation stage even before the authorization check (which will provide the mapping of an authenticated user to a local account UID/GID). So you do not need to do worry about it; the grid software will do this for you. However, you should make sure that your site has the most up-to-date list of Trusted CAs. There are multiple trust authorities in OSG (think of it as a different DMV for each state). If you do not have an up-to-date list of CAs it is possible that some of your users transactions at your site will start to fail. A current CRL list for each CA is also necessary, since without one transactions for users of that CA will fail.", 
            "title": "Why maintain up-to-date Trusted CA /CRL information"
        }, 
        {
            "location": "/common/ca/#how-to-ensure-you-are-get-up-to-date-cacrl-information", 
            "text": "If you installed CAs using rpm packages ( osg-ca-certs , igtf-ca-certs ) (Options 1, 4), you will need to install the software described in  the CA update document , and enable  osg-ca-certs-updater  service to keep the CAs automatically updated. If you do not install the updater, you will have to regularly run yum update to keep the CAs updated.  If you use Option 2 (i.e.  osg-update-certs ) then make sure that you have the corresponding service enabled.   root@host # /sbin/service osg-update-certs-cron  status\n   Periodic osg-update-certs is enabled.   Ensure that fetch-crl cron is enabled\\   root@host # /sbin/service fetch-crl-cron  status\n  Periodic fetch-crl is enabled.", 
            "title": "How to ensure you are get up-to-date CA/CRL information"
        }, 
        {
            "location": "/common/ca/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/common/ca/#useful-configuration-and-log-files", 
            "text": "Configuration files:     Package  File Description  Location  Comment      All CA Packages  CA File Location  /etc/grid-security/certificates     All CA Packages  Index files  /etc/grid-security/certificates/INDEX.html  or  /etc/grid-security/certificates/INDEX.txt  Latest version also available at  http://repo.grid.iu.edu/pacman/cadist/    All CA Packages  Change Log  /etc/grid-security/certificates/CHANGES  Latest version also available at  http://repo.grid.iu.edu/pacman/cadist/CHANGES    osg-ca-certs or igtf-ca-certs  contain only CA files      osg-ca-scripts  Configuration File for osg-update-certs  /etc/osg/osg-update-certs.conf  This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.    fetch-crl-2.x  Configuration file  /etc/fetch-crl.conf     fetch-crl-3.x  Configuration file  /etc/fetch-crl3.conf      The index and change log files contain a summary of all the CA distributed and their version.  Logs files:     Package  File Description  Location      osg-ca-scripts  Log file of osg-update-certs  /var/log/osg-update-certs.log    osg-ca-scripts  Stdout of osg-update-certs  /var/log/osg-ca-certs-status.system.out    osg-ca-scripts  Stdout of osg-ca-manage  /var/log/osg-ca-manage.system.out    osg-ca-scripts  Stdout of initial CA setup  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/common/ca/#tests", 
            "text": "To test the host certificate of a server  openssl s_client  can be used. Here is an example with the gatekeeper:  user@host $ openssl s_client -showcerts -cert /etc/grid-security/hostcert.pem -key /etc/grid-security/hostkey.pem -CApath /etc/grid-security/certificates/ -debug -connect osg-gk.mwt2.org:2119", 
            "title": "Tests"
        }, 
        {
            "location": "/common/ca/#frequently-asked-questions", 
            "text": "", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/common/ca/#location-of-certificates", 
            "text": "/etc/grid-security/certificates", 
            "title": "Location of Certificates?"
        }, 
        {
            "location": "/common/ca/#what-is-the-version-of-osg-ca-package-i-have-installed-and-what-are-its-contents", 
            "text": "The version of the CA package ca be found at  /etc/grid-security/certificates/INDEX.html  or  /etc/grid-security/certificates/INDEX.txt . The changes file can be found at  /etc/grid-security/certificates/CHANGES .", 
            "title": "What is the version of OSG CA package I have installed and what are its contents?"
        }, 
        {
            "location": "/common/ca/#contents-of-osg-ca-package", 
            "text": "The OSG CA Distribution contains:   IGTF Distribution of Authority Root Certificates  (CAs accredited by the  International Grid Trust Federation )  Purdue TeraGrid CA   Details of CAs in OSG distribution can be found  here . For additional details what is in the current release, see the  distribution site  and  change log .", 
            "title": "Contents of OSG CA package?"
        }, 
        {
            "location": "/common/ca/#how-can-i-add-or-remove-a-particular-ca-file", 
            "text": "Add and remove of CA files are supported only if you CA files are being installed using  osg-update-certs , which is included in the  osg-ca-scripts  package (option 2), for all other options no support for adding and removing a particular CA file is provided by OSG. The preferred approach to add or remove a CA is to use  osg-ca-manage . For adding a new CA  osg-ca-manage add [--dir  local_dir ] --hash  CA_hash  may be used, while a CA is removed using  osg-ca-manage remove --hash  CA_hash .", 
            "title": "How can I add or remove a particular CA file?"
        }, 
        {
            "location": "/common/ca/#are-there-any-log-files-or-configuration-files-associated-with-ca-certificate-package", 
            "text": "If CA files are installed using  osg-ca-certs  or  igtf-ca-certs  rpms (i.e. options 1, 4) no log or configuration files are present.  Log and configuration files are however present for  osg-ca-scripts  rpm package (option 2).  Config files:  /etc/osg/osg-update-certs.conf  Log files:  /var/log/osg-update-certs.log ,  /var/log/osg-ca-certs-status.system.out ,  /var/log/osg-ca-manage.system.out ,  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Are there any log files or configuration files associated with CA certificate package?"
        }, 
        {
            "location": "/common/ca/#are-ca-packages-automatically-updated", 
            "text": "If CA files are installed using  osg-ca-certs  or  igtf-ca-certs  rpms (i.e. options 1, 4), you will need to install the software described in  OSG CA certs updater , and enable osg-ca-certs-updater service to keep the CAs automatically updated.  If CA files are being installed using  osg-ca-scripts  rpm package (option 2), CA files are kept up-to-date as long as  osg-update-certs-cron  service the package provides has been started.", 
            "title": "Are CA packages automatically updated?"
        }, 
        {
            "location": "/common/ca/#how-do-i-manually-update-my-ca-package", 
            "text": "For Option 1: run one of the following  yum update osg-ca-certs  or  yum update igtf-ca-certs  depending on the rpm package you installed.  For Option 4: run  yum update osg-ca-certs  For Option 2: You do not need to do a manual update, make sure  osg-update-certs-cron  is enabled using  root@host # /sbin/service osg-update-certs-cron  status  If the service is disabled, enable it using  root@host # /sbin/service osg-update-certs-cron  start  If for some extraordinary reason you need to manually update the CA you could run  osg-ca-manage [--force] refreshCA .", 
            "title": "How do I manually update my CA package?"
        }, 
        {
            "location": "/common/ca/#where-are-the-configuration-files-for-fetch-crl", 
            "text": "/etc/fetch-crl.conf  or  /etc/fetch-crl3.conf  for fetch-crl 2.x or 3.x respectively", 
            "title": "Where are the configuration files for fetch-crl?"
        }, 
        {
            "location": "/common/ca/#references", 
            "text": "Some guides on x509 certificates:   Useful commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html  Install GSI authentication on a server:  http://security.ncsa.illinois.edu/research/wssec/gsihttps/  Certificates how-to:  http://www.nordugrid.org/documents/certificate_howto.html   Some examples about verifying the certificates:   http://gagravarr.org/writing/openssl-certs/others.shtml  http://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/  http://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html   Related software:   Description, manual and examples of  osg-ca-manage  osg-ca-certs-updater", 
            "title": "References"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/", 
            "text": "OSG CA Certificates Updater\n\n\nAbout this Document\n\n\nThis document explains the installation and use of \nosg-ca-certs-updater\n, a package that provides automatic updates of CA certificates.\n\n\nRequirements\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstall instructions\n\n\nRun the following command to install the latest version of the updater.\n\n\nroot@host#\n yum install osg-ca-certs-updater\n\n\n\n\n\nServices\n\n\nStarting and Enabling Services\n\n\nRun the following to enable the updater. This will persist until the machine is rebooted.\n\n\nroot@host#\n service osg-ca-certs-updater-cron start\n\n\n\n\n\nRun the following to enable the updater when the machine is rebooted.\n\n\nroot@host#\n chkconfig osg-ca-certs-updater-cron on\n\n\n\n\n\nRun both commands if you wish for the service to activate immediately and remain active throughout reboots.\n\n\nStopping and Disabling Services\n\n\nEnter the following to disable the updater. This will persist until the machine is rebooted.\n\n\nroot@host#\n service osg-ca-certs-updater-cron stop\n\n\n\n\n\nEnter the following to disable the updater when the machine is rebooted.\n\n\nroot@host#\n chkconfig osg-ca-certs-updater-cron off\n\n\n\n\n\nRun both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.\n\n\nConfiguration\n\n\nWhile there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the \ncron\n entry of the service. This entry is located in the file \n/etc/cron.d/osg-ca-certs-updater\n. Please see the Unix manual page for \ncrontab\n in section 5 for an explanation of the format. The manual page can be accessed by the command \nman 5 crontab\n. The valid command-line arguments can be listed by running \nosg-ca-certs-updater --help\n. Reasonable defaults have been provided, namely:\n\n\n\n\nAttempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day.\n\n\nRun the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day.\n\n\nDelay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories.\n\n\nDo not warn the administrator about update failures that have happened less than 72 hours since the last successful update.\n\n\nLog errors only.\n\n\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nConfiguration file\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nosg-ca-certs-updater\n\n\nCron entry for periodically launching the updater\n\n\n/etc/cron.d/osg-ca-certs-updater\n\n\nCommand-line arguments to the updater can be specified here\n\n\n\n\n\n\nosg-release\n\n\nRepo definition files for production OSG repositories\n\n\n/etc/yum.repos.d/osg.repo\n or \n/etc/yum.repos.d/osg-el6.repo\n\n\nMake sure these repositories are enabled and reachable from the host you are trying to update\n\n\n\n\n\n\n\n\nLog files\n\n\nLogging is performed to the console by default. Please see the manual for your \ncron\n daemon to find out how it handles console output.\n\n\nA logfile can be specified via the \n-l\n / \n--logfile\n command-line option.\n\n\nIf logging to syslog via the \n-s\n / \n--log-to-syslog\n option, the updater will write to the \nuser\n section of the syslog. The file \n/etc/syslog.conf\n determines where syslog messages are saved.\n\n\nHow to get Help?\n\n\nTo get assistance please use \nHelp Procedure\n.\n\n\nReferences\n\n\nSome guides on X.509 certificates:\n\n\n\n\nUseful commands: \nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html\n\n\nInstall GSI authentication on a server: \nhttp://security.ncsa.illinois.edu/research/wssec/gsihttps/\n\n\nCertificates how-to: \nhttp://www.nordugrid.org/documents/certificate_howto.html\n\n\n\n\nSome examples about verifying the certificates:\n\n\n\n\nhttp://gagravarr.org/writing/openssl-certs/others.shtml\n\n\nhttp://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/\n\n\nhttp://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html", 
            "title": "OSG CA Certificates Updater"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#osg-ca-certificates-updater", 
            "text": "", 
            "title": "OSG CA Certificates Updater"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#about-this-document", 
            "text": "This document explains the installation and use of  osg-ca-certs-updater , a package that provides automatic updates of CA certificates.", 
            "title": "About this Document"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#requirements", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Requirements"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#install-instructions", 
            "text": "Run the following command to install the latest version of the updater.  root@host#  yum install osg-ca-certs-updater", 
            "title": "Install instructions"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#services", 
            "text": "", 
            "title": "Services"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#starting-and-enabling-services", 
            "text": "Run the following to enable the updater. This will persist until the machine is rebooted.  root@host#  service osg-ca-certs-updater-cron start  Run the following to enable the updater when the machine is rebooted.  root@host#  chkconfig osg-ca-certs-updater-cron on  Run both commands if you wish for the service to activate immediately and remain active throughout reboots.", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#stopping-and-disabling-services", 
            "text": "Enter the following to disable the updater. This will persist until the machine is rebooted.  root@host#  service osg-ca-certs-updater-cron stop  Enter the following to disable the updater when the machine is rebooted.  root@host#  chkconfig osg-ca-certs-updater-cron off  Run both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#configuration", 
            "text": "While there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the  cron  entry of the service. This entry is located in the file  /etc/cron.d/osg-ca-certs-updater . Please see the Unix manual page for  crontab  in section 5 for an explanation of the format. The manual page can be accessed by the command  man 5 crontab . The valid command-line arguments can be listed by running  osg-ca-certs-updater --help . Reasonable defaults have been provided, namely:   Attempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day.  Run the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day.  Delay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories.  Do not warn the administrator about update failures that have happened less than 72 hours since the last successful update.  Log errors only.", 
            "title": "Configuration"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#useful-configuration-and-log-files", 
            "text": "", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#configuration-file", 
            "text": "Package  File Description  Location  Comment      osg-ca-certs-updater  Cron entry for periodically launching the updater  /etc/cron.d/osg-ca-certs-updater  Command-line arguments to the updater can be specified here    osg-release  Repo definition files for production OSG repositories  /etc/yum.repos.d/osg.repo  or  /etc/yum.repos.d/osg-el6.repo  Make sure these repositories are enabled and reachable from the host you are trying to update", 
            "title": "Configuration file"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#log-files", 
            "text": "Logging is performed to the console by default. Please see the manual for your  cron  daemon to find out how it handles console output.  A logfile can be specified via the  -l  /  --logfile  command-line option.  If logging to syslog via the  -s  /  --log-to-syslog  option, the updater will write to the  user  section of the syslog. The file  /etc/syslog.conf  determines where syslog messages are saved.", 
            "title": "Log files"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#how-to-get-help", 
            "text": "To get assistance please use  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#references", 
            "text": "Some guides on X.509 certificates:   Useful commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html  Install GSI authentication on a server:  http://security.ncsa.illinois.edu/research/wssec/gsihttps/  Certificates how-to:  http://www.nordugrid.org/documents/certificate_howto.html   Some examples about verifying the certificates:   http://gagravarr.org/writing/openssl-certs/others.shtml  http://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/  http://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html", 
            "title": "References"
        }, 
        {
            "location": "/common/yum/", 
            "text": "YUM Repositories\n\n\nAbout This Document\n\n\nThis document introduces YUM repositories and how OSG uses them.\n\n\nRepositories\n\n\nOSG hosts four public-facing repositories at \nrepo.grid.iu.edu\n:\n\n\n\n\nrelease\n: This repository contains software that we are willing to support and can be used by the general community.\n\n\ncontrib\n: RPMs contributed from outside the OSG.\n\n\ntesting\n: This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed version.\n\n\ndevelopment\n: This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage.\n\n\n\n\nOSG's RPM packages rely also on external packages provided by supported OSes and EPEL. You must have the following repositories available and enabled:\n\n\n\n\nyour OS repositories (SL 6/7, CentOS 6/7, or RHEL 6/7 repositories)\n\n\nEPEL repositories\n\n\nthe OSG repositories you'd like to use\n\n\n\n\nIf one of these repositories is missing you may have missing dependencies.\n\n\n\n\nWarning\n\n\nWe did not test other repositories. If you use packages from other repositories, like \njpackage\n, \ndag\n, or \nrpmforge\n, you may encounter problems.\n\n\n\n\nEnabling Repositories\n\n\nIn \nour advice on using yum\n you will learn many tricks and tips on using yum.\n\n\nTo use the packages in a repository without adding special options to the yum command the repository must be enabled.\n\n\nInstall the Yum Repositories required by OSG\n\n\nThe OSG RPMs currently support Red Hat Enterprise Linux 6, 7, and variants.\n\n\nOSG RPMs are distributed via the OSG yum repositories. Some packages depend on packages distributed via the \nEPEL\n repositories. So both repositories must be enabled.\n\n\nInstall EPEL\n\n\n\n\nInstall the EPEL repository, if not already present. \nNote:\n This enables EPEL by default. Choose the right version to match your OS version.\n#\n EPEL \n6\n \n(\nFor RHEL \n6\n, CentOS \n6\n, and SL \n6\n)\n\n\n[root@client ~] #\n rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n\n#\n EPEL \n7\n \n(\nFor RHEL \n7\n, CentOS \n7\n, and SL \n7\n)\n \n\n[root@client ~] #\n rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nif you have your own mirror or configuration of the EPEL repository, you \nMUST\n verify that the OSG repository has a better yum priority than EPEL (\ndetails\n). Otherwise, you will have strange dependency resolution (\ndepsolving\n) issues.\n\n\n\n\nInstall the Yum priorities package\n\n\nFor packages that exist in both OSG and EPEL repositories, it is important to prefer the OSG ones or else OSG software installs may fail. Installing the Yum priorities package enables the repository priority system to work.\n\n\n\n\n\n\nInstall the Yum priorities package:\n\n\n[root@client ~]#\n yum install yum-plugin-priorities\n\n\n\n\n\n\n\n\n\nEnsure that \n/etc/yum.conf\n has the following line in the \n[main]\n section (particularly when using ROCKS), thereby enabling Yum plugins, including the priorities one:\n\n\nplugins=1\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf you do not have a required key you can force the installation using \n--nogpgcheck=\n; e.g., \nyum install --nogpgcheck yum-priorities\n.\n\n\n\n\nInstall OSG Repositories\n\n\nIf you are upgrading from one OSG series to another, remove the old OSG repository definition files and clean the Yum cache:\n\n\n[root@client ~]$\n yum clean all \n\n[root@client ~]$\n rpm -e osg-release\n\n\n\n\n\nThis step ensures that local changes to \n*.repo\n files will not block the installation of the new OSG repositories. After this step, \n*.repo\n files that have been changed will exist in \n/etc/yum.repos.d/\n with the \n*.rpmsave\n extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the \n*.rpmsave\n files to the new \n*.repo\n files.\n\n\nInstall the OSG repositories:\n\n\n[root@client ~]$\n rpm -Uvh \nURL\n\n\n\n\n\n\nWhere \nURL\n is one of the following:\n\n\n\n\n\n\n\n\nSeries\n\n\nEL6 URL (for RHEL 6, CentOS 6, or SL 6)\n\n\nEL7 URL (for RHEL 7, CentOS 7, or SL 7)\n\n\n\n\n\n\n\n\n\n\nOSG 3.3\n\n\nhttps://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm\n\n\nhttps://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\n\n\n\n\nOSG 3.4\n\n\nhttps://repo.grid.iu.edu/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n\nhttps://repo.grid.iu.edu/osg/3.4/osg-3.4-el7-release-latest.rpm\n\n\n\n\n\n\n\n\nPriorities\n\n\n Make sure you installed the Yum priorities plugin, as described above. Not doing so is a common mistake that causes failed installations.\n\n\nThe only OSG repository enabled by default is the release one. If you want to enable another one, such as \nosg-testing\n, then edit its file (e.g. \n/etc/yum.repos.d/osg-testing.repo\n) and change the enabled option from 0 to 1:\n\n\n[osg-testing]\n\n\nname\n=\nOSG Software for Enterprise Linux 5 - Testing - $basearch\n\n\n#baseurl=http://repo.grid.iu.edu/osg/3.2/el5/testing/$basearch\n\n\nmirrorlist\n=\nhttp://repo.grid.iu.edu/mirror/osg/3.2/el5/testing/$basearch\n\n\nfailovermethod\n=\npriority\n\n\npriority\n=\n98\n\n\nenabled\n=\n1\n\n\ngpgcheck\n=\n1\n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\n\n\nWarning\n\n\nif you have your own mirror or configuration of the EPEL repository, you \nMUST\n verify that the OSG repository has a better yum priority than EPEL. Otherwise, you will have strange dependency resolution issues.\n\n\n\n\nReference\n\n\n\n\nBasic use of Yum\n\n\nBest practices in using Yum", 
            "title": "Yum Repos"
        }, 
        {
            "location": "/common/yum/#yum-repositories", 
            "text": "", 
            "title": "YUM Repositories"
        }, 
        {
            "location": "/common/yum/#about-this-document", 
            "text": "This document introduces YUM repositories and how OSG uses them.", 
            "title": "About This Document"
        }, 
        {
            "location": "/common/yum/#repositories", 
            "text": "OSG hosts four public-facing repositories at  repo.grid.iu.edu :   release : This repository contains software that we are willing to support and can be used by the general community.  contrib : RPMs contributed from outside the OSG.  testing : This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed version.  development : This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage.   OSG's RPM packages rely also on external packages provided by supported OSes and EPEL. You must have the following repositories available and enabled:   your OS repositories (SL 6/7, CentOS 6/7, or RHEL 6/7 repositories)  EPEL repositories  the OSG repositories you'd like to use   If one of these repositories is missing you may have missing dependencies.   Warning  We did not test other repositories. If you use packages from other repositories, like  jpackage ,  dag , or  rpmforge , you may encounter problems.", 
            "title": "Repositories"
        }, 
        {
            "location": "/common/yum/#enabling-repositories", 
            "text": "In  our advice on using yum  you will learn many tricks and tips on using yum.  To use the packages in a repository without adding special options to the yum command the repository must be enabled.", 
            "title": "Enabling Repositories"
        }, 
        {
            "location": "/common/yum/#install-the-yum-repositories-required-by-osg", 
            "text": "The OSG RPMs currently support Red Hat Enterprise Linux 6, 7, and variants.  OSG RPMs are distributed via the OSG yum repositories. Some packages depend on packages distributed via the  EPEL  repositories. So both repositories must be enabled.", 
            "title": "Install the Yum Repositories required by OSG"
        }, 
        {
            "location": "/common/yum/#install-epel", 
            "text": "Install the EPEL repository, if not already present.  Note:  This enables EPEL by default. Choose the right version to match your OS version. #  EPEL  6   ( For RHEL  6 , CentOS  6 , and SL  6 )  [root@client ~] #  rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm #  EPEL  7   ( For RHEL  7 , CentOS  7 , and SL  7 )   [root@client ~] #  rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm     Warning  if you have your own mirror or configuration of the EPEL repository, you  MUST  verify that the OSG repository has a better yum priority than EPEL ( details ). Otherwise, you will have strange dependency resolution ( depsolving ) issues.", 
            "title": "Install EPEL"
        }, 
        {
            "location": "/common/yum/#install-the-yum-priorities-package", 
            "text": "For packages that exist in both OSG and EPEL repositories, it is important to prefer the OSG ones or else OSG software installs may fail. Installing the Yum priorities package enables the repository priority system to work.    Install the Yum priorities package:  [root@client ~]#  yum install yum-plugin-priorities    Ensure that  /etc/yum.conf  has the following line in the  [main]  section (particularly when using ROCKS), thereby enabling Yum plugins, including the priorities one:  plugins=1     Note  If you do not have a required key you can force the installation using  --nogpgcheck= ; e.g.,  yum install --nogpgcheck yum-priorities .", 
            "title": "Install the Yum priorities package"
        }, 
        {
            "location": "/common/yum/#install-osg-repositories", 
            "text": "If you are upgrading from one OSG series to another, remove the old OSG repository definition files and clean the Yum cache:  [root@client ~]$  yum clean all  [root@client ~]$  rpm -e osg-release  This step ensures that local changes to  *.repo  files will not block the installation of the new OSG repositories. After this step,  *.repo  files that have been changed will exist in  /etc/yum.repos.d/  with the  *.rpmsave  extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the  *.rpmsave  files to the new  *.repo  files.  Install the OSG repositories:  [root@client ~]$  rpm -Uvh  URL   Where  URL  is one of the following:     Series  EL6 URL (for RHEL 6, CentOS 6, or SL 6)  EL7 URL (for RHEL 7, CentOS 7, or SL 7)      OSG 3.3  https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm  https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm    OSG 3.4  https://repo.grid.iu.edu/osg/3.4/osg-3.4-el6-release-latest.rpm  https://repo.grid.iu.edu/osg/3.4/osg-3.4-el7-release-latest.rpm", 
            "title": "Install OSG Repositories"
        }, 
        {
            "location": "/common/yum/#priorities", 
            "text": "Make sure you installed the Yum priorities plugin, as described above. Not doing so is a common mistake that causes failed installations.  The only OSG repository enabled by default is the release one. If you want to enable another one, such as  osg-testing , then edit its file (e.g.  /etc/yum.repos.d/osg-testing.repo ) and change the enabled option from 0 to 1:  [osg-testing]  name = OSG Software for Enterprise Linux 5 - Testing - $basearch  #baseurl=http://repo.grid.iu.edu/osg/3.2/el5/testing/$basearch  mirrorlist = http://repo.grid.iu.edu/mirror/osg/3.2/el5/testing/$basearch  failovermethod = priority  priority = 98  enabled = 1  gpgcheck = 1  gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG    Warning  if you have your own mirror or configuration of the EPEL repository, you  MUST  verify that the OSG repository has a better yum priority than EPEL. Otherwise, you will have strange dependency resolution issues.", 
            "title": "Priorities"
        }, 
        {
            "location": "/common/yum/#reference", 
            "text": "Basic use of Yum  Best practices in using Yum", 
            "title": "Reference"
        }, 
        {
            "location": "/worker-node/install-wn/", 
            "text": "Installing and Using the Worker Node Client From RPMs\n\n\nAbout This Guide\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the reference section below for contents of the Worker Node Client.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:\n\n\n\n\nInstall using RPMs and Yum (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\nUse from OASIS\n - useful when worker nodes already mount \nCVMFS\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstall the Worker Node Client\n\n\nInstall the Worker Node Client RPM:\n\n\n[root@client ~]#\n yum install osg-wn-client\n\n\n\n\n\nServices\n\n\nFetch-CRL is the only service required to support the WN Client.\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nfetch-crl-boot\n will begin fetching CRLS, which can take a few minutes and fail on transient errors. You can add configuration to ignore these transient errors in \n/etc/fetch-crl.conf\n:\n\n\nnoerrors\n\n\n\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice SERVICE-NAME start\n\n\n\n\n\n\nStop a service\n\n\nservice SERVICE-NAME stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig SERVICE-NAME on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig SERVICE-NAME off\n\n\n\n\n\n\n\n\nValidating the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to \n/etc/osg/wn-client\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.\n\n\nReference\n\n\nPlease see the documentation on using \nyum and RPM\n, \nthe best practices\n for using yum to install software, and using \nyum repositories\n.\n\n\nWorker node contents\n\n\nThe worker node may be updated from time to time. As of OSG 3.3.21 in February 2017, the OSG worker node client contains:\n\n\n\n\nOSG Certificates\n\n\ncurl\n\n\nFetch CRL\n\n\nFTS client\n\n\ngfal2\n\n\nglobus-url-copy (GridFTP client)\n\n\nglobus-xio-udt-driver\n\n\nldapsearch\n\n\nMyProxy\n\n\nosg-system-profiler\n\n\nosg-version\n\n\nUberFTP\n\n\nvo-client (includes /etc/vomses file)\n\n\nVOMS client\n\n\nwget\n\n\nxrdcp\n\n\n\n\nTo see the currently installed version of the worker node package, run the following command:\n\n\n[root@client ~]# rpm -q --requires osg-wn-client\n\n\n\n\n\nClick \nhere\n for more details on using RPM to see what was installed.", 
            "title": "Worker Node (RPM install)"
        }, 
        {
            "location": "/worker-node/install-wn/#installing-and-using-the-worker-node-client-from-rpms", 
            "text": "", 
            "title": "Installing and Using the Worker Node Client From RPMs"
        }, 
        {
            "location": "/worker-node/install-wn/#about-this-guide", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the reference section below for contents of the Worker Node Client.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:   Install using RPMs and Yum (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes  Use from OASIS  - useful when worker nodes already mount  CVMFS   This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/worker-node/install-wn/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-wn/#install-the-worker-node-client", 
            "text": "Install the Worker Node Client RPM:  [root@client ~]#  yum install osg-wn-client", 
            "title": "Install the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn/#services", 
            "text": "Fetch-CRL is the only service required to support the WN Client.     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info      Note  fetch-crl-boot  will begin fetching CRLS, which can take a few minutes and fail on transient errors. You can add configuration to ignore these transient errors in  /etc/fetch-crl.conf :  noerrors   As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service SERVICE-NAME start    Stop a service  service SERVICE-NAME stop    Enable a service to start during boot  chkconfig SERVICE-NAME on    Disable a service from starting during boot  chkconfig SERVICE-NAME off", 
            "title": "Services"
        }, 
        {
            "location": "/worker-node/install-wn/#validating-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to  /etc/osg/wn-client", 
            "title": "Validating the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-wn/#reference", 
            "text": "Please see the documentation on using  yum and RPM ,  the best practices  for using yum to install software, and using  yum repositories .", 
            "title": "Reference"
        }, 
        {
            "location": "/worker-node/install-wn/#worker-node-contents", 
            "text": "The worker node may be updated from time to time. As of OSG 3.3.21 in February 2017, the OSG worker node client contains:   OSG Certificates  curl  Fetch CRL  FTS client  gfal2  globus-url-copy (GridFTP client)  globus-xio-udt-driver  ldapsearch  MyProxy  osg-system-profiler  osg-version  UberFTP  vo-client (includes /etc/vomses file)  VOMS client  wget  xrdcp   To see the currently installed version of the worker node package, run the following command:  [root@client ~]# rpm -q --requires osg-wn-client  Click  here  for more details on using RPM to see what was installed.", 
            "title": "Worker node contents"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/", 
            "text": "Installing and Using the Worker Node Client from Tarballs\n\n\nIntroduction\n\n\nThis document is intended to guide users through the process of installing the worker node software and configuring the installed worker node software. Contents of the worker node client can be found \nhere\n.  Although this document is oriented to system administrators, any unprivileged user may install and use the client.\n\n\nIf you are installing the OSG worker node client following these instruction, remember to configure the \ngrid_dir\n option on your CE - see \nbelow\n.\n\n\nAbout This Guide\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:  This guide is useful when installing onto a shared filesystem for distribution to worker nodes.  Other options include installing \nvia RPMs\n or providing the client \nvia OASIS (CVMFS)\n.\n\n\nBefore starting, ensure the host has \na supported operating system\n.\n\n\nDownload, Installation and Configuration\n\n\nDownload the WN Client\n\n\nPlease pick the \nosg-wn-client\n tarball that is appropriate for your distribution and architecture. You will find them in \nhttps://repo.grid.iu.edu/tarball-install/\n .\n\n\nThe latest available the tarballs for OSG 3.3 are:\n\n\n\n\nBinaries for 32-bit RHEL6\n\n\nBinaries for 64-bit RHEL6\n\n\nBinaries for RHEL7\n\n\n\n\nFor OSG 3.4:\n\n\n\n\nBinaries for 64-bit RHEL6\n\n\nBinaries for RHEL7\n\n\n\n\nInstall the WN Client\n\n\n\n\nUnpack the tarball.\n\n\nMove the directory that was created to where you want the tarball client to be.\n\n\nRun \nosg-post-install\n (\nPATH_TO_CLIENT\n/osg/osg-post-install\n) to fix the directories in the installation.\n\n\nSource the setup \nsource \nPATH_TO_CLIENT\n/setup.sh\n (or \nsetup.csh\n depending on the shell).\n\n\nDownload and set up CA certificiates using \nosg-ca-manage\n (See the \nCA management documentation\n for the available options).\n\n\nDownload CRLs using \nfetch-crl\n.\n\n\n\n\n\n\nWarning\n\n\nOnce \nosg-post-install\n is run to relocate the install, it cannot be run again.  You will need to unpack a fresh copy.\n\n\n\n\nExample installation (in \n/home/user/test-install\n, the \nPATH_TO_CLIENT\n/\n is \n/home/user/test-install/osg-wn-client\n ):\n\n\n[root@client ~] #\n mkdir /home/user/test-install\n\n[root@client ~] #\n \ncd\n /home/user/test-install\n\n[root@client ~] #\n wget http://repo.grid.iu.edu/tarball-install/3.4/osg-wn-client-latest.el6.x86_64.tar.gz\n\n[root@client ~] #\n tar xzf osg-wn-client-latest.el6.x86_64.tar.gz\n\n[root@client ~] #\n \ncd\n osg-wn-client\n\n[root@client ~] #\n ./osg/osg-post-install\n\n[root@client ~] #\n \nsource\n setup.sh\n\n[root@client ~] #\n osg-ca-manage setupCA --url osg\n\n[root@client ~] #\n fetch-crl\n\n\n\n\n\nConfigure the CE\n\n\nUsing the wn-client software installed from the tarball will require a few changes on the compute element so that the resource's configuration can be correctly reported.\n\n\nSet \ngrid_dir\n in the \nStorage\n section of your OSG-Configure configs: \nCE configuration instructions\n. \ngrid_dir\n is used as the \n$OSG_GRID\n environment variable in running jobs - see \nthe environment variables document\n. Pilot jobs source \n$OSG_GRID/setup.sh\n before performing any work. The value set for \ngrid_dir\n must be the path of the wn-client installation directory. This is the path returned by \necho $OSG_LOCATION\n once you source the setup file created by this installation.\n\n\nServices\n\n\nThe WN client is a collection of client programs that do not require service startup or shutdown. The only services are \nosg-update-certs\n that keeps the CA certificates up-to-date and \nfetch-crl\n that keeps the CRLs up-to-date. Following the instructions below you'll add the services to your crontab that will take care to run them periodically until you remove them.\n\n\nAuto-updating certificates and CRLs\n\n\nYou must create cron jobs to run \nfetch-crl\n and \nosg-update-certs\n to update your CRLs and certificates automatically.\n\n\nHere is what they should look like. (Note: fill in \nOSG_LOCATION\n with the full path of your tarball install, including \nosg-wn-client\n that is created by the tarball).\n\n\n# Cron job to update certs.\n# Runs every hour by default, though does not update certs until they\nre at\n# least 24 hours old.  There is a random sleep time for up to 45 minutes (2700\n# seconds) to avoid overloading cert servers.\n10 * * * *   ( . \nOSG_LOCATION\n/setup.sh \n osg-update-certs --random-sleep 2700 --called-from-cron )\n\n\n\n\n\n# Cron job to update CRLs\n# Runs every 6 hours at, 45 minutes +/- 3 minutes.\n42 */6 * * *   ( . \nOSG_LOCATION\n/setup.sh \n fetch-crl -q -r 360 )\n\n\n\n\n\nYou might want to configure proxy settings in \n$OSG_LOCATION/etc/fetch-crl.conf\n.\n\n\nStarting and Enabling Services\n\n\nTo start the services you must edit your cron with \ncrontab -e\n and add the lines above.\n\n\nStopping and Disabling Services\n\n\nTo stop the services you must edit your cron with \ncrontab -e\n and remove or comment the lines above.\n\n\nValiding the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \n$OSG_GRID\n is set to the directory of your worker node client installation\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "Worker Node (tarball install)"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#installing-and-using-the-worker-node-client-from-tarballs", 
            "text": "", 
            "title": "Installing and Using the Worker Node Client from Tarballs"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#introduction", 
            "text": "This document is intended to guide users through the process of installing the worker node software and configuring the installed worker node software. Contents of the worker node client can be found  here .  Although this document is oriented to system administrators, any unprivileged user may install and use the client.  If you are installing the OSG worker node client following these instruction, remember to configure the  grid_dir  option on your CE - see  below .", 
            "title": "Introduction"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#about-this-guide", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:  This guide is useful when installing onto a shared filesystem for distribution to worker nodes.  Other options include installing  via RPMs  or providing the client  via OASIS (CVMFS) .  Before starting, ensure the host has  a supported operating system .", 
            "title": "About This Guide"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#download-installation-and-configuration", 
            "text": "", 
            "title": "Download, Installation and Configuration"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#download-the-wn-client", 
            "text": "Please pick the  osg-wn-client  tarball that is appropriate for your distribution and architecture. You will find them in  https://repo.grid.iu.edu/tarball-install/  .  The latest available the tarballs for OSG 3.3 are:   Binaries for 32-bit RHEL6  Binaries for 64-bit RHEL6  Binaries for RHEL7   For OSG 3.4:   Binaries for 64-bit RHEL6  Binaries for RHEL7", 
            "title": "Download the WN Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#install-the-wn-client", 
            "text": "Unpack the tarball.  Move the directory that was created to where you want the tarball client to be.  Run  osg-post-install  ( PATH_TO_CLIENT /osg/osg-post-install ) to fix the directories in the installation.  Source the setup  source  PATH_TO_CLIENT /setup.sh  (or  setup.csh  depending on the shell).  Download and set up CA certificiates using  osg-ca-manage  (See the  CA management documentation  for the available options).  Download CRLs using  fetch-crl .    Warning  Once  osg-post-install  is run to relocate the install, it cannot be run again.  You will need to unpack a fresh copy.   Example installation (in  /home/user/test-install , the  PATH_TO_CLIENT /  is  /home/user/test-install/osg-wn-client  ):  [root@client ~] #  mkdir /home/user/test-install [root@client ~] #   cd  /home/user/test-install [root@client ~] #  wget http://repo.grid.iu.edu/tarball-install/3.4/osg-wn-client-latest.el6.x86_64.tar.gz [root@client ~] #  tar xzf osg-wn-client-latest.el6.x86_64.tar.gz [root@client ~] #   cd  osg-wn-client [root@client ~] #  ./osg/osg-post-install [root@client ~] #   source  setup.sh [root@client ~] #  osg-ca-manage setupCA --url osg [root@client ~] #  fetch-crl", 
            "title": "Install the WN Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#configure-the-ce", 
            "text": "Using the wn-client software installed from the tarball will require a few changes on the compute element so that the resource's configuration can be correctly reported.  Set  grid_dir  in the  Storage  section of your OSG-Configure configs:  CE configuration instructions .  grid_dir  is used as the  $OSG_GRID  environment variable in running jobs - see  the environment variables document . Pilot jobs source  $OSG_GRID/setup.sh  before performing any work. The value set for  grid_dir  must be the path of the wn-client installation directory. This is the path returned by  echo $OSG_LOCATION  once you source the setup file created by this installation.", 
            "title": "Configure the CE"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#services", 
            "text": "The WN client is a collection of client programs that do not require service startup or shutdown. The only services are  osg-update-certs  that keeps the CA certificates up-to-date and  fetch-crl  that keeps the CRLs up-to-date. Following the instructions below you'll add the services to your crontab that will take care to run them periodically until you remove them.", 
            "title": "Services"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#auto-updating-certificates-and-crls", 
            "text": "You must create cron jobs to run  fetch-crl  and  osg-update-certs  to update your CRLs and certificates automatically.  Here is what they should look like. (Note: fill in  OSG_LOCATION  with the full path of your tarball install, including  osg-wn-client  that is created by the tarball).  # Cron job to update certs.\n# Runs every hour by default, though does not update certs until they re at\n# least 24 hours old.  There is a random sleep time for up to 45 minutes (2700\n# seconds) to avoid overloading cert servers.\n10 * * * *   ( .  OSG_LOCATION /setup.sh   osg-update-certs --random-sleep 2700 --called-from-cron )  # Cron job to update CRLs\n# Runs every 6 hours at, 45 minutes +/- 3 minutes.\n42 */6 * * *   ( .  OSG_LOCATION /setup.sh   fetch-crl -q -r 360 )  You might want to configure proxy settings in  $OSG_LOCATION/etc/fetch-crl.conf .", 
            "title": "Auto-updating certificates and CRLs"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#starting-and-enabling-services", 
            "text": "To start the services you must edit your cron with  crontab -e  and add the lines above.", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#stopping-and-disabling-services", 
            "text": "To stop the services you must edit your cron with  crontab -e  and remove or comment the lines above.", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#validing-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  $OSG_GRID  is set to the directory of your worker node client installation", 
            "title": "Validing the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/", 
            "text": "Configuring a Site to Use the Worker Node Client Software From OASIS\n\n\nAbout This Guide\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:\n\n\n\n\nUse the Worker Node Client software from OASIS (this guide) - useful when \nCVMFS\n is already mounted on your worker nodes\n\n\nInstall using RPMs and Yum\n - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\n\n\nAlso note that, once configured to use OASIS, each use of Worker Node Client software will cause that software and its associated files to be downloaded from a CVMFS server or your local cache thereof. This may result in extra network activity, especially if Worker Node Client tools are used heavily.\n\n\nConfiguring Your Site to Use the Worker Node Client From OASIS\n\n\nBelow are the one-time steps that you must perform to configure your site to use the Worker Node Client software from OASIS.\n\n\nOn every worker node, \ninstall and configure CVMFS\n\n\nDetermine the OASIS path to the Worker Node Client software for your worker nodes:\n\n\n\n\n\n\n\n\nWorker Node OS\n\n\nUse\u2026\n\n\n\n\n\n\n\n\n\n\nEL\u00a06 (32-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386\n\n\n\n\n\n\nEL\u00a06 (64-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64\n\n\n\n\n\n\nEL\u00a07 (64-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64\n\n\n\n\n\n\n\n\nOn the CE, in the \n/etc/osg/config.d/10-storage.ini\n file, set the \ngrid_dir\n configuration setting to the path from the previous step.\n\n\nFor more information, see the \nOSG environment variables reference page\n and the \nCE configuration instructions\n.\n\n\nOnce you finish making changes to configuration files on your CE, validate, fix, and apply the configuration:\n\n\n[root@client ~] #\n osg-configure -v\n\n[root@client ~] #\n osg-configure -c\n\n\n\n\n\nValidating the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to the directory of your WN Client installation\n\n\n\n\nManually Using the Worker Node Client From OASIS\n\n\nIf you must log onto a worker node and use the Worker Node Client software directly during your login session, consult the following table for the command to set up your environment:\n\n\n\n\n\n\n\n\nWorker Node OS\n\n\nRun the following command\u2026\n\n\n\n\n\n\n\n\n\n\nEL\u00a06 (32-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386/setup.sh\n\n\n\n\n\n\nEL\u00a06 (64-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64/setup.sh\n\n\n\n\n\n\nEL\u00a07 (64-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64/setup.sh\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nSome possible issues that may come up:\n\n\n\n\n\n\nA missing softlink to the CA certs directory. To check this, run:\n\n\n[user@client ~] $\n ls -l /cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.3/current/el6-x86_64/etc/grid-security/\n\n\n\n\n\nand check that \ncertificates\n is linked to somewhere. The fix is to yum update the \noasis-config\n package to version 4 or higher. A known workaround is to run:\n\n\n[user@client ~] $\n \nexport\n \nX509_CERT_DIR\n=\n/cvmfs/oasis.opensciencegrid.org/mis/certificates\n\n\n\n\n\nbefore any commands.\n\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "Worker Node (OASIS install)"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#configuring-a-site-to-use-the-worker-node-client-software-from-oasis", 
            "text": "", 
            "title": "Configuring a Site to Use the Worker Node Client Software From OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#about-this-guide", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:   Use the Worker Node Client software from OASIS (this guide) - useful when  CVMFS  is already mounted on your worker nodes  Install using RPMs and Yum  - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes   This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system   Also note that, once configured to use OASIS, each use of Worker Node Client software will cause that software and its associated files to be downloaded from a CVMFS server or your local cache thereof. This may result in extra network activity, especially if Worker Node Client tools are used heavily.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#configuring-your-site-to-use-the-worker-node-client-from-oasis", 
            "text": "Below are the one-time steps that you must perform to configure your site to use the Worker Node Client software from OASIS.  On every worker node,  install and configure CVMFS  Determine the OASIS path to the Worker Node Client software for your worker nodes:     Worker Node OS  Use\u2026      EL\u00a06 (32-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386    EL\u00a06 (64-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64    EL\u00a07 (64-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64     On the CE, in the  /etc/osg/config.d/10-storage.ini  file, set the  grid_dir  configuration setting to the path from the previous step.  For more information, see the  OSG environment variables reference page  and the  CE configuration instructions .  Once you finish making changes to configuration files on your CE, validate, fix, and apply the configuration:  [root@client ~] #  osg-configure -v [root@client ~] #  osg-configure -c", 
            "title": "Configuring Your Site to Use the Worker Node Client From OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#validating-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to the directory of your WN Client installation", 
            "title": "Validating the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#manually-using-the-worker-node-client-from-oasis", 
            "text": "If you must log onto a worker node and use the Worker Node Client software directly during your login session, consult the following table for the command to set up your environment:     Worker Node OS  Run the following command\u2026      EL\u00a06 (32-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386/setup.sh    EL\u00a06 (64-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64/setup.sh    EL\u00a07 (64-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64/setup.sh", 
            "title": "Manually Using the Worker Node Client From OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#troubleshooting", 
            "text": "Some possible issues that may come up:    A missing softlink to the CA certs directory. To check this, run:  [user@client ~] $  ls -l /cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.3/current/el6-x86_64/etc/grid-security/  and check that  certificates  is linked to somewhere. The fix is to yum update the  oasis-config  package to version 4 or higher. A known workaround is to run:  [user@client ~] $   export   X509_CERT_DIR = /cvmfs/oasis.opensciencegrid.org/mis/certificates  before any commands.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-cvmfs/", 
            "text": "Install CVMFS\n\n\nHere we describe how to install the\n\nCVMFS\n (Cern-VM file system) client.\nThis document is intended for system administrators who wish to\ninstall this client to have access to files distributed by CVMFS\nservers via HTTP.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.4.3.\nThe version of CVMFS installed should be \n= 2.4.1\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the \ncvmfs\n Linux user\n\n\nGroup IDs:\n If they do not exist already, the installation will create the Linux users \ncvmfs\n and \nfuse\n\n\nNetwork ports:\n You will need network access to a local squid server such as the \nsquid distributed by OSG\n. The squid will need out-bound access to cvmfs stratum 1 servers.\n\n\nHost choice:\n -  Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details \nbelow\n)\n\n\nFUSE\n: CVMFS requires FUSE\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nInstalling CVMFS\n\n\nThe following will install CVMFS from the OSG yum repository. It will also install fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution which is called OASIS. To simplify installation, OSG provides convenience RPMs that install all required software with a single command.\n\n\n\n\n\n\nClean yum cache:\n\n\n[root@client ~ ] $\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\n[root@client ~ ] $\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall CVMFS software:\n\n\n[root@client ~] $\n yum install osg-oasis\n\n\n\n\n\n\n\n\n\nSetup of FUSE and automount\n\n\nFUSE and automount are required for CVMFS and the steps to configure them are different on EL6 vs EL7. Follow the section that is appropriate for your host's OS:\n\n\n\n\nFor EL6 hosts\n\n\nFor EL7 hosts\n\n\n\n\nFor EL6 hosts\n\n\n\n\n\n\nCreate or edit \n/etc/fuse.conf\n so that it contains:\n\n\nuser_allow_other\n\n\n\n\n\n\n\n\n\nCreate or edit \n/etc/auto.master\n to have the following contents to enable automounting:\n\n\n/cvmfs /etc/auto.cvmfs\n\n\n\n\n\n\n\n\n\nRestart autofs to make the change take effect:\n\n\n[root@client ~] $\n service autofs restart\n\n\n\n\n\n\n\n\n\nFor EL7 hosts\n\n\n\n\n\n\nAdd the following to \n/etc/auto.master.d/cvmfs.autofs\n:\n\n\n/cvmfs /etc/auto.cvmfs\n\n\n\n\n\n\n\n\n\nRestart autofs to make the change take effect:\n\n\n[root@client ~] $\n systemctl restart autofs\n\n\n\n\n\n\n\n\n\n\n\nConfiguring CVMFS\n\n\nCreate or edit \n/etc/cvmfs/default.local\n, a file that controls the\nCVMFS configuration. Below is a sample configuration, but please note\nthat you will need to \nedit the parts in \nred\n. In\nparticular, the \nCVMFS_HTTP_PROXY\n line below must be edited for your\nsite.\n\n\nCVMFS_REPOSITORIES=\n`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr \n \n ,`\n\nCVMFS_QUOTA_LIMIT=\n20000\n\nCVMFS_HTTP_PROXY=\nhttp://squid.example.com:3128\n\n\n\n\n\n\n\nCVMFS by default allows any repository to be mounted, no matter what\nthe setting of \nCVMFS_REPOSITORIES\n is; that variable is used by support\ntools such as \ncvmfs_config\n and \ncvmfs_talk\n when they need to know a\nlist of repositories.  The recommended \nCVMFS_REPOSITORIES\n setting\nabove is so that those tools will use two common repositories plus any\nadditional that have been mounted.  You may want to choose a different\nset of always-known repositories.\n\n\nSet up a list of CVMFS HTTP proxies to retrieve from in\n\nCVMFS_HTTP_PROXY\n. If you do not have any squid at your site follow\nthe instructions to \ninstall squid from OSG\n.\nVertical bars separating proxies means to load balance between them\nand try them all before continuing. A semicolon between proxies means\nto try that one only after the previous ones have failed. A special\nproxy called DIRECT can be placed last in the list to indicate\ndirectly connecting to servers if all other proxies fail. A DIRECT\nproxy is acceptable for small sites but discouraged for large sites\nbecause of the potential load that could be put upon globally shared\nservers.\n\n\nSet up the cache limit in \nCVMFS_QUOTA_LIMIT\n (in MegaBytes). The\nrecommended value for most applications is 20000 MB. This is the\ncombined limit for all but the osgstorage.org repositories. This cache\nwill be stored in \n/var/lib/cvmfs\n by default; to override the\nlocation, set \nCVMFS_CACHE_BASE\n in \n/etc/cvmfs/default.local\n. Note\nthat an additional 1000 MB is allocated for a separate osgstorage.org\nrepositories cache in \n$CVMFS_CACHE_BASE/osgstorage\n. To be safe, make\nsure that at least 20% more than \n$CVMFS_QUOTA_LIMIT\n + 1000 MB of space\nstays available for CVMFS in that filesystem. This is very important,\nsince if that space is not available it can cause many I/O errors and\napplication crashes. Many system administrators choose to put the\ncache space in a separate filesystem, which is a good way to manage\nit.\n\n\n\n\nWarning\n\n\nIf you use SELinux and change \nCVMFS_CACHE_BASE\n, then the\nnew cache directory must be labelled with SELinux type\n\ncvmfs_cache_t\n. This can be done by executing the following command:\n\n\n[user@client ~] $\n chcon -R -t cvmfs_cache_t \n$CVMFS_CACHE_BASE\n\n\n\n\n\n\n\n\nValidating CVMFS\n\n\nAfter CVMFS is installed, you should be able to see the \n/cvmfs\n\ndirectory. But note that it will initially appear to be empty:\n\n\n[user@client ~] $\n ls /cvmfs\n\n[user@client ~] $\n\n\n\n\n\n\n\nDirectories within \n/cvmfs\n will not be mounted until you examine them. For instance:\n\n\n[user@client ~] $\n ls /cvmfs\n\n[user@client ~] $\n ls -l /cvmfs/atlas.cern.ch\n\ntotal 1\n\n\ndrwxr-xr-x 8 cvmfs cvmfs 3 Apr 13 14:50 repo\n\n\n[user@client ~] $\n ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\n\ntotal 1\n\n\nlrwxrwxrwx 1 cvmfs cvmfs 18 May 13  2015 cms -\n /cvmfs/cms.cern.ch\n\n\n[user@client ~] $\n ls -l /cvmfs/glast.egi.eu\n\ntotal 5\n\n\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n\n\n[user@client ~] $\n ls -l /cvmfs/nova.osgstorage.org\n\ntotal 6\n\n\nlrwxrwxrwx 1 cvmfs cvmfs   43 Jun 14  2016 analysis -\n pnfs/fnal.gov/usr/nova/persistent/analysis/\n\n\nlrwxrwxrwx 1 cvmfs cvmfs   32 Jan 19 11:40 flux -\n pnfs/fnal.gov/usr/nova/data/flux\n\n\ndrwxr-xr-x 3 cvmfs cvmfs 4096 Jan 19 11:39 pnfs\n\n\n[user@client ~] $\n ls /cvmfs\n\natlas.cern.ch                   glast.egi.eu         oasis.opensciencegrid.org\n\n\nconfig-osg.opensciencegrid.org  nova.osgstorage.org\n\n\n\n\n\n\n\n\nTroubleshooting problems\n\n\nIf no directories exist under \n/cvmfs/\n, you can try the following\nsteps to debug:\n\n\n\n\nMount it manually with \nmkdir -p /mnt/cvmfs\n and then\n  \nmount -t cvmfs REPOSITORYNAME /mnt/cvmfs\n where REPOSITORYNAME is\n  the repository, for example config-osg.opensciencegrid.org (which is\n  the best one to try first because other repositories require it to\n  be mounted).  If this works, then CVMFS is working, but there is a\n  problem with automount.\n\n\nIf that doesn't work and doesn't give any explanatory errors, try\n   \ncvmfs_config chksetup\n or \ncvmfs_config showconfig REPOSITORYNAME\n\n   to verify your setup.\n\n\nIf chksetup reports access problems to proxies, it may be caused by\n  access control settings in the squids.\n\n\nIf you have changed settings in \n/etc/cvmfs/default.local\n, and they\n  do not seem to be taking effect, note that there are other\n  configuration files that can override the settings. See the comments\n  at the beginning of \n/etc/cvmfs/default.conf\n regarding the order in\n  which configuration files are evaluated and look for old files that\n  may have been left from a previous installation.\n\n\nMore things to try are in the\n  \nupstream documentation\n.\n\n\n\n\nStarting and Stopping services\n\n\nOnce it is set up, CVMFS is always automatically started when one of\nthe repositories are accessed; there are no system services to start.\n\n\nCVMFS can be stopped via:\n\n\n[root@client ~] $\n cvmfs_config umount\n\nUnmounting /cvmfs/config-osg.opensciencegrid.org: OK\n\n\nUnmounting /cvmfs/atlas.cern.ch: OK\n\n\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\n\n\nUnmounting /cvmfs/glast.egi.eu: OK\n\n\nUnmounting /cvmfs/nova.osgstorage.org: OK\n\n\n\n\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug reporting and OSG-specific issues, see our \nhelp procedure\n\n\nFor community support and best-effort software team support contact\n   \n.\n\n\nFor general CERN VM FileSystem support contact \n.\n\n\n\n\nReferences\n\n\n\n\nhttp://cernvm.cern.ch/portal/filesystem/techinformation\n\n\nhttps://cvmfs.readthedocs.io/en/latest/\n\n\n\n\nUsers and Groups\n\n\nThis installation will create one user unless it already exists\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\n\n\n\n\n\n\nThe installation will also create a cvmfs group and default the cvmfs\nuser to that group. In addition, if the fuse rpm is not for some\nreason already installed, installing cvmfs will also install fuse and\nthat will create another group:\n\n\n\n\n\n\n\n\nGroup\n\n\nComment\n\n\nGroup members\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\nnone\n\n\n\n\n\n\nfuse\n\n\nFUSE service account\n\n\ncvmfs", 
            "title": "CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#install-cvmfs", 
            "text": "Here we describe how to install the CVMFS  (Cern-VM file system) client.\nThis document is intended for system administrators who wish to\ninstall this client to have access to files distributed by CVMFS\nservers via HTTP.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.4.3.\nThe version of CVMFS installed should be  = 2.4.1", 
            "title": "Install CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the  cvmfs  Linux user  Group IDs:  If they do not exist already, the installation will create the Linux users  cvmfs  and  fuse  Network ports:  You will need network access to a local squid server such as the  squid distributed by OSG . The squid will need out-bound access to cvmfs stratum 1 servers.  Host choice:  -  Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details  below )  FUSE : CVMFS requires FUSE   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#installing-cvmfs", 
            "text": "The following will install CVMFS from the OSG yum repository. It will also install fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution which is called OASIS. To simplify installation, OSG provides convenience RPMs that install all required software with a single command.    Clean yum cache:  [root@client ~ ] $  yum clean all --enablerepo = *    Update software:  [root@client ~ ] $  yum update  This command will update  all  packages    Install CVMFS software:  [root@client ~] $  yum install osg-oasis", 
            "title": "Installing CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#setup-of-fuse-and-automount", 
            "text": "FUSE and automount are required for CVMFS and the steps to configure them are different on EL6 vs EL7. Follow the section that is appropriate for your host's OS:   For EL6 hosts  For EL7 hosts", 
            "title": "Setup of FUSE and automount"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#for-el6-hosts", 
            "text": "Create or edit  /etc/fuse.conf  so that it contains:  user_allow_other    Create or edit  /etc/auto.master  to have the following contents to enable automounting:  /cvmfs /etc/auto.cvmfs    Restart autofs to make the change take effect:  [root@client ~] $  service autofs restart", 
            "title": "For EL6 hosts"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#for-el7-hosts", 
            "text": "Add the following to  /etc/auto.master.d/cvmfs.autofs :  /cvmfs /etc/auto.cvmfs    Restart autofs to make the change take effect:  [root@client ~] $  systemctl restart autofs", 
            "title": "For EL7 hosts"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#configuring-cvmfs", 
            "text": "Create or edit  /etc/cvmfs/default.local , a file that controls the\nCVMFS configuration. Below is a sample configuration, but please note\nthat you will need to  edit the parts in  red . In\nparticular, the  CVMFS_HTTP_PROXY  line below must be edited for your\nsite.  CVMFS_REPOSITORIES= `echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr     ,` \nCVMFS_QUOTA_LIMIT= 20000 \nCVMFS_HTTP_PROXY= http://squid.example.com:3128   \nCVMFS by default allows any repository to be mounted, no matter what\nthe setting of  CVMFS_REPOSITORIES  is; that variable is used by support\ntools such as  cvmfs_config  and  cvmfs_talk  when they need to know a\nlist of repositories.  The recommended  CVMFS_REPOSITORIES  setting\nabove is so that those tools will use two common repositories plus any\nadditional that have been mounted.  You may want to choose a different\nset of always-known repositories.  Set up a list of CVMFS HTTP proxies to retrieve from in CVMFS_HTTP_PROXY . If you do not have any squid at your site follow\nthe instructions to  install squid from OSG .\nVertical bars separating proxies means to load balance between them\nand try them all before continuing. A semicolon between proxies means\nto try that one only after the previous ones have failed. A special\nproxy called DIRECT can be placed last in the list to indicate\ndirectly connecting to servers if all other proxies fail. A DIRECT\nproxy is acceptable for small sites but discouraged for large sites\nbecause of the potential load that could be put upon globally shared\nservers.  Set up the cache limit in  CVMFS_QUOTA_LIMIT  (in MegaBytes). The\nrecommended value for most applications is 20000 MB. This is the\ncombined limit for all but the osgstorage.org repositories. This cache\nwill be stored in  /var/lib/cvmfs  by default; to override the\nlocation, set  CVMFS_CACHE_BASE  in  /etc/cvmfs/default.local . Note\nthat an additional 1000 MB is allocated for a separate osgstorage.org\nrepositories cache in  $CVMFS_CACHE_BASE/osgstorage . To be safe, make\nsure that at least 20% more than  $CVMFS_QUOTA_LIMIT  + 1000 MB of space\nstays available for CVMFS in that filesystem. This is very important,\nsince if that space is not available it can cause many I/O errors and\napplication crashes. Many system administrators choose to put the\ncache space in a separate filesystem, which is a good way to manage\nit.   Warning  If you use SELinux and change  CVMFS_CACHE_BASE , then the\nnew cache directory must be labelled with SELinux type cvmfs_cache_t . This can be done by executing the following command:  [user@client ~] $  chcon -R -t cvmfs_cache_t  $CVMFS_CACHE_BASE", 
            "title": "Configuring CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#validating-cvmfs", 
            "text": "After CVMFS is installed, you should be able to see the  /cvmfs \ndirectory. But note that it will initially appear to be empty:  [user@client ~] $  ls /cvmfs [user@client ~] $   \nDirectories within  /cvmfs  will not be mounted until you examine them. For instance:  [user@client ~] $  ls /cvmfs [user@client ~] $  ls -l /cvmfs/atlas.cern.ch total 1  drwxr-xr-x 8 cvmfs cvmfs 3 Apr 13 14:50 repo  [user@client ~] $  ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft total 1  lrwxrwxrwx 1 cvmfs cvmfs 18 May 13  2015 cms -  /cvmfs/cms.cern.ch  [user@client ~] $  ls -l /cvmfs/glast.egi.eu total 5  drwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast  [user@client ~] $  ls -l /cvmfs/nova.osgstorage.org total 6  lrwxrwxrwx 1 cvmfs cvmfs   43 Jun 14  2016 analysis -  pnfs/fnal.gov/usr/nova/persistent/analysis/  lrwxrwxrwx 1 cvmfs cvmfs   32 Jan 19 11:40 flux -  pnfs/fnal.gov/usr/nova/data/flux  drwxr-xr-x 3 cvmfs cvmfs 4096 Jan 19 11:39 pnfs  [user@client ~] $  ls /cvmfs atlas.cern.ch                   glast.egi.eu         oasis.opensciencegrid.org  config-osg.opensciencegrid.org  nova.osgstorage.org", 
            "title": "Validating CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#troubleshooting-problems", 
            "text": "If no directories exist under  /cvmfs/ , you can try the following\nsteps to debug:   Mount it manually with  mkdir -p /mnt/cvmfs  and then\n   mount -t cvmfs REPOSITORYNAME /mnt/cvmfs  where REPOSITORYNAME is\n  the repository, for example config-osg.opensciencegrid.org (which is\n  the best one to try first because other repositories require it to\n  be mounted).  If this works, then CVMFS is working, but there is a\n  problem with automount.  If that doesn't work and doesn't give any explanatory errors, try\n    cvmfs_config chksetup  or  cvmfs_config showconfig REPOSITORYNAME \n   to verify your setup.  If chksetup reports access problems to proxies, it may be caused by\n  access control settings in the squids.  If you have changed settings in  /etc/cvmfs/default.local , and they\n  do not seem to be taking effect, note that there are other\n  configuration files that can override the settings. See the comments\n  at the beginning of  /etc/cvmfs/default.conf  regarding the order in\n  which configuration files are evaluated and look for old files that\n  may have been left from a previous installation.  More things to try are in the\n   upstream documentation .", 
            "title": "Troubleshooting problems"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#starting-and-stopping-services", 
            "text": "Once it is set up, CVMFS is always automatically started when one of\nthe repositories are accessed; there are no system services to start.  CVMFS can be stopped via:  [root@client ~] $  cvmfs_config umount Unmounting /cvmfs/config-osg.opensciencegrid.org: OK  Unmounting /cvmfs/atlas.cern.ch: OK  Unmounting /cvmfs/oasis.opensciencegrid.org: OK  Unmounting /cvmfs/glast.egi.eu: OK  Unmounting /cvmfs/nova.osgstorage.org: OK", 
            "title": "Starting and Stopping services"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug reporting and OSG-specific issues, see our  help procedure  For community support and best-effort software team support contact\n    .  For general CERN VM FileSystem support contact  .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#references", 
            "text": "http://cernvm.cern.ch/portal/filesystem/techinformation  https://cvmfs.readthedocs.io/en/latest/", 
            "title": "References"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#users-and-groups", 
            "text": "This installation will create one user unless it already exists     User  Comment      cvmfs  CernVM-FS service account     The installation will also create a cvmfs group and default the cvmfs\nuser to that group. In addition, if the fuse rpm is not for some\nreason already installed, installing cvmfs will also install fuse and\nthat will create another group:     Group  Comment  Group members      cvmfs  CernVM-FS service account  none    fuse  FUSE service account  cvmfs", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/worker-node/install-singularity/", 
            "text": "Install Singularity\n\n\nSingularity\n is a tool that creates\ndocker-like process containers but without giving extra privileges to\nunprivileged users.  It is used by grid pilot jobs (which are\nsubmitted by per-VO grid workload management systems) to isolate user\njobs from the pilot's files and processes and from other users' files\nand processes.  It also supplies a chroot environment in order to run\nuser jobs in different operating system images under one Linux kernel.\n\n\nFor operating system kernels older than the one released for\nRed Hat Enterprise Linux (RHEL) 7.4,\nsingularity needs to use kernel capabilities that are only accessible\nto the root user, so it has to be installed with setuid-root\nexecutables.  Securing setuid-root programs is difficult, but singularity\nkeeps that privileged code to a\n\nminimum\n to keep the\nvulnerability low.  Beginning with the kernel released with RHEL 7.4,\nthere is a new\n\ntechnology preview feature\n\nto allow unprivileged bind mounts in user name spaces, which allows\nsingularity to run as an unprivileged user.  The OSG has installed\nsingularity in \ncvmfs\n, so if you have a RHEL 7.4 kernel\nor later you\ncan avoid installing singularity at all and reduce vulnerability even\nfurther.  The RHEL 7.4 kernel (version 3.10.0-693) is available as a\nsecurity update for all RHEL 7 based versions, even on systems that\nhave not updated to RHEL 7.4.\n\n\nThe document is intended for system administrators who wish to either\ninstall singularity or enable it to be run as an unprivileged user.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.4.3.\nThe version of singularity installed should be \n= 2.3.1\n\n\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nIf you're installing singularity, prepare the \nrequired Yum repositories\n\n\n\n\nUsing Singularity via CVMFS (EL 7 only)\n\n\nIf the operating system is an EL 7 variant, and has been updated to EL\n7.4 or the 7.4 kernel (3.10.0-693 or greater), you can skip\ninstallation altogether and use singularity through CVMFS:\n\n\n\n\nSet the \nnamespace.unpriv_enable=1\n boot option.  The easiest way\n    to do this is to add it in \n/etc/sysconfig/grub\n to the end of the\n    GRUB_CMDLINE_LINUX variable, before the ending double-quote.\n\n\n\n\nUpdate the grub configuration:\n\n\n[root@client ~] $\n grub2-mkconfig -o /boot/grub2/grub.cfg\n\n\n\n\n\n\n\n\n\nSet a sysctl option as follows:\n\n\n[root@client ~] $\n \necho\n \nuser.max_user_namespaces = 15000\n \n\\\n\n    \n /etc/sysctl.d/90-max_user_namespaces.conf\n\n\n\n\n\n\n\n\n\nReboot\n\n\n\n\nIf you haven't yet installed \ncvmfs\n, do so.\n\n\nLog in as an ordinary unprivileged user and verify that singularity\n    works:\n[user@client ~] $\n /cvmfs/oasis.opensciencegrid.org/mis/singularity/el7-x86_64/bin/singularity \n\\\n\n        \nexec\n -C -H \n$HOME\n:/srv /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6 \n\\\n\n        ps -ef\n\nWARNING: Container does not have an exec helper script, calling \ncat\n directly\n\n\nUID        PID  PPID  C STIME TTY          TIME CMD\n\n\nuser         1     0  0 21:34 ?        00:00:00 ps -ef\n\n\n\n\n\n\n\n\n\n\nInstalling singularity\n\n\nTo install singularity, make sure that your host is up to date before installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\n[root@client ~ ] $\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\n[root@client ~ ] $\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nThe singularity packages are split into two parts, choose the command that corresponds to your host:\n\n\n\n\n\n\nIf you are installing singularity on a worker node, where images do not need to be created of a manipulated, install just the smaller part to limit the amount of setuid-root code that is installed:\n\n\n[root@client ~] $\n yum install singularity-runtime\n\n\n\n\n\n\n\n\n\nIf you want a full singularity installation, run the following command:\n\n\n[root@client ~] $\n yum install singularity\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguring singularity\n\n\nThe default configuration of singularity is sufficient.  If you want\nto see what options are available, see \n/etc/singularity/singularity.conf\n.\n\n\nValidating singularity\n\n\nAfter singularity is installed, as an ordinary user run the following\ncommand to verify it:\n\n\n[user@client ~] $\n singlarity \nexec\n -C -H \n$HOME\n:/srv /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6 ps -ef\n\nWARNING: Container does not have an exec helper script, calling \ncat\n directly\n\n\nUID        PID  PPID  C STIME TTY          TIME CMD\n\n\nuser         1     0  0 21:34 ?        00:00:00 ps -ef\n\n\n\n\n\n\n\n\nStarting and Stopping services\n\n\nsingularity has no services to start or stop.", 
            "title": "Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#install-singularity", 
            "text": "Singularity  is a tool that creates\ndocker-like process containers but without giving extra privileges to\nunprivileged users.  It is used by grid pilot jobs (which are\nsubmitted by per-VO grid workload management systems) to isolate user\njobs from the pilot's files and processes and from other users' files\nand processes.  It also supplies a chroot environment in order to run\nuser jobs in different operating system images under one Linux kernel.  For operating system kernels older than the one released for\nRed Hat Enterprise Linux (RHEL) 7.4,\nsingularity needs to use kernel capabilities that are only accessible\nto the root user, so it has to be installed with setuid-root\nexecutables.  Securing setuid-root programs is difficult, but singularity\nkeeps that privileged code to a minimum  to keep the\nvulnerability low.  Beginning with the kernel released with RHEL 7.4,\nthere is a new technology preview feature \nto allow unprivileged bind mounts in user name spaces, which allows\nsingularity to run as an unprivileged user.  The OSG has installed\nsingularity in  cvmfs , so if you have a RHEL 7.4 kernel\nor later you\ncan avoid installing singularity at all and reduce vulnerability even\nfurther.  The RHEL 7.4 kernel (version 3.10.0-693) is available as a\nsecurity update for all RHEL 7 based versions, even on systems that\nhave not updated to RHEL 7.4.  The document is intended for system administrators who wish to either\ninstall singularity or enable it to be run as an unprivileged user.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.4.3.\nThe version of singularity installed should be  = 2.3.1", 
            "title": "Install Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  If you're installing singularity, prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-singularity/#using-singularity-via-cvmfs-el-7-only", 
            "text": "If the operating system is an EL 7 variant, and has been updated to EL\n7.4 or the 7.4 kernel (3.10.0-693 or greater), you can skip\ninstallation altogether and use singularity through CVMFS:   Set the  namespace.unpriv_enable=1  boot option.  The easiest way\n    to do this is to add it in  /etc/sysconfig/grub  to the end of the\n    GRUB_CMDLINE_LINUX variable, before the ending double-quote.   Update the grub configuration:  [root@client ~] $  grub2-mkconfig -o /boot/grub2/grub.cfg    Set a sysctl option as follows:  [root@client ~] $   echo   user.max_user_namespaces = 15000   \\ \n      /etc/sysctl.d/90-max_user_namespaces.conf    Reboot   If you haven't yet installed  cvmfs , do so.  Log in as an ordinary unprivileged user and verify that singularity\n    works: [user@client ~] $  /cvmfs/oasis.opensciencegrid.org/mis/singularity/el7-x86_64/bin/singularity  \\ \n         exec  -C -H  $HOME :/srv /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6  \\ \n        ps -ef WARNING: Container does not have an exec helper script, calling  cat  directly  UID        PID  PPID  C STIME TTY          TIME CMD  user         1     0  0 21:34 ?        00:00:00 ps -ef", 
            "title": "Using Singularity via CVMFS (EL 7 only)"
        }, 
        {
            "location": "/worker-node/install-singularity/#installing-singularity", 
            "text": "To install singularity, make sure that your host is up to date before installing the required packages:    Clean yum cache:  [root@client ~ ] $  yum clean all --enablerepo = *    Update software:  [root@client ~ ] $  yum update  This command will update  all  packages    The singularity packages are split into two parts, choose the command that corresponds to your host:    If you are installing singularity on a worker node, where images do not need to be created of a manipulated, install just the smaller part to limit the amount of setuid-root code that is installed:  [root@client ~] $  yum install singularity-runtime    If you want a full singularity installation, run the following command:  [root@client ~] $  yum install singularity", 
            "title": "Installing singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#configuring-singularity", 
            "text": "The default configuration of singularity is sufficient.  If you want\nto see what options are available, see  /etc/singularity/singularity.conf .", 
            "title": "Configuring singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#validating-singularity", 
            "text": "After singularity is installed, as an ordinary user run the following\ncommand to verify it:  [user@client ~] $  singlarity  exec  -C -H  $HOME :/srv /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6 ps -ef WARNING: Container does not have an exec helper script, calling  cat  directly  UID        PID  PPID  C STIME TTY          TIME CMD  user         1     0  0 21:34 ?        00:00:00 ps -ef", 
            "title": "Validating singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#starting-and-stopping-services", 
            "text": "singularity has no services to start or stop.", 
            "title": "Starting and Stopping services"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/", 
            "text": "HTCondor-CE Overview\n\n\nAbout this Document\n\n\nThis document serves as an introduction to HTCondor-CE, how it works, and how it differs from a GRAM CE.\n\n\nDocument Requirements\n\n\nBefore continuing with this document, make sure that you are familiar with the following concepts:\n\n\n\n\nAn OSG site plan\n\n\nWhat is a batch system and which one will you use (\nHTCondor\n, PBS, LSF, SGE, or SLURM)?\n\n\nSecurity in the OSG via \nGSI\n (i.e., \nCertificate authorities\n, user and host \ncertificates\n, proxies)\n\n\n\n\n\n\nPilot jobs, frontends, and factories (i.e., \nGlideinWMS\n, \nAutoPyFactory\n)\n\n\n\n\nWhat is a Compute Element?\n\n\nAn OSG Compute Element (CE) is the entry point for the OSG to your local resources: a layer of software that you install on a machine that can submit jobs into your local batch system. At the heart of the CE is the \njob gateway\n software, which is responsible for handling incoming jobs, authorizing them, and delegating them to your batch system for execution. Historically, the OSG only had one option for a job gateway solution, Globus Toolkit\u2019s GRAM-based gatekeeper, but now offers the HTCondor-CE as an alternative.\n\n\nToday in OSG, most jobs that arrive at a CE (called \ngrid jobs\n) are \nnot\n end-user jobs, but rather pilot jobs submitted from factories. Successful pilot jobs create and make available an environment for actual end-user jobs to match and ultimately run within the pilot job container. Eventually pilot jobs remove themselves, typically after a period of inactivity.\n\n\nWhat is HTCondor-CE?\n\n\nHTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for the OSG. It is configured to use the \nJobRouter daemon\n to delegate jobs by transforming and submitting them to the site\u2019s batch system.\n\n\nHow is HTCondor-CE different from a GRAM CE?\n\n\nThe biggest difference you will see between an HTCondor-CE and a GRAM CE is in the way that jobs are submitted to your batch system; HTCondor-CE uses the built-in JobRouter daemon whereas GRAM CE uses jobmanager scripts written in Perl. Customizing your site\u2019s CE now requires editing configuration files instead of editing jobmanager scripts.\n\n\nListed below are some other benefits to switching to HTCondor-CE:\n\n\n\n\nScalability:\n HTCondor-CE is capable of supporting job workloads of large sites (see \nscale testing results\n)\n\n\nDebugging tools:\n HTCondor-CE offers \nmany tools to help troubleshoot\n issues with jobs\n\n\nRouting as configuration:\n HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs\n\n\n\n\nHow Jobs Run\n\n\nOnce an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the JobRouter creates a transformed copy (called the \nrouted job\n) and submits the copy to the batch system (called the \nbatch system job\n). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter.\n\n\nOn HTCondor batch systems\n\n\nFor a site with an HTCondor \nbatch system\n, the JobRouter can use HTCondor protocols to place a transformed copy of the grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the same. Thus, there are three representations of your job, each with its own ID (see diagram below):\n\n\n\n\nSubmit host: the HTCondor job ID in the original queue\n\n\nHTCondor-CE: the grid job\u2019s ID\n\n\nHTCondor batch system: the routed job\u2019s ID\n\n\n\n\n\n\nIn an HTCondor-CE/HTCondor setup, files are transferred from HTCondor-CE\u2019s spool directory to the batch system\u2019s spool directory using internal HTCondor protocols.\n\n\n\n\nNote\n\n\nThe JobRouter copies the job directly into the batch system and does not make use of \ncondor_submit\n. This means that if the HTCondor batch system is configured to add attributes to incoming jobs when they are submitted (i.e., \nSUBMIT_EXPRS\n), these attributes will not be added to the routed jobs.\n\n\n\n\nOn other batch systems\n\n\nFor non-HTCondor batch systems, the JobRouter transforms the grid job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below):\n\n\n\n\nSubmit host: the HTCondor job ID in the original queue\n\n\nHTCondor-CE: the grid job\u2019s ID and the routed job\u2019s ID\n\n\nHTCondor batch system: the batch system\u2019s job ID\n\n\n\n\nAlthough the following figure specifies the PBS case, it applies to all non-HTCondor batch systems:\n\n\n\n\nWith non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its spool directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.\n\n\nOver SSH\n\n\nHTCondor-CE-Bosco\n is a special configuration of HTCondor-CE that can submit jobs to a remote cluster over SSH. The HTCondor-CE-Bosco provides a simple starting point for opportunistic resource owners that want to start contributing to the OSG with minimal effort: an organization will be able to accept OSG jobs by allowing SSH access to a submit node in their cluster.\n\n\n\n\nHTCondor-CE-Bosco is intended for small sites or as an introduction to the OSG. If your site intends to run thousands of OSG jobs, you will need to host a standard \nHTCondor-CE\n because HTCondor-CE-Bosco has not yet been optimized for such loads.\n\n\nHow the CE is Customized\n\n\nAside from the \nbasic configuration\n required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all):\n\n\n\n\nDeciding which VOs are allowed to run at your site:\n The method of limiting the VOs that are allowed to run on your site has not changed between GRAM and HTCondor-CE\u2019s: select an authorization system, GUMS or edg-mkgridmap, and configure it accordingly.\n\n\nHow to filter and transform the grid jobs to be run on your batch system:\n Filtering and transforming grid jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the \nJobRouter recipes\n page.\n\n\n\n\n\n\nNote\n\n\nIf you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in \n/etc/condor/\n and the other in \n/etc/condor-ce\n) and will need to make sure to differentiate the two when editing any configuration.\n\n\n\n\nHow Security Works\n\n\nIn the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certifcates to users and hosts. When these users and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust.\n\n\nDue to the OSG's distributed nature, a user's job may end up at any number of sites, potentially needing to re-authenticate at multiple points. Instead of sending the user's certificate with the job for this re-authentication, trust can be delegated to a proxy that is generated from the user certificate, which is then attached to the job and expires after some set time for added security.\n\n\nIn its default configuration, HTCondor-CE uses GSI-based authentication and authorization (the same as Globus GRAM) to verify the certificate chain, which will work with existing GUMS servers or grid mapfiles. Additionally, it can be reconfigured to provide alternate authentication mechanisms such as Kerberos, SSL, shared secret, or even IP-based authentication. More information about authorization methods can be found \nhere\n.\n\n\nNext steps\n\n\nIf you're transitioning from a GRAM CE to HTCondor-CE, the process is the same as if you were setting up a completely new CE, whether you're installing it on a new machine or alongside your GRAM CE.\n\n\n\n\nInstall \nHTCondor-CE\n or \nHTCondor-CE-Bosco\n\n\nSetting up \njob routes\n\n\nSubmitting\n jobs to HTCondor-CE\n\n\nTroubleshooting\n HTCondor-CE\n\n\nRegister the CE with OIM\n\n\nRegister with the OSG GlideinWMS factories and/or the ATLAS AutoPyFactory", 
            "title": "HTCondor-CE Overview"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#htcondor-ce-overview", 
            "text": "", 
            "title": "HTCondor-CE Overview"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#about-this-document", 
            "text": "This document serves as an introduction to HTCondor-CE, how it works, and how it differs from a GRAM CE.", 
            "title": "About this Document"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#document-requirements", 
            "text": "Before continuing with this document, make sure that you are familiar with the following concepts:   An OSG site plan  What is a batch system and which one will you use ( HTCondor , PBS, LSF, SGE, or SLURM)?  Security in the OSG via  GSI  (i.e.,  Certificate authorities , user and host  certificates , proxies)    Pilot jobs, frontends, and factories (i.e.,  GlideinWMS ,  AutoPyFactory )", 
            "title": "Document Requirements"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#what-is-a-compute-element", 
            "text": "An OSG Compute Element (CE) is the entry point for the OSG to your local resources: a layer of software that you install on a machine that can submit jobs into your local batch system. At the heart of the CE is the  job gateway  software, which is responsible for handling incoming jobs, authorizing them, and delegating them to your batch system for execution. Historically, the OSG only had one option for a job gateway solution, Globus Toolkit\u2019s GRAM-based gatekeeper, but now offers the HTCondor-CE as an alternative.  Today in OSG, most jobs that arrive at a CE (called  grid jobs ) are  not  end-user jobs, but rather pilot jobs submitted from factories. Successful pilot jobs create and make available an environment for actual end-user jobs to match and ultimately run within the pilot job container. Eventually pilot jobs remove themselves, typically after a period of inactivity.", 
            "title": "What is a Compute Element?"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#what-is-htcondor-ce", 
            "text": "HTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for the OSG. It is configured to use the  JobRouter daemon  to delegate jobs by transforming and submitting them to the site\u2019s batch system.", 
            "title": "What is HTCondor-CE?"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-is-htcondor-ce-different-from-a-gram-ce", 
            "text": "The biggest difference you will see between an HTCondor-CE and a GRAM CE is in the way that jobs are submitted to your batch system; HTCondor-CE uses the built-in JobRouter daemon whereas GRAM CE uses jobmanager scripts written in Perl. Customizing your site\u2019s CE now requires editing configuration files instead of editing jobmanager scripts.  Listed below are some other benefits to switching to HTCondor-CE:   Scalability:  HTCondor-CE is capable of supporting job workloads of large sites (see  scale testing results )  Debugging tools:  HTCondor-CE offers  many tools to help troubleshoot  issues with jobs  Routing as configuration:  HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs", 
            "title": "How is HTCondor-CE different from a GRAM CE?"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-jobs-run", 
            "text": "Once an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the JobRouter creates a transformed copy (called the  routed job ) and submits the copy to the batch system (called the  batch system job ). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter.", 
            "title": "How Jobs Run"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#on-htcondor-batch-systems", 
            "text": "For a site with an HTCondor  batch system , the JobRouter can use HTCondor protocols to place a transformed copy of the grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the same. Thus, there are three representations of your job, each with its own ID (see diagram below):   Submit host: the HTCondor job ID in the original queue  HTCondor-CE: the grid job\u2019s ID  HTCondor batch system: the routed job\u2019s ID    In an HTCondor-CE/HTCondor setup, files are transferred from HTCondor-CE\u2019s spool directory to the batch system\u2019s spool directory using internal HTCondor protocols.   Note  The JobRouter copies the job directly into the batch system and does not make use of  condor_submit . This means that if the HTCondor batch system is configured to add attributes to incoming jobs when they are submitted (i.e.,  SUBMIT_EXPRS ), these attributes will not be added to the routed jobs.", 
            "title": "On HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#on-other-batch-systems", 
            "text": "For non-HTCondor batch systems, the JobRouter transforms the grid job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below):   Submit host: the HTCondor job ID in the original queue  HTCondor-CE: the grid job\u2019s ID and the routed job\u2019s ID  HTCondor batch system: the batch system\u2019s job ID   Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems:   With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its spool directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.", 
            "title": "On other batch systems"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#over-ssh", 
            "text": "HTCondor-CE-Bosco  is a special configuration of HTCondor-CE that can submit jobs to a remote cluster over SSH. The HTCondor-CE-Bosco provides a simple starting point for opportunistic resource owners that want to start contributing to the OSG with minimal effort: an organization will be able to accept OSG jobs by allowing SSH access to a submit node in their cluster.   HTCondor-CE-Bosco is intended for small sites or as an introduction to the OSG. If your site intends to run thousands of OSG jobs, you will need to host a standard  HTCondor-CE  because HTCondor-CE-Bosco has not yet been optimized for such loads.", 
            "title": "Over SSH"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-the-ce-is-customized", 
            "text": "Aside from the  basic configuration  required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all):   Deciding which VOs are allowed to run at your site:  The method of limiting the VOs that are allowed to run on your site has not changed between GRAM and HTCondor-CE\u2019s: select an authorization system, GUMS or edg-mkgridmap, and configure it accordingly.  How to filter and transform the grid jobs to be run on your batch system:  Filtering and transforming grid jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the  JobRouter recipes  page.    Note  If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in  /etc/condor/  and the other in  /etc/condor-ce ) and will need to make sure to differentiate the two when editing any configuration.", 
            "title": "How the CE is Customized"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-security-works", 
            "text": "In the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certifcates to users and hosts. When these users and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust.  Due to the OSG's distributed nature, a user's job may end up at any number of sites, potentially needing to re-authenticate at multiple points. Instead of sending the user's certificate with the job for this re-authentication, trust can be delegated to a proxy that is generated from the user certificate, which is then attached to the job and expires after some set time for added security.  In its default configuration, HTCondor-CE uses GSI-based authentication and authorization (the same as Globus GRAM) to verify the certificate chain, which will work with existing GUMS servers or grid mapfiles. Additionally, it can be reconfigured to provide alternate authentication mechanisms such as Kerberos, SSL, shared secret, or even IP-based authentication. More information about authorization methods can be found  here .", 
            "title": "How Security Works"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#next-steps", 
            "text": "If you're transitioning from a GRAM CE to HTCondor-CE, the process is the same as if you were setting up a completely new CE, whether you're installing it on a new machine or alongside your GRAM CE.   Install  HTCondor-CE  or  HTCondor-CE-Bosco  Setting up  job routes  Submitting  jobs to HTCondor-CE  Troubleshooting  HTCondor-CE  Register the CE with OIM  Register with the OSG GlideinWMS factories and/or the ATLAS AutoPyFactory", 
            "title": "Next steps"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/", 
            "text": "Installing and Maintaining HTCondor-CE\n\n\nThe \nHTCondor-CE\n software is a \njob gateway\n for an OSG Compute Element (CE). As such, HTCondor-CE is the entry point for jobs coming from the OSG \u2014 it handles authorization and delegation of jobs to your local batch system. In OSG today, most CEs accept \npilot jobs\n from a factory, which in turn are able to accept and run end-user jobs.  See the \nHTCondor-CE Overview\n for a much more detailed introduction.\n\n\nUse this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the OSG software repositories.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux users \ncondor\n (UID 4716) and \ngratia\n (UID 42401)\n\n\nService certificate:\n The HTCondor-CE service uses a host certificate at \n/etc/grid-security/hostcert.pem\n and an accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nNetwork ports:\n The pilot factories must be able to contact your HTCondor-CE service on ports 9619 and 9620 for condor versions \n 8.3.2 (TCP)\n\n\nHost choice:\n HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling HTCondor-CE\n\n\nAn HTCondor-CE installation consists of the job gateway (i.e., the HTCondor-CE job router) and other support software (e.g., GridFTP, a Gratia probe, authentication software). To simplify installation, OSG provides convenience RPMs that install all required software with a single command.\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nIf your batch system is already installed via non-RPM means and is in the following list, install the appropriate 'empty' RPM. Otherwise, skip to the next step.\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen run the following command\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nyum install empty-condor --enablerepo=osg-empty\n\n\n\n\n\n\nSLURM\n\n\nyum install empty-slurm --enablerepo=osg-empty\n\n\n\n\n\n\n\n\n\n\n\n\nIf your HTCondor batch system is already installed via non-OSG RPM means, add the line below to \n/etc/yum.repos.d/osg.repo\n. Otherwise, skip to the next step.\n\n\nexclude=condor empty-condor\n\n\n\n\n\n\n\n\n\nSelect the appropriate convenience RPM(s):\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen use the following package(s)\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nosg-ce-condor\n\n\n\n\n\n\nLSF\n\n\nosg-ce-lsf\n\n\n\n\n\n\nPBS\n\n\nosg-ce-pbs\n\n\n\n\n\n\nSGE\n\n\nosg-ce-sge\n\n\n\n\n\n\nSLURM\n\n\nosg-ce-slurm\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the CE software:\n\n\n[root@client ~] $ yum install *PACKAGE(S)*\n\n\n\n\n\n\n\n\n\nConfiguring HTCondor-CE\n\n\nThere are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on \noptional configurations\n.\n\n\nEnabling HTCondor-CE\n\n\nIf you are installing HTCondor-CE on a new host, the default configuration is correct and you can skip this step and continue onto \nConfiguring the batch system\n! However, if you are updating a host that used a Globus GRAM job gateway (aka the Globus gatekeeper), you must disable GRAM and enable the HTCondor job gateway. Edit the gateway configuration file \n/etc/osg/config.d/10-gateway.ini\n so that it reads:\n\n\ngram_gateway_enabled = False\nhtcondor_gateway_enabled = True\n\n\n\n\n\nConfiguring the batch system\n\n\nEnable your batch system by editing the \nenabled\n field in the \n/etc/osg/config.d/20-\nYOUR BATCH SYSTEM\n.ini\n\n\nenabled = \nTrue\n\n\n\n\n\n\nIf you are using HTCondor as your \nlocal batch system\n (i.e., in addition to your HTCondor-CE), skip to the \nconfiguring authentication\n section. For other batch systems (e.g., PBS, LSF, SGE, SLURM), keep reading.\n\n\nBatch systems other than HTCondor\n\n\nNon-HTCondor batch systems require additional configuration to support file transfer to your site's worker nodes.\n\n\nSharing the spool directory\n\n\nTo transfer files between the CE and the batch system, HTCondor-CE requires a shared file system. The current recommendation is to run a dedicated NFS server (whose installation is beyond the scope of this document) on the \nCE host\n. In this setup, HTCondor-CE writes to the local spool directory and the NFS server shares the directory with all of the worker nodes.\n\n\n\n\nNote\n\n\nIf you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE daemons can write to the spool directory.\n\n\n\n\nBy default, the spool directory is \n/var/lib/condor-ce\n but you can control this by setting \nSPOOL\n in \n/etc/condor-ce/config.d/99-local.conf\n (create this file if it doesn't exist). For example, the following sets the \nSPOOL\n directory to \n/home/condor\n:\n\n\nSPOOL=/home/condor\n\n\n\n\n\n\n\nNote\n\n\nThe shared spool directory must be readable and writeable by the \ncondor\n user for HTCondor-CE to function correctly.\n\n\n\n\nDisable worker node proxy renewal\n\n\nWorker node proxy renewal is not used by HTCondor-CE and leaving it on will cause some jobs to be held. Edit \n/etc/blah.config\n on the HTCondor-CE host and set the following values:\n\n\nblah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no\nblah_disable_limited_proxy=yes\n\n\n\n\n\n\n\nNote\n\n\nThere should be no whitespace around the \n=\n.\n\n\n\n\nConfiguring authentication\n\n\nIn OSG 3.3, there are three methods to manage authentication for incoming jobs: the \nLCMAPS VOMS plugin\n, \nedg-mkgridmap\n and \nGUMS\n. edg-mkgridmap is easy to set up and maintain, and GUMS has more features and capabilities. The LCMAPS VOMS plugin is the new OSG-preferred authentication, offering the simplicity of edg-mkgridmap and many of GUMS' rich feature set. If you need to support \npool accounts\n, GUMS is the only software with that capability.\n\n\nIn OSG 3.4, the LCMAPS VOMS plugin is the only available authentication solution.\n\n\nAuthentication with the LCMAPS VOMS plugin\n\n\nTo configure your CE to use the LCMAPS VOMS plugin:\n\n\n\n\n\n\nIf you are using OSG 3.3, add the following line to \n/etc/sysconfig/condor-ce\n:\n\n\nexport LLGT_VOMS_ENABLE_CREDENTIAL_CHECK=1\n\n\n\n\n\n\n\n\n\nFollow the instructions in \nthe LCMAPS VOMS plugin installation and configuration document\n to prepare the LCMAPS VOMS plugin\n\n\n\n\n\n\n\n\nNote\n\n\nIf your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the \ncondor_mapfile\n. If this is not the desired behavior, set \nGSI_AUTHZ_CONF=/dev/null\n in the local HTCondor configuration.\n\n\n\n\nAuthentication with edg-mkgridmap\n\n\n\n\nWarning\n\n\nedg-mkgridmap is unavailable in OSG 3.4\n\n\n\n\nTo configure your CE to use edg-mkgridmap:\n\n\n\n\nFollow the configuration instructions in \nthe edg-mkgridmap document\n to define the VOs that your site accepts\n\n\nSet some critical gridmap attributes by editing the \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor-CE host:\nauthorization_method = gridmap\n\n\n\n\n\n\n\n\n\nAuthentication with GUMS\n\n\n\n\nWarning\n\n\nGUMS is unavailable in OSG 3.4\n\n\n\n\n\n\nFollow the instructions in \nthe GUMS installation and configuration document\n to prepare GUMS\n\n\nSet some critical GUMS attributes by editing the \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor-CE host:\nauthorization_method = xacml\ngums_host = \nYOUR GUMS HOSTNAME\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the \ncondor_mapfile\n. If this is not the desired behavior, set \nGSI_AUTHZ_CONF=/dev/null\n in the local HTCondor configuration.\n\n\n\n\nConfiguring CE collector advertising\n\n\nTo split jobs between the various sites of the OSG, information about each site's capabilities are uploaded to a central collector. The job factories then query the central collector for idle resources and submit pilot jobs to the available sites. To advertise your site, you will need to enter some information about the worker nodes of your clusters.\n\n\nPlease see the \nSubcluster / Resource Entry configuration document\n about configuring the data that will be uploaded to the central collector.\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n directory does not apply those settings to software automatically. Settings that are made outside of the OSG directory take effect immediately or at least when the relevant service is restarted. For the OSG settings, use the \nosg-configure\n tool to validate (to a limited extent) and apply the settings to the relevant software components. The \nosg-configure\n software is included automatically in an HTCondor-CE installation.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n directory\n\n\n\n\nNote\n\n\nThis document describes the critical settings for HTCondor-CE and related software. You may need to configure other software that is installed on your HTCondor-CE host, too.\n\n\n\n\n\n\n\n\nValidate the configuration settings\n\n\nroot@host #\n osg-configure -v\n\n\n\n\n\n\n\n\n\nFix any errors (at least) that \nosg-configure\n reports.\n\n\n\n\n\n\nOnce the validation command succeeds without errors, apply the configuration settings:\n\n\nroot@host # osg-configure -c\n\n\n\n\n\n\n\n\n\nGenerate a \nuser-vo-map\n file with your authentication set up (skip this step if you're using the LCMAPS VOMS plugin):\n\n\n\n\n\n\nIf you're using edg-mkgridmap, run the following:\n\n\nroot@host #\n edg-mkgridmap\n\n\n\n\n\n\n\n\n\nIf you're using GUMS, run the following:\n\n\nroot@host #\n gums-host-cron\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to \nthe section on using HTCondor-CE\n.\n\n\n\n\nTransforming and filtering jobs\n\n\nConfiguring for multiple network interfaces\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nAccounting with multiple CEs or local user jobs\n\n\nHTCondor accounting groups\n\n\nInstalling the HTCondor-CE View\n\n\n\n\nTransforming and filtering jobs\n\n\nIf you need to modify or filter jobs, more information can be found in the \nJob Router Recipes\n document.\n\n\n\n\nNote\n\n\nIf you need to assign jobs to HTCondor accounting groups, refer to \nthis\n section.\n\n\n\n\nConfiguring for multiple network interfaces\n\n\nIf you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname and interface to use when communicating to each other. Set \nNETWORK_HOSTNAME\n and \nNETWORK_INTERFACE\n to the hostname and IP address of your public interface, respectively, in \n/etc/condor-ce/config.d/99-local.conf\n directory with the line:\n\n\nNETWORK_HOSTNAME = \ncondorce.example.com\n\nNETWORK_INTERFACE = \n127.0.0.1\n\n\n\n\n\n\nReplacing \ncondorce.example.com\n text with your public interface\u2019s hostname and \n127.0.0.1\n with your public interface\u2019s IP address.\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nIf you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and scheduler universes. Local and scheduler universes are HTCondor-CE\u2019s analogue to GRAM\u2019s managed fork: they allow jobs to be run on the CE itself, mainly for remote troubleshooting. Pilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another worker node.\n\n\nThe two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison.\n\n\n\n\n\n\nTo change the default limit\n on the number of locally run jobs (the current default is 20), add the following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n TotalLocalJobsRunning + TotalSchedulerJobsRunning \n \nJOB-LIMIT\n\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\nTo only allow a specific user\n to start locally run jobs, add the following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n target.Owner \n=\n?\n=\n \nUSERNAME\n\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\nTo disable\n locally run jobs, add the following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n False\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nRSV requires the ability to start local universe jobs so if you are using RSV, you need to allow local universe jobs from the \nrsv\n user.\n\n\n\n\nAccounting with multiple CEs or local user jobs\n\n\n\n\nNote\n\n\nFor non-HTCondor batch systems only\n\n\n\n\nIf your site has multiple CEs or you have non-grid users submitting to the same local batch system, the OSG accounting software needs to be configured so that it doesn't over report the number of jobs. Use the following table to determine which file requires editing:\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen edit the following file on your CE(s)\u2026\n\n\n\n\n\n\n\n\n\n\nLSF\n\n\n/etc/gratia/pbs-lsf/ProbeConfig\n\n\n\n\n\n\nPBS\n\n\n/etc/gratia/pbs-lsf/ProbeConfig\n\n\n\n\n\n\nSGE\n\n\n/etc/gratia/sge/ProbeConfig\n\n\n\n\n\n\nSLURM\n\n\n/etc/gratia/slurm/ProbeConfig\n\n\n\n\n\n\n\n\nThen edit the value of \nSuppressNoDNRecords\n so that it reads:\n\n\nSuppressNoDNRecords=\n1\n\n\n\n\n\n\nHTCondor accounting groups\n\n\n\n\nNote\n\n\nFor HTCondor batch systems only\n\n\n\n\nIf you want to provide fairshare on a group basis, as opposed to a Unix user basis, you can use HTCondor accounting groups. They are independent of the Unix groups the user may already be in and are \ndocumented in the HTCondor manual\n. If you are using HTCondor accounting groups, you can map jobs from the CE into HTCondor accounting groups based on their UID, their DN, or their VOMS attributes.\n\n\n\n\n\n\nTo map UIDs to an accounting group,\n add entries to \n/etc/osg/uid_table.txt\n with the following form:\n\n\nuid GroupName\n\n\n\n\n\nThe following is an example \nuid_table.txt\n:\n\n\nuscms02 TestGroup\nosg     other.osgedu\n\n\n\n\n\n\n\n\n\nTo map DNs or VOMS attributes to an accounting group,\n add lines to \n/etc/osg/extattr_table.txt\n with the following form:\n\n\nSubjectOrAttribute\n GroupName\n\n\n\n\n\n\nThe \nSubjectOrAttribute\n can be a Perl regular expression. The following is an example \nextattr_table.txt\n:\n\n\ncmsprio cms.other.prio\ncms\\/Role=production cms.prod\n\\/DC=com\\/DC=DigiCert-Grid\\/O=Open\\ Science\\ Grid\\/OU=People\\/CN=Brian\\ Lin\\ 1047 osg.test\n.* other\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nEntries in \n/etc/osg/uid_table.txt\n are honored over \n/etc/osg/extattr_table.txt\n if a job would match to lines in both files.\n\n\n\n\nInstall and run the HTCondor-CE View\n\n\nThe HTCondor-CE View is an optional web interface to the status of your CE. To run the View,\n\n\n\n\n\n\nBegin by installing the package htcondor-ce-view:\n\n\nroot@host #\n yum install htcondor-ce-view\n\n\n\n\n\n\n\n\n\nNext, uncomment the \nDAEMON_LIST\n configuration located at \n/etc/condor-ce/config.d/05-ce-view.conf\n:\n\n\nDAEMON_LIST\n \n=\n \n$(\nDAEMON_LIST\n)\n, CEVIEW, GANGLIAD\n\n\n\n\n\n\n\n\n\nRestart the CE service:\n\n\nroot@host #\n service condor-ce restart\n\n\n\n\n\n\n\n\n\nVerify the service by entering your CE's hostname into your web browser\n\n\n\n\n\n\nThe website is served on port 80 by default. To change this default, edit the value of \nHTCONDORCE_VIEW_PORT\n in \n/etc/condor-ce/config.d/05-ce-view.conf\n.\n\n\nUsing HTCondor-CE\n\n\nAs a site administrator, there are a few ways in which you might use the HTCondor-CE:\n\n\n\n\nManaging the HTCondor-CE and associated services\n\n\nUsing HTCondor-CE administrative tools to monitor and maintain the job gateway\n\n\nUsing HTCondor-CE user tools to test gateway operations\n\n\n\n\nManaging HTCondor-CE and associated services\n\n\nIn addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nYour batch system\n\n\ncondor\n or \npbs_server\n or \u2026\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\ncondor-ce\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nSome of the HTCondor-CE administrative and user tools are documented in \nthe HTCondor-CE troubleshooting guide\n.\n\n\nValidating HTCondor-CE\n\n\nThere are different ways to make sure that your HTCondor-CE host is working well:\n\n\n\n\nPerform automated validation by running \nRSV\n\n\nManually verify your HTCondor-CE using \nHTCondor-CE troubleshooting guide\n; useful tools include:\n\n\ncondor_ce_run\n\n\ncondor_ce_trace\n\n\ncondor_submit\n\n\n\n\n\n\n\n\nTroubleshooting HTCondor-CE\n\n\nFor information on how to troubleshoot your HTCondor-CE, please refer to \nthe HTCondor-CE troubleshooting guide\n.\n\n\nRegistering the CE\n\n\nTo be part of the OSG Production Grid, your CE must be registered in the \nOSG Information Management System\n (OIM). To register your resource:\n\n\n\n\nObtain, install, and verify your user certificate\n (which you may have done already)\n\n\nRegister your site and CE in OIM\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\nConfiguration\n\n\nThe following directories contain the configuration for HTCondor-CE. The directories are parsed in the order presented and thus configuration within the final directory will override configuration specified in the previous directories.\n\n\n\n\n\n\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\n/usr/share/condor-ce/config.d/\n\n\nConfiguration defaults (overwritten on package updates)\n\n\n\n\n\n\n/etc/condor-ce/config.d/\n\n\nFiles in this directory are parsed in alphanumeric order (i.e., \n99-local.conf\n will override values in \n01-ce-auth.conf\n)\n\n\n\n\n\n\n\n\nFor a detailed order of the way configuration files are parsed, run the following command:\n\n\nuser@host $\n condor_ce_config_val -config\n\n\n\n\n\nUsers\n\n\nThe following users are needed by HTCondor-CE at all sites:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncondor\n\n\nThe HTCondor-CE will be run as root, but perform most of its operations as the \ncondor\n user.\n\n\n\n\n\n\ngratia\n\n\nRuns the Gratia probes to collect accounting data\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nFile\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n\n\n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHtcondor-CE\n\n\ntcp\n\n\n9619\n\n\nX\n\n\n\n\nHTCondor-CE shared port\n\n\n\n\n\n\n\n\nAllow inbound and outbound network connection to all internal site servers, such as GUMS and the batch system head-node only ephemeral outgoing ports are necessary.", 
            "title": "Install HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#installing-and-maintaining-htcondor-ce", 
            "text": "The  HTCondor-CE  software is a  job gateway  for an OSG Compute Element (CE). As such, HTCondor-CE is the entry point for jobs coming from the OSG \u2014 it handles authorization and delegation of jobs to your local batch system. In OSG today, most CEs accept  pilot jobs  from a factory, which in turn are able to accept and run end-user jobs.  See the  HTCondor-CE Overview  for a much more detailed introduction.  Use this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the OSG software repositories.", 
            "title": "Installing and Maintaining HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux users  condor  (UID 4716) and  gratia  (UID 42401)  Service certificate:  The HTCondor-CE service uses a host certificate at  /etc/grid-security/hostcert.pem  and an accompanying key at  /etc/grid-security/hostkey.pem  Network ports:  The pilot factories must be able to contact your HTCondor-CE service on ports 9619 and 9620 for condor versions   8.3.2 (TCP)  Host choice:  HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#installing-htcondor-ce", 
            "text": "An HTCondor-CE installation consists of the job gateway (i.e., the HTCondor-CE job router) and other support software (e.g., GridFTP, a Gratia probe, authentication software). To simplify installation, OSG provides convenience RPMs that install all required software with a single command.    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    If your batch system is already installed via non-RPM means and is in the following list, install the appropriate 'empty' RPM. Otherwise, skip to the next step.     If your batch system is\u2026  Then run the following command\u2026      HTCondor  yum install empty-condor --enablerepo=osg-empty    SLURM  yum install empty-slurm --enablerepo=osg-empty       If your HTCondor batch system is already installed via non-OSG RPM means, add the line below to  /etc/yum.repos.d/osg.repo . Otherwise, skip to the next step.  exclude=condor empty-condor    Select the appropriate convenience RPM(s):     If your batch system is\u2026  Then use the following package(s)\u2026      HTCondor  osg-ce-condor    LSF  osg-ce-lsf    PBS  osg-ce-pbs    SGE  osg-ce-sge    SLURM  osg-ce-slurm       Install the CE software:  [root@client ~] $ yum install *PACKAGE(S)*", 
            "title": "Installing HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-htcondor-ce", 
            "text": "There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on  optional configurations .", 
            "title": "Configuring HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#enabling-htcondor-ce", 
            "text": "If you are installing HTCondor-CE on a new host, the default configuration is correct and you can skip this step and continue onto  Configuring the batch system ! However, if you are updating a host that used a Globus GRAM job gateway (aka the Globus gatekeeper), you must disable GRAM and enable the HTCondor job gateway. Edit the gateway configuration file  /etc/osg/config.d/10-gateway.ini  so that it reads:  gram_gateway_enabled = False\nhtcondor_gateway_enabled = True", 
            "title": "Enabling HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-the-batch-system", 
            "text": "Enable your batch system by editing the  enabled  field in the  /etc/osg/config.d/20- YOUR BATCH SYSTEM .ini  enabled =  True   If you are using HTCondor as your  local batch system  (i.e., in addition to your HTCondor-CE), skip to the  configuring authentication  section. For other batch systems (e.g., PBS, LSF, SGE, SLURM), keep reading.", 
            "title": "Configuring the batch system"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#batch-systems-other-than-htcondor", 
            "text": "Non-HTCondor batch systems require additional configuration to support file transfer to your site's worker nodes.", 
            "title": "Batch systems other than HTCondor"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#sharing-the-spool-directory", 
            "text": "To transfer files between the CE and the batch system, HTCondor-CE requires a shared file system. The current recommendation is to run a dedicated NFS server (whose installation is beyond the scope of this document) on the  CE host . In this setup, HTCondor-CE writes to the local spool directory and the NFS server shares the directory with all of the worker nodes.   Note  If you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE daemons can write to the spool directory.   By default, the spool directory is  /var/lib/condor-ce  but you can control this by setting  SPOOL  in  /etc/condor-ce/config.d/99-local.conf  (create this file if it doesn't exist). For example, the following sets the  SPOOL  directory to  /home/condor :  SPOOL=/home/condor   Note  The shared spool directory must be readable and writeable by the  condor  user for HTCondor-CE to function correctly.", 
            "title": "Sharing the spool directory"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#disable-worker-node-proxy-renewal", 
            "text": "Worker node proxy renewal is not used by HTCondor-CE and leaving it on will cause some jobs to be held. Edit  /etc/blah.config  on the HTCondor-CE host and set the following values:  blah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no\nblah_disable_limited_proxy=yes   Note  There should be no whitespace around the  = .", 
            "title": "Disable worker node proxy renewal"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-authentication", 
            "text": "In OSG 3.3, there are three methods to manage authentication for incoming jobs: the  LCMAPS VOMS plugin ,  edg-mkgridmap  and  GUMS . edg-mkgridmap is easy to set up and maintain, and GUMS has more features and capabilities. The LCMAPS VOMS plugin is the new OSG-preferred authentication, offering the simplicity of edg-mkgridmap and many of GUMS' rich feature set. If you need to support  pool accounts , GUMS is the only software with that capability.  In OSG 3.4, the LCMAPS VOMS plugin is the only available authentication solution.", 
            "title": "Configuring authentication"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#authentication-with-the-lcmaps-voms-plugin", 
            "text": "To configure your CE to use the LCMAPS VOMS plugin:    If you are using OSG 3.3, add the following line to  /etc/sysconfig/condor-ce :  export LLGT_VOMS_ENABLE_CREDENTIAL_CHECK=1    Follow the instructions in  the LCMAPS VOMS plugin installation and configuration document  to prepare the LCMAPS VOMS plugin     Note  If your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the  condor_mapfile . If this is not the desired behavior, set  GSI_AUTHZ_CONF=/dev/null  in the local HTCondor configuration.", 
            "title": "Authentication with the LCMAPS VOMS plugin"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#authentication-with-edg-mkgridmap", 
            "text": "Warning  edg-mkgridmap is unavailable in OSG 3.4   To configure your CE to use edg-mkgridmap:   Follow the configuration instructions in  the edg-mkgridmap document  to define the VOs that your site accepts  Set some critical gridmap attributes by editing the  /etc/osg/config.d/10-misc.ini  file on the HTCondor-CE host: authorization_method = gridmap", 
            "title": "Authentication with edg-mkgridmap"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#authentication-with-gums", 
            "text": "Warning  GUMS is unavailable in OSG 3.4    Follow the instructions in  the GUMS installation and configuration document  to prepare GUMS  Set some critical GUMS attributes by editing the  /etc/osg/config.d/10-misc.ini  file on the HTCondor-CE host: authorization_method = xacml\ngums_host =  YOUR GUMS HOSTNAME      Note  If your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the  condor_mapfile . If this is not the desired behavior, set  GSI_AUTHZ_CONF=/dev/null  in the local HTCondor configuration.", 
            "title": "Authentication with GUMS"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-ce-collector-advertising", 
            "text": "To split jobs between the various sites of the OSG, information about each site's capabilities are uploaded to a central collector. The job factories then query the central collector for idle resources and submit pilot jobs to the available sites. To advertise your site, you will need to enter some information about the worker nodes of your clusters.  Please see the  Subcluster / Resource Entry configuration document  about configuring the data that will be uploaded to the central collector.", 
            "title": "Configuring CE collector advertising"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#applying-configuration-settings", 
            "text": "Making changes to the OSG configuration files in the  /etc/osg/config.d  directory does not apply those settings to software automatically. Settings that are made outside of the OSG directory take effect immediately or at least when the relevant service is restarted. For the OSG settings, use the  osg-configure  tool to validate (to a limited extent) and apply the settings to the relevant software components. The  osg-configure  software is included automatically in an HTCondor-CE installation.    Make all changes to  .ini  files in the  /etc/osg/config.d  directory   Note  This document describes the critical settings for HTCondor-CE and related software. You may need to configure other software that is installed on your HTCondor-CE host, too.     Validate the configuration settings  root@host #  osg-configure -v    Fix any errors (at least) that  osg-configure  reports.    Once the validation command succeeds without errors, apply the configuration settings:  root@host # osg-configure -c    Generate a  user-vo-map  file with your authentication set up (skip this step if you're using the LCMAPS VOMS plugin):    If you're using edg-mkgridmap, run the following:  root@host #  edg-mkgridmap    If you're using GUMS, run the following:  root@host #  gums-host-cron", 
            "title": "Applying configuration settings"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to  the section on using HTCondor-CE .   Transforming and filtering jobs  Configuring for multiple network interfaces  Limiting or disabling locally running jobs on the CE  Accounting with multiple CEs or local user jobs  HTCondor accounting groups  Installing the HTCondor-CE View", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#transforming-and-filtering-jobs", 
            "text": "If you need to modify or filter jobs, more information can be found in the  Job Router Recipes  document.   Note  If you need to assign jobs to HTCondor accounting groups, refer to  this  section.", 
            "title": "Transforming and filtering jobs"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-for-multiple-network-interfaces", 
            "text": "If you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname and interface to use when communicating to each other. Set  NETWORK_HOSTNAME  and  NETWORK_INTERFACE  to the hostname and IP address of your public interface, respectively, in  /etc/condor-ce/config.d/99-local.conf  directory with the line:  NETWORK_HOSTNAME =  condorce.example.com \nNETWORK_INTERFACE =  127.0.0.1   Replacing  condorce.example.com  text with your public interface\u2019s hostname and  127.0.0.1  with your public interface\u2019s IP address.", 
            "title": "Configuring for multiple network interfaces"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#limiting-or-disabling-locally-running-jobs-on-the-ce", 
            "text": "If you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and scheduler universes. Local and scheduler universes are HTCondor-CE\u2019s analogue to GRAM\u2019s managed fork: they allow jobs to be run on the CE itself, mainly for remote troubleshooting. Pilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another worker node.  The two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison.    To change the default limit  on the number of locally run jobs (the current default is 20), add the following to  /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  TotalLocalJobsRunning + TotalSchedulerJobsRunning    JOB-LIMIT  START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )     To only allow a specific user  to start locally run jobs, add the following to  /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  target.Owner  = ? =   USERNAME  START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )     To disable  locally run jobs, add the following to  /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  False START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )      Note  RSV requires the ability to start local universe jobs so if you are using RSV, you need to allow local universe jobs from the  rsv  user.", 
            "title": "Limiting or disabling locally running jobs on the CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#accounting-with-multiple-ces-or-local-user-jobs", 
            "text": "Note  For non-HTCondor batch systems only   If your site has multiple CEs or you have non-grid users submitting to the same local batch system, the OSG accounting software needs to be configured so that it doesn't over report the number of jobs. Use the following table to determine which file requires editing:     If your batch system is\u2026  Then edit the following file on your CE(s)\u2026      LSF  /etc/gratia/pbs-lsf/ProbeConfig    PBS  /etc/gratia/pbs-lsf/ProbeConfig    SGE  /etc/gratia/sge/ProbeConfig    SLURM  /etc/gratia/slurm/ProbeConfig     Then edit the value of  SuppressNoDNRecords  so that it reads:  SuppressNoDNRecords= 1", 
            "title": "Accounting with multiple CEs or local user jobs"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#htcondor-accounting-groups", 
            "text": "Note  For HTCondor batch systems only   If you want to provide fairshare on a group basis, as opposed to a Unix user basis, you can use HTCondor accounting groups. They are independent of the Unix groups the user may already be in and are  documented in the HTCondor manual . If you are using HTCondor accounting groups, you can map jobs from the CE into HTCondor accounting groups based on their UID, their DN, or their VOMS attributes.    To map UIDs to an accounting group,  add entries to  /etc/osg/uid_table.txt  with the following form:  uid GroupName  The following is an example  uid_table.txt :  uscms02 TestGroup\nosg     other.osgedu    To map DNs or VOMS attributes to an accounting group,  add lines to  /etc/osg/extattr_table.txt  with the following form:  SubjectOrAttribute  GroupName   The  SubjectOrAttribute  can be a Perl regular expression. The following is an example  extattr_table.txt :  cmsprio cms.other.prio\ncms\\/Role=production cms.prod\n\\/DC=com\\/DC=DigiCert-Grid\\/O=Open\\ Science\\ Grid\\/OU=People\\/CN=Brian\\ Lin\\ 1047 osg.test\n.* other     Note  Entries in  /etc/osg/uid_table.txt  are honored over  /etc/osg/extattr_table.txt  if a job would match to lines in both files.", 
            "title": "HTCondor accounting groups"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#install-and-run-the-htcondor-ce-view", 
            "text": "The HTCondor-CE View is an optional web interface to the status of your CE. To run the View,    Begin by installing the package htcondor-ce-view:  root@host #  yum install htcondor-ce-view    Next, uncomment the  DAEMON_LIST  configuration located at  /etc/condor-ce/config.d/05-ce-view.conf :  DAEMON_LIST   =   $( DAEMON_LIST ) , CEVIEW, GANGLIAD    Restart the CE service:  root@host #  service condor-ce restart    Verify the service by entering your CE's hostname into your web browser    The website is served on port 80 by default. To change this default, edit the value of  HTCONDORCE_VIEW_PORT  in  /etc/condor-ce/config.d/05-ce-view.conf .", 
            "title": "Install and run the HTCondor-CE View"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#using-htcondor-ce", 
            "text": "As a site administrator, there are a few ways in which you might use the HTCondor-CE:   Managing the HTCondor-CE and associated services  Using HTCondor-CE administrative tools to monitor and maintain the job gateway  Using HTCondor-CE user tools to test gateway operations", 
            "title": "Using HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#managing-htcondor-ce-and-associated-services", 
            "text": "In addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    Your batch system  condor  or  pbs_server  or \u2026     HTCondor-CE  condor-ce      Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME", 
            "title": "Managing HTCondor-CE and associated services"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#using-htcondor-ce-tools", 
            "text": "Some of the HTCondor-CE administrative and user tools are documented in  the HTCondor-CE troubleshooting guide .", 
            "title": "Using HTCondor-CE tools"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#validating-htcondor-ce", 
            "text": "There are different ways to make sure that your HTCondor-CE host is working well:   Perform automated validation by running  RSV  Manually verify your HTCondor-CE using  HTCondor-CE troubleshooting guide ; useful tools include:  condor_ce_run  condor_ce_trace  condor_submit", 
            "title": "Validating HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#troubleshooting-htcondor-ce", 
            "text": "For information on how to troubleshoot your HTCondor-CE, please refer to  the HTCondor-CE troubleshooting guide .", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#registering-the-ce", 
            "text": "To be part of the OSG Production Grid, your CE must be registered in the  OSG Information Management System  (OIM). To register your resource:   Obtain, install, and verify your user certificate  (which you may have done already)  Register your site and CE in OIM", 
            "title": "Registering the CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuration", 
            "text": "The following directories contain the configuration for HTCondor-CE. The directories are parsed in the order presented and thus configuration within the final directory will override configuration specified in the previous directories.     Location  Comment      /usr/share/condor-ce/config.d/  Configuration defaults (overwritten on package updates)    /etc/condor-ce/config.d/  Files in this directory are parsed in alphanumeric order (i.e.,  99-local.conf  will override values in  01-ce-auth.conf )     For a detailed order of the way configuration files are parsed, run the following command:  user@host $  condor_ce_config_val -config", 
            "title": "Configuration"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#users", 
            "text": "The following users are needed by HTCondor-CE at all sites:     User  Comment      condor  The HTCondor-CE will be run as root, but perform most of its operations as the  condor  user.    gratia  Runs the Gratia probes to collect accounting data", 
            "title": "Users"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#certificates", 
            "text": "File  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem    Host key  root  /etc/grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Certificates"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Htcondor-CE  tcp  9619  X   HTCondor-CE shared port     Allow inbound and outbound network connection to all internal site servers, such as GUMS and the batch system head-node only ephemeral outgoing ports are necessary.", 
            "title": "Networking"
        }, 
        {
            "location": "/compute-element/job-router-recipes/", 
            "text": "Writing Routes For HTCondor-CE\n\n\nThe \nJobRouter\n is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in \n/etc/condor-ce/config.d/02-ce-*.conf\n that provide enough basic functionality for a small site.\n\n\nIf you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems.\n\n\nQuirks and Pitfalls\n\n\n\n\nThe JobRouter matches jobs to routes in a round-robin fashion (for HTCondor versions \n 8.7.1). This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\nIf a value is set in \nJOB_ROUTER_DEFAULTS\n with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n. Do this at your own risk as it may cause the CE to break.\n\n\nMake sure to run \ncondor_ce_reconfig\n after changing your routes, otherwise they will not take effect.\n\n\nBefore the last square bracket, make sure all lines end in a line continuation character (backslash). You can inspect the syntax of your routes with \ncondor_ce_config_val JOB_ROUTER_ENTRIES\n to see if HTCondor-CE has ingested them properly.\n\n\nDo \nnot\n set the job environment through the JobRouter. Instead, add any changes to the \n[Local Settings]\n section in \n/etc/osg/config.d/40-localsettings.ini\n and run osg-configure, as documented \nhere\n.\n\n\nHTCondor batch system only: Local universe jobs are excluded from any routing.\n\n\n\n\nHow Job Routes are Constructed\n\n\nEach job route\u2019s \nClassAd\n is constructed by combining each entry from the \nJOB_ROUTER_ENTRIES\n with the \nJOB_ROUTER_DEFAULTS\n. Attributes that are \nset_\n in \nJOB_ROUTER_ENTRIES\n will override those \nset_\n in \nJOB_ROUTER_DEFAULTS\n\n\nJOB_ROUTER_ENTRIES\n\n\nJOB_ROUTER_ENTRIES\n is a configuration variable whose default is set in \n/etc/condor-ce/config.d/02-ce-*.conf\n but may be overriden by the administrator in \n/etc/condor-ce/config.d/99-local.conf\n. This document outlines the many changes you can make to \nJOB_ROUTER_ENTRIES\n to fit your site\u2019s needs.\n\n\nJOB_ROUTER_DEFAULTS\n\n\nJOB_ROUTER_DEFAULTS\n is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents, run the following command:\n\n\nuser@host $\n condor_ce_config_val JOB_ROUTER_DEFAULTS \n|\n sed \ns/;/;\\n/g\n\n\n\n\n\n\n\n\nWarning\n\n\nIf a value is set in \nJOB_ROUTER_DEFAULTS\n with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n. Do this at your own risk as it may cause the CE to break.\n\n\n\n\n\n\nWarning\n\n\nDo \nnot\n set the \nJOB_ROUTER_DEFAULTS\n configuration variable yourself. This will cause the CE to stop functioning.\n\n\n\n\nGeneric Routes\n\n\nThis section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in \n/etc/condor-ce/config.d/99-local.conf\n, not the original \n02-ce-*.conf\n.\n\n\nRequired fields\n\n\nThe minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in \n/usr/share/condor-ce/config.d/02-ce-\nbatch system\n-defaults.conf\n, provided by the \nosg-ce-\nbatch system\n packages.\n\n\nBatch system\n\n\nEach route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the \nTargetUniverse\n attribute needs to be set to \n5\n or \n\"vanilla\"\n. For all other batch systems, the \nTargetUniverse\n attribute needs to be set to \n9\n or \n\"grid\"\n and the \nGridResource\n attribute needs to be set to \n\"batch \nbatch system\n\"\n (where \nbatch system\n can be one of \npbs\n (for both users of \npbs\n and \nSLURM\n), \nlsf\n, or \nsge\n).\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     \nTargetUniverse = 5; \\\n\n     name = \nRoute jobs to HTCondor\n; \\\n] \\\n[ \\\n     \nGridResource = \nbatch pbs\n; \\\n\n     \nTargetUniverse = 9; \\\n\n     name = \nRoute jobs to PBS\n; \\\n]\n\n\n\n\n\nRoute name\n\n\nTo identify routes, you will need to assign a name to the route with the \nname\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     \nname = \nRoute jobs to HTCondor\n; \\\n\n]\n\n\n\n\n\nThe name of the route will be useful in debugging since it shows up in the output of \ncondor_ce_job_router_info\n, the \nJobRouterLog\n, and in the ClassAd of the routed job, which can be viewed with \ncondor_ce_q\n or \ncondor_ce_history\n.\n\n\nWriting multiple routes\n\n\nIf your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues), you will need to write multiple job routes. Each route is enclosed by square brackets and unless they're the last closing bracket, they need to be followed by the line continuation character. The following routes takes incoming jobs that have a \nqueue\n attribute set to \n\"analy\"\n and routes them to the site's HTCondor batch system. Any other jobs will be sent to that site's PBS batch system.\n\n\n\n\nNote\n\n\nFor versions of HTCondor \n 8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\n\n\nJOB_ROUTER_ENTRIES = \n[ \\\n\n     TargetUniverse = 5; \\\n     name = \nRoute jobs to HTCondor\n; \\\n     Requirements = (TARGET.queue =?= \nanaly\n); \\\n\n] \\\n[ \\\n\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRoute jobs to PBS\n; \\\n     Requirements = (TARGET.queue =!= \nanaly\n); \\\n\n]\n\n\n\n\n\n\nWriting comments\n\n\nTo write comments you can use C-style comments, text enclosed by \n/* */\n. If the comment is at the end of a line, it still has to be followed by the line continuation character.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nC-style comments\n; \\\n     \n/* This is a comment */ \\\n\n]\n\n\n\n\n\nYou can also use \n#\n to comment out single lines:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nHash comments\n; \\\n     \n# BrokenAttribute = \ncommented out\n; \\\n\n]\n\n\n\n\n\nSetting attributes for all routes\n\n\nTo set an attribute that will be applied to all routes, you will need to ensure that \nMERGE_JOB_ROUTER_DEFAULT_ADS\n is set to \nTrue\n (check the value with \ncondor_ce_config_val\n) and use the \nset_\n function in the \nJOB_ROUTER_DEFAULTS\n. The following configuration sets the \nPeriodic_Hold\n attribute for all routes:\n\n\n# Use the defaults generated by the condor_ce_router_defaults script.  To add\n\n\n# additional defaults, add additional lines of the form:\n\n\n#\n\n\n#   JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;]\n\n\n#\n\n\nMERGE_JOB_ROUTER_DEFAULT_ADS\n=\nTrue\n\nJOB_ROUTER_DEFAULTS\n \n=\n \n$(\nJOB_ROUTER_DEFAULTS\n)\n \n[\nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n]\n\n\n\n\n\n\nFiltering jobs based on\u2026\n\n\nTo filter jobs, use the \nRequirements\n attribute. Jobs will evaluate against the ClassAd expression set in the \nRequirements\n and if the expression evaluates to \nTRUE\n, the route will match. More information on the syntax of ClassAd's can be found in the \nHTCondor manual\n. For an example on how incoming jobs interact with filtering in job routes, consult \nthis document\n.\n\n\nWhen setting requirements, you need to prefix job attributes that you are filtering with \nTARGET.\n so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a \nqueue = \"analy\"\n attribute, then the following job route will not match:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by queue\n; \\\n     queue = \nnot-analy\n; \\\n     \nRequirements = (queue =?= \nanaly\n); \\\n\n]\n\n\n\n\n\nThis is because when evaluating the route requirement, the job route will compare its own \nqueue\n attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the \nHTCondor manual\n.\n\n\n\n\nNote\n\n\nIf you have an HTCondor batch system, note the difference with \nset_requirements\n.\n\n\n\n\n\n\nNote\nFor versions of HTCondor \n 8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\n\n\n\n\nGlidein queue\n\n\nTo filter jobs based on their glidein queue attribute, your routes will need a \nRequirements\n expression using the incoming job's \nqueue\n attribute. The following entry routes jobs to the PBS queue if the incoming job (specified by \nTARGET\n) is an \nanaly\n (Analysis) glidein:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by queue\n; \\\n     \nRequirements = (TARGET.queue =?= \nanaly\n); \\\n\n]\n\n\n\n\n\nJob submitter\n\n\nTo filter jobs based on who submitted it, your routes will need a \nRequirements\n expression using the incoming job's \nOwner\n attribute. The following entry routes jobs to the HTCondor batch system iff the submitter is \nusatlas2\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by job submitter\n; \\\n     \nRequirements = (TARGET.Owner =?= \nusatlas2\n); \\\n\n]\n\n\n\n\n\nAlternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system iff the submitter's name begins with \nusatlas\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nFiltering by job submitter (regular expression)\n; \\\n     \nRequirements = regexp(\n^usatlas\n, TARGET.Owner); \\\n\n]\n\n\n\n\n\nVOMS attribute\n\n\nTo filter jobs based on the subject of the job's proxy, your routes will need a \nRequirements\n expression using the incoming job's \nx509UserProxyFirstFQAN\n attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains \n/cms/Role=Pilot\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nFiltering by VOMS attribute (regex)\n; \\\n     \nRequirements = regexp(\n\\/cms\\/Role\\=pilot\n, TARGET.x509UserProxyFirstFQAN); \\\n\n]\n\n\n\n\n\nSetting a default\u2026\n\n\nThis section outlines how to set default job limits, memory, cores, queue, and maximum walltime. For an example on how users can override these defaults, consult \nthis document\n.\n\n\nMaximum number of jobs\n\n\nTo set a default limit to the maximum number of jobs per route, you can edit the configuration variable \nCONDORCE_MAX_JOBS\n in \n/etc/condor-ce/config.d/01-ce-router.conf\n:\n\n\nCONDORCE_MAX_JOBS = 10000\n\n\n\n\n\n\n\nNote\n\n\nThe above configuration  is to be placed directly into the HTCondor-CE configuration, not into a job route.\n\n\n\n\nMaximum memory\n\n\nTo set a default maximum memory for routed jobs, set the attribute \ndefault_maxMemory\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRequest memory\n; \\\n     /* Set the requested memory to 1 GB */ \\\n     \nset_default_maxMemory = 1000; \\\n\n]\n\n\n\n\n\nNumber of cores to request\n\n\nTo set a default number of cores for routed jobs, set the attribute \ndefault_xcount\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRequest CPU\n; \\\n     /* Set the requested cores to 8 */ \\\n     \nset_default_xcount = 8; \\\n\n]\n\n\n\n\n\nMaximum walltime\n\n\nTo set a default maximum walltime (in minutes) for routed jobs, set the attribute \ndefault_maxWallTime\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting WallTime\n; \\\n     /* Set the max walltime to 1 hr */ \\\n     \nset_default_maxWallTime = 60; \\\n\n]\n\n\n\n\n\nEditing attributes\u2026\n\n\nThe following functions are operations that affect job attributes and are evaluated in the following order:\n\n\n\n\ncopy_*\n\n\ndelete_*\n\n\nset_*\n\n\neval_set_*\n\n\n\n\nAfter each job route\u2019s ClassAd is \nconstructed\n, the above operations are evaluated in order. For example, if the attribute \nfoo\n is set using \neval_set_foo\n in the \nJOB_ROUTER_DEFAULTS\n, you'll be unable to use \ndelete_foo\n to remote it from your jobs since the attribute is set using \neval_set_foo\n after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in \nJOB_ROUTER_DEFAULTS\n get overriden by the same operation in \nJOB_ROUTER_ENTRIES\n. So to 'delete' \nfoo\n, we would add \neval_set_foo = \"\"\n to the route in the \nJOB_ROUTER_ENTRIES\n, resulting in \nfoo\n being absent from the routed job.\n\n\nMore documentation can be found in the \nHTCondor manual\n.\n\n\nCopying attributes\n\n\nTo copy the value of an attribute of the incoming job to an attribute of the routed job, use \ncopy_\n. The following route copies the \nenvironment\n attribute of the incoming job and sets the attribute \nOriginal_Environment\n on the routed job to the same value:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nCopying attributes\n; \\\n     \ncopy_environment = \nOriginal_Environment\n; \\\n\n]\n\n\n\n\n\nRemoving attributes\n\n\nTo remove an attribute of the incoming job from the routed job, use \ndelete_\n. The following route removes the \nenvironment\n attribute from the routed job:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nCopying attributes\n; \\\n     \ndelete_environment = True; \\\n\n]\n\n\n\n\n\nSetting attributes\n\n\nTo set an attribute on the routed job, use \nset_\n. The following route sets the Job's \nRank\n attribute to 5:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting an attribute\n; \\\n     \nset_Rank = 5; \\\n\n]\n\n\n\n\n\nSetting attributes with ClassAd expressions\n\n\nTo set an attribute to a ClassAd expression to be evaluated, use \nset_eval\n. The following route sets the \nExperiment\n attribute to \natlas.osguser\n if the Owner of the incoming job is \nosguser\n:\n\n\n\n\nNote\n\n\nIf a value is set in JOB_ROUTER_DEFAULTS with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n.\n\n\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting an attribute with a !ClassAd expression\n; \\\n     \neval_set_Experiment = strcat(\natlas.\n, Owner); \\\n\n]\n\n\n\n\n\nLimiting the number of...\n\n\nThis section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route).\n\n\n\n\nNote\n\n\nIf you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via \nuser priorities and group accounting\n.\n\n\n\n\nTotal jobs\n\n\nTo set a limit on the number of jobs for a specific route, set the \nMaxJobs\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nLimit the total number of jobs to 100\n; \\\n     \nMaxJobs = 100; \\\n\n] \\\n[ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nLimit the total number of jobs to 75\n; \\\n     \nMaxJobs = 75; \\\n\n]\n\n\n\n\n\nIdle jobs\n\n\nTo set a limit on the number of idle jobs for a specific route, set the \nMaxIdleJobs\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nLimit the total number of idle jobs to 100\n; \\\n     \nMaxIdleJobs = 100; \\\n\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name = \nLimit the total number of idle jobs to 75\n; \\\n     \nMaxIdleJobs = 75; \\\n\n]\n\n\n\n\n\nDebugging routes\n\n\nTo help debug expressions in your routes, you can use the \ndebug()\n function. First, set the debug mode for the JobRouter by editing a file in \n/etc/condor-ce/config.d/\n to read\n\n\nJOB_ROUTER_DEBUG = D_FULLDEBUG\n\n\n\n\n\nThen wrap the problematic attribute in \ndebug()\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nDebugging a difficult !ClassAd expression\n; \\\n     \neval_set_Experiment = debug(strcat(\natlas\n, Name)); \\\n\n]\n\n\n\n\n\nYou will find the debugging output in \n/var/log/condor-ce/JobRouterLog\n.\n\n\nRoutes for HTCondor Batch Systems\n\n\nThis section contains information about job routes that can be used if you are running an HTCondor batch system at your site.\n\n\nSetting periodic hold, release or remove\n\n\nTo release, remove or put a job on hold if it meets certain criteria, use the \nPERIODIC_*\n family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting \nPERIODIC_EXPR_INTERVAL\n in your CE's configuration.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nSetting periodic statements\n; \\\n     \n/* Puts the routed job on hold if the job\ns been idle and has been started at least once or if the job has tried to start more than once */ \\\n     set_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n     /* Remove routed jobs if their walltime is longer than 3 days and 5 minutes */ \\\n     set_Periodic_Remove = ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ); \\\n     /* Release routed jobs if the condor_starter couldn\nt start the executable and \nVMGAHP_ERR_INTERNAL\n is in the HoldReason */ \\\n     set_Periodic_Release = HoldReasonCode == 6 \n regexp(\nVMGAHP_ERR_INTERNAL\n, HoldReason); \\\n\n]\n\n\n\n\n\nSetting routed job requirements\n\n\nIf you need to set requirements on your routed job, you will need to use \nset_Requirements\n instead of \nRequirements\n. The \nRequirements\n attribute filters jobs coming into your CE into different job routes whereas \nset_requirements\n will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the \nHTCondor manual\n.\n\n\nTo ensure that your job lands on a Linux machine in your pool:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     \nset_Requirements =  OpSys == \nLINUX\n; \\\n\n]\n\n\n\n\n\nSetting accounting groups\n\n\nTo assign jobs to an HTCondor accounting group to manage fair share on your local batch system, we recommend using \nUID and ExtAttr tables\n.\n\n\nRoutes for non-HTCondor Batch Systems\n\n\nThis section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site.\n\n\nSetting a default batch queue\n\n\nTo set a default queue for routed jobs, set the attribute \ndefault_queue\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting batch system queues\n; \\\n     \nset_default_queue = \nosg_queue\n; \\\n\n]\n\n\n\n\n\nSetting batch system directives\n\n\nTo write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in \n/etc/blahp/\n (e.g., if your local batch system is PBS, edit \n/etc/blahp/pbs_local_submit_attributes.sh\n). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via the \ndefault_remote_cerequirements\n attribute, which can take the following form:\n\n\ndefault_remote_cerequirements = \nfoo == X \n bar == \\\nY\\\n \n ...\n\n\n\n\n\n\nThis sets \nfoo\n to value \nX\n and \nbar\n to the string \nY\n (escaped double-quotes are required for string values) in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the \nx509UserProxyFirstFQAN\n attribute of the job submitted to a PBS batch system\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting job submit variables\n; \\\n     \nset_default_remote_cerequirements = strcat(\nWalltime == 3600 \n AccountingGroup == \\\n, x509UserProxyFirstFQAN, \n\\\n); \\\n\n]\n\n\n\n\n\nWith \n/etc/blahp/pbs_local_submit_attributes.sh\n containing.\n\n\n1\n2\n3\n#!/bin/bash\n\n\necho\n \n#PBS -l walltime=\n$Walltime\n\n\necho\n \n#PBS -A \n$AccountingGroup\n\n\n\n\n\n\n\nThis results in the following being appended to the script that gets submitted to your batch system:\n\n\n#PBS -l walltime=3600\n#PBS -A \nCE job\ns x509UserProxyFirstFQAN attribute\n\n\n\n\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\nExample Configurations\n\n\nAGLT2's job routes\n\n\nAtlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes.\n\n\n\n\nSetting various HTCondor-specific attributes like \nRank\n, \nAccountingGroup\n, \nJobPrio\n and \nPeriodic_Remove\n (see the \nHTCondor manual\n for more). Some of these are site-specific like \nLastandFrac\n, \nIdleMP8Pressure\n, \nlocalQue\n, \nIsAnalyJob\n and \nJobMemoryLimit\n.\n\n\nThere is a difference between \nRequirements\n and \nset_requirements\n. The \nRequirements\n attribute matches jobs to specific routes while the \nset_requirements\n sets the \nRequirements\n attribute on the \nrouted\n job, which confines which machines that the routed job can land on.\n\n\n\n\nSource: \nhttps://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content\n\n\nJOB_ROUTER_ENTRIES\n \n=\n \n\\\n\n/* Still to \ndo\n on all routes, get job requirements and add them here */ \n\\\n\n/* ***** Route no \n1\n ***** */ \n\\\n\n/* ***** Analysis queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue\n==\nanaly\n;\n \n\\\n\n    \nName\n \n=\n \nAnalysis Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \neval_set_IdleMP8Pressure\n \n=\n \n$(\nIdleMP8Pressure\n)\n;\n \n\\\n\n    \neval_set_LastAndFrac\n \n=\n \n$(\nLastAndFrac\n)\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n \n \n(\nIfThenElse\n((\nOwner\n \n==\n \natlasconnect\n \n||\n \nOwner\n \n==\n \nmuoncal\n)\n,IfThenElse\n(\nIdleMP8Pressure,\n(\nTARGET.PARTITIONED \n=\n!\n=\n TRUE\n)\n,True\n)\n,IfThenElse\n(\nLastAndFrac,\n(\nTARGET.PARTITIONED \n=\n!\n=\n TRUE\n)\n,True\n)))\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.analy.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nAnalysis\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n True\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n5\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n2\n ***** */ \n\\\n\n/* ***** splitterNT queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue \n==\n \nsplitterNT\n;\n \n\\\n\n    \nName\n \n=\n \nSplitter ntuple queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n \ngroup_calibrate.muoncal\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nSplitter\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n10\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n3\n ***** */ \n\\\n\n/* ***** splitter queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue \n==\n \nsplitter\n;\n \n\\\n\n    \nName\n \n=\n \nSplitter queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n \ngroup_calibrate.muoncal\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nSplitter\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n15\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n4\n ***** */ \n\\\n\n/* ***** xrootd queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue \n==\n \nxrootd\n;\n \n\\\n\n    \nName\n \n=\n \nXrootd queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.analy.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nAnalysis\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n True\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n35\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n5\n ***** */ \n\\\n\n/* ***** Tier3Test queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue \n==\n \nTier3Test\n;\n \n\\\n\n    \nName\n \n=\n \nTier3 Test Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n \n \n(\n \nIS_TIER3_TEST_QUEUE\n \n=\n?\n=\n True \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.analy.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nTier3Test\n;\n \n\\\n\n    \nset_IsTier3TestJob\n \n=\n True\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n True\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n20\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n6\n ***** */ \n\\\n\n/* ***** mp8 queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue\n==\nmp8\n;\n \n\\\n\n    \nName\n \n=\n \nMCORE Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n \n \n((\n TARGET.Cpus \n==\n \n8\n \n TARGET.CPU_TYPE \n=\n?\n=\n \nmp8\n \n)\n \n||\n TARGET.PARTITIONED \n=\n?\n=\n True \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.mcore.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nMP8\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n25\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n0\n.0\n;\n \n\\\n\n    \neval_set_RequestCpus\n \n=\n \n8\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n33552000\n;\n \n\\\n\n    \nset_Slot_Type\n \n=\n \nmp8\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n7\n ***** */ \n\\\n\n/* ***** Installation queue, triggered by usatlas2 user ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue is undefined \n target.Owner \n==\n \nusatlas2\n;\n \n\\\n\n    \nName\n \n=\n \nInstall Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n \n \n(\n TARGET.IS_INSTALL_QUE \n=\n?\n=\n True \n)\n \n \n(\nTARGET.AGLT2_SITE \n==\n \nUM\n \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.other.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nDefault\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_IsInstallJob\n \n=\n True\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n15\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n8\n ***** */ \n\\\n\n/* ***** Default queue \nfor\n usatlas1 user ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue is undefined \n regexp\n(\nusatlas1\n,target.Owner\n)\n;\n \n\\\n\n    \nName\n \n=\n \nATLAS Production Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.prod.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nDefault\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n9\n ***** */ \n\\\n\n/* ***** Default queue \nfor\n any other usatlas account ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue is undefined \n \n(\nregexp\n(\nusatlas2\n,target.Owner\n)\n \n||\n regexp\n(\nusatlas3\n,target.Owner\n))\n;\n \n\\\n\n    \nName\n \n=\n \nOther ATLAS Production\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.other.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nDefault\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n10\n ***** */ \n\\\n\n/* ***** Anything \nelse\n. Set queue as Default and assign to other VOs  ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue is undefined \n ifThenElse\n(\nregexp\n(\nusatlas\n,target.Owner\n)\n,false,true\n)\n;\n \n\\\n\n    \nName\n \n=\n \nOther Jobs\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_VOgener.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nDefault\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n\n\n\n\n\n\nBNL's job routes\n\n\nAtlas BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes:\n\n\n\n\nSetting various HTCondor-specific attributes like \nJobLeaseDuration\n, \nRequirements\n and \nPeriodic_Hold\n (see the \nHTCondor manual\n for more). Some of these are site-specific like \nRACF_Group\n, \nExperiment\n, \nJob_Type\n and \nVO\n.\n\n\nJobs are split into different routes based on the \nGlideIn\n queue that they're in.\n\n\nThere is a difference between \nRequirements\n and \nset_requirements\n. The \nRequirements\n attribute matches \nincoming\n jobs to specific routes while the \nset_requirements\n sets the \nRequirements\n attribute on the \nrouted\n job, which confines which machines that the routed job can land on.\n\n\n\n\nSource: \nhttp://www.usatlas.bnl.gov/twiki/bin/view/Admins/HTCondorCE.html\n\n\n###############################################################################\n\n\n#\n\n\n# HTCondor-CE HTCondor batch system configuration file.\n\n\n#\n\n\n###############################################################################\n\n\n\n# Submit the job to the site Condor\n\n\n\nJOB_ROUTER_ENTRIES\n \n=\n \n\\\n\n   \n[\n \n\\\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nBNL_Condor_Pool_long\n;\n \n\\\n\n     \nRequirements\n \n=\n target.queue\n==\nanalysis.long\n;\n \n\\\n\n     \neval_set_RACF_Group\n \n=\n \nlong\n;\n \n\\\n\n     \nset_Experiment\n \n=\n \natlas\n;\n \n\\\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n \n\\\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n \n\\\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n \n\\\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n \n\\\n\n   \n]\n \n\\\n\n   \n[\n \n\\\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nBNL_Condor_Pool_short\n;\n \n\\\n\n     \nRequirements\n \n=\n target.queue\n==\nanalysis.short\n;\n \n\\\n\n     \neval_set_RACF_Group\n \n=\n \nshort\n;\n \n\\\n\n     \nset_Experiment\n \n=\n \natlas\n;\n \n\\\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n \n\\\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n \n\\\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n \n\\\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n \n\\\n\n   \n]\n \n\\\n\n   \n[\n \n\\\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nBNL_Condor_Pool_grid\n;\n \n\\\n\n     \nRequirements\n \n=\n target.queue\n==\ngrid\n;\n \n\\\n\n     \neval_set_RACF_Group\n \n=\n \ngrid\n;\n \n\\\n\n     \nset_Experiment\n \n=\n \natlas\n;\n \n\\\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n \n\\\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n \n\\\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n \n\\\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n \n\\\n\n   \n]\n \n\\\n\n   \n[\n \n\\\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nBNL_Condor_Pool\n;\n \n\\\n\n     \nRequirements\n \n=\n target.queue is undefined\n;\n \n\\\n\n     \neval_set_RACF_Group\n \n=\n \ngrid\n;\n \n\\\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \nrcf\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n     \nset_Experiment\n \n=\n \natlas\n;\n \n\\\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n \n\\\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n \n\\\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n \n\\\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n \n\\\n\n   \n]", 
            "title": "Job Router Recipes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-routes-for-htcondor-ce", 
            "text": "The  JobRouter  is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in  /etc/condor-ce/config.d/02-ce-*.conf  that provide enough basic functionality for a small site.  If you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems.", 
            "title": "Writing Routes For HTCondor-CE"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#quirks-and-pitfalls", 
            "text": "The JobRouter matches jobs to routes in a round-robin fashion (for HTCondor versions   8.7.1). This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.  If a value is set in  JOB_ROUTER_DEFAULTS  with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break.  Make sure to run  condor_ce_reconfig  after changing your routes, otherwise they will not take effect.  Before the last square bracket, make sure all lines end in a line continuation character (backslash). You can inspect the syntax of your routes with  condor_ce_config_val JOB_ROUTER_ENTRIES  to see if HTCondor-CE has ingested them properly.  Do  not  set the job environment through the JobRouter. Instead, add any changes to the  [Local Settings]  section in  /etc/osg/config.d/40-localsettings.ini  and run osg-configure, as documented  here .  HTCondor batch system only: Local universe jobs are excluded from any routing.", 
            "title": "Quirks and Pitfalls"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#how-job-routes-are-constructed", 
            "text": "Each job route\u2019s  ClassAd  is constructed by combining each entry from the  JOB_ROUTER_ENTRIES  with the  JOB_ROUTER_DEFAULTS . Attributes that are  set_  in  JOB_ROUTER_ENTRIES  will override those  set_  in  JOB_ROUTER_DEFAULTS", 
            "title": "How Job Routes are Constructed"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job_router_entries", 
            "text": "JOB_ROUTER_ENTRIES  is a configuration variable whose default is set in  /etc/condor-ce/config.d/02-ce-*.conf  but may be overriden by the administrator in  /etc/condor-ce/config.d/99-local.conf . This document outlines the many changes you can make to  JOB_ROUTER_ENTRIES  to fit your site\u2019s needs.", 
            "title": "JOB_ROUTER_ENTRIES"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job_router_defaults", 
            "text": "JOB_ROUTER_DEFAULTS  is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents, run the following command:  user@host $  condor_ce_config_val JOB_ROUTER_DEFAULTS  |  sed  s/;/;\\n/g    Warning  If a value is set in  JOB_ROUTER_DEFAULTS  with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break.    Warning  Do  not  set the  JOB_ROUTER_DEFAULTS  configuration variable yourself. This will cause the CE to stop functioning.", 
            "title": "JOB_ROUTER_DEFAULTS"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#generic-routes", 
            "text": "This section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in  /etc/condor-ce/config.d/99-local.conf , not the original  02-ce-*.conf .", 
            "title": "Generic Routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#required-fields", 
            "text": "The minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in  /usr/share/condor-ce/config.d/02-ce- batch system -defaults.conf , provided by the  osg-ce- batch system  packages.", 
            "title": "Required fields"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#batch-system", 
            "text": "Each route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the  TargetUniverse  attribute needs to be set to  5  or  \"vanilla\" . For all other batch systems, the  TargetUniverse  attribute needs to be set to  9  or  \"grid\"  and the  GridResource  attribute needs to be set to  \"batch  batch system \"  (where  batch system  can be one of  pbs  (for both users of  pbs  and  SLURM ),  lsf , or  sge ).  JOB_ROUTER_ENTRIES = [ \\\n      TargetUniverse = 5; \\ \n     name =  Route jobs to HTCondor ; \\\n] \\\n[ \\\n      GridResource =  batch pbs ; \\ \n      TargetUniverse = 9; \\ \n     name =  Route jobs to PBS ; \\\n]", 
            "title": "Batch system"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#route-name", 
            "text": "To identify routes, you will need to assign a name to the route with the  name  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n      name =  Route jobs to HTCondor ; \\ \n]  The name of the route will be useful in debugging since it shows up in the output of  condor_ce_job_router_info , the  JobRouterLog , and in the ClassAd of the routed job, which can be viewed with  condor_ce_q  or  condor_ce_history .", 
            "title": "Route name"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-multiple-routes", 
            "text": "If your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues), you will need to write multiple job routes. Each route is enclosed by square brackets and unless they're the last closing bracket, they need to be followed by the line continuation character. The following routes takes incoming jobs that have a  queue  attribute set to  \"analy\"  and routes them to the site's HTCondor batch system. Any other jobs will be sent to that site's PBS batch system.   Note  For versions of HTCondor   8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.   JOB_ROUTER_ENTRIES =  [ \\ \n     TargetUniverse = 5; \\\n     name =  Route jobs to HTCondor ; \\\n     Requirements = (TARGET.queue =?=  analy ); \\ ] \\\n[ \\ \n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Route jobs to PBS ; \\\n     Requirements = (TARGET.queue =!=  analy ); \\ ]", 
            "title": "Writing multiple routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-comments", 
            "text": "To write comments you can use C-style comments, text enclosed by  /* */ . If the comment is at the end of a line, it still has to be followed by the line continuation character.  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  C-style comments ; \\\n      /* This is a comment */ \\ \n]  You can also use  #  to comment out single lines:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Hash comments ; \\\n      # BrokenAttribute =  commented out ; \\ \n]", 
            "title": "Writing comments"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes-for-all-routes", 
            "text": "To set an attribute that will be applied to all routes, you will need to ensure that  MERGE_JOB_ROUTER_DEFAULT_ADS  is set to  True  (check the value with  condor_ce_config_val ) and use the  set_  function in the  JOB_ROUTER_DEFAULTS . The following configuration sets the  Periodic_Hold  attribute for all routes:  # Use the defaults generated by the condor_ce_router_defaults script.  To add  # additional defaults, add additional lines of the form:  #  #   JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;]  #  MERGE_JOB_ROUTER_DEFAULT_ADS = True JOB_ROUTER_DEFAULTS   =   $( JOB_ROUTER_DEFAULTS )   [ set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ; ]", 
            "title": "Setting attributes for all routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#filtering-jobs-based-on", 
            "text": "To filter jobs, use the  Requirements  attribute. Jobs will evaluate against the ClassAd expression set in the  Requirements  and if the expression evaluates to  TRUE , the route will match. More information on the syntax of ClassAd's can be found in the  HTCondor manual . For an example on how incoming jobs interact with filtering in job routes, consult  this document .  When setting requirements, you need to prefix job attributes that you are filtering with  TARGET.  so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a  queue = \"analy\"  attribute, then the following job route will not match:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by queue ; \\\n     queue =  not-analy ; \\\n      Requirements = (queue =?=  analy ); \\ \n]  This is because when evaluating the route requirement, the job route will compare its own  queue  attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the  HTCondor manual .   Note  If you have an HTCondor batch system, note the difference with  set_requirements .    Note For versions of HTCondor   8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.", 
            "title": "Filtering jobs based on\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#glidein-queue", 
            "text": "To filter jobs based on their glidein queue attribute, your routes will need a  Requirements  expression using the incoming job's  queue  attribute. The following entry routes jobs to the PBS queue if the incoming job (specified by  TARGET ) is an  analy  (Analysis) glidein:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by queue ; \\\n      Requirements = (TARGET.queue =?=  analy ); \\ \n]", 
            "title": "Glidein queue"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job-submitter", 
            "text": "To filter jobs based on who submitted it, your routes will need a  Requirements  expression using the incoming job's  Owner  attribute. The following entry routes jobs to the HTCondor batch system iff the submitter is  usatlas2 :  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by job submitter ; \\\n      Requirements = (TARGET.Owner =?=  usatlas2 ); \\ \n]  Alternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system iff the submitter's name begins with  usatlas :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Filtering by job submitter (regular expression) ; \\\n      Requirements = regexp( ^usatlas , TARGET.Owner); \\ \n]", 
            "title": "Job submitter"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#voms-attribute", 
            "text": "To filter jobs based on the subject of the job's proxy, your routes will need a  Requirements  expression using the incoming job's  x509UserProxyFirstFQAN  attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains  /cms/Role=Pilot :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Filtering by VOMS attribute (regex) ; \\\n      Requirements = regexp( \\/cms\\/Role\\=pilot , TARGET.x509UserProxyFirstFQAN); \\ \n]", 
            "title": "VOMS attribute"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-a-default", 
            "text": "This section outlines how to set default job limits, memory, cores, queue, and maximum walltime. For an example on how users can override these defaults, consult  this document .", 
            "title": "Setting a default\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-number-of-jobs", 
            "text": "To set a default limit to the maximum number of jobs per route, you can edit the configuration variable  CONDORCE_MAX_JOBS  in  /etc/condor-ce/config.d/01-ce-router.conf :  CONDORCE_MAX_JOBS = 10000   Note  The above configuration  is to be placed directly into the HTCondor-CE configuration, not into a job route.", 
            "title": "Maximum number of jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-memory", 
            "text": "To set a default maximum memory for routed jobs, set the attribute  default_maxMemory :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Request memory ; \\\n     /* Set the requested memory to 1 GB */ \\\n      set_default_maxMemory = 1000; \\ \n]", 
            "title": "Maximum memory"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#number-of-cores-to-request", 
            "text": "To set a default number of cores for routed jobs, set the attribute  default_xcount :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Request CPU ; \\\n     /* Set the requested cores to 8 */ \\\n      set_default_xcount = 8; \\ \n]", 
            "title": "Number of cores to request"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-walltime", 
            "text": "To set a default maximum walltime (in minutes) for routed jobs, set the attribute  default_maxWallTime :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting WallTime ; \\\n     /* Set the max walltime to 1 hr */ \\\n      set_default_maxWallTime = 60; \\ \n]", 
            "title": "Maximum walltime"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#editing-attributes", 
            "text": "The following functions are operations that affect job attributes and are evaluated in the following order:   copy_*  delete_*  set_*  eval_set_*   After each job route\u2019s ClassAd is  constructed , the above operations are evaluated in order. For example, if the attribute  foo  is set using  eval_set_foo  in the  JOB_ROUTER_DEFAULTS , you'll be unable to use  delete_foo  to remote it from your jobs since the attribute is set using  eval_set_foo  after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in  JOB_ROUTER_DEFAULTS  get overriden by the same operation in  JOB_ROUTER_ENTRIES . So to 'delete'  foo , we would add  eval_set_foo = \"\"  to the route in the  JOB_ROUTER_ENTRIES , resulting in  foo  being absent from the routed job.  More documentation can be found in the  HTCondor manual .", 
            "title": "Editing attributes\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#copying-attributes", 
            "text": "To copy the value of an attribute of the incoming job to an attribute of the routed job, use  copy_ . The following route copies the  environment  attribute of the incoming job and sets the attribute  Original_Environment  on the routed job to the same value:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Copying attributes ; \\\n      copy_environment =  Original_Environment ; \\ \n]", 
            "title": "Copying attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#removing-attributes", 
            "text": "To remove an attribute of the incoming job from the routed job, use  delete_ . The following route removes the  environment  attribute from the routed job:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Copying attributes ; \\\n      delete_environment = True; \\ \n]", 
            "title": "Removing attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes", 
            "text": "To set an attribute on the routed job, use  set_ . The following route sets the Job's  Rank  attribute to 5:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting an attribute ; \\\n      set_Rank = 5; \\ \n]", 
            "title": "Setting attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes-with-classad-expressions", 
            "text": "To set an attribute to a ClassAd expression to be evaluated, use  set_eval . The following route sets the  Experiment  attribute to  atlas.osguser  if the Owner of the incoming job is  osguser :   Note  If a value is set in JOB_ROUTER_DEFAULTS with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES .   JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting an attribute with a !ClassAd expression ; \\\n      eval_set_Experiment = strcat( atlas. , Owner); \\ \n]", 
            "title": "Setting attributes with ClassAd expressions"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#limiting-the-number-of", 
            "text": "This section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route).   Note  If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via  user priorities and group accounting .", 
            "title": "Limiting the number of..."
        }, 
        {
            "location": "/compute-element/job-router-recipes/#total-jobs", 
            "text": "To set a limit on the number of jobs for a specific route, set the  MaxJobs  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Limit the total number of jobs to 100 ; \\\n      MaxJobs = 100; \\ \n] \\\n[ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Limit the total number of jobs to 75 ; \\\n      MaxJobs = 75; \\ \n]", 
            "title": "Total jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#idle-jobs", 
            "text": "To set a limit on the number of idle jobs for a specific route, set the  MaxIdleJobs  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Limit the total number of idle jobs to 100 ; \\\n      MaxIdleJobs = 100; \\ \n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name =  Limit the total number of idle jobs to 75 ; \\\n      MaxIdleJobs = 75; \\ \n]", 
            "title": "Idle jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#debugging-routes", 
            "text": "To help debug expressions in your routes, you can use the  debug()  function. First, set the debug mode for the JobRouter by editing a file in  /etc/condor-ce/config.d/  to read  JOB_ROUTER_DEBUG = D_FULLDEBUG  Then wrap the problematic attribute in  debug() :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Debugging a difficult !ClassAd expression ; \\\n      eval_set_Experiment = debug(strcat( atlas , Name)); \\ \n]  You will find the debugging output in  /var/log/condor-ce/JobRouterLog .", 
            "title": "Debugging routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#routes-for-htcondor-batch-systems", 
            "text": "This section contains information about job routes that can be used if you are running an HTCondor batch system at your site.", 
            "title": "Routes for HTCondor Batch Systems"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-periodic-hold-release-or-remove", 
            "text": "To release, remove or put a job on hold if it meets certain criteria, use the  PERIODIC_*  family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting  PERIODIC_EXPR_INTERVAL  in your CE's configuration.  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Setting periodic statements ; \\\n      /* Puts the routed job on hold if the job s been idle and has been started at least once or if the job has tried to start more than once */ \\\n     set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\\n     /* Remove routed jobs if their walltime is longer than 3 days and 5 minutes */ \\\n     set_Periodic_Remove = ( RemoteWallClockTime   (3*24*60*60 + 5*60) ); \\\n     /* Release routed jobs if the condor_starter couldn t start the executable and  VMGAHP_ERR_INTERNAL  is in the HoldReason */ \\\n     set_Periodic_Release = HoldReasonCode == 6   regexp( VMGAHP_ERR_INTERNAL , HoldReason); \\ \n]", 
            "title": "Setting periodic hold, release or remove"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-routed-job-requirements", 
            "text": "If you need to set requirements on your routed job, you will need to use  set_Requirements  instead of  Requirements . The  Requirements  attribute filters jobs coming into your CE into different job routes whereas  set_requirements  will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the  HTCondor manual .  To ensure that your job lands on a Linux machine in your pool:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n      set_Requirements =  OpSys ==  LINUX ; \\ \n]", 
            "title": "Setting routed job requirements"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-accounting-groups", 
            "text": "To assign jobs to an HTCondor accounting group to manage fair share on your local batch system, we recommend using  UID and ExtAttr tables .", 
            "title": "Setting accounting groups"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#routes-for-non-htcondor-batch-systems", 
            "text": "This section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site.", 
            "title": "Routes for non-HTCondor Batch Systems"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-a-default-batch-queue", 
            "text": "To set a default queue for routed jobs, set the attribute  default_queue :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting batch system queues ; \\\n      set_default_queue =  osg_queue ; \\ \n]", 
            "title": "Setting a default batch queue"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-batch-system-directives", 
            "text": "To write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in  /etc/blahp/  (e.g., if your local batch system is PBS, edit  /etc/blahp/pbs_local_submit_attributes.sh ). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via the  default_remote_cerequirements  attribute, which can take the following form:  default_remote_cerequirements =  foo == X   bar == \\ Y\\    ...   This sets  foo  to value  X  and  bar  to the string  Y  (escaped double-quotes are required for string values) in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the  x509UserProxyFirstFQAN  attribute of the job submitted to a PBS batch system  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting job submit variables ; \\\n      set_default_remote_cerequirements = strcat( Walltime == 3600   AccountingGroup == \\ , x509UserProxyFirstFQAN,  \\ ); \\ \n]  With  /etc/blahp/pbs_local_submit_attributes.sh  containing.  1\n2\n3 #!/bin/bash  echo   #PBS -l walltime= $Walltime  echo   #PBS -A  $AccountingGroup    This results in the following being appended to the script that gets submitted to your batch system:  #PBS -l walltime=3600\n#PBS -A  CE job s x509UserProxyFirstFQAN attribute", 
            "title": "Setting batch system directives"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  The HTCondor-CE troubleshooting guide  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#example-configurations", 
            "text": "", 
            "title": "Example Configurations"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#aglt2s-job-routes", 
            "text": "Atlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes.   Setting various HTCondor-specific attributes like  Rank ,  AccountingGroup ,  JobPrio  and  Periodic_Remove  (see the  HTCondor manual  for more). Some of these are site-specific like  LastandFrac ,  IdleMP8Pressure ,  localQue ,  IsAnalyJob  and  JobMemoryLimit .  There is a difference between  Requirements  and  set_requirements . The  Requirements  attribute matches jobs to specific routes while the  set_requirements  sets the  Requirements  attribute on the  routed  job, which confines which machines that the routed job can land on.   Source:  https://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content  JOB_ROUTER_ENTRIES   =   \\ \n/* Still to  do  on all routes, get job requirements and add them here */  \\ \n/* ***** Route no  1  ***** */  \\ \n/* ***** Analysis queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue == analy ;   \\ \n     Name   =   Analysis Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     eval_set_IdleMP8Pressure   =   $( IdleMP8Pressure ) ;   \\ \n     eval_set_LastAndFrac   =   $( LastAndFrac ) ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  )     ( IfThenElse (( Owner   ==   atlasconnect   ||   Owner   ==   muoncal ) ,IfThenElse ( IdleMP8Pressure, ( TARGET.PARTITIONED  = ! =  TRUE ) ,True ) ,IfThenElse ( LastAndFrac, ( TARGET.PARTITIONED  = ! =  TRUE ) ,True ))) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.analy. ,Owner ) ;   \\ \n     set_localQue   =   Analysis ;   \\ \n     set_IsAnalyJob   =  True ;   \\ \n     set_JobPrio   =   5 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  2  ***** */  \\ \n/* ***** splitterNT queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue  ==   splitterNT ;   \\ \n     Name   =   Splitter ntuple queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =   group_calibrate.muoncal ;   \\ \n     set_localQue   =   Splitter ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_JobPrio   =   10 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  3  ***** */  \\ \n/* ***** splitter queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue  ==   splitter ;   \\ \n     Name   =   Splitter queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =   group_calibrate.muoncal ;   \\ \n     set_localQue   =   Splitter ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_JobPrio   =   15 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  4  ***** */  \\ \n/* ***** xrootd queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue  ==   xrootd ;   \\ \n     Name   =   Xrootd queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.analy. ,Owner ) ;   \\ \n     set_localQue   =   Analysis ;   \\ \n     set_IsAnalyJob   =  True ;   \\ \n     set_JobPrio   =   35 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  5  ***** */  \\ \n/* ***** Tier3Test queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue  ==   Tier3Test ;   \\ \n     Name   =   Tier3 Test Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  )     (   IS_TIER3_TEST_QUEUE   = ? =  True  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.analy. ,Owner ) ;   \\ \n     set_localQue   =   Tier3Test ;   \\ \n     set_IsTier3TestJob   =  True ;   \\ \n     set_IsAnalyJob   =  True ;   \\ \n     set_JobPrio   =   20 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  6  ***** */  \\ \n/* ***** mp8 queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue == mp8 ;   \\ \n     Name   =   MCORE Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  )     ((  TARGET.Cpus  ==   8    TARGET.CPU_TYPE  = ? =   mp8   )   ||  TARGET.PARTITIONED  = ? =  True  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.mcore. ,Owner ) ;   \\ \n     set_localQue   =   MP8 ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_JobPrio   =   25 ;   \\ \n     set_Rank   =   0 .0 ;   \\ \n     eval_set_RequestCpus   =   8 ;   \\ \n     set_JobMemoryLimit   =   33552000 ;   \\ \n     set_Slot_Type   =   mp8 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  7  ***** */  \\ \n/* ***** Installation queue, triggered by usatlas2 user ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue is undefined   target.Owner  ==   usatlas2 ;   \\ \n     Name   =   Install Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  )     (  TARGET.IS_INSTALL_QUE  = ? =  True  )     ( TARGET.AGLT2_SITE  ==   UM   ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.other. ,Owner ) ;   \\ \n     set_localQue   =   Default ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_IsInstallJob   =  True ;   \\ \n     set_JobPrio   =   15 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  8  ***** */  \\ \n/* ***** Default queue  for  usatlas1 user ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue is undefined   regexp ( usatlas1 ,target.Owner ) ;   \\ \n     Name   =   ATLAS Production Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.prod. ,Owner ) ;   \\ \n     set_localQue   =   Default ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  9  ***** */  \\ \n/* ***** Default queue  for  any other usatlas account ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue is undefined    ( regexp ( usatlas2 ,target.Owner )   ||  regexp ( usatlas3 ,target.Owner )) ;   \\ \n     Name   =   Other ATLAS Production ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.other. ,Owner ) ;   \\ \n     set_localQue   =   Default ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  10  ***** */  \\ \n/* ***** Anything  else . Set queue as Default and assign to other VOs  ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue is undefined   ifThenElse ( regexp ( usatlas ,target.Owner ) ,false,true ) ;   \\ \n     Name   =   Other Jobs ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_VOgener. ,Owner ) ;   \\ \n     set_localQue   =   Default ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]", 
            "title": "AGLT2's job routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#bnls-job-routes", 
            "text": "Atlas BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes:   Setting various HTCondor-specific attributes like  JobLeaseDuration ,  Requirements  and  Periodic_Hold  (see the  HTCondor manual  for more). Some of these are site-specific like  RACF_Group ,  Experiment ,  Job_Type  and  VO .  Jobs are split into different routes based on the  GlideIn  queue that they're in.  There is a difference between  Requirements  and  set_requirements . The  Requirements  attribute matches  incoming  jobs to specific routes while the  set_requirements  sets the  Requirements  attribute on the  routed  job, which confines which machines that the routed job can land on.   Source:  http://www.usatlas.bnl.gov/twiki/bin/view/Admins/HTCondorCE.html  ###############################################################################  #  # HTCondor-CE HTCondor batch system configuration file.  #  ###############################################################################  # Submit the job to the site Condor  JOB_ROUTER_ENTRIES   =   \\ \n    [   \\ \n      GridResource   =   condor localhost localhost ;   \\ \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ;   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   BNL_Condor_Pool_long ;   \\ \n      Requirements   =  target.queue == analysis.long ;   \\ \n      eval_set_RACF_Group   =   long ;   \\ \n      set_Experiment   =   atlas ;   \\ \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n      set_Job_Type   =   cas ;   \\ \n      set_JobLeaseDuration   =   3600 ;   \\ \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ;   \\ \n      eval_set_VO   =  x509UserProxyVOName ;   \\ \n    ]   \\ \n    [   \\ \n      GridResource   =   condor localhost localhost ;   \\ \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ;   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   BNL_Condor_Pool_short ;   \\ \n      Requirements   =  target.queue == analysis.short ;   \\ \n      eval_set_RACF_Group   =   short ;   \\ \n      set_Experiment   =   atlas ;   \\ \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n      set_Job_Type   =   cas ;   \\ \n      set_JobLeaseDuration   =   3600 ;   \\ \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ;   \\ \n      eval_set_VO   =  x509UserProxyVOName ;   \\ \n    ]   \\ \n    [   \\ \n      GridResource   =   condor localhost localhost ;   \\ \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ;   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   BNL_Condor_Pool_grid ;   \\ \n      Requirements   =  target.queue == grid ;   \\ \n      eval_set_RACF_Group   =   grid ;   \\ \n      set_Experiment   =   atlas ;   \\ \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n      set_Job_Type   =   cas ;   \\ \n      set_JobLeaseDuration   =   3600 ;   \\ \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ;   \\ \n      eval_set_VO   =  x509UserProxyVOName ;   \\ \n    ]   \\ \n    [   \\ \n      GridResource   =   condor localhost localhost ;   \\ \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ;   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   BNL_Condor_Pool ;   \\ \n      Requirements   =  target.queue is undefined ;   \\ \n      eval_set_RACF_Group   =   grid ;   \\ \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   rcf   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n      set_Experiment   =   atlas ;   \\ \n      set_Job_Type   =   cas ;   \\ \n      set_JobLeaseDuration   =   3600 ;   \\ \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ;   \\ \n      eval_set_VO   =  x509UserProxyVOName ;   \\ \n    ]", 
            "title": "BNL's job routes"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/", 
            "text": "HTCondor-CE Troubleshooting Guide\n\n\nIn this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of common issues with suggested troubleshooting steps.\n\n\nKnown Issues\n\n\nSUBMIT_EXPRS are not applied to jobs on the local HTCondor\n\n\nIf you are adding attributes to jobs submitted to your HTCondor pool with \nSUBMIT_EXPRS\n, these will \nnot\n be applied to jobs that are entering your pool from the HTCondor-CE. To get around this, you will want to add the attributes to your \njob routes\n. If the CE is the only entry point for jobs into your pool, you can get rid of \nSUBMIT_EXPRS\n on your backend. Otherwise, you will have to maintain your list of attributes both in your list of routes and in your \nSUBMIT_EXPRS\n.\n\n\nGeneral Troubleshooting Items\n\n\nMaking sure packages are up-to-date\n\n\nIt is important to make sure that the HTCondor-CE and related RPMs are up-to-date.\n\n\nroot@host #\n yum update \nhtcondor-ce*\n blahp condor\n\n\n\n\n\nIf you just want to see the packages to update, but do not want to perform the update now, answer \nN\n at the prompt.\n\n\nVerify package contents\n\n\nIf the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the contents of your packages (ignoring changes to configuration files):\n\n\nuser@host $\n rpm -q --verify htcondor-ce htcondor-ce-client blahp \n|\n awk \n$2 != \nc\n {print $0}\n\n\n\n\n\n\nIf the verification command returns output, this means that your packages have been changed. To fix this, you can reinstall the packages:\n\n\nuser@host $\n yum reinstall htcondor-ce htcondor-ce-client blahp\n\n\n\n\n\n\n\nNote\n\n\nThe reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an \n.rpmnew\n suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration.\n\n\n\n\nVerify clocks are synchronized\n\n\nLike all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is synchronized using a utility such as \nntpd\n. Additionally, HTCondor itself is sensitive to time skews on the NFS server. If you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew.\n\n\nHTCondor-CE Troubleshooting Items\n\n\nThis section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. Before troubleshooting, we recommend increasing the log level:\n\n\n\n\n\n\nWrite the following into \n/etc/condor-ce/config.d/99-local.conf\n to increase the log level for all daemons:\n\n\nALL_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nEnsure that the configuration is in place:\n\n\nroot@host # condor_ce_reconfig\n\n\n\n\n\n\n\n\n\nReproduce the issue\n\n\n\n\n\n\n\n\nNote\n\n\nBefore spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running \ncondor_ce_reconfig\n.\n\n\n\n\nDaemons fail to start\n\n\nIf there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to startup. Check the following subsections in order:\n\n\nSymptoms\n\n\nDaemon startup failure may manifest in many ways, the following are few symptoms of the problem.\n\n\n\n\n\n\nThe service fails to start:\n\n\nroot@host #\n service condor-ce start\n\nStarting Condor-CE daemons: [ FAIL ]\n\n\n\n\n\n\n\n\n\n\ncondor_ce_q\n fails with a lengthy error message:\n\n\nuser@host $\n condor_ce_q\n\nError:\n\n\n\nExtra Info: You probably saw this error because the condor_schedd is not running\n\n\non the machine you are trying to query. If the condor_schedd is not running, the\n\n\nCondor system will not be able to find an address and port to connect to and\n\n\nsatisfy this request. Please make sure the Condor daemons are running and try\n\n\nagain.\n\n\n\nExtra Info: If the condor_schedd is running on the machine you are trying to\n\n\nquery and you still see the error, the most likely cause is that you have setup\n\n\na personal Condor, you have not defined SCHEDD_NAME in your condor_config file,\n\n\nand something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define\n\n\neither or both of those settings in your config file, or you must use the -name\n\n\noption to condor_q. Please see the Condor manual for details on SCHEDD_NAME and\n\n\nSCHEDD_ADDRESS_FILE.\n\n\n\n\n\n\n\n\n\n\nNext actions\n\n\n\n\nIf the MasterLog is filled with \nERROR:SECMAN...TCP connection to collector...failed\n:\n This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in \nthis\n section of the install guide.\n\n\nIf the MasterLog is filled with \nDC_AUTHENTICATE\n errors:\n The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in \n/etc/condor-ce/condor_mapfile\n.\n\n\nIf the SchedLog is filled with \nCan\u2019t find address for negotiator\n:\n You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one.\n\n\n\n\nJobs fail to submit to the CE\n\n\nIf a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the culprit, then you may have encountered an authentication or authorization issue. You may see error messages like the following in your \nSchedLog\n:\n\n\n08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done!\n\n\n\n\n\nNext actions\n\n\n\n\nCheck GUMS or grid-mapfile\n and ensure that the user's DN is known to your \nauthentication method\n\n\nCheck for lcmaps errors\n in \n/var/log/messages\n\n\nIf you do not see helpful error messages in \n/var/log/messages\n,\n adjust the debug level by adding \nexport LCMAPS_DEBUG_LEVEL=5\n to \n/etc/sysconfig/condor-ce\n, restarting the condor-ce service, and checking \n/var/log/messages\n for errors again.\n\n\n\n\nJobs stay idle on the CE\n\n\nCheck the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is busy.\n\n\nIdle jobs on CE: Is the job router handling the incoming job?\n\n\nJobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things if you suspect that the jobs are not being matched. Check if the JobRouter sees a job before that by looking at the \njob router log\n and looking for the text \nsrc=\nJOB-ID\n\u2026claimed job\n.\n\n\nNext actions\n\n\nUse \ncondor_ce_job_router_info\n to see why your idle job does not match any routes\n\n\nIdle jobs on CE: Verify correct operation between the CE and your local batch system\n\n\nFor HTCondor batch systems\n\n\nHTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch system interaction will appear in the \nJobRouterLog\n.\n\n\nNext actions\n\n\n\n\nCheck the \nJobRouterLog\n for failures.\n\n\nVerify that the local HTCondor is functional.\n\n\nUse \ncondor_ce_config_val\n to verify that the \nJOB_ROUTER_SCHEDD2_NAME\n, \nJOB_ROUTER_SCHEDD2_POOL\n, and \nJOB_ROUTER_SCHEDD2_SPOOL\n configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively.\n\n\nUse \ncondor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE\n and verify that it is set to \n.*\n.\n\n\n\n\nFor non-HTCondor batch systems\n\n\nHTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch system interaction will appear in the \nGridmanagerLog\n. Look for \ngm state change\u2026\n lines to figure out where the issures are occuring.\n\n\nNext actions\n\n\n\n\nIf you see failures in the GridmanagerLog during job submission:\n Save the submit files by adding the appropriate entry to \nblah.config\n and submit it \nmanually\n to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the \nbatch system\n_binpath\n in \n/etc/blah.config\n.\n\n\n\n\nIf you see failures in the GridmanagerLog during queries for job status:\n Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in \n/usr/libexec/blahp/\nbatch system\n_status.sh\n (e.g., \n/usr/libexec/blahp/lsf_status.sh\n) that take the argument \nbatch system/YYYMMDD/job ID\n (e.g., \nlsf/20141008/65053\n). Run the appropriate status script for your batch system and upon success, you should see the following output:\n\n\nroot@host #\n /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053\n\n[ BatchjobId = \n894862\n; JobStatus = 4; ExitCode = 0; WorkerNode = \natl-prod08\n ]\n\n\n\n\n\n\nIf the script fails, \nrequest help\n from the OSG.\n\n\n\n\n\n\nIdle jobs on CE: Make sure the underlying batch system can run jobs\n\n\nHTCondor-CE communicates directly with an HTCondor batch system schedd, so if jobs are not running, examine the \nSchedLog\n and diagnose the problem from there. For other batch systems, the BLAHP is used to submit jobs using your batch system\u2019s job submission binaries, whose location is specified in \n/etc/blah.config\n.\n\n\nProcedure\n\n\n\n\nManually create and submit a simple job (e.g., one that runs \nsleep\n)\n\n\nCheck for errors in the submission itself\n\n\nWatch the job in the batch system queue (e.g., using \ncondor_q\n)\n\n\nIf the job does not run, check for errors on the batch system\n\n\n\n\nNext actions\n\n\nIf the underlying batch system does not run a simple manual job, it will probably not run a job coming from HTCondor-CE. Once you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again.\n\n\nIdle jobs on CE: Verify ability to change permissions on key files\n\n\nHTCondor-CE needs the ability to write and chown files in its \nspool\n directory and if it cannot, jobs will not run at all. Spool permission errors can appear in the \nSchedLog\n and the \nJobRouterLog\n.\n\n\nSymptoms\n\n\n09/17/14 14:45:42 Error: Unable to chown \n/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env\n from 12345 to 54321\n\n\n\n\n\nNext actions\n\n\n\n\nAs root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions.\n\n\nVerify that there aren't any underlying file system issues in the specified location\n\n\n\n\nJobs stay idle on a remote host submitting to the CE\n\n\nIf you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not see a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not authorized to run there. Note that jobs may take several minutes or longer if the CE is busy.\n\n\nRemote idle jobs: Can you contact the CE?\n\n\nTo check basic connectivity to a CE, use \ncondor_ce_ping\n:\n\n\nSymptoms\n\n\nuser@host $\n condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\nERROR: couldn\nt locate condorce.example.com!\n\n\n\n\n\n\nNext actions\n\n\n\n\nMake sure that the HTCondor-CE daemons are running with \ncondor_ce_status\n.\n\n\nVerify that your CE is reachable from your submit host, replacing \ncondorce.example.com\n with the hostname of your CE:\nuser@host $\n ping condorce.example.com\n\n\n\n\n\n\n\n\n\nRemote idle jobs: Are you authorized to run jobs on the CE?\n\n\nThe CE will only accept jobs from users that authenticate via LCMAPS, grid mapfile, or GUMS. You can use \ncondor_ce_ping\n to check if you are authorized and what user your proxy is being mapped to.\n\n\nSymptoms\n\n\nuser@host $\n condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\nRemote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nLocal  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nSession ID:                  condorce:3343:1412790611:0\n\n\nInstruction:                 WRITE\n\n\nCommand:                     60021\n\n\nEncryption:                  none\n\n\nIntegrity:                   MD5\n\n\nAuthenticated using:         GSI\n\n\nAll authentication methods:  GSI\n\n\nRemote Mapping:              gsi@unmapped\n\n\nAuthorized:                  FALSE\n\n\n\n\n\n\nNotice the failures in the above message: \nRemote Mapping: gsi@unmapped\n and \nAuthorized: FALSE\n\n\nNext actions\n\n\n\n\nVerify that an \nauthentication method\n is set up on the CE\n\n\nVerify that your user DN is mapped to an existing system user\n\n\n\n\nJobs go on hold\n\n\nJobs will be put on held with a \nHoldReason\n attribute that can be inspected with \ncondor_ce_q\n:\n\n\nuser@host $\n condor_ce_q -l \nJOB-ID\n -attr HoldReason\n\nHoldReason = \nCE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to non-existent route or entry in JOB_ROUTER_ENTRIES.\n\n\n\n\n\n\nHeld jobs: Missing/expired user proxy\n\n\nHTCondor-CE requires a valid user proxy for each job that is submitted. You can check the status of your proxy with the following\n\n\nuser@host $\n voms-proxy-info -all\n\n\n\n\n\nNext actions\n\n\nEnsure that the owner of the job generates their proxy with \nvoms-proxy-init\n.\n\n\nHeld jobs: Invalid job universe\n\n\nThe HTCondor-CE only accepts jobs that have \nuniverse\n in their submit files set to \nvanilla\n, \nstandard\n, \nlocal\n, or \nscheduler\n. These universes also have corresponding integer values that can be found in the \nHTCondor manual\n.\n\n\nNext actions\n\n\n\n\nEnsure jobs submitted locally, from the CE host, are submitted with \nuniverse = vanilla\n\n\n\n\nEnsure jobs submitted from a remote submit point are submitted with:\n\n\nuniverse = grid\ngrid_resource = condor condorce.example.com condorce.example.com:9619\n\n\n\n\n\nreplacing \ncondorce.example.com\n with the hostname of the CE.\n\n\n\n\n\n\nHeld jobs: Non-existent route or entry in JOB_ROUTER_ENTRIES\n\n\nJobs on the CE will be put on hold if they do not match any job routes within 30 minutes.\n\n\nNext actions\n\n\nUse \ncondor_ce_job_router_info\n to see why your idle job does not match any routes.\n\n\nIdentifying the corresponding job ID on the local batch system\n\n\nWhen troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID and the resultant job ID on the batch system. The methods for finding the resultant job ID differs between batch systems.\n\n\nHTCondor batch systems\n\n\n\n\n\n\nTo inspect the CE\u2019s job ad, use \ncondor_ce_q\n or \ncondor_ce_history\n:\n\n\n\n\n\n\nUse \ncondor_ce_q\n if the job is still in the CE\u2019s queue:\n\n\nuser@host $\n condor_ce_q \nJOB-ID\n -af RoutedToJobId\n\n\n\n\n\n\n\n\n\nUse \ncondor_ce_history\n if the job has left the CE\u2019s queue:\n\n\nuser@host $\n condor_ce_history \nJOB-ID\n -af RoutedToJobId\n\n\n\n\n\n\n\n\n\n\n\n\n\nParse the \nJobRouterLog\n for the CE\u2019s job ID.\n\n\n\n\n\n\nNon-HTCondor batch systems\n\n\nWhen HTCondor-CE records the corresponding batch system job ID, it is written in the form \nBATCH-SYSTEM\n/\nDATE\n/\nJOB ID\n:\n\n\nlsf/20141206/482046\n\n\n\n\n\n\n\n\n\nTo inspect the CE\u2019s job ad, use \ncondor_ce_q\n:\n\n\nuser@host $\n condor_ce_q \nJOB-ID\n -af GridJobId\n\n\n\n\n\n\n\n\n\nParse the \nGridmanagerLog\n for the CE\u2019s job ID.\n\n\n\n\n\n\nJobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only)\n\n\nBy design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. Therefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps:\n\n\n\n\nIdentify the misbehaving job ID in your batch system queue\n\n\n\n\nFind the job's corresponding CE job ID:\n\n\nuser@host $\n condor_q \nJOB-ID\n -af RoutedFromJobId\n\n\n\n\n\n\n\n\n\nUse \ncondor_ce_rm\n to remove the CE job from the queue\n\n\n\n\n\n\nMissing HTCondor tools\n\n\nMost of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. If you are trying to use HTCondor-CE tools and you see the following error:\n\n\nuser@host $\n condor_ce_job_router_info\n\n/usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found\n\n\n\n\n\n\nThis means that the \ncondor_job_router_info\n (note this is not the CE version), is not in your \nPATH\n.\n\n\nNext Actions\n\n\n\n\nEither the condor RPM is missing or there are some other issues with it (try \nrpm --verify condor\n).\n\n\nYou have installed HTCondor in a non-standard location that is not in your \nPATH\n.\n\n\nThe \ncondor_job_router_info\n tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming).\n\n\n\n\nHTCondor-CE Troubleshooting Tools\n\n\nHTCondor-CE has its own separate set of of the HTCondor tools with \nce\n in the name (i.e., \ncondor_ce_submit\n vs \ncondor_submit\n). Some of the the commands are only for the CE (e.g., \ncondor_ce_run\n and \ncondor_ce_trace\n) but many of them are just HTCondor commands configured to interact with the CE (e.g., \ncondor_ce_q\n, \ncondor_ce_status\n). It is important to differentiate the two: \ncondor_ce_config_val\n will provide configuration values for your HTCondor-CE while \ncondor_config_val\n will provide configuration values for your HTCondor batch system. If you are not running an HTCondor batch system, the non-CE commands will return errors.\n\n\ncondor_ce_trace\n\n\nUsage\n\n\ncondor_ce_trace\n is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job.\n\n\n\n\nNote\n\n\nYou must have generated a proxy (e.g., \nvoms-proxy-init\n) and your DN must be added to your \nchosen authentication method\n.\n\n\n\n\nuser@host $\n condor_ce_trace condorce.example.com\n\n\n\n\n\nReplacing the \ncondorce.example.com\n with the hostname of the CE. If you are familiar with the output of condor commands, the command also takes a \n--debug\n option that displays verbose condor output.\n\n\nTroubleshooting\n\n\n\n\nIf the command fails with \u201cFailed ping\u2026\u201d:\n Make sure that the HTCondor-CE daemons are running on the CE\n\n\nIf you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line:\n Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our \ninstallation document\n.\n\n\nIf the job submits but does not complete:\n Look at the status of the job and perform the relevant \ntroubleshooting steps\n.\n\n\n\n\ncondor_ce_run\n\n\nUsage\n\n\nSimilar to \nglobus-job-run\n, \ncondor_ce_run\n is a tool that submits a simple job to your CE, so it is useful for quickly submitting jobs through your CE. To submit a job to the CE and run the \nenv\n command on the remote batch system:\n\n\n\n\nNote\n\n\nYou must have generated a proxy (e.g., \nvoms-proxy-init\n) and your DN must be added to your \nchosen authentication method\n.\n\n\n\n\nuser@host $\n condor_ce_run -r condorce.example.com:9619 /bin/env\n\n\n\n\n\nReplacing the \ncondorce.example.com\n with the hostname of the CE. If you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you can run commands locally on the CE with \ncondor_ce_run\n with the \n-l\n option. The following example outputs the JobRouterLog of the CE in question:\n\n\nuser@host $\n condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog\n\n\n\n\n\nReplacing the \ncondorce.example.com\n text with the hostname of the CE. To disable this feature on your CE, consult \nthis\n section of the install documentation.\n\n\nTroubleshooting\n\n\n\n\nIf you do not see any results:\n \ncondor_ce_run\n does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use \ncondor_ce_q\n in a separate terminal to track the job on the CE. If you never see any results, use \ncondor_ce_trace\n to pinpoint errors.\n\n\nIf you see an error message that begins with \u201cFailed to\u2026\u201d:\n Check connectivity to the CE with \ncondor_ce_trace\n or \ncondor_ce_ping\n\n\n\n\ncondor_ce_submit\n\n\nSee the \nsubmitting to HTCondor-CE\n document for details.\n\n\ncondor_ce_ping\n\n\nUsage\n\n\nUse the following \ncondor_ce_ping\n command to test your ability to submit jobs to an HTCondor-CE, replacing \ncondorce.example.com\n with the hostname of your CE:\n\n\nuser@host $\n condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\n\n\n\n\nThe following shows successful output where I am able to submit jobs (\nAuthorized: TRUE\n) as the glow user (\nRemote Mapping: glow@users.opensciencegrid.org\n):\n\n\nRemote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nLocal  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nSession ID:                  condorce:27407:1412286981:3\n\n\nInstruction:                 WRITE\n\n\nCommand:                     60021\n\n\nEncryption:                  none\n\n\nIntegrity:                   MD5\n\n\nAuthenticated using:         GSI\n\n\nAll authentication methods:  GSI\n\n\nRemote Mapping:              glow@users.opensciencegrid.org\n\n\nAuthorized:                  TRUE\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_ping\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_ping\n takes the same arguments as \ncondor_ping\n and is documented in the \nHTCondor manual\n.\n\n\n\n\nTroubleshooting\n\n\n\n\n\n\nIf you see \u201cERROR: couldn\u2019t locate (null)\u201d\n, that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE:\n\n\nMASTER_DEBUG = D_FULLDEBUG\nSCHEDD_DEBUG = D_FULLDEBUG\n\n\n\n\n\nThen look in the \nMasterLog\n and \nSchedLog\n for any errors.\n\n\n\n\n\n\nIf you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line\n, this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our \ninstallation document\n.\n\n\n\n\n\n\ncondor_ce_q\n\n\nUsage\n\n\ncondor_ce_q\n can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619\n\n\n\n\n\nTo inspect the full ClassAd for a specific job, specify the \n-l\n flag and the job ID:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l \nJOB-ID\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_q\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_q\n takes the same arguments as \ncondor_q\n and is documented in the \nHTCondor manual\n.\n\n\n\n\nTroubleshooting\n\n\nIf the jobs that you are submiting to a CE are not completing, \ncondor_ce_q\n can tell you the status of your jobs.\n\n\n\n\n\n\nIf the schedd is not running:\n You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with:\n\n\nMASTER_DEBUG = D_FULLDEBUG\nSCHEDD_DEBUG = D_FULLDEBUG\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\nThen look in the \nMasterLog\n and \nSchedLog\n on the CE for any errors.\n\n\n\n\n\n\nIf there are issues with contacting the collector:\n You will see the following message:\n\n\nuser@host $\n condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu\n\n\n-- Failed to fetch ads from: \n129.59.197.223:9620?sock`33630_8b33_4\n : ce1.accre.vanderbilt.edu\n\n\n\n\n\n\nThis may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the \nALLOW_READ\n configuration value is not set:\n\n\nuser@host $\n condor_ce_config_val -v ALLOW_READ\n\nNot defined: ALLOW_READ\n\n\n\n\n\n\nIf it is defined, remove it from the file that is returned in the output.\n\n\n\n\n\n\nIf a job is held:\n There should be an accompanying \nHoldReason\n that will tell you why it is being held. The \nHoldReason\n is in the job\u2019s ClassAd, so you can use the long form of \ncondor_ce_q\n to extract its value:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l \nJob ID\n \n|\n grep HoldReason\n\n\n\n\n\n\n\n\n\nIf a job is idle:\n The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the \ncondor_ce_job_router_info\n.\n\n\n\n\n\n\ncondor_ce_history\n\n\nUsage\n\n\ncondor_ce_history\n can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE:\n\n\nuser@host $\n condor_ce_history -name condorce.example.com -pool condorce.example.com:9619\n\n\n\n\n\nTo inspect the full ClassAd for a specific job, specify the \n-l\n flag and the job ID:\n\n\nuser@host $\n condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l \nJob ID\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_history\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_history\n takes the same arguments as \ncondor_history\n and is documented in the \nHTCondor manual\n.\n\n\n\n\ncondor_ce_job_router_info\n\n\nUsage\n\n\nUse the \ncondor_ce_job_router_info\n command to help troubleshoot your routes and how jobs will match to them. To see all of your routes (the output is long because it combines your routes with the \nJOB_ROUTER_DEFAULTS\n configuration variable):\n\n\nroot@host #\n condor_ce_job_router_info -config\n\n\n\n\n\nTo see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of \ncondor_ce_q\n (replace the \nJOB-ID\n with the job ID that you are interested in):\n\n\nroot@host #\n condor_ce_q -l \nJOB-ID\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\nTo inspect a job that has already left the queue, use \ncondor_ce_history\n instead of \ncondor_ce_q\n:\n\n\nroot@host #\n condor_ce_history -l \nJOB-ID\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\n\n\nNote\n\n\nIf the proxy for the job has expired, the job will not match any routes. To work around this constraint:\n\n\n\n\nroot@host #\n condor_ce_history -l \nJOB-ID\n \n|\n sed \ns/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date \n+1 sec\n`/\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\nAlternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file:\n\n\nroot@host #\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads \nJOBAD-FILE\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\nIf the job does not match any route:\n You can identify this case when you see \n0 candidate jobs found\n in the \ncondor_job_router_info\n output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to \ntrue\n. When troubleshooting, look at all of the expressions prior to the \ntarget.ProcId \n= 0\n expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again.\n\n\n\n\nIf your job matches more than one route:\n the tool will tell you by showing all matching routes after the job ID:\n\n\nChecking Job src=162,0 against all routes\nRoute Matches: Local_PBS\nRoute Matches: Condor_Test\n\n\n\n\n\nTo troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is \nhighlighted below\n:\n\n\nUmbrella constraint: ((target.x509userproxysubject =!= UNDEFINED) \n\n(target.x509UserProxyExpiration =!= UNDEFINED) \n\n(time() \n target.x509UserProxyExpiration) \n\n(target.JobUniverse =?= 5 || target.JobUniverse =?= 1)) \n\n\n( (target.osgTestPBS is true) || (true) )\n \n\n(target.ProcId \n= 0 \n target.JobStatus == 1 \n\n(target.StageInStart is undefined || target.StageInFinish isnt undefined) \n\ntarget.Managed isnt \nScheddDone\n \n\ntarget.Managed isnt \nExtenal\n \n\ntarget.Owner isnt Undefined \n\ntarget.RoutedBy isnt \nhtcondor-ce\n)\n\n\n\n\n\nBoth routes evaluate to \ntrue\n for the job\u2019s ClassAd because it contained \nosgTestPBS = true\n. Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the \njob route configuration page\n for more details.\n\n\n\n\n\n\nIf it is unclear why jobs are matching a route:\n wrap the route's requirements expression in \ndebug()\n and check the \nJobRouterLog\n for more information.\n\n\n\n\n\n\ncondor_ce_router_q\n\n\nUsage\n\n\nIf you have multiple job routes and many jobs, \ncondor_ce_router_q\n is a useful tool to see how jobs are being routed and their statuses:\n\n\nuser@host $\n condor_ce_router_q\n\n\n\n\n\ncondor_ce_router_q\n takes the same options as \ncondor_router_q\n and \ncondor_q\n and is documented in the \nHTCondor manual\n\n\ncondor_ce_status\n\n\nUsage\n\n\nTo see the daemons running on a CE, you can run the following:\n\n\nuser@host $\n condor_ce_status -any -name condorce.example.com -pool condorce.example.com:9619\n\n\n\n\n\nReplacing \ncondorce.example.com\n  with the hostname of the CE.\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_status\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_status\n takes the same arguments as \ncondor_status\n and is documented in the \nHTCondor manual\n.\n\n\n\n\nTroubleshooting\n\n\nTo list the daemons that are configured to run:\n\n\nuser@host $\n condor_ce_config_val -v DAEMON_LIST\n\nDAEMON_LIST: MASTER COLLECTOR SCHEDD JOB_ROUTER, SHARED_PORT, SHARED_PORT\n\n\n  Defined in \n/etc/condor-ce/config.d/03-ce-shared-port.conf\n, line 9.\n\n\n\n\n\n\nIf you do not see these daemons in the output of \ncondor_ce_status\n, check the \nMaster log\n for errors.\n\n\ncondor_ce_config_val\n\n\nUsage\n\n\nTo see the value of configuration variables and where they are set, use \ncondor_ce_config_val\n. Primarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. To see the value of a single variable and where it is set:\n\n\nuser@host $\n condor_ce_config_val -v \nCONFIGURATION-VARIABLE\n\n\n\n\n\n\nTo see a list of all configuration variables and their values:\n\n\nuser@host $\n condor_ce_config_val -dump\n\n\n\n\n\nTo see a list of all the files that are used to create your configuration and the order that they are parsed, use the following command:\n\n\nuser@host $\n condor_ce_config_val -config\n\n\n\n\n\ncondor_ce_config_val\n takes the same arguments as \ncondor_config_val\n and is documented in the \nHTCondor manual\n.\n\n\ncondor_ce_reconfig\n\n\nUsage\n\n\nTo ensure that your configuration changes have taken effect, run \ncondor_ce_reconfig\n.\n\n\nuser@host $\n condor_ce_reconfig\n\n\n\n\n\ncondor_ce_{on,off,restart}\n\n\nUsage\n\n\nTo turn on/off/restart HTCondor-CE daemons, use the following commands:\n\n\nroot@host #\n condor_ce_on\n\nroot@host #\n condor_ce_off\n\nroot@host #\n condor_ce_restart\n\n\n\n\n\nThe HTCondor-CE service uses the previous commands with default values. Using these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart:\n\n\n\n\n\n\nIf you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command:\n\n\nroot@host #\n condor_ce_restart -fast\n\n\n\n\n\nThis will cause HTCondor-CE to restart and quickly reconnect to all running jobs.\n\n\n\n\n\n\nIf you need to stop running new jobs, run the following:\n\n\nroot@host #\n condor_ce_off -peaceful\n\n\n\n\n\nThis will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down.\n\n\n\n\n\n\nHTCondor-CE Troubleshooting Data\n\n\nThe following files are located on the CE host.\n\n\nMasterLog\n\n\nThe HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if they fail to start.\n\n\n\n\nLocation: \n/var/log/condor-ce/MasterLog\n\n\nKey contents: Start-up, shut-down, and communication with other HTCondor daemons\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nMASTER_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for:\n\n\nSuccessful daemon start-up. The following line shows that the Collector daemon started successfully:\n\n\n10/07/14 14:20:27 Started DaemonCore process \n/usr/sbin/condor_collector -f -port 9619\n, pid and pgroup = 7318\n\n\n\n\n\nSchedLog\n\n\nThe HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. It contains valuable information when trying to troubleshoot authentication issues.\n\n\n\n\nLocation: \n/var/log/condor-ce/SchedLog\n\n\nKey contents:\n\n\nEvery job submitted to the CE\n\n\nUser authorization events\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nSCHEDD_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is submitted to the CE queue:\n\n\n10/07/14 16:52:17 Submitting new job 234.0\n\n\n\n\n\nIn this example, the ID of the submitted job is \n234.0\n.\n\n\n\n\n\n\nJob owner is authorized and mapped:\n\n\n10/07/14 16:52:17 Command=QMGMT_WRITE_CMD, peer=\n131.225.154.68:42262\n\n10/07/14 16:52:17 AuthMethod=GSI, AuthId=/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Lin 1047,\n                  /GLOW/Role=NULL/Capability=NULL, \nCondorId=glow@users.opensciencegrid.org\n\n\n\n\n\n\nIn this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the \nglow\n user.\n\n\n\n\n\n\nUser job submission fails\n due to improper authentication or authorization:\n\n\n08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189\n                  failed: AUTHENTICATE:1003:Failed to authenticate with any\n                  method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to\n                  authenticate because the remote (client) side was not able to acquire its\n                  credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to\n                  lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to \ngsi@unmapped\n\n                  from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE:\n                  reason: WRITE authorization policy contains no matching ALLOW entry for this\n                  request; identifiers used for this host:\n                  72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip\n                  address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done\n\n\n\n\n\n\n\n\n\nMissing negotiator:\n\n\n10/18/14 17:32:21 Can\nt find address for negotiator\n10/18/14 17:32:21 Failed to send RESCHEDULE to unknown daemon:\n\n\n\n\n\nSince HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes:\n\n\n06/23/15 11:15:03 Number of Active Workers 0\n\n\n\n\n\n\n\n\n\nCorrupted \njob_queue.log\n:\n\n\n02/07/17 10:55:49 WARNING: Encountered corrupt log record _654 (byte offset 5046225)\n02/07/17 10:55:49 103 1354325.0 PeriodicRemove ( StageInFinish \n 0 ) 105\n02/07/17 10:55:49 Lines following corrupt log record _654 (up to 3):\n02/07/17 10:55:49 103 1346101.0 RemoteWallClockTime 116668.000000\n02/07/17 10:55:49 104 1346101.0 WallClockCheckpoint\n02/07/17 10:55:49 104 1346101.0 ShadowBday\n02/07/17 10:55:49 ERROR \nError: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction,\n                  recovery failed\n at line 1080 in file /builddir/build/BUILD/condor-8.4.8/src/condor_utils/classad_log.cpp\n\n\n\n\n\nThis means \n/var/lib/condor-ce/spool/job_queue.log\n has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the \nLines following corrupt log record...\n line. The most common culprit of the corruption is that the disk containing the \njob_queue.log\n has filled up. To avoid this problem, you can change the location of \njob_queue.log\n by setting \nJOB_QUEUE_LOG\n in \n/etc/condor-ce/config.d/\n to a path, preferably one on a large SSD.\n\n\n\n\n\n\nJobRouterLog\n\n\nThe HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to troubleshoot issues with job routing.\n\n\n\n\nLocation: \n/var/log/condor-ce/JobRouterLog\n\n\nKey contents:\n\n\nEvery attempt to route a job\n\n\nRouting success messages\n\n\nJob attribute changes, based on chosen route\n\n\nJob submission errors to an HTCondor batch system\n\n\nCorresponding job IDs on an HTCondor batch system\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nJOB_ROUTER_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nApply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnown Errors\n\n\n\n\n\n\nIf you have \nD_FULLDEBUG\n turned on for the job router, you will see errors like the following:\n\n\n06/12/15 14:00:28 HOOK_UPDATE_JOB_INFO not configured.\n\n\n\n\n\nYou can safely ignore these.\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is considered for routing:\n\n\n09/17/14 15:00:56 JobRouter (src=86.0,route=Local_LSF): found candidate job\n\n\n\n\n\nIn parentheses are the original HTCondor-CE job ID (e.g., \n86.0\n) and the route (e.g., \nLocal_LSF\n).\n\n\n\n\n\n\nJob is successfully routed:\n\n\n09/17/14 15:00:57 JobRouter (src=86.0,route=Local_LSF): claimed job\n\n\n\n\n\n\n\n\n\nFinding the corresponding job ID on your HTCondor batch system:\n\n\n09/17/14 15:00:57 JobRouter (src=86.0,dest=205.0,route=Local_Condor): claimed job\n\n\n\n\n\nIn parentheses are the original HTCondor-CE job ID (e.g., \n86.0\n) and the resultant job ID on the HTCondor batch system (e.g., \n205.0\n)\n\n\n\n\n\n\nIf your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the \ncondor_ce_job_router_info\n\n\n\n\nHTCondor batch systems only\n: The following error occurs when the job router daemon cannot submit the routed job:\n10/19/14 13:09:15 Can\nt resolve collector condorce.example.com; skipping\n10/19/14 13:09:15 ERROR (pool condorce.example.com) Can\nt find address of schedd\n10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job\n\n\n\n\n\n\n\n\n\nGridmanagerLog\n\n\nThe HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. It contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. Details on how to read the Gridmanager log can be found on the \nHTCondor Wiki\n.\n\n\n\n\nLocation: \n/var/log/condor-ce/GridmanagerLog.\nJOB-OWNER\n\n\nKey contents:\n\n\nEvery attempt to submit a job to a batch system or other grid resource\n\n\nStatus updates of submitted jobs\n\n\nCorresponding job IDs on non-HTCondor batch systems\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nMAX_GRIDMANAGER_LOG = 6h\nMAX_NUM_GRIDMANAGER_LOG = 8\nGRIDMANAGER_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is submitted to the batch system:\n\n\n09/17/14 09:51:34 [12997] (85.0) gm state change: GM_SUBMIT_SAVE -\n GM_SUBMITTED\n\n\n\n\n\nEvery state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)).\n\n\n\n\n\n\nJob status being updated:\n\n\n09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -\n GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563] \n- \nBLAH_JOB_STATUS 3 lsf/20140917/482046\n\n09/17/14 15:07:24 [25543] GAHP[25563] -\n \nS\n\n09/17/14 15:07:25 [25543] GAHP[25563] \n- \nRESULTS\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \nR\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \nS\n \n1\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \n3\n \n0\n \nNo Error\n \n4\n \n[ BatchjobId = \n482046\n; JobStatus = 4; ExitCode = 0; WorkerNode = \natl-prod08\n ]\n\n\n\n\n\n\nThe first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here.\n\n\n\n\n\n\nFinding the corresponding job ID on your non-HTCondor batch system:\n\n\n09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -\n GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563] \n- \nBLAH_JOB_STATUS 3 lsf/20140917/482046\n\n\n\n\n\n\nOn the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses, \n(87.0)\n. At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes, \nlsf/20140917/482046\n.\n\n\n\n\n\n\nJob completion on the batch system:\n\n\n09/17/14 15:07:25 [25543] (87.0) gm state change: GM_TRANSFER_OUTPUT -\n GM_DONE_SAVE\n\n\n\n\n\n\n\n\n\nSharedPortLog\n\n\nThe HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the collector. This log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found \nhere\n.\n\n\n\n\nLocation: \n/var/log/condor-ce/SharedPortLog\n\n\nKey contents: Every attempt to connect to HTCondor-CE (except collector queries)\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nSHARED_PORT_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nMessages log\n\n\nThe messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. If there are issues with the \nauthentication setup\n, the errors may appear here.\n\n\n\n\nLocation: \n/var/log/messages\n\n\nKey contents: User authentication\n\n\n\n\nWhat to look for\n\n\nA user is mapped:\n\n\nOct 6 10:35:32 osgserv06 htondor-ce-llgt[12147]: Callout to \nLCMAPS\n returned local user (service condor): \nosgglow01\n\n\n\n\n\n\nBLAHP Configuration File\n\n\nHTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client tools.\n\n\n\n\nLocation: \n/etc/blah.config\n\n\nKey contents:\n\n\nLocations of the batch system's client binaries and logs\n\n\nLocation to save files that are submitted to the local batch system\n\n\n\n\n\n\n\n\nYou can also tell the BLAHP to save the files that are being submitted to the local batch system to \nDIR-NAME\n by adding the following line:\n\n\nblah_debug_save_submit_info=\nDIR_NAME\n\n\n\n\n\n\nThe BLAHP will then create a directory with the format \nbl_*\n for each submission to the local jobmanager with the submit file and proxy used.\n\n\n\n\nNote\n\n\nWhitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within \nDIR_NAME\n.\n\n\n\n\nGetting Help\n\n\nIf you are still experiencing issues after using this document, please let us know!\n\n\n\n\nGather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.)\n\n\n\n\nGather system information:\n\n\nroot@host # osg-system-profiler\n\n\n\n\n\n\n\n\n\nStart a support request using \na web interface\n or by email to \n\n\n\n\nDescribe issue and expected or desired behavior\n\n\nInclude basic HTCondor-CE and related information\n\n\nAttach the osg-system-profiler output\n\n\n\n\n\n\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nConfiguring HTCondor-CE job routes\n\n\nSubmitting jobs to HTCondor-CE", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-guide", 
            "text": "In this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of common issues with suggested troubleshooting steps.", 
            "title": "HTCondor-CE Troubleshooting Guide"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#submit_exprs-are-not-applied-to-jobs-on-the-local-htcondor", 
            "text": "If you are adding attributes to jobs submitted to your HTCondor pool with  SUBMIT_EXPRS , these will  not  be applied to jobs that are entering your pool from the HTCondor-CE. To get around this, you will want to add the attributes to your  job routes . If the CE is the only entry point for jobs into your pool, you can get rid of  SUBMIT_EXPRS  on your backend. Otherwise, you will have to maintain your list of attributes both in your list of routes and in your  SUBMIT_EXPRS .", 
            "title": "SUBMIT_EXPRS are not applied to jobs on the local HTCondor"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#general-troubleshooting-items", 
            "text": "", 
            "title": "General Troubleshooting Items"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#making-sure-packages-are-up-to-date", 
            "text": "It is important to make sure that the HTCondor-CE and related RPMs are up-to-date.  root@host #  yum update  htcondor-ce*  blahp condor  If you just want to see the packages to update, but do not want to perform the update now, answer  N  at the prompt.", 
            "title": "Making sure packages are up-to-date"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#verify-package-contents", 
            "text": "If the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the contents of your packages (ignoring changes to configuration files):  user@host $  rpm -q --verify htcondor-ce htcondor-ce-client blahp  |  awk  $2 !=  c  {print $0}   If the verification command returns output, this means that your packages have been changed. To fix this, you can reinstall the packages:  user@host $  yum reinstall htcondor-ce htcondor-ce-client blahp   Note  The reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an  .rpmnew  suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration.", 
            "title": "Verify package contents"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#verify-clocks-are-synchronized", 
            "text": "Like all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is synchronized using a utility such as  ntpd . Additionally, HTCondor itself is sensitive to time skews on the NFS server. If you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew.", 
            "title": "Verify clocks are synchronized"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-items", 
            "text": "This section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. Before troubleshooting, we recommend increasing the log level:    Write the following into  /etc/condor-ce/config.d/99-local.conf  to increase the log level for all daemons:  ALL_DEBUG = D_FULLDEBUG    Ensure that the configuration is in place:  root@host # condor_ce_reconfig    Reproduce the issue     Note  Before spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running  condor_ce_reconfig .", 
            "title": "HTCondor-CE Troubleshooting Items"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#daemons-fail-to-start", 
            "text": "If there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to startup. Check the following subsections in order:  Symptoms  Daemon startup failure may manifest in many ways, the following are few symptoms of the problem.    The service fails to start:  root@host #  service condor-ce start Starting Condor-CE daemons: [ FAIL ]     condor_ce_q  fails with a lengthy error message:  user@host $  condor_ce_q Error:  Extra Info: You probably saw this error because the condor_schedd is not running  on the machine you are trying to query. If the condor_schedd is not running, the  Condor system will not be able to find an address and port to connect to and  satisfy this request. Please make sure the Condor daemons are running and try  again.  Extra Info: If the condor_schedd is running on the machine you are trying to  query and you still see the error, the most likely cause is that you have setup  a personal Condor, you have not defined SCHEDD_NAME in your condor_config file,  and something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define  either or both of those settings in your config file, or you must use the -name  option to condor_q. Please see the Condor manual for details on SCHEDD_NAME and  SCHEDD_ADDRESS_FILE.     Next actions   If the MasterLog is filled with  ERROR:SECMAN...TCP connection to collector...failed :  This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in  this  section of the install guide.  If the MasterLog is filled with  DC_AUTHENTICATE  errors:  The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in  /etc/condor-ce/condor_mapfile .  If the SchedLog is filled with  Can\u2019t find address for negotiator :  You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one.", 
            "title": "Daemons fail to start"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-fail-to-submit-to-the-ce", 
            "text": "If a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the culprit, then you may have encountered an authentication or authorization issue. You may see error messages like the following in your  SchedLog :  08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done!  Next actions   Check GUMS or grid-mapfile  and ensure that the user's DN is known to your  authentication method  Check for lcmaps errors  in  /var/log/messages  If you do not see helpful error messages in  /var/log/messages ,  adjust the debug level by adding  export LCMAPS_DEBUG_LEVEL=5  to  /etc/sysconfig/condor-ce , restarting the condor-ce service, and checking  /var/log/messages  for errors again.", 
            "title": "Jobs fail to submit to the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-stay-idle-on-the-ce", 
            "text": "Check the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is busy.", 
            "title": "Jobs stay idle on the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-is-the-job-router-handling-the-incoming-job", 
            "text": "Jobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things if you suspect that the jobs are not being matched. Check if the JobRouter sees a job before that by looking at the  job router log  and looking for the text  src= JOB-ID \u2026claimed job .  Next actions  Use  condor_ce_job_router_info  to see why your idle job does not match any routes", 
            "title": "Idle jobs on CE: Is the job router handling the incoming job?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-verify-correct-operation-between-the-ce-and-your-local-batch-system", 
            "text": "", 
            "title": "Idle jobs on CE: Verify correct operation between the CE and your local batch system"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#for-htcondor-batch-systems", 
            "text": "HTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch system interaction will appear in the  JobRouterLog .  Next actions   Check the  JobRouterLog  for failures.  Verify that the local HTCondor is functional.  Use  condor_ce_config_val  to verify that the  JOB_ROUTER_SCHEDD2_NAME ,  JOB_ROUTER_SCHEDD2_POOL , and  JOB_ROUTER_SCHEDD2_SPOOL  configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively.  Use  condor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE  and verify that it is set to  .* .", 
            "title": "For HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#for-non-htcondor-batch-systems", 
            "text": "HTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch system interaction will appear in the  GridmanagerLog . Look for  gm state change\u2026  lines to figure out where the issures are occuring.  Next actions   If you see failures in the GridmanagerLog during job submission:  Save the submit files by adding the appropriate entry to  blah.config  and submit it  manually  to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the  batch system _binpath  in  /etc/blah.config .   If you see failures in the GridmanagerLog during queries for job status:  Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in  /usr/libexec/blahp/ batch system _status.sh  (e.g.,  /usr/libexec/blahp/lsf_status.sh ) that take the argument  batch system/YYYMMDD/job ID  (e.g.,  lsf/20141008/65053 ). Run the appropriate status script for your batch system and upon success, you should see the following output:  root@host #  /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053 [ BatchjobId =  894862 ; JobStatus = 4; ExitCode = 0; WorkerNode =  atl-prod08  ]   If the script fails,  request help  from the OSG.", 
            "title": "For non-HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-make-sure-the-underlying-batch-system-can-run-jobs", 
            "text": "HTCondor-CE communicates directly with an HTCondor batch system schedd, so if jobs are not running, examine the  SchedLog  and diagnose the problem from there. For other batch systems, the BLAHP is used to submit jobs using your batch system\u2019s job submission binaries, whose location is specified in  /etc/blah.config .  Procedure   Manually create and submit a simple job (e.g., one that runs  sleep )  Check for errors in the submission itself  Watch the job in the batch system queue (e.g., using  condor_q )  If the job does not run, check for errors on the batch system   Next actions  If the underlying batch system does not run a simple manual job, it will probably not run a job coming from HTCondor-CE. Once you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again.", 
            "title": "Idle jobs on CE: Make sure the underlying batch system can run jobs"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-verify-ability-to-change-permissions-on-key-files", 
            "text": "HTCondor-CE needs the ability to write and chown files in its  spool  directory and if it cannot, jobs will not run at all. Spool permission errors can appear in the  SchedLog  and the  JobRouterLog .  Symptoms  09/17/14 14:45:42 Error: Unable to chown  /var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env  from 12345 to 54321  Next actions   As root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions.  Verify that there aren't any underlying file system issues in the specified location", 
            "title": "Idle jobs on CE: Verify ability to change permissions on key files"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-stay-idle-on-a-remote-host-submitting-to-the-ce", 
            "text": "If you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not see a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not authorized to run there. Note that jobs may take several minutes or longer if the CE is busy.", 
            "title": "Jobs stay idle on a remote host submitting to the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#remote-idle-jobs-can-you-contact-the-ce", 
            "text": "To check basic connectivity to a CE, use  condor_ce_ping :  Symptoms  user@host $  condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE ERROR: couldn t locate condorce.example.com!   Next actions   Make sure that the HTCondor-CE daemons are running with  condor_ce_status .  Verify that your CE is reachable from your submit host, replacing  condorce.example.com  with the hostname of your CE: user@host $  ping condorce.example.com", 
            "title": "Remote idle jobs: Can you contact the CE?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#remote-idle-jobs-are-you-authorized-to-run-jobs-on-the-ce", 
            "text": "The CE will only accept jobs from users that authenticate via LCMAPS, grid mapfile, or GUMS. You can use  condor_ce_ping  to check if you are authorized and what user your proxy is being mapped to.  Symptoms  user@host $  condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE Remote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Local  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Session ID:                  condorce:3343:1412790611:0  Instruction:                 WRITE  Command:                     60021  Encryption:                  none  Integrity:                   MD5  Authenticated using:         GSI  All authentication methods:  GSI  Remote Mapping:              gsi@unmapped  Authorized:                  FALSE   Notice the failures in the above message:  Remote Mapping: gsi@unmapped  and  Authorized: FALSE  Next actions   Verify that an  authentication method  is set up on the CE  Verify that your user DN is mapped to an existing system user", 
            "title": "Remote idle jobs: Are you authorized to run jobs on the CE?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-go-on-hold", 
            "text": "Jobs will be put on held with a  HoldReason  attribute that can be inspected with  condor_ce_q :  user@host $  condor_ce_q -l  JOB-ID  -attr HoldReason HoldReason =  CE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to non-existent route or entry in JOB_ROUTER_ENTRIES.", 
            "title": "Jobs go on hold"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-missingexpired-user-proxy", 
            "text": "HTCondor-CE requires a valid user proxy for each job that is submitted. You can check the status of your proxy with the following  user@host $  voms-proxy-info -all  Next actions  Ensure that the owner of the job generates their proxy with  voms-proxy-init .", 
            "title": "Held jobs: Missing/expired user proxy"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-invalid-job-universe", 
            "text": "The HTCondor-CE only accepts jobs that have  universe  in their submit files set to  vanilla ,  standard ,  local , or  scheduler . These universes also have corresponding integer values that can be found in the  HTCondor manual .  Next actions   Ensure jobs submitted locally, from the CE host, are submitted with  universe = vanilla   Ensure jobs submitted from a remote submit point are submitted with:  universe = grid\ngrid_resource = condor condorce.example.com condorce.example.com:9619  replacing  condorce.example.com  with the hostname of the CE.", 
            "title": "Held jobs: Invalid job universe"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-non-existent-route-or-entry-in-job_router_entries", 
            "text": "Jobs on the CE will be put on hold if they do not match any job routes within 30 minutes.  Next actions  Use  condor_ce_job_router_info  to see why your idle job does not match any routes.", 
            "title": "Held jobs: Non-existent route or entry in JOB_ROUTER_ENTRIES"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#identifying-the-corresponding-job-id-on-the-local-batch-system", 
            "text": "When troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID and the resultant job ID on the batch system. The methods for finding the resultant job ID differs between batch systems.", 
            "title": "Identifying the corresponding job ID on the local batch system"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-batch-systems", 
            "text": "To inspect the CE\u2019s job ad, use  condor_ce_q  or  condor_ce_history :    Use  condor_ce_q  if the job is still in the CE\u2019s queue:  user@host $  condor_ce_q  JOB-ID  -af RoutedToJobId    Use  condor_ce_history  if the job has left the CE\u2019s queue:  user@host $  condor_ce_history  JOB-ID  -af RoutedToJobId      Parse the  JobRouterLog  for the CE\u2019s job ID.", 
            "title": "HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#non-htcondor-batch-systems", 
            "text": "When HTCondor-CE records the corresponding batch system job ID, it is written in the form  BATCH-SYSTEM / DATE / JOB ID :  lsf/20141206/482046    To inspect the CE\u2019s job ad, use  condor_ce_q :  user@host $  condor_ce_q  JOB-ID  -af GridJobId    Parse the  GridmanagerLog  for the CE\u2019s job ID.", 
            "title": "Non-HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-removed-from-the-local-htcondor-pool-become-resubmitted-htcondor-batch-systems-only", 
            "text": "By design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. Therefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps:   Identify the misbehaving job ID in your batch system queue   Find the job's corresponding CE job ID:  user@host $  condor_q  JOB-ID  -af RoutedFromJobId    Use  condor_ce_rm  to remove the CE job from the queue", 
            "title": "Jobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only)"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#missing-htcondor-tools", 
            "text": "Most of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. If you are trying to use HTCondor-CE tools and you see the following error:  user@host $  condor_ce_job_router_info /usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found   This means that the  condor_job_router_info  (note this is not the CE version), is not in your  PATH .  Next Actions   Either the condor RPM is missing or there are some other issues with it (try  rpm --verify condor ).  You have installed HTCondor in a non-standard location that is not in your  PATH .  The  condor_job_router_info  tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming).", 
            "title": "Missing HTCondor tools"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-tools", 
            "text": "HTCondor-CE has its own separate set of of the HTCondor tools with  ce  in the name (i.e.,  condor_ce_submit  vs  condor_submit ). Some of the the commands are only for the CE (e.g.,  condor_ce_run  and  condor_ce_trace ) but many of them are just HTCondor commands configured to interact with the CE (e.g.,  condor_ce_q ,  condor_ce_status ). It is important to differentiate the two:  condor_ce_config_val  will provide configuration values for your HTCondor-CE while  condor_config_val  will provide configuration values for your HTCondor batch system. If you are not running an HTCondor batch system, the non-CE commands will return errors.", 
            "title": "HTCondor-CE Troubleshooting Tools"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_trace", 
            "text": "", 
            "title": "condor_ce_trace"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage", 
            "text": "condor_ce_trace  is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job.   Note  You must have generated a proxy (e.g.,  voms-proxy-init ) and your DN must be added to your  chosen authentication method .   user@host $  condor_ce_trace condorce.example.com  Replacing the  condorce.example.com  with the hostname of the CE. If you are familiar with the output of condor commands, the command also takes a  --debug  option that displays verbose condor output.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting", 
            "text": "If the command fails with \u201cFailed ping\u2026\u201d:  Make sure that the HTCondor-CE daemons are running on the CE  If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line:  Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our  installation document .  If the job submits but does not complete:  Look at the status of the job and perform the relevant  troubleshooting steps .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_run", 
            "text": "", 
            "title": "condor_ce_run"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_1", 
            "text": "Similar to  globus-job-run ,  condor_ce_run  is a tool that submits a simple job to your CE, so it is useful for quickly submitting jobs through your CE. To submit a job to the CE and run the  env  command on the remote batch system:   Note  You must have generated a proxy (e.g.,  voms-proxy-init ) and your DN must be added to your  chosen authentication method .   user@host $  condor_ce_run -r condorce.example.com:9619 /bin/env  Replacing the  condorce.example.com  with the hostname of the CE. If you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you can run commands locally on the CE with  condor_ce_run  with the  -l  option. The following example outputs the JobRouterLog of the CE in question:  user@host $  condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog  Replacing the  condorce.example.com  text with the hostname of the CE. To disable this feature on your CE, consult  this  section of the install documentation.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_1", 
            "text": "If you do not see any results:   condor_ce_run  does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use  condor_ce_q  in a separate terminal to track the job on the CE. If you never see any results, use  condor_ce_trace  to pinpoint errors.  If you see an error message that begins with \u201cFailed to\u2026\u201d:  Check connectivity to the CE with  condor_ce_trace  or  condor_ce_ping", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_submit", 
            "text": "See the  submitting to HTCondor-CE  document for details.", 
            "title": "condor_ce_submit"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_ping", 
            "text": "", 
            "title": "condor_ce_ping"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_2", 
            "text": "Use the following  condor_ce_ping  command to test your ability to submit jobs to an HTCondor-CE, replacing  condorce.example.com  with the hostname of your CE:  user@host $  condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE  The following shows successful output where I am able to submit jobs ( Authorized: TRUE ) as the glow user ( Remote Mapping: glow@users.opensciencegrid.org ):  Remote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Local  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Session ID:                  condorce:27407:1412286981:3  Instruction:                 WRITE  Command:                     60021  Encryption:                  none  Integrity:                   MD5  Authenticated using:         GSI  All authentication methods:  GSI  Remote Mapping:              glow@users.opensciencegrid.org  Authorized:                  TRUE    Note  If you run the  condor_ce_ping  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_ping  takes the same arguments as  condor_ping  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_2", 
            "text": "If you see \u201cERROR: couldn\u2019t locate (null)\u201d , that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE:  MASTER_DEBUG = D_FULLDEBUG\nSCHEDD_DEBUG = D_FULLDEBUG  Then look in the  MasterLog  and  SchedLog  for any errors.    If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line , this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our  installation document .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_q", 
            "text": "", 
            "title": "condor_ce_q"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_3", 
            "text": "condor_ce_q  can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619  To inspect the full ClassAd for a specific job, specify the  -l  flag and the job ID:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l  JOB-ID    Note  If you run the  condor_ce_q  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_q  takes the same arguments as  condor_q  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_3", 
            "text": "If the jobs that you are submiting to a CE are not completing,  condor_ce_q  can tell you the status of your jobs.    If the schedd is not running:  You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with:  MASTER_DEBUG = D_FULLDEBUG\nSCHEDD_DEBUG = D_FULLDEBUG  To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig  Then look in the  MasterLog  and  SchedLog  on the CE for any errors.    If there are issues with contacting the collector:  You will see the following message:  user@host $  condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu -- Failed to fetch ads from:  129.59.197.223:9620?sock`33630_8b33_4  : ce1.accre.vanderbilt.edu   This may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the  ALLOW_READ  configuration value is not set:  user@host $  condor_ce_config_val -v ALLOW_READ Not defined: ALLOW_READ   If it is defined, remove it from the file that is returned in the output.    If a job is held:  There should be an accompanying  HoldReason  that will tell you why it is being held. The  HoldReason  is in the job\u2019s ClassAd, so you can use the long form of  condor_ce_q  to extract its value:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l  Job ID   |  grep HoldReason    If a job is idle:  The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the  condor_ce_job_router_info .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_history", 
            "text": "", 
            "title": "condor_ce_history"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_4", 
            "text": "condor_ce_history  can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE:  user@host $  condor_ce_history -name condorce.example.com -pool condorce.example.com:9619  To inspect the full ClassAd for a specific job, specify the  -l  flag and the job ID:  user@host $  condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l  Job ID    Note  If you run the  condor_ce_history  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_history  takes the same arguments as  condor_history  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_job_router_info", 
            "text": "", 
            "title": "condor_ce_job_router_info"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_5", 
            "text": "Use the  condor_ce_job_router_info  command to help troubleshoot your routes and how jobs will match to them. To see all of your routes (the output is long because it combines your routes with the  JOB_ROUTER_DEFAULTS  configuration variable):  root@host #  condor_ce_job_router_info -config  To see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of  condor_ce_q  (replace the  JOB-ID  with the job ID that you are interested in):  root@host #  condor_ce_q -l  JOB-ID   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -  To inspect a job that has already left the queue, use  condor_ce_history  instead of  condor_ce_q :  root@host #  condor_ce_history -l  JOB-ID   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -   Note  If the proxy for the job has expired, the job will not match any routes. To work around this constraint:   root@host #  condor_ce_history -l  JOB-ID   |  sed  s/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date  +1 sec `/   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -  Alternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file:  root@host #  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads  JOBAD-FILE", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_4", 
            "text": "If the job does not match any route:  You can identify this case when you see  0 candidate jobs found  in the  condor_job_router_info  output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to  true . When troubleshooting, look at all of the expressions prior to the  target.ProcId  = 0  expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again.   If your job matches more than one route:  the tool will tell you by showing all matching routes after the job ID:  Checking Job src=162,0 against all routes\nRoute Matches: Local_PBS\nRoute Matches: Condor_Test  To troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is  highlighted below :  Umbrella constraint: ((target.x509userproxysubject =!= UNDEFINED)  \n(target.x509UserProxyExpiration =!= UNDEFINED)  \n(time()   target.x509UserProxyExpiration)  \n(target.JobUniverse =?= 5 || target.JobUniverse =?= 1))   ( (target.osgTestPBS is true) || (true) )   \n(target.ProcId  = 0   target.JobStatus == 1  \n(target.StageInStart is undefined || target.StageInFinish isnt undefined)  \ntarget.Managed isnt  ScheddDone   \ntarget.Managed isnt  Extenal   \ntarget.Owner isnt Undefined  \ntarget.RoutedBy isnt  htcondor-ce )  Both routes evaluate to  true  for the job\u2019s ClassAd because it contained  osgTestPBS = true . Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the  job route configuration page  for more details.    If it is unclear why jobs are matching a route:  wrap the route's requirements expression in  debug()  and check the  JobRouterLog  for more information.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_router_q", 
            "text": "", 
            "title": "condor_ce_router_q"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_6", 
            "text": "If you have multiple job routes and many jobs,  condor_ce_router_q  is a useful tool to see how jobs are being routed and their statuses:  user@host $  condor_ce_router_q  condor_ce_router_q  takes the same options as  condor_router_q  and  condor_q  and is documented in the  HTCondor manual", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_status", 
            "text": "", 
            "title": "condor_ce_status"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_7", 
            "text": "To see the daemons running on a CE, you can run the following:  user@host $  condor_ce_status -any -name condorce.example.com -pool condorce.example.com:9619  Replacing  condorce.example.com   with the hostname of the CE.   Note  If you run the  condor_ce_status  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_status  takes the same arguments as  condor_status  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_5", 
            "text": "To list the daemons that are configured to run:  user@host $  condor_ce_config_val -v DAEMON_LIST DAEMON_LIST: MASTER COLLECTOR SCHEDD JOB_ROUTER, SHARED_PORT, SHARED_PORT    Defined in  /etc/condor-ce/config.d/03-ce-shared-port.conf , line 9.   If you do not see these daemons in the output of  condor_ce_status , check the  Master log  for errors.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_config_val", 
            "text": "", 
            "title": "condor_ce_config_val"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_8", 
            "text": "To see the value of configuration variables and where they are set, use  condor_ce_config_val . Primarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. To see the value of a single variable and where it is set:  user@host $  condor_ce_config_val -v  CONFIGURATION-VARIABLE   To see a list of all configuration variables and their values:  user@host $  condor_ce_config_val -dump  To see a list of all the files that are used to create your configuration and the order that they are parsed, use the following command:  user@host $  condor_ce_config_val -config  condor_ce_config_val  takes the same arguments as  condor_config_val  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_reconfig", 
            "text": "", 
            "title": "condor_ce_reconfig"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_9", 
            "text": "To ensure that your configuration changes have taken effect, run  condor_ce_reconfig .  user@host $  condor_ce_reconfig", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_onoffrestart", 
            "text": "", 
            "title": "condor_ce_{on,off,restart}"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_10", 
            "text": "To turn on/off/restart HTCondor-CE daemons, use the following commands:  root@host #  condor_ce_on root@host #  condor_ce_off root@host #  condor_ce_restart  The HTCondor-CE service uses the previous commands with default values. Using these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart:    If you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command:  root@host #  condor_ce_restart -fast  This will cause HTCondor-CE to restart and quickly reconnect to all running jobs.    If you need to stop running new jobs, run the following:  root@host #  condor_ce_off -peaceful  This will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-data", 
            "text": "The following files are located on the CE host.", 
            "title": "HTCondor-CE Troubleshooting Data"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#masterlog", 
            "text": "The HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if they fail to start.   Location:  /var/log/condor-ce/MasterLog  Key contents: Start-up, shut-down, and communication with other HTCondor daemons   Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  MASTER_DEBUG = D_FULLDEBUG    To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "MasterLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for", 
            "text": "Successful daemon start-up. The following line shows that the Collector daemon started successfully:  10/07/14 14:20:27 Started DaemonCore process  /usr/sbin/condor_collector -f -port 9619 , pid and pgroup = 7318", 
            "title": "What to look for:"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#schedlog", 
            "text": "The HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. It contains valuable information when trying to troubleshoot authentication issues.   Location:  /var/log/condor-ce/SchedLog  Key contents:  Every job submitted to the CE  User authorization events     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  SCHEDD_DEBUG = D_FULLDEBUG    To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "SchedLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_1", 
            "text": "Job is submitted to the CE queue:  10/07/14 16:52:17 Submitting new job 234.0  In this example, the ID of the submitted job is  234.0 .    Job owner is authorized and mapped:  10/07/14 16:52:17 Command=QMGMT_WRITE_CMD, peer= 131.225.154.68:42262 \n10/07/14 16:52:17 AuthMethod=GSI, AuthId=/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Lin 1047,\n                  /GLOW/Role=NULL/Capability=NULL,  CondorId=glow@users.opensciencegrid.org   In this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the  glow  user.    User job submission fails  due to improper authentication or authorization:  08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189\n                  failed: AUTHENTICATE:1003:Failed to authenticate with any\n                  method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to\n                  authenticate because the remote (client) side was not able to acquire its\n                  credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to\n                  lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to  gsi@unmapped \n                  from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE:\n                  reason: WRITE authorization policy contains no matching ALLOW entry for this\n                  request; identifiers used for this host:\n                  72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip\n                  address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done    Missing negotiator:  10/18/14 17:32:21 Can t find address for negotiator\n10/18/14 17:32:21 Failed to send RESCHEDULE to unknown daemon:  Since HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes:  06/23/15 11:15:03 Number of Active Workers 0    Corrupted  job_queue.log :  02/07/17 10:55:49 WARNING: Encountered corrupt log record _654 (byte offset 5046225)\n02/07/17 10:55:49 103 1354325.0 PeriodicRemove ( StageInFinish   0 ) 105\n02/07/17 10:55:49 Lines following corrupt log record _654 (up to 3):\n02/07/17 10:55:49 103 1346101.0 RemoteWallClockTime 116668.000000\n02/07/17 10:55:49 104 1346101.0 WallClockCheckpoint\n02/07/17 10:55:49 104 1346101.0 ShadowBday\n02/07/17 10:55:49 ERROR  Error: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction,\n                  recovery failed  at line 1080 in file /builddir/build/BUILD/condor-8.4.8/src/condor_utils/classad_log.cpp  This means  /var/lib/condor-ce/spool/job_queue.log  has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the  Lines following corrupt log record...  line. The most common culprit of the corruption is that the disk containing the  job_queue.log  has filled up. To avoid this problem, you can change the location of  job_queue.log  by setting  JOB_QUEUE_LOG  in  /etc/condor-ce/config.d/  to a path, preferably one on a large SSD.", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobrouterlog", 
            "text": "The HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to troubleshoot issues with job routing.   Location:  /var/log/condor-ce/JobRouterLog  Key contents:  Every attempt to route a job  Routing success messages  Job attribute changes, based on chosen route  Job submission errors to an HTCondor batch system  Corresponding job IDs on an HTCondor batch system     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  JOB_ROUTER_DEBUG = D_FULLDEBUG    Apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "JobRouterLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#known-errors", 
            "text": "If you have  D_FULLDEBUG  turned on for the job router, you will see errors like the following:  06/12/15 14:00:28 HOOK_UPDATE_JOB_INFO not configured.  You can safely ignore these.", 
            "title": "Known Errors"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_2", 
            "text": "Job is considered for routing:  09/17/14 15:00:56 JobRouter (src=86.0,route=Local_LSF): found candidate job  In parentheses are the original HTCondor-CE job ID (e.g.,  86.0 ) and the route (e.g.,  Local_LSF ).    Job is successfully routed:  09/17/14 15:00:57 JobRouter (src=86.0,route=Local_LSF): claimed job    Finding the corresponding job ID on your HTCondor batch system:  09/17/14 15:00:57 JobRouter (src=86.0,dest=205.0,route=Local_Condor): claimed job  In parentheses are the original HTCondor-CE job ID (e.g.,  86.0 ) and the resultant job ID on the HTCondor batch system (e.g.,  205.0 )    If your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the  condor_ce_job_router_info   HTCondor batch systems only : The following error occurs when the job router daemon cannot submit the routed job: 10/19/14 13:09:15 Can t resolve collector condorce.example.com; skipping\n10/19/14 13:09:15 ERROR (pool condorce.example.com) Can t find address of schedd\n10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#gridmanagerlog", 
            "text": "The HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. It contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. Details on how to read the Gridmanager log can be found on the  HTCondor Wiki .   Location:  /var/log/condor-ce/GridmanagerLog. JOB-OWNER  Key contents:  Every attempt to submit a job to a batch system or other grid resource  Status updates of submitted jobs  Corresponding job IDs on non-HTCondor batch systems     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  MAX_GRIDMANAGER_LOG = 6h\nMAX_NUM_GRIDMANAGER_LOG = 8\nGRIDMANAGER_DEBUG = D_FULLDEBUG    To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "GridmanagerLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_3", 
            "text": "Job is submitted to the batch system:  09/17/14 09:51:34 [12997] (85.0) gm state change: GM_SUBMIT_SAVE -  GM_SUBMITTED  Every state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)).    Job status being updated:  09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -  GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563]  -  BLAH_JOB_STATUS 3 lsf/20140917/482046 \n09/17/14 15:07:24 [25543] GAHP[25563] -   S \n09/17/14 15:07:25 [25543] GAHP[25563]  -  RESULTS \n09/17/14 15:07:25 [25543] GAHP[25563] -   R \n09/17/14 15:07:25 [25543] GAHP[25563] -   S   1 \n09/17/14 15:07:25 [25543] GAHP[25563] -   3   0   No Error   4   [ BatchjobId =  482046 ; JobStatus = 4; ExitCode = 0; WorkerNode =  atl-prod08  ]   The first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here.    Finding the corresponding job ID on your non-HTCondor batch system:  09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -  GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563]  -  BLAH_JOB_STATUS 3 lsf/20140917/482046   On the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses,  (87.0) . At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes,  lsf/20140917/482046 .    Job completion on the batch system:  09/17/14 15:07:25 [25543] (87.0) gm state change: GM_TRANSFER_OUTPUT -  GM_DONE_SAVE", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#sharedportlog", 
            "text": "The HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the collector. This log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found  here .   Location:  /var/log/condor-ce/SharedPortLog  Key contents: Every attempt to connect to HTCondor-CE (except collector queries)   Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  SHARED_PORT_DEBUG = D_FULLDEBUG    To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "SharedPortLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#messages-log", 
            "text": "The messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. If there are issues with the  authentication setup , the errors may appear here.   Location:  /var/log/messages  Key contents: User authentication", 
            "title": "Messages log"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_4", 
            "text": "A user is mapped:  Oct 6 10:35:32 osgserv06 htondor-ce-llgt[12147]: Callout to  LCMAPS  returned local user (service condor):  osgglow01", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#blahp-configuration-file", 
            "text": "HTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client tools.   Location:  /etc/blah.config  Key contents:  Locations of the batch system's client binaries and logs  Location to save files that are submitted to the local batch system     You can also tell the BLAHP to save the files that are being submitted to the local batch system to  DIR-NAME  by adding the following line:  blah_debug_save_submit_info= DIR_NAME   The BLAHP will then create a directory with the format  bl_*  for each submission to the local jobmanager with the submit file and proxy used.   Note  Whitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within  DIR_NAME .", 
            "title": "BLAHP Configuration File"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#getting-help", 
            "text": "If you are still experiencing issues after using this document, please let us know!   Gather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.)   Gather system information:  root@host # osg-system-profiler    Start a support request using  a web interface  or by email to    Describe issue and expected or desired behavior  Include basic HTCondor-CE and related information  Attach the osg-system-profiler output", 
            "title": "Getting Help"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  Configuring HTCondor-CE job routes  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/", 
            "text": "Submitting Jobs to an HTCondor-CE\n\n\nThis document outlines methods of manual submission to an HTCondor-CE. It is intended for site administrators wishing to verify the functionality of their HTCondor-CE installation and developers writing software to submit jobs to an HTCondor-CE (e.g., pilot jobs).\n\n\n\n\nNote\n\n\nMost incoming jobs are pilots from factories and that manual submission does not reflect the standard method that jobs are submitted to OSG CE\u2019s.\n\n\n\n\nSubmitting Jobs...\n\n\nThere are two main methods for submitting files to an HTCondor-CE: using the tools bundled with the \nhtcondor-ce-client\n package and using the \ncondor_submit\n command with a submit file. Both methods will test end-to-end job submission but the former method is simpler while the latter will walk you through writing your own submit file.\n\n\nBefore attempting to submit jobs, you will need to generate a proxy from a user certificate before running any jobs. To generate a proxy, run the following command on the host you plan on submitting from:\n\n\nuser@host $\n voms-proxy-init\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nThere are two HTCondor-CE tools that allow users to test the functionality of their HTCondor-CE: \ncondor_ce_trace\n and \ncondor_ce_run\n. The former is the preferred tool as it provides useful feedback if failure occurs while the latter is simply an automated submission tool. These commands may be run from any host that has \nhtcondor-ce-client\n installed, which you may wish to do if you are testing availability of your CE from an external source.\n\n\ncondor_ce_trace\n\n\ncondor_ce_trace\n is a Python script that uses HTCondor's Python bindings to run diagnostics, including job submission, against your HTCondor-CE. To submit a job with \ncondor_ce_trace\n, run the following command:\n\n\nuser@host $\n condor_ce_trace --debug condorce.example.com\n\n\n\n\n\nReplacing \ncondorce.example.com\n with the hostname of the CE you wish to test. On success, you will see \nJob status: Completed\n and the environment of the job on the worker node it landed on. If you do not get the expected output, refer to the \ntroubleshooting guide\n.\n\n\nRequesting resources\n\n\ncondor_ce_trace\n doesn't make any specific resource requests so its jobs are only given the default resources by the CE. To request specific resources (or other job attributes), you can specify the \n--attribute\n option on the command line:\n\n\nuser@host $\n condor_ce_trace --debug --attribute\n=\n+resource1=value1\n...--attribute\n=\n+resourceN=valueN\n condorce.example.com\n\n\n\n\n\nTo submit a job that requests 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command:\n\n\nuser@host $\n condor_ce_trace --debug --attribute\n=\n+xcount=4\n --attribute\n=\n+maxMemory=4000\n --attribute\n=\n+maxWallTime=120\n --attribute\n=\n+remote_queue=osg\n condorce.example.com\n\n\n\n\n\nFor a list of other attributes that can be set with the \n--attribute\n option, consult the \njob attributes\n section.\n\n\ncondor_ce_run\n\n\ncondor_ce_run\n is a Python script that calls \ncondor_submit\n on a generated submit file and tracks its progress with \ncondor_q\n. To submit a job with \ncondor_ce_run\n, run the following command:\n\n\nuser@host $\n condor_ce_run -r condorce.example.com:9619 /bin/env\n\n\n\n\n\nReplacing \ncondorce.example.com\n with the hostname of the CE you wish to test. The command will not return any output until it completes: When it does you will see the environment of the job on the worker noded it landed on. If you do not get the expected output, refer to the \ntroubleshooting guide\n.\n\n\nUsing a submit file...\n\n\nIf you are familiar with HTCondor, submitting a job to an HTCondor-CE using a submit file follows the same procedure as submitting a job to an HTCondor batch system: Write a submit file and use \ncondor_submit\n (or in one of our cases, \ncondor_ce_submit\n) to submit the job. This is by virtue of the fact that HTCondor-CE is just a special configuration of HTCondor. The major differences occur in the specific attributes for the submit files outlined below.\n\n\nFrom the CE host\n\n\nThis method uses \ncondor_ce_submit\n to submit directly to an HTCondor-CE. The only reason we use \ncondor_ce_submit\n in this case is to take advantage of the already running daemons on the CE host.\n\n\n\n\n\n\nWrite a submit file, \nce_test.sub\n:\n\n\n# Required for local HTCondor-CE submission\nuniverse = vanilla\nuse_x509userproxy = true\n+Owner = undefined\n\n# Files\nexecutable = \nce_test.sh\n\noutput = ce_test.out\nerror = ce_test.err\nlog = ce_test.log\n\n# File transfer behavior\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n\n# Optional resource requests\n#+xcount = 4            # Request 4 cores\n#+maxMemory = 4000      # Request 4GB of RAM\n#+maxWallTime = 120     # Request 2 hrs of wall clock time\n#+remote_queue = \nosg\n  # Request the OSG queue\n\n# Run job once\nqueue\n\n\n\n\n\nReplacing \nce_test.sh\n with the path to the executable you wish to run.\n\n\n\n\n\n\nYou can use any executable you choose for the \nexecutable\n field. If you don't have one in mind, you may use the following example test script:\n\n\n1\n2\n3\n4\n5\n#!/bin/bash\n\n\ndate\nhostname\nenv\n\n\n\n\n\n\n\n\n\n\nMark the test script as executable:\n\n\nuser@host $\n chmod +x ce_test.sh\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit the job:\n\n\nuser@host $\n condor_ce_submit ce_test.sub\n\n\n\n\n\n\n\n\n\nFrom another host\n\n\nFor this method, you will need a functional HTCondor submit node. If you do not have one readily available, you can install the \ncondor\n package from the OSG repository to get a simple submit node:\n\n\n\n\n\n\nInstall HTCondor:\n\n\nroot@host #\n yum install condor\n\n\n\n\n\n\n\n\n\nStart the \ncondor\n service:\n\n\nroot@host $\n service condor start\n\n\n\n\n\n\n\n\n\nOnce the \ncondor\n service is running, write a submit file and submit your job:\n\n\n\n\n\n\nWrite a submit file, \nce_test.sub\n:\n\n\n# Required for remote HTCondor-CE submission\nuniverse = grid\nuse_x509userproxy = true\ngrid_resource = condor \ncondorce.example.com condorce.example.com\n:9619\n\n# Files\nexecutable = \nce_test.sh\n\noutput = ce_test.out\nerror = ce_test.err\nlog = ce_test.log\n\n# File transfer behavior\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n\n# Optional resource requests\n#+xcount = 4            # Request 4 cores\n#+maxMemory = 4000      # Request 4GB of RAM\n#+maxWallTime = 120     # Request 2 hrs of wall clock time\n#+remote_queue = \nosg\n  # Request the OSG queue\n\n# Run job once\nqueue\n\n\n\n\n\n\n\nNote\n\n\nThe \ngrid_resource\n line should start with \ncondor\n and is not related to which batch system you are using.\n\n\n\n\n\n\n\n\nYou can use any executable you choose for the \nexecutable\n field. If you don't have one in mind, you may use the following example test script:\n\n\n1\n2\n3\n4\n5\n#!/bin/bash\n\n\ndate\nhostname\nenv\n\n\n\n\n\n\n\n\n\n\nMark the test script as executable:\n\n\nuser@host $\n chmod +x ce_test.sh\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit the job:\n\n\nuser@host $\n condor_submit ce_test.sub\n\n\n\n\n\n\n\n\n\nTracking job progress\n\n\nWhen the job completes, stdout will be placed into \nce_test.out\n, stderr will be placed into \nce_test.err\n, and HTCondor logging information will be placed in \nce_test.log\n. You can track job progress by looking at the condor queue by running the following command on the CE host:\n\n\nuser@host $\n condor_ce_q\n\n\n\n\n\nUsing the following table to determine job status:\n\n\n\n\n\n\n\n\nThis value in the \nST\n column...\n\n\nMeans that the job is...\n\n\n\n\n\n\n\n\n\n\nI\n\n\nidle\n\n\n\n\n\n\nC\n\n\ncomplete\n\n\n\n\n\n\nX\n\n\nbeing removed\n\n\n\n\n\n\nH\n\n\nheld\n\n\n\n\n\n\n\n\ntransferring input\n\n\n\n\n\n\n\n\ntransferring output\n\n\n\n\n\n\n\n\nHow Job Routes Affect Your Job\n\n\nUpon successful submission of your job, the Job Router takes control of your job by matching it to routes and submitting a transformed job to your batch system.\n\n\nMatching\n\n\nFirst, the Job Router checks if your job \nmatches any routes\n. It does this by checking the routes \nRequirements\n expression against the job and selecting the first match. If your job does not match any routes, the job will be put on hold and eventually removed from the CE queue without completing.\n\n\n\n\nNote\n\n\nFor versions of HTCondor \n 8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\n\n\nExamples\n\n\nThe following three routes only perform filtering and submission of routed jobs to an HTCondor batch system. The only differences are in the types of jobs that they match:\n\n\n\n\nRoute 1:\n Matches jobs whose attribute \nfoo\n is equal to \nbar\n.\n\n\nRoute 2:\n Matches jobs whose attribute \nfoo\n is equal to \nbaz\n.\n\n\nRoute 3:\n Matches jobs whose attribute \nfoo\n is neither equal to \nbar\n nor \nbaz\n.\n\n\n\n\n\n\nNote\n\n\nSetting a custom attribute for job submission requires the \n+\n prefix in your submit file but it is unnecessary in the job routes.\n\n\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nRoute 1\n; \\\n     Requirements = (TARGET.foo =?= \nbar\n); \\\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name = \nRoute 2\n; \\\n     Requirements = (TARGET.foo =?= \nbaz\n); \\\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name = \nRoute 3\n; \\\n     Requirements = (TARGET.foo =!= \nbar\n) \n (TARGET.foo =!= \nbaz\n); \\\n]\n\n\n\n\n\nIf a user could submitted their job with \n+foo=bar\n, the job would match \nRoute 1\n.\n\n\nRoute defaults\n\n\nRoute defaults\n can be set for batch system queue, maximum memory, number of cores to request, and maximum walltime. The submitting user can override any of these by setting the corresponding \nattribute\n in their job.\n\n\nExamples\n\n\nThe following route takes all incoming jobs and submits them to an HTCondor batch system requesting 1GB of memory.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nRoute 1\n; \\\n     set_default_maxMemory = 1000; \\\n]\n\n\n\n\n\nA user could submit their job with the attribute \n+maxMemory=2000\n and that job would be submitted requesting 2GB memory instead of the default of 1GB.\n\n\nTroubleshooting Your Jobs\n\n\nTroubleshooting\n\n\nAll interactions between \ncondor_submit\n and the CE will be recorded in the file specified by the \nlog\n attribute in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion:\n\n\n000 (786.000.000) 12/09 16:49:55 Job submitted from host: \n131.225.154.68:53134\n\n...\n027 (786.000.000) 12/09 16:50:09 Job submitted to grid resource\n    GridResource: condor condorce.example.com condorce.example.com:9619\n    GridJobId: condor condorce.example.com condorce.example.com:9619 796.0\n...\n005 (786.000.000) 12/09 16:52:19 Job terminated.\n        (1) Normal termination (return value 0)\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n        0  -  Run Bytes Sent By Job\n        0  -  Run Bytes Received By Job\n        0  -  Total Bytes Sent By Job\n        0  -  Total Bytes Received By Job\n\n\n\n\n\nIf there are issues contacting the CE, you will see error messages about a 'Down Globus Resource':\n\n\n020 (788.000.000) 12/09 16:56:17 Detected Down Globus Resource\n    RM-Contact: fermicloud133.fnal.gov\n...\n026 (788.000.000) 12/09 16:56:17 Detected Down Grid Resource\n    GridResource: condor condorce.example.com condorce.example.com:9619\n\n\n\n\n\nThis indicates a communication issue with your CE that can be diagnosed with \ncondor_ce_ping\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\n\n\nJob attributes\n\n\nThe following table is a reference of job attributes that can be included in HTCondor submit files and their GlobusRSL equivalents. A more comprehensive list of submit file attributes specific to HTCondor-CE can be found in the \nHTCondor manual\n.\n\n\n\n\n\n\n\n\nHTCondor Attribute\n\n\nGlobus RSL\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\narguments\n\n\narguments\n\n\nArguments that will be provided to the executable for the job.\n\n\n\n\n\n\nerror\n\n\nstderr\n\n\nPath to the file on the client machine that stores stderr from the job.\n\n\n\n\n\n\nexecutable\n\n\nexecutable\n\n\nPath to the file on the client machine that the job will execute.\n\n\n\n\n\n\ninput\n\n\nstdin\n\n\nPath to the file on the client machine that stores input to be piped into the stdin of the job.\n\n\n\n\n\n\n+maxMemory\n\n\nmaxMemory\n\n\nThe amount of memory in MB that you wish to allocate to the job.\n\n\n\n\n\n\n+maxWallTime\n\n\nmaxWallTime\n\n\nThe maximum walltime (in minutes) the job is allowed to run before it is removed.\n\n\n\n\n\n\noutput\n\n\nstdout\n\n\nPath to the file on the client machine that stores stdout from the job.\n\n\n\n\n\n\n+remote_queue\n\n\nqueue\n\n\nAssign job to the target queue in the scheduler. Note that the queue name should be in quotes.\n\n\n\n\n\n\ntransfer_input_files\n\n\nfile_stage_in\n\n\nA comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started.\n\n\n\n\n\n\ntransfer_output_files\n\n\ntransfer_output_files\n\n\nA comma-delimited list of all the files and directories to be transferred back to the client, after the job completes.\n\n\n\n\n\n\n+xcount\n\n\nxcount\n\n\nThe number of cores to allocate for the job.\n\n\n\n\n\n\n\n\nIf you are setting an attribute to a string value, make sure enclose the string in double-quotes (\n\"\n), otherwise HTCondor-CE will try to find an attribute by that name.", 
            "title": "Submitting to HTCondor-CE"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#submitting-jobs-to-an-htcondor-ce", 
            "text": "This document outlines methods of manual submission to an HTCondor-CE. It is intended for site administrators wishing to verify the functionality of their HTCondor-CE installation and developers writing software to submit jobs to an HTCondor-CE (e.g., pilot jobs).   Note  Most incoming jobs are pilots from factories and that manual submission does not reflect the standard method that jobs are submitted to OSG CE\u2019s.", 
            "title": "Submitting Jobs to an HTCondor-CE"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#submitting-jobs", 
            "text": "There are two main methods for submitting files to an HTCondor-CE: using the tools bundled with the  htcondor-ce-client  package and using the  condor_submit  command with a submit file. Both methods will test end-to-end job submission but the former method is simpler while the latter will walk you through writing your own submit file.  Before attempting to submit jobs, you will need to generate a proxy from a user certificate before running any jobs. To generate a proxy, run the following command on the host you plan on submitting from:  user@host $  voms-proxy-init", 
            "title": "Submitting Jobs..."
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#using-htcondor-ce-tools", 
            "text": "There are two HTCondor-CE tools that allow users to test the functionality of their HTCondor-CE:  condor_ce_trace  and  condor_ce_run . The former is the preferred tool as it provides useful feedback if failure occurs while the latter is simply an automated submission tool. These commands may be run from any host that has  htcondor-ce-client  installed, which you may wish to do if you are testing availability of your CE from an external source.", 
            "title": "Using HTCondor-CE tools"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#condor_ce_trace", 
            "text": "condor_ce_trace  is a Python script that uses HTCondor's Python bindings to run diagnostics, including job submission, against your HTCondor-CE. To submit a job with  condor_ce_trace , run the following command:  user@host $  condor_ce_trace --debug condorce.example.com  Replacing  condorce.example.com  with the hostname of the CE you wish to test. On success, you will see  Job status: Completed  and the environment of the job on the worker node it landed on. If you do not get the expected output, refer to the  troubleshooting guide .", 
            "title": "condor_ce_trace"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#requesting-resources", 
            "text": "condor_ce_trace  doesn't make any specific resource requests so its jobs are only given the default resources by the CE. To request specific resources (or other job attributes), you can specify the  --attribute  option on the command line:  user@host $  condor_ce_trace --debug --attribute = +resource1=value1 ...--attribute = +resourceN=valueN  condorce.example.com  To submit a job that requests 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command:  user@host $  condor_ce_trace --debug --attribute = +xcount=4  --attribute = +maxMemory=4000  --attribute = +maxWallTime=120  --attribute = +remote_queue=osg  condorce.example.com  For a list of other attributes that can be set with the  --attribute  option, consult the  job attributes  section.", 
            "title": "Requesting resources"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#condor_ce_run", 
            "text": "condor_ce_run  is a Python script that calls  condor_submit  on a generated submit file and tracks its progress with  condor_q . To submit a job with  condor_ce_run , run the following command:  user@host $  condor_ce_run -r condorce.example.com:9619 /bin/env  Replacing  condorce.example.com  with the hostname of the CE you wish to test. The command will not return any output until it completes: When it does you will see the environment of the job on the worker noded it landed on. If you do not get the expected output, refer to the  troubleshooting guide .", 
            "title": "condor_ce_run"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#using-a-submit-file", 
            "text": "If you are familiar with HTCondor, submitting a job to an HTCondor-CE using a submit file follows the same procedure as submitting a job to an HTCondor batch system: Write a submit file and use  condor_submit  (or in one of our cases,  condor_ce_submit ) to submit the job. This is by virtue of the fact that HTCondor-CE is just a special configuration of HTCondor. The major differences occur in the specific attributes for the submit files outlined below.", 
            "title": "Using a submit file..."
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#from-the-ce-host", 
            "text": "This method uses  condor_ce_submit  to submit directly to an HTCondor-CE. The only reason we use  condor_ce_submit  in this case is to take advantage of the already running daemons on the CE host.    Write a submit file,  ce_test.sub :  # Required for local HTCondor-CE submission\nuniverse = vanilla\nuse_x509userproxy = true\n+Owner = undefined\n\n# Files\nexecutable =  ce_test.sh \noutput = ce_test.out\nerror = ce_test.err\nlog = ce_test.log\n\n# File transfer behavior\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n\n# Optional resource requests\n#+xcount = 4            # Request 4 cores\n#+maxMemory = 4000      # Request 4GB of RAM\n#+maxWallTime = 120     # Request 2 hrs of wall clock time\n#+remote_queue =  osg   # Request the OSG queue\n\n# Run job once\nqueue  Replacing  ce_test.sh  with the path to the executable you wish to run.    You can use any executable you choose for the  executable  field. If you don't have one in mind, you may use the following example test script:  1\n2\n3\n4\n5 #!/bin/bash \n\ndate\nhostname\nenv     Mark the test script as executable:  user@host $  chmod +x ce_test.sh      Submit the job:  user@host $  condor_ce_submit ce_test.sub", 
            "title": "From the CE host"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#from-another-host", 
            "text": "For this method, you will need a functional HTCondor submit node. If you do not have one readily available, you can install the  condor  package from the OSG repository to get a simple submit node:    Install HTCondor:  root@host #  yum install condor    Start the  condor  service:  root@host $  service condor start    Once the  condor  service is running, write a submit file and submit your job:    Write a submit file,  ce_test.sub :  # Required for remote HTCondor-CE submission\nuniverse = grid\nuse_x509userproxy = true\ngrid_resource = condor  condorce.example.com condorce.example.com :9619\n\n# Files\nexecutable =  ce_test.sh \noutput = ce_test.out\nerror = ce_test.err\nlog = ce_test.log\n\n# File transfer behavior\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n\n# Optional resource requests\n#+xcount = 4            # Request 4 cores\n#+maxMemory = 4000      # Request 4GB of RAM\n#+maxWallTime = 120     # Request 2 hrs of wall clock time\n#+remote_queue =  osg   # Request the OSG queue\n\n# Run job once\nqueue   Note  The  grid_resource  line should start with  condor  and is not related to which batch system you are using.     You can use any executable you choose for the  executable  field. If you don't have one in mind, you may use the following example test script:  1\n2\n3\n4\n5 #!/bin/bash \n\ndate\nhostname\nenv     Mark the test script as executable:  user@host $  chmod +x ce_test.sh      Submit the job:  user@host $  condor_submit ce_test.sub", 
            "title": "From another host"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#tracking-job-progress", 
            "text": "When the job completes, stdout will be placed into  ce_test.out , stderr will be placed into  ce_test.err , and HTCondor logging information will be placed in  ce_test.log . You can track job progress by looking at the condor queue by running the following command on the CE host:  user@host $  condor_ce_q  Using the following table to determine job status:     This value in the  ST  column...  Means that the job is...      I  idle    C  complete    X  being removed    H  held     transferring input     transferring output", 
            "title": "Tracking job progress"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#how-job-routes-affect-your-job", 
            "text": "Upon successful submission of your job, the Job Router takes control of your job by matching it to routes and submitting a transformed job to your batch system.", 
            "title": "How Job Routes Affect Your Job"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#matching", 
            "text": "First, the Job Router checks if your job  matches any routes . It does this by checking the routes  Requirements  expression against the job and selecting the first match. If your job does not match any routes, the job will be put on hold and eventually removed from the CE queue without completing.   Note  For versions of HTCondor   8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.   Examples  The following three routes only perform filtering and submission of routed jobs to an HTCondor batch system. The only differences are in the types of jobs that they match:   Route 1:  Matches jobs whose attribute  foo  is equal to  bar .  Route 2:  Matches jobs whose attribute  foo  is equal to  baz .  Route 3:  Matches jobs whose attribute  foo  is neither equal to  bar  nor  baz .    Note  Setting a custom attribute for job submission requires the  +  prefix in your submit file but it is unnecessary in the job routes.   JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Route 1 ; \\\n     Requirements = (TARGET.foo =?=  bar ); \\\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name =  Route 2 ; \\\n     Requirements = (TARGET.foo =?=  baz ); \\\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name =  Route 3 ; \\\n     Requirements = (TARGET.foo =!=  bar )   (TARGET.foo =!=  baz ); \\\n]  If a user could submitted their job with  +foo=bar , the job would match  Route 1 .", 
            "title": "Matching"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#route-defaults", 
            "text": "Route defaults  can be set for batch system queue, maximum memory, number of cores to request, and maximum walltime. The submitting user can override any of these by setting the corresponding  attribute  in their job.  Examples  The following route takes all incoming jobs and submits them to an HTCondor batch system requesting 1GB of memory.  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Route 1 ; \\\n     set_default_maxMemory = 1000; \\\n]  A user could submit their job with the attribute  +maxMemory=2000  and that job would be submitted requesting 2GB memory instead of the default of 1GB.", 
            "title": "Route defaults"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#troubleshooting-your-jobs", 
            "text": "", 
            "title": "Troubleshooting Your Jobs"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#troubleshooting", 
            "text": "All interactions between  condor_submit  and the CE will be recorded in the file specified by the  log  attribute in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion:  000 (786.000.000) 12/09 16:49:55 Job submitted from host:  131.225.154.68:53134 \n...\n027 (786.000.000) 12/09 16:50:09 Job submitted to grid resource\n    GridResource: condor condorce.example.com condorce.example.com:9619\n    GridJobId: condor condorce.example.com condorce.example.com:9619 796.0\n...\n005 (786.000.000) 12/09 16:52:19 Job terminated.\n        (1) Normal termination (return value 0)\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n        0  -  Run Bytes Sent By Job\n        0  -  Run Bytes Received By Job\n        0  -  Total Bytes Sent By Job\n        0  -  Total Bytes Received By Job  If there are issues contacting the CE, you will see error messages about a 'Down Globus Resource':  020 (788.000.000) 12/09 16:56:17 Detected Down Globus Resource\n    RM-Contact: fermicloud133.fnal.gov\n...\n026 (788.000.000) 12/09 16:56:17 Detected Down Grid Resource\n    GridResource: condor condorce.example.com condorce.example.com:9619  This indicates a communication issue with your CE that can be diagnosed with  condor_ce_ping .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#job-attributes", 
            "text": "The following table is a reference of job attributes that can be included in HTCondor submit files and their GlobusRSL equivalents. A more comprehensive list of submit file attributes specific to HTCondor-CE can be found in the  HTCondor manual .     HTCondor Attribute  Globus RSL  Summary      arguments  arguments  Arguments that will be provided to the executable for the job.    error  stderr  Path to the file on the client machine that stores stderr from the job.    executable  executable  Path to the file on the client machine that the job will execute.    input  stdin  Path to the file on the client machine that stores input to be piped into the stdin of the job.    +maxMemory  maxMemory  The amount of memory in MB that you wish to allocate to the job.    +maxWallTime  maxWallTime  The maximum walltime (in minutes) the job is allowed to run before it is removed.    output  stdout  Path to the file on the client machine that stores stdout from the job.    +remote_queue  queue  Assign job to the target queue in the scheduler. Note that the queue name should be in quotes.    transfer_input_files  file_stage_in  A comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started.    transfer_output_files  transfer_output_files  A comma-delimited list of all the files and directories to be transferred back to the client, after the job completes.    +xcount  xcount  The number of cores to allocate for the job.     If you are setting an attribute to a string value, make sure enclose the string in double-quotes ( \" ), otherwise HTCondor-CE will try to find an attribute by that name.", 
            "title": "Job attributes"
        }, 
        {
            "location": "/data/gridftp/", 
            "text": "Installing and Maintaining a GridFTP Server\n\n\nAbout This Guide\n\n\nThis page explains how to install the stand-alone Globus GridFTP server.\n\n\nThe GridFTP package contains components necessary to set up a stand-alone gsiftp server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances:\n\n\n\n\nYou are serving VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and your site has more than 250 cores\n\n\nYour site will be managing more than 50 TB of disk space\n\n\nA simple front-end to a filesystem allowing access over WAN - for example NFS.\n\n\n\n\n\n\nNote\n\n\nThis document is for a standalone GridFTP server on top of POSIX storage.  \nSee this page\n for installation and configuration of a GridFTP server on top of the Hadoop Distributed File System.\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process you will need to fulfill these prerequisites.\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\nService certificate: The GridFTP service uses a host certificate at \n/etc/grid-security/hostcert.pem\n and an accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nNetwork ports: GridFTP listens on TCP port 2811 and the list of ports configured by the \nGLOBUS_TCP_SOURCE_RANGE\n environment variable.\n\n\n\n\nInstalling GridFTP\n\n\nFirst, you will need to install the GridFTP meta-package:\n\n\n[root@client ~] #\n yum install osg-gridftp\n\n\n\n\n\nConfiguring GridFTP\n\n\nConfiguring authentication\n\n\nIn OSG 3.3, there are three methods to manage authentication for incoming jobs: the \nLCMAPS VOMS plugin\n, \nedg-mkgridmap\n and \nGUMS\n. Of these, GUMS has the most features and capabilities. The LCMAPS VOMS plugin is the new OSG-preferred authentication, offering the simplicity of edg-mkgridmap and many of GUMS' rich feature set. If you need to support \"\npool accounts\n\", GUMS is the only option with that capability.\n\n\nIn OSG 3.4, the LCMAPS VOMS plugin is the only available authentication solution.\n\n\nAuthentication with the LCMAPS VOMS plugin\n\n\n\n\n\n\nAdd the following line to \n/etc/sysconfig/globus-gridftp-server\n:\n\n\nexport\n \nLLGT_VOMS_ENABLE_CREDENTIAL_CHECK\n=\n1\n\n\n\n\n\n\nThis should \nonly\n be done for OSG 3.3; it is unnecessary for OSG 3.4.\n\n\n\n\n\n\nFollow the instructions in \nthe LCMAPS VOMS plugin installation and configuration document\n to prepare the LCMAPS VOMS plugin.\n\n\n\n\n\n\n\n\nNote\n\n\nThis is the suggested mechanism for all new installs.\n\n\n\n\nAuthentication with edg-mkgridmap\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated. Support is scheduled to end as of June 2018.\n\n\n\n\nBy default, GridFTP uses a gridmap file, found in \n/etc/grid-security/grid-mapfile\n. This is the file generated by \nedg-mkgridmap\n if you follow the default \ninstall instructions\n.  No further configuration is needed.\n\n\nAuthentication with GUMS\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated. Support is scheduled to end as of June 2018.\n\n\n\n\nIf you want to use GUMS security (recommended), you will need to enable it using the following steps:\n\n\n\n\n\n\nEdit \n/etc/grid-security/gsi-authz.conf\n and uncomment the globus callout.\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\n\nNote that this used to be the full path to the library (\n/usr/lib64\n or \n/usr/lib\n), but now we rely on the linker for proper resolution in this file.\n\n\n\n\n\n\nEdit \n/etc/lcmaps.db\n to include your service endpoint:\n\n\n...\ngumsclient = \nlcmaps_gums_client.mod\n\n             \n-resourcetype ce\n\n             \n-actiontype execute-now\n\n             \n-capath /etc/grid-security/certificates\n\n             \n-cert   /etc/grid-security/hostcert.pem\n\n             \n-key    /etc/grid-security/hostkey.pem\n\n             \n--cert-owner root\n\n# Change this URL to your GUMS server\n             \n--endpoint https://\ngums.example.com:8443\n/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\n\n\n\n\n\nEnabling Gratia GridFTP transfer probe\n\n\nThe \nGratia GridFTP probe\n collects the information about the Gridftp transfers and forwards it to central Gratia collector. You need to enable the probe first. To do this, edit \n/etc/gratia/gridftp-transfer/ProbeConfig\n to set:\n\n\nEnableProbe=\n1\n\n\n\n\n\n\nAll other configuration settings should be suitable for most purposes. However, you can edit them if needed. The probe runs every 30 minutes as a cron job.\n\n\nOptional configuration\n\n\nModifying the environment\n\n\nEnvironment variables are stored in \n/etc/sysconfig/globus-gridftp-server\n which is sourced on service startup.  If you want to change LCMAPS log levels, or GridFTP port ranges, you can edit them there.\n\n\n#Uncomment and modify for firewalls\n\n\n#export GLOBUS_TCP_PORT_RANGE=min,max\n\n\n#export GLOBUS_TCP_SOURCE_RANGE=min,max\n\n\n\n\n\n\nNote that the variables \nGLOBUS_TCP_PORT_RANGE\n and \nGLOBUS_TCP_SOURCE_RANGE\n can be set here to allow GridFTP to navigate around firewall rules (these affect the inbound and outbound ports, respectively).\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/globus-gridftp-server\n and choose a higher debug level:\n\n\n# level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2\n\n\n\n\n\nOutput goes to \n/var/log/messages\n by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.\n\n\nConfiguring a multi-homed server\n\n\nThe GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file \n/etc/gridftp.conf\n.\n\n\nTo use a single interface you can set \nhostname\n to the Hostname or IP address to use:\n\n\nhostname IP-TO-USE\n\n\n\n\n\nYou can also set separately the \ncontrol_interface\n, \ndata_interface\n and \nipc_interface\n.  On systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting \ndata_interface\n:\n\n\ncontrol_interface IP-TO-USE\ndata_interface IP-TO-USE\nipc_interface IP-TO-USE\n\n\n\n\n\nFor more options available for the GridFTP server, read the comments in the configuration file (\n/etc/gridftp.conf\n) or see the \nGlobus manual\n.\n\n\nManaging GridFTP\n\n\nIn addition to the GridFTP service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nGridFTP\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\n\n\nValidating GridFTP\n\n\nThe GridFTP service can be validated by using globus-url-copy. You will need to run \ngrid-proxy-init\n or \nvoms-proxy-init\n in order to get a valid user proxy in order to communicate with the GridFTP server.\n\n\n[root@client ~] #\n globus-url-copy file:///tmp/zero.source gsiftp://yourhost.yourdomain/tmp/zero\n\n[root@client ~] #\n \necho\n \n$?\n\n\n0\n\n\n\n\n\n\nRun the validation as an unprivileged user; when invoked as root, \nglobus-url-copy\n will attempt to use the host certificate instead of your user certificate, with confusing results.\n\n\nGetting Help\n\n\nFor assistance, please use \nthis page\n.\n\n\nReference\n\n\n\n\nGlobus GridFTP administration manual\n\n\nGlobus GridFTP tutorial\n\n\n\n\nConfiguration and Log Files\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nEnvironment variables for GridFTP and LCMAPS\n\n\n\n\n\n\n\n\n/usr/share/osg/sysconfig/globus-gridftp-server-plugin\n\n\nWhere environment variables for GridFTP plugin are included\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\nGridFTP Gratia Probe configuration\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/cron.d/gratia-probe-gridftp-transfer.cron\n\n\nCron tab file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nGridFTP transfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nGridFTP authorization log\n\n\n\n\n\n\nGratia probe\n\n\n/var/logs/gratia\n\n\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n and \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nInstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates.\n\n\nUsers\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any Unix username that can be mapped by LCMAPS VOMS, \nedg-mkgridmap\n, or GUMS should be created on the GridFTP host.\n\n\nFor example, VOs newly-added to the LCMAPS VOMS configuration will not be able to transfer files until the corresponding Unix user account is created.\n\n\nNetworking\n\n\nFor more details on overall firewall configuration, please see our \nfirewall documentation\n.\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_PORT_RANGE\n\n\nX\n\n\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_SOURCE_RANGE\n\n\n\n\nX\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP control channel\n\n\ntcp\n\n\n2811\n\n\nX", 
            "title": "GridFTP Server"
        }, 
        {
            "location": "/data/gridftp/#installing-and-maintaining-a-gridftp-server", 
            "text": "", 
            "title": "Installing and Maintaining a GridFTP Server"
        }, 
        {
            "location": "/data/gridftp/#about-this-guide", 
            "text": "This page explains how to install the stand-alone Globus GridFTP server.  The GridFTP package contains components necessary to set up a stand-alone gsiftp server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances:   You are serving VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and your site has more than 250 cores  Your site will be managing more than 50 TB of disk space  A simple front-end to a filesystem allowing access over WAN - for example NFS.    Note  This document is for a standalone GridFTP server on top of POSIX storage.   See this page  for installation and configuration of a GridFTP server on top of the Hadoop Distributed File System.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/data/gridftp/#before-starting", 
            "text": "Before starting the installation process you will need to fulfill these prerequisites.   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates  Service certificate: The GridFTP service uses a host certificate at  /etc/grid-security/hostcert.pem  and an accompanying key at  /etc/grid-security/hostkey.pem  Network ports: GridFTP listens on TCP port 2811 and the list of ports configured by the  GLOBUS_TCP_SOURCE_RANGE  environment variable.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/gridftp/#installing-gridftp", 
            "text": "First, you will need to install the GridFTP meta-package:  [root@client ~] #  yum install osg-gridftp", 
            "title": "Installing GridFTP"
        }, 
        {
            "location": "/data/gridftp/#configuring-gridftp", 
            "text": "", 
            "title": "Configuring GridFTP"
        }, 
        {
            "location": "/data/gridftp/#configuring-authentication", 
            "text": "In OSG 3.3, there are three methods to manage authentication for incoming jobs: the  LCMAPS VOMS plugin ,  edg-mkgridmap  and  GUMS . Of these, GUMS has the most features and capabilities. The LCMAPS VOMS plugin is the new OSG-preferred authentication, offering the simplicity of edg-mkgridmap and many of GUMS' rich feature set. If you need to support \" pool accounts \", GUMS is the only option with that capability.  In OSG 3.4, the LCMAPS VOMS plugin is the only available authentication solution.", 
            "title": "Configuring authentication"
        }, 
        {
            "location": "/data/gridftp/#authentication-with-the-lcmaps-voms-plugin", 
            "text": "Add the following line to  /etc/sysconfig/globus-gridftp-server :  export   LLGT_VOMS_ENABLE_CREDENTIAL_CHECK = 1   This should  only  be done for OSG 3.3; it is unnecessary for OSG 3.4.    Follow the instructions in  the LCMAPS VOMS plugin installation and configuration document  to prepare the LCMAPS VOMS plugin.     Note  This is the suggested mechanism for all new installs.", 
            "title": "Authentication with the LCMAPS VOMS plugin"
        }, 
        {
            "location": "/data/gridftp/#authentication-with-edg-mkgridmap", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated. Support is scheduled to end as of June 2018.   By default, GridFTP uses a gridmap file, found in  /etc/grid-security/grid-mapfile . This is the file generated by  edg-mkgridmap  if you follow the default  install instructions .  No further configuration is needed.", 
            "title": "Authentication with edg-mkgridmap"
        }, 
        {
            "location": "/data/gridftp/#authentication-with-gums", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated. Support is scheduled to end as of June 2018.   If you want to use GUMS security (recommended), you will need to enable it using the following steps:    Edit  /etc/grid-security/gsi-authz.conf  and uncomment the globus callout.  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  Note that this used to be the full path to the library ( /usr/lib64  or  /usr/lib ), but now we rely on the linker for proper resolution in this file.    Edit  /etc/lcmaps.db  to include your service endpoint:  ...\ngumsclient =  lcmaps_gums_client.mod \n              -resourcetype ce \n              -actiontype execute-now \n              -capath /etc/grid-security/certificates \n              -cert   /etc/grid-security/hostcert.pem \n              -key    /etc/grid-security/hostkey.pem \n              --cert-owner root \n# Change this URL to your GUMS server\n              --endpoint https:// gums.example.com:8443 /gums/services/GUMSXACMLAuthorizationServicePort", 
            "title": "Authentication with GUMS"
        }, 
        {
            "location": "/data/gridftp/#enabling-gratia-gridftp-transfer-probe", 
            "text": "The  Gratia GridFTP probe  collects the information about the Gridftp transfers and forwards it to central Gratia collector. You need to enable the probe first. To do this, edit  /etc/gratia/gridftp-transfer/ProbeConfig  to set:  EnableProbe= 1   All other configuration settings should be suitable for most purposes. However, you can edit them if needed. The probe runs every 30 minutes as a cron job.", 
            "title": "Enabling Gratia GridFTP transfer probe"
        }, 
        {
            "location": "/data/gridftp/#optional-configuration", 
            "text": "", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/data/gridftp/#modifying-the-environment", 
            "text": "Environment variables are stored in  /etc/sysconfig/globus-gridftp-server  which is sourced on service startup.  If you want to change LCMAPS log levels, or GridFTP port ranges, you can edit them there.  #Uncomment and modify for firewalls  #export GLOBUS_TCP_PORT_RANGE=min,max  #export GLOBUS_TCP_SOURCE_RANGE=min,max   Note that the variables  GLOBUS_TCP_PORT_RANGE  and  GLOBUS_TCP_SOURCE_RANGE  can be set here to allow GridFTP to navigate around firewall rules (these affect the inbound and outbound ports, respectively).  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/globus-gridftp-server  and choose a higher debug level:  # level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2  Output goes to  /var/log/messages  by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.", 
            "title": "Modifying the environment"
        }, 
        {
            "location": "/data/gridftp/#configuring-a-multi-homed-server", 
            "text": "The GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file  /etc/gridftp.conf .  To use a single interface you can set  hostname  to the Hostname or IP address to use:  hostname IP-TO-USE  You can also set separately the  control_interface ,  data_interface  and  ipc_interface .  On systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting  data_interface :  control_interface IP-TO-USE\ndata_interface IP-TO-USE\nipc_interface IP-TO-USE  For more options available for the GridFTP server, read the comments in the configuration file ( /etc/gridftp.conf ) or see the  Globus manual .", 
            "title": "Configuring a multi-homed server"
        }, 
        {
            "location": "/data/gridftp/#managing-gridftp", 
            "text": "In addition to the GridFTP service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    GridFTP  globus-gridftp-server", 
            "title": "Managing GridFTP"
        }, 
        {
            "location": "/data/gridftp/#validating-gridftp", 
            "text": "The GridFTP service can be validated by using globus-url-copy. You will need to run  grid-proxy-init  or  voms-proxy-init  in order to get a valid user proxy in order to communicate with the GridFTP server.  [root@client ~] #  globus-url-copy file:///tmp/zero.source gsiftp://yourhost.yourdomain/tmp/zero [root@client ~] #   echo   $?  0   Run the validation as an unprivileged user; when invoked as root,  globus-url-copy  will attempt to use the host certificate instead of your user certificate, with confusing results.", 
            "title": "Validating GridFTP"
        }, 
        {
            "location": "/data/gridftp/#getting-help", 
            "text": "For assistance, please use  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/gridftp/#reference", 
            "text": "Globus GridFTP administration manual  Globus GridFTP tutorial", 
            "title": "Reference"
        }, 
        {
            "location": "/data/gridftp/#configuration-and-log-files", 
            "text": "Service/Process  Configuration File  Description      GridFTP  /etc/sysconfig/globus-gridftp-server  Environment variables for GridFTP and LCMAPS     /usr/share/osg/sysconfig/globus-gridftp-server-plugin  Where environment variables for GridFTP plugin are included    Gratia Probe  /etc/gratia/gridftp-transfer/ProbeConfig  GridFTP Gratia Probe configuration    Gratia Probe  /etc/cron.d/gratia-probe-gridftp-transfer.cron  Cron tab file        Service/Process  Log File  Description      GridFTP  /var/log/gridftp.log  GridFTP transfer log     /var/log/gridftp-auth.log  GridFTP authorization log    Gratia probe  /var/logs/gratia", 
            "title": "Configuration and Log Files"
        }, 
        {
            "location": "/data/gridftp/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem  and  /etc/grid-security/hostkey.pem     Instructions  to request a service certificate.  You will also need a copy of CA certificates.", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/gridftp/#users", 
            "text": "For this package to function correctly, you will have to create the users needed for grid operation. Any Unix username that can be mapped by LCMAPS VOMS,  edg-mkgridmap , or GUMS should be created on the GridFTP host.  For example, VOs newly-added to the LCMAPS VOMS configuration will not be able to transfer files until the corresponding Unix user account is created.", 
            "title": "Users"
        }, 
        {
            "location": "/data/gridftp/#networking", 
            "text": "For more details on overall firewall configuration, please see our  firewall documentation .     Service Name  Protocol  Port Number  Inbound  Outbound  Comment      GridFTP data channels  tcp  GLOBUS_TCP_PORT_RANGE  X   contiguous range of ports is necessary.    GridFTP data channels  tcp  GLOBUS_TCP_SOURCE_RANGE   X  contiguous range of ports is necessary.    GridFTP control channel  tcp  2811  X", 
            "title": "Networking"
        }, 
        {
            "location": "/data/frontier-squid/", 
            "text": "Install the Frontier Squid HTTP Caching Proxy\n\n\nFrontier Squid is a distribution of the well-known \nsquid HTTP caching\nproxy software\n that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has\n\nmany advantages\n\nover regular squid for common grid applications, especially Frontier\nand CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.\n\n\nThis document is intended for System Administrators who are installing\n\nfrontier-squid\n, the OSG distribution of the Frontier Squid software.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.4.0.\nThe version of frontier-squid installed should be \n= 3.5.24-3.1.\nWhen using an OSG Version \n 3.4.0 and a frontier-squid version in the\n2.7STABLE9 series, refer to the\n\nold upstream install documentation\n\ninstead of the current links included below. There are some\nincompatibilities between the two versions, so if you are upgrading\nfrom a 2.7STABLE9 version to a 3.5 version, see the\n\nupstream documentation on upgrading\n.\n\n\n\n\nFrontier Squid is Recommended\n\n\nOSG recommends that all sites run a caching proxy for HTTP and HTTPS\nto help reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.\n\n\nFor large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid\nsoftware still will be installed on the CE, but it need not be\nenabled. Instead, install your proxy service on the separate host and\nthen configure the CE host to refer to the proxy on that host.\n\n\nThe \nosg-configure\n configuration tool (version 1.0.45 and later)\nwarns users who have not added the proxy location to their CE\nconfiguration. In the future, a proxy will be required and\nosg-configure will fail if the proxy location is not set.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the \nsquid\n Linux user\n\n\nNetwork ports:\n Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP)\n\n\nHost choice:\n If you will be supporting the Frontier application at your site, review the\n\nupstream documentation Hardware considerations section\n to determine how to size your equipment.\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nInstalling Frontier Squid\n\n\nTo install Frontier Squid, make sure that your host is up to date before installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\n[root@client ~ ] $\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\n[root@client ~ ] $\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall Frontier Squid:\n\n\n[root@client ~ ] $\n yum install frontier-squid\n\n\n\n\n\n\n\n\n\nConfiguring Frontier Squid\n\n\nConfiguring the Frontier Squid Service\n\n\nTo configure the Frontier Squid service itself:\n\n\n\n\nFollow the\n    \nConfiguration section of the upstream Frontier Squid documentation\n.\n\n\nEnable, start, and test the service (as described below).\n\n\nEnable WLCG monitoring as described in the\n    \nupstream documentation on enabling monitoring\n\n    and\n    \nregister the squid in OIM\n.\n\n\n\n\n\n\nNote\n\n\nAn important difference between the standard Squid software and\nthe Frontier Squid variant is that Frontier Squid changes are in\n\n/etc/squid/customize.sh\n instead of \n/etc/squid/squid.conf\n.\n\n\n\n\nConfiguring the OSG CE\n\n\nTo configure the OSG Compute Element (CE) to know about your Frontier Squid service:\n\n\n\n\n\n\nOn your CE host, edit \n/etc/osg/config.d/01-squid.ini\n\n\n\n\nMake sure that \nenabled\n is set to \nTrue\n\n\nSet \nlocation\n to the hostname and port of your Frontier Squid\n    service (e.g., \nmy.squid.host.edu:3128\n)\n\n\nLeave the other settings at \nDEFAULT\n unless you have specific\n    reasons to change them\n\n\n\n\n\n\n\n\nRun \nosg-configure\n to propagate the changes on your CE.\n\n\n\n\n\n\n\n\nNote\n\n\nYou may want to finish other CE configuration tasks before running\n\nosg-configure\n. Just be sure to run it once before starting CE\nservices.\n\n\n\n\nUsing Frontier-Squid\n\n\nStart the frontier-squid service and enable it to start at boot time. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice frontier-squid start\n\n\nsystemctl start frontier-squid\n\n\n\n\n\n\nStop a  service\n\n\nservice frontier-squid stop\n\n\nsystemctl stop frontier-squid\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig frontier-squid on\n\n\nsystemctl enable frontier-squid\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig frontier-squid off\n\n\nsystemctl disable frontier-squid\n\n\n\n\n\n\n\n\nValidating Frontier Squid\n\n\nAs any user on another computer, do the following (where\n\nyoursquid.your.domain\n is the fully qualified domain name of your\nsquid server):\n\n\n[user@client ~] $\n \nexport\n \nhttp_proxy\n=\nhttp://\nyoursquid.your.domain\n:3128\n\n[user@client ~] $\n wget -qdO/dev/null http://frontier.cern.ch \n2\n1\n|\ngrep X-Cache\n\nX-Cache: MISS from \nyoursquid.your.domain\n\n\n[user@client ~] $\n wget -qdO/dev/null http://frontier.cern.ch \n2\n1\n|\ngrep X-Cache\n\nX-Cache: HIT from \nyoursquid.your.domain\n\n\n\n\n\n\nIf the grep doesn't print anything, try removing it from the pipeline\nto see if errors are obvious. If the second try says MISS again,\nsomething is probably wrong with the squid cache writes. Look at the squid\n\naccess.log file\n\nto try to see what's wrong.\n\n\nIf your squid will be supporting the Frontier application, it is also\ngood to do the test in the\n\nupstream documentation Testing the installation section\n.\n\n\nReference\n\n\nUsers\n\n\nThe frontier-squid installation will create one user account unless it\nalready exists.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nsquid\n\n\nReduced privilege user that the squid process runs under. Set the default gid of the \"squid\" user to be a group that is also called \"squid\".\n\n\n\n\n\n\n\n\nThe package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the\n\nupstream documentation Preparation section\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nSquid\n\n\ntcp\n\n\n3128\n\n\n\u2713\n\n\n\u2713\n\n\nAlso limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously\n\n\n\n\n\n\nSquid monitor\n\n\nudp\n\n\n3401\n\n\n\u2713\n\n\n\n\nAlso limited in squid ACLs. Should be limited to monitoring server addresses\n\n\n\n\n\n\n\n\nThe addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the\n\nupstream documentation Enabling monitoring section\n.\n\n\nFrontier Squid Log Files\n\n\nLog file contents are explained in the\n\nupstream documentation Log file contents section\n.", 
            "title": "HTTP Cache"
        }, 
        {
            "location": "/data/frontier-squid/#install-the-frontier-squid-http-caching-proxy", 
            "text": "Frontier Squid is a distribution of the well-known  squid HTTP caching\nproxy software  that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has many advantages \nover regular squid for common grid applications, especially Frontier\nand CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.  This document is intended for System Administrators who are installing frontier-squid , the OSG distribution of the Frontier Squid software.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.4.0.\nThe version of frontier-squid installed should be  = 3.5.24-3.1.\nWhen using an OSG Version   3.4.0 and a frontier-squid version in the\n2.7STABLE9 series, refer to the old upstream install documentation \ninstead of the current links included below. There are some\nincompatibilities between the two versions, so if you are upgrading\nfrom a 2.7STABLE9 version to a 3.5 version, see the upstream documentation on upgrading .", 
            "title": "Install the Frontier Squid HTTP Caching Proxy"
        }, 
        {
            "location": "/data/frontier-squid/#frontier-squid-is-recommended", 
            "text": "OSG recommends that all sites run a caching proxy for HTTP and HTTPS\nto help reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.  For large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid\nsoftware still will be installed on the CE, but it need not be\nenabled. Instead, install your proxy service on the separate host and\nthen configure the CE host to refer to the proxy on that host.  The  osg-configure  configuration tool (version 1.0.45 and later)\nwarns users who have not added the proxy location to their CE\nconfiguration. In the future, a proxy will be required and\nosg-configure will fail if the proxy location is not set.", 
            "title": "Frontier Squid is Recommended"
        }, 
        {
            "location": "/data/frontier-squid/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If it does not exist already, the installation will create the  squid  Linux user  Network ports:  Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP)  Host choice:  If you will be supporting the Frontier application at your site, review the upstream documentation Hardware considerations section  to determine how to size your equipment.   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/frontier-squid/#installing-frontier-squid", 
            "text": "To install Frontier Squid, make sure that your host is up to date before installing the required packages:    Clean yum cache:  [root@client ~ ] $  yum clean all --enablerepo = *    Update software:  [root@client ~ ] $  yum update  This command will update  all  packages    Install Frontier Squid:  [root@client ~ ] $  yum install frontier-squid", 
            "title": "Installing Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-frontier-squid", 
            "text": "", 
            "title": "Configuring Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-the-frontier-squid-service", 
            "text": "To configure the Frontier Squid service itself:   Follow the\n     Configuration section of the upstream Frontier Squid documentation .  Enable, start, and test the service (as described below).  Enable WLCG monitoring as described in the\n     upstream documentation on enabling monitoring \n    and\n     register the squid in OIM .    Note  An important difference between the standard Squid software and\nthe Frontier Squid variant is that Frontier Squid changes are in /etc/squid/customize.sh  instead of  /etc/squid/squid.conf .", 
            "title": "Configuring the Frontier Squid Service"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-the-osg-ce", 
            "text": "To configure the OSG Compute Element (CE) to know about your Frontier Squid service:    On your CE host, edit  /etc/osg/config.d/01-squid.ini   Make sure that  enabled  is set to  True  Set  location  to the hostname and port of your Frontier Squid\n    service (e.g.,  my.squid.host.edu:3128 )  Leave the other settings at  DEFAULT  unless you have specific\n    reasons to change them     Run  osg-configure  to propagate the changes on your CE.     Note  You may want to finish other CE configuration tasks before running osg-configure . Just be sure to run it once before starting CE\nservices.", 
            "title": "Configuring the OSG CE"
        }, 
        {
            "location": "/data/frontier-squid/#using-frontier-squid", 
            "text": "Start the frontier-squid service and enable it to start at boot time. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service frontier-squid start  systemctl start frontier-squid    Stop a  service  service frontier-squid stop  systemctl stop frontier-squid    Enable a service to start on boot  chkconfig frontier-squid on  systemctl enable frontier-squid    Disable a service from starting on boot  chkconfig frontier-squid off  systemctl disable frontier-squid", 
            "title": "Using Frontier-Squid"
        }, 
        {
            "location": "/data/frontier-squid/#validating-frontier-squid", 
            "text": "As any user on another computer, do the following (where yoursquid.your.domain  is the fully qualified domain name of your\nsquid server):  [user@client ~] $   export   http_proxy = http:// yoursquid.your.domain :3128 [user@client ~] $  wget -qdO/dev/null http://frontier.cern.ch  2 1 | grep X-Cache X-Cache: MISS from  yoursquid.your.domain  [user@client ~] $  wget -qdO/dev/null http://frontier.cern.ch  2 1 | grep X-Cache X-Cache: HIT from  yoursquid.your.domain   If the grep doesn't print anything, try removing it from the pipeline\nto see if errors are obvious. If the second try says MISS again,\nsomething is probably wrong with the squid cache writes. Look at the squid access.log file \nto try to see what's wrong.  If your squid will be supporting the Frontier application, it is also\ngood to do the test in the upstream documentation Testing the installation section .", 
            "title": "Validating Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/data/frontier-squid/#users", 
            "text": "The frontier-squid installation will create one user account unless it\nalready exists.     User  Comment      squid  Reduced privilege user that the squid process runs under. Set the default gid of the \"squid\" user to be a group that is also called \"squid\".     The package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the upstream documentation Preparation section .", 
            "title": "Users"
        }, 
        {
            "location": "/data/frontier-squid/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Squid  tcp  3128  \u2713  \u2713  Also limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously    Squid monitor  udp  3401  \u2713   Also limited in squid ACLs. Should be limited to monitoring server addresses     The addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the upstream documentation Enabling monitoring section .", 
            "title": "Networking"
        }, 
        {
            "location": "/data/frontier-squid/#frontier-squid-log-files", 
            "text": "Log file contents are explained in the upstream documentation Log file contents section .", 
            "title": "Frontier Squid Log Files"
        }, 
        {
            "location": "/other/gsissh/", 
            "text": "Installing GSI OpenSSH\n\n\nThis document gives instructions on installing and using the GSI OpenSSH server available in the OSG repository and configuring it so that you can use on your cluster.\n\n\nRequirements\n\n\nHost and OS\n\n\nThe GSI OpenSSH rpms will require an user account and group in order for the privilege separation to work.\n\n\nUsers and Groups\n\n\nThe RPM installation will try to create the \ngsisshd\n user and group and the \n/var/empty/gsisshd\n directory with the correct ownership if they are not present. If you are using a configuration management system or ROCKS, you should make sure that these users and groups are created before installing the RPMs to avoid potential issues. The gsisshd user should have an empty home directory. By default, this is home directory set to \n/var/empty/gsisshd\n and belongs to the \ngsisshd\n user and group. You may change it if needed to something else as long as the ownerships remain the same.\n\n\nInstallation procedure\n\n\nPrior to install, make sure you have:\n\n \nYum repositories correctly configured\n for OSG.\n\n \nCA certificates installed\n\n\nGSI OpenSSH Installation\n\n\nStart with installing GSI OpenSSH from the repository\n\n\n[root@server]#\n yum install gsi-openssh-server gsi-openssh-clients\n\n\n\n\n\nIn addition, you'll need to install CA certificates in order for GSIOpenSSH to work. You can follow the instructions below in order to install them:\n\n\nConfiguration and Operations\n\n\nUseful configuration and log files\n\n\nConfiguration Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/sshd_config\n\n\nConfiguration file\n\n\n\n\n\n\ngsisshd\n\n\n/etc/sysconfig/gsisshd\n\n\nEnvironment variables for gsisshd\n\n\n\n\n\n\ngsisshd\n\n\n/etc/lcmaps.db\n\n\nLCMAPS configuration\n\n\n\n\n\n\n\n\nLog Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/var/log/messages\n\n\nAll log messages\n\n\n\n\n\n\n\n\nOther Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostcert.pem\n\n\nHost certificate\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostkey.pem\n\n\nX.509 host key\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/ssh_host_rsa_key\n\n\nRSA Host key\n\n\n\n\n\n\n\n\nConfiguration\n\n\nIn order to get a running instance of the GSI OpenSSH server, you'll\nneed to change the default configuration. However, before you go any\nfurther, you'll need to decide whether you want GSI OpenSSH to be your \nprimary ssh service or not (e.g. whether the GSI OpenSSH service will \nreplace your existing SSH service). If you choose not to replace your \nexisting service, you'll need to change the port setting in the GSI \nOpenSSH configuration to another port (e.g. 2222) so that you can run \nboth SSH services at the same time. Regardless of your choice, you \nshould probably have both services use the same host key. In order \nto do this, symlink \n/etc/gsissh/ssh_host_dsa_key\n and \n/etc/gsissh/ssh_host_rsa_key\n \nto \n/etc/ssh/ssh_host_dsa_key\n and \n/etc/ssh/ssh_host_rsa_key\n respectively. \n\n\n\n\nNote\n\n\nRegardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (have ! in the password field of \n/etc/shadow\n).\n\n\n\n\nUsing a gridmap file for authorization\n\n\nIn order to use gsissh, you'll need to create mappings in your \n\n/etc/grid-security/grid-mapfile\n for the DNs that you will \nallow to login. The mappings should be entered one to a line, \nwith each line consisting of DN followed by the account the DN \nshould map to. Also, you should ensure that the \n\n/etc/grid-security/gsi-authz.conf\n file is empty or that all \nof the lines in the file are commented out using a \n#\n at the beginning of the line.\n\n\n\n\nNote\n\n\nThe mappings will not consider VOMS extensions so the first mapping that matches will be used regardless of the VO role or VO present in the users proxy\n\n\n\n\nAn example of the \n/etc/grid-security/grid-mapfile\n follows:\n\n\n/DC=org/DC=doegrids/OU=People/CN=USER NAME 123456\n useraccount\n\n\n\n\n\nUsing LCMAPS and GUMS for authorization\n\n\nIn order to use LCMAPS callouts with GSI OpenSSH, you'll first need to edit \n/etc/grid-security/gsi-authz.conf\n to indicate that Globus should do a GSI callout for authorization. The file should contain the following:\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\n\nso that LCMAPS is used. Next, install the lcmaps rpms:\n\n\n[root@server]#\n yum install lcmaps lcas-lcmaps-gt4-interface\n\n\n\n\n\nFinally, you'll need to modify \n/etc/lcmaps.db\n so that the \ngumsclient\n entry has the correct endpoint for your gums server.\n\n\nStarting and Enabling Services\n\n\nTo start the services:\n\n\n\n\n\n\nTo start GSI OpenSSH you can use the service command, e.g.:\n\n\nservice gsisshd start\n\n\n\n\n\n\nYou should also enable the appropriate services so that they are automatically started when your system is powered on:\n\n\n\n\n\n\nTo enable OpenSSH by default on the node:\n\n\nchkconfig gsisshd on\n\n\n\n\n\n\nStopping and Disabling Services\n\n\nTo stop the services:\n\n\n\n\n\n\nTo stop OpenSSH you can use: \\\npre class=\u201crootscreen\u201d>\n\n\nservice gsisshd stop\n\n\n\n\n\n\nIn addition, you can disable services by running the following commands. However, you don't need to do this normally.\n\n\n\n\n\n\nOptionally, to disable OpenSSH:\n\n\nchkconfig gsisshd off\n\n\n\n\n\n\nTroubleshooting\n\n\nYou can get information on troubleshooting errors on the \nNCSA page\n.\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/gsisshd\n and choose a higher debug level:\n\n\n# level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2\n\n\n\n\n\nOutput goes to \n/var/log/messages\n by default.\n\n\nTest GSI OpenSSH\n\n\nAfter starting the \ngsisshd\n service you can check if it is running correctly\n\n\n$ grid-proxy-init\nYour identity: /DC\n=\nch/DC\n=\ncern/OU\n=\nOrganic Units/OU\n=\nUsers/CN\n=\nUser Name\nEnter GRID pass phrase \nfor\n this identity:\nCreating proxy ............................................................................................... Done\nYour proxy is valid \nuntil\n: Sat Apr \n23\n \n08\n:18:27 \n2016\n\n$ gsissh localhost -p \n2222\n\nLast login: Tue Sep \n18\n \n16\n:08:03 \n2012\n from itb4.uchicago.edu\n$\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "GSI-enabled SSH"
        }, 
        {
            "location": "/other/gsissh/#installing-gsi-openssh", 
            "text": "This document gives instructions on installing and using the GSI OpenSSH server available in the OSG repository and configuring it so that you can use on your cluster.", 
            "title": "Installing GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/other/gsissh/#host-and-os", 
            "text": "The GSI OpenSSH rpms will require an user account and group in order for the privilege separation to work.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/other/gsissh/#users-and-groups", 
            "text": "The RPM installation will try to create the  gsisshd  user and group and the  /var/empty/gsisshd  directory with the correct ownership if they are not present. If you are using a configuration management system or ROCKS, you should make sure that these users and groups are created before installing the RPMs to avoid potential issues. The gsisshd user should have an empty home directory. By default, this is home directory set to  /var/empty/gsisshd  and belongs to the  gsisshd  user and group. You may change it if needed to something else as long as the ownerships remain the same.", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/other/gsissh/#installation-procedure", 
            "text": "Prior to install, make sure you have:   Yum repositories correctly configured  for OSG.   CA certificates installed", 
            "title": "Installation procedure"
        }, 
        {
            "location": "/other/gsissh/#gsi-openssh-installation", 
            "text": "Start with installing GSI OpenSSH from the repository  [root@server]#  yum install gsi-openssh-server gsi-openssh-clients  In addition, you'll need to install CA certificates in order for GSIOpenSSH to work. You can follow the instructions below in order to install them:", 
            "title": "GSI OpenSSH Installation"
        }, 
        {
            "location": "/other/gsissh/#configuration-and-operations", 
            "text": "", 
            "title": "Configuration and Operations"
        }, 
        {
            "location": "/other/gsissh/#useful-configuration-and-log-files", 
            "text": "Configuration Files     Service or Process  Configuration File  Description      gsisshd  /etc/gsissh/sshd_config  Configuration file    gsisshd  /etc/sysconfig/gsisshd  Environment variables for gsisshd    gsisshd  /etc/lcmaps.db  LCMAPS configuration     Log Files     Service or Process  Log File  Description      gsisshd  /var/log/messages  All log messages     Other Files     Service or Process  File  Description      gsisshd  /etc/grid-security/hostcert.pem  Host certificate    gsisshd  /etc/grid-security/hostkey.pem  X.509 host key    gsisshd  /etc/gsissh/ssh_host_rsa_key  RSA Host key", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/other/gsissh/#configuration", 
            "text": "In order to get a running instance of the GSI OpenSSH server, you'll\nneed to change the default configuration. However, before you go any\nfurther, you'll need to decide whether you want GSI OpenSSH to be your \nprimary ssh service or not (e.g. whether the GSI OpenSSH service will \nreplace your existing SSH service). If you choose not to replace your \nexisting service, you'll need to change the port setting in the GSI \nOpenSSH configuration to another port (e.g. 2222) so that you can run \nboth SSH services at the same time. Regardless of your choice, you \nshould probably have both services use the same host key. In order \nto do this, symlink  /etc/gsissh/ssh_host_dsa_key  and  /etc/gsissh/ssh_host_rsa_key  \nto  /etc/ssh/ssh_host_dsa_key  and  /etc/ssh/ssh_host_rsa_key  respectively.    Note  Regardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (have ! in the password field of  /etc/shadow ).", 
            "title": "Configuration"
        }, 
        {
            "location": "/other/gsissh/#using-a-gridmap-file-for-authorization", 
            "text": "In order to use gsissh, you'll need to create mappings in your  /etc/grid-security/grid-mapfile  for the DNs that you will \nallow to login. The mappings should be entered one to a line, \nwith each line consisting of DN followed by the account the DN \nshould map to. Also, you should ensure that the  /etc/grid-security/gsi-authz.conf  file is empty or that all \nof the lines in the file are commented out using a  #  at the beginning of the line.   Note  The mappings will not consider VOMS extensions so the first mapping that matches will be used regardless of the VO role or VO present in the users proxy   An example of the  /etc/grid-security/grid-mapfile  follows:  /DC=org/DC=doegrids/OU=People/CN=USER NAME 123456  useraccount", 
            "title": "Using a gridmap file for authorization"
        }, 
        {
            "location": "/other/gsissh/#using-lcmaps-and-gums-for-authorization", 
            "text": "In order to use LCMAPS callouts with GSI OpenSSH, you'll first need to edit  /etc/grid-security/gsi-authz.conf  to indicate that Globus should do a GSI callout for authorization. The file should contain the following:  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  so that LCMAPS is used. Next, install the lcmaps rpms:  [root@server]#  yum install lcmaps lcas-lcmaps-gt4-interface  Finally, you'll need to modify  /etc/lcmaps.db  so that the  gumsclient  entry has the correct endpoint for your gums server.", 
            "title": "Using LCMAPS and GUMS for authorization"
        }, 
        {
            "location": "/other/gsissh/#starting-and-enabling-services", 
            "text": "To start the services:    To start GSI OpenSSH you can use the service command, e.g.:  service gsisshd start    You should also enable the appropriate services so that they are automatically started when your system is powered on:    To enable OpenSSH by default on the node:  chkconfig gsisshd on", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/other/gsissh/#stopping-and-disabling-services", 
            "text": "To stop the services:    To stop OpenSSH you can use: \\ pre class=\u201crootscreen\u201d>  service gsisshd stop    In addition, you can disable services by running the following commands. However, you don't need to do this normally.    Optionally, to disable OpenSSH:  chkconfig gsisshd off", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/other/gsissh/#troubleshooting", 
            "text": "You can get information on troubleshooting errors on the  NCSA page .  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/gsisshd  and choose a higher debug level:  # level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2  Output goes to  /var/log/messages  by default.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/other/gsissh/#test-gsi-openssh", 
            "text": "After starting the  gsisshd  service you can check if it is running correctly  $ grid-proxy-init\nYour identity: /DC = ch/DC = cern/OU = Organic Units/OU = Users/CN = User Name\nEnter GRID pass phrase  for  this identity:\nCreating proxy ............................................................................................... Done\nYour proxy is valid  until : Sat Apr  23   08 :18:27  2016 \n$ gsissh localhost -p  2222 \nLast login: Tue Sep  18   16 :08:03  2012  from itb4.uchicago.edu\n$", 
            "title": "Test GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/", 
            "text": "Installing and Maintaining the LCMAPS VOMS Plugin\n\n\nAbout This Guide\n\n\nLCMAPS is a software library for mapping grid certificates of incoming connections to specific Unix accounts. The LCMAPS VOMS plugin enables LCMAPS to make mapping decisions based on the VOMS attributes of grid certificates, e.g., \n/cms/Role=production/Capability=NULL\n. Starting in OSG 3.4, the LCMAPS VOMS plugin will replace GUMS and edg-mkgridmap as the authentication method at OSG sites.\n\n\nThe OSG provides a default set of mappings from VOMS attributes to Unix accounts. By configuring LCMAPS, you can override these mappings, including changing the Unix account that a VO is mapped to, banning based on VOMS attributes, banning a specific user, or adding a VO, VO group, VO role, and/or user that is not in the OSG's set of mappings.\n\n\nUse this page to learn how to install and configure the LCMAPS VOMS plugin to authenticate users to access your resources on a per-VO basis.\n\n\nInstalling the LCMAPS VOMS Plugin\n\n\nTo install the LCMAPS VOMS plugin, make sure that your host is up to date before installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\n[root@client ~ ] $\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\n[root@client ~ ] $\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall \nlcmaps\n, the default mapfile, and the configuration tools:\n\n\n[root@server]#\n yum install lcmaps vo-client-lcmaps-voms osg-configure-misc\n\n\n\n\n\n\n\n\n\nConfiguring the LCMAPS VOMS Plugin\n\n\nThe following section describes the steps required to configure the LCMAPS VOMS plugin for authentication. If you are using OSG 3.3 packages, there are software-specific instructions that must be followed for \nHTCondor-CE\n, \nGridFTP\n, and \nXRootD\n. To check if you are running OSG 3.3, run the following command:\n\n\n[root@server]#\n rpm -q --queryformat\n=\n%{VERSION}\\n\n osg-release\n\n\n\n\n\nAdditionally, there is \noptional configuration\n if you need to make changes to the default mappings.\n\n\nEnabling the LCMAPS VOMS plugin\n\n\nTo configure your host to use LCMAPS VOMS plugin authentication, edit \n/etc/osg/config.d/10-misc.ini\n and set the following options:\n\n\nglexec_location\n \n=\n \nUNAVAILABLE\n\n\nedit_lcmaps_db\n \n=\n \nTrue\n\n\nauthorization_method\n \n=\n \nvomsmap\n\n\n\n\n\n\nSupporting mapped VOs and users\n\n\nUnix accounts must exist for each VO, VO role, VO group, or user you choose to support in the \nmapfiles\n:\n\n\n\n\n\n\nConsult the default VO mappings in \n/usr/share/osg/voms-mapfile-default\n to determine the mapped Unix account names. Each of the mapfiles has the following format:\n\n\nVO, VO role, VO group or user\n \nUnix account\n\n\n\n\n\n\n\n\n\n\nCreate Unix accounts for each VO, VO role, VO group, and user that you wish to support\n\n\n\n\nEdit \n/etc/osg/config.d/30-gip.ini\n and specify the supported VOs per \nSubcluster or ResourceEntry section\n:\n\n\n\n\nallowed_vos\n=\nVO1,VO2...\n\n\n\n\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n directory does not apply those settings to software automatically. For the OSG settings, use the \nosg-configure\n tool to validate (to a limited extent) and apply the settings to the relevant software components. If instead you wish to manage the LCMAPS VOMS plugin configuration yourself, skip to the \nmanual configuration section\n.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n directory.\n\n\n\n\nNote\n\n\nThis document only describes the critical settings for the LCMAPS VOMS plugin and related software. You may need to configure other software that is installed on your host, too.\n\n\n\n\n\n\n\n\nValidate the configuration settings:\n\n\n[root@server]#\n osg-configure -v\n\n\n\n\n\n\n\n\n\nOnce the validation command succeeds without errors, apply the configuration settings:\n\n\n[root@server]#\n osg-configure -c\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following subsections contain information on migration from \nedg-mkgridmap\n, mapping or banning users by their certificates' Distinguished Names (DNs) or by their proxies' VOMS attributes. Any optional configuration is to be performed after the installation and configuration sections above.\n\n\nFor a table of the configuration files and their order of evaluation, consult the \nreference section\n.\n\n\n\n\nMigrating from edg-mkgridmap\n\n\nMapping VOs\n\n\nMapping users\n\n\nBanning VOs\n\n\nBanning users\n\n\n\n\nMigrating from edg-mkgridmap\n\n\nThe program edg-mkgridmap (found in the package \nedg-mkgridmap\n), used for authentication on HTCondor-CE, GridFTP, and XRootD hosts, is no longer available starting in OSG 3.4. The LCMAPS VOMS plugin (package \nlcmaps-plugins-voms\n) now provides the same functionality. To migrate from edg-mkgridmap to the LCMAPS VOMS plugin, perform the following procedure:\n\n\n\n\n\n\nConfigure user DN mappings:\n\n\n\n\n\n\nIf you have a local grid mapfile (see \nthe EDG-mkgridmap docs\n), replace the contents of \n/etc/grid-security/grid-mapfile\n with the contents of the local grid mapfile.\n\n\n\n\n\n\nIf you do not have a local grid mapfile, remove \n/etc/grid-security/grid-mapfile\n.\n\n\n\n\n\n\n\n\n\n\nIf you are remaining on OSG 3.3, ensure that the you have set \nexport LLGT_VOMS_ENABLE_CREDENTIAL_CHECK=1\n in the appropriate file and restart the service. If you have updated your host to OSG 3.4, skip to the next step.\n\n\n\n\n\n\n\n\nIf your host is a(n)...\n\n\nAdd the aforementioned line to...\n\n\nAnd restart this service...\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\n/etc/sysconfig/condor-ce\n\n\ncondor-ce\n\n\n\n\n\n\nGridFTP server\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\n\n\n\n\nIf you are converting an HTCondor-CE host, remove the HTCondor-CE \nGRIDMAP\n configuration. Otherwise, skip to the next step.\n\n\n\n\n\n\nFind the location of this configuration using the following command:\n\n\n[root@ce]#\n condor_ce_config_val -v GRIDMAP\n\n\n\n\n\n\n\n\n\nDelete the line that sets the \nGRIDMAP\n configuration variable\n\n\n\n\nReconfigure HTCondor-CE:\n[root@ce]#\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemove edg-mkgridmap and related packages:\n\n\n[root@ce]#\n yum erase edg-mkgridmap\n\n\n\n\n\n\n\nWarning\n\n\nIn the output from this command, yum should \nnot\n list other packages than the one. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.3 (or 3.4) versions (they should have \".osg33\" or \".osg34\" in their versions), and try again.\n\n\n\n\n\n\n\n\nMapping VOs\n\n\n/etc/grid-security/voms-mapfile\n is used to map VOs, VO roles, or VO groups to Unix accounts based on their VOMS attributes. An example of the format of a \nvoms-mapfile\n follows:\n\n\n# map GLOW jobs in the chtc group to the \nglow1\n Unix account.\n\n/GLOW/chtc/*\n glow1\n# map GLOW jobs with the htpc role to the \nglow2\n Unix account.\n\n/GLOW/Role=htpc/*\n glow2\n# map other GLOW jobs to the \nglow\n Unix account.\n\n/GLOW/*\n glow\n\n\n\n\n\nEach non-commented line is a shell-style pattern which is compared against the user's VOMS attributes, and a Unix account that the user will be mapped to if the pattern matches.\nThe patterns are compared in the order they are listed in. Therefore, more general patterns should be placed later in the file.\n\n\n\n\nNote\n\n\nThe Unix account must exist for the user to be mapped. If a VO's Unix account is missing, that VO will not be able to access your resources.\n\n\n\n\nMapping users\n\n\n/etc/grid-security/grid-mapfile\n is used to map specific users to Unix accounts based on their certificates' DNs. An example of the format of a \ngrid-mapfile\n follows:\n\n\n# map Matyas\ns FNAL DN to the \nmatyas\n Unix account\n\n/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\n matyas\n\n\n\n\n\n\n\nNote\n\n\nThe Unix account must exist for the user to be mapped. If a user's Unix account is missing, that user will not be able to access your resources.\n\n\n\n\nBanning VOs\n\n\n/etc/grid-security/ban-voms-mapfile\n is used to ban an entire VO or a role withing a VO from accessing resources on your machine. An example of the format of a \nban-voms-mapfile\n follows:\n\n\n# ban CMS production jobs\n\n/cms/Role=production/*\n\n\n\n\n\n\nEach non-commented line is a shell-style pattern which is compared against a user's VOMS attributes. If the pattern matches, that user will be unable to access your resources.\n\n\n\n\nWarning\n\n\n/etc/grid-security/ban-voms-mapfile\n \nmust\n exist, even if you are not banning any VOs. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.\n\n\n\n\nBanning users\n\n\n/etc/grid-security/ban-mapfile\n is used to ban specific users from accessing your resources based on their certificates' DNs. An example of the format of a \nban-mapfile\n follows:\n\n\n# ban Matyas\ns FNAL DN\n\n/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\n\n\n\n\n\n\n\n\nWarning\n\n\n/etc/grid-security/ban-mapfile\n \nmust\n exist, even if you are not banning any users. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.\n\n\n\n\nValidating the LCMAPS VOMS plugin VO mappings\n\n\nTo validate the LCMAPS VOMS plugin by itself, use the following procedure to test mapping your own cert to a user:\n\n\n\n\nVerify your DN is \nnot\n in \n/etc/grid-security/grid-mapfile\n, or else it will generate a false positive\n\n\nVerify your DN is \nnot\n in \n/etc/grid-security/ban-mapfile\n, or else it will generate a false negative\n\n\n\n\nInstall the \nllrun\n and \nvoms-clients\n packages:\n\n\n[root@host]#\n yum install llrun voms-clients\n\n\n\n\n\n\n\n\n\nAs an unprivileged user, create a VOMS proxy (filling in \nYOUR_VO\n with a VO you are a member of):\n\n\n[you@client]$\n voms-proxy-init -voms \nYOUR_VO\n\n\n\n\n\n\n\n\n\n\nVerify that your credentials are mapped as expected:\n\n\n[you@client]$\n llrun -s -l \nmode\n=\npem,policy\n=\nauthorize_only,db\n=\n/etc/lcmaps.db \n\\\n\n    -p/tmp/x509up_u\n`\nid -u\n`\n\n\n\n\n\n\n\n\n\n\nIf you did not get correctly mapped, check your proxy's FQAN by running:\n\n\n[you@client]$\n voms-proxy-info -fqan\n\n\n\n\n\nand make sure it matches one of the patterns in \n/etc/grid-security/voms-mapfile\n or \n/usr/share/osg/voms-mapfile-default\n, and does not match any patterns in \n/etc/grid-security/ban-voms-mapfile\n.\n\n\nTroubleshooting the LCMAPS VOMS plugin\n\n\nLCMAPS logs to \njournalctl\n (EL7) or \n/var/log/messages\n (EL6) and the verbosity of the logging can be increased by setting the \nLCMAPS_DEBUG_LEVEL\n environment variable. You can also change the destination of the logging by setting the \nLCMAPS_LOG_FILE\n environment variable.\n\n\n\n\n\n\nUse the table below to choose the appropriate file to edit:\n\n\n\n\n\n\n\n\nIf your host is a(n)...\n\n\nEdit this file...\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\n/etc/sysconfig/condor-ce\n\n\n\n\n\n\nGridFTP server\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\n\n\n\n\n\n\nAdd the following to the file chosen in the previous step:\n\n\nexport\n \nLCMAPS_DEBUG_LEVEL\n=\n5\n\n\n# optional (uncomment the following line to output log messages to a file):\n\n\n# export LCMAPS_LOG_FILE=/tmp/lcmaps.log\n\n\n\n\n\n\n\n\n\n\nUse the table below to choose the appropriate service to restart:\n\n\n\n\n\n\n\n\nIf your host is a(n)...\n\n\nRestart the following service...\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\ncondor-ce\n\n\n\n\n\n\nGridFTP server\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting mapping with HTCondor-CE\n\n\nHTCondor-CE caches auth lookups for 30 minutes by default. If you are testing changes to your various mapfiles with HTCondor-CE, you will need to disable this caching.\n\n\nTo do this, create a file in \n/etc/condor-ce/config.d\n called e.g. \n99-disablegsicache.conf\n with the following line:\n\n\nGSS_ASSIST_GRIDMAP_CACHE_EXPIRATION=0\n\n\n\n\n\nand then restart \ncondor-ce\n.\n\n\nOnce you are satisfied that your mappings are working, you may remove this file and restart \ncondor-ce\n in order to reduce the load on your CE caused by authentication.\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nConfiguration Files\n\n\nThe files are evaluated in the following order, with earlier files taking precedence over later ones:\n\n\n\n\n\n\n\n\nFile\n\n\nProvider\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\n/etc/grid-security/ban-mapfile\n\n\nAdmin\n\n\nBan DNs\n\n\n\n\n\n\n/etc/grid-security/ban-voms-mapfile\n\n\nAdmin\n\n\nBan VOs\n\n\n\n\n\n\n/etc/grid-security/grid-mapfile\n\n\nAdmin\n\n\nMap DNs\n\n\n\n\n\n\n/etc/grid-security/voms-mapfile\n\n\nAdmin\n\n\nMap VOs\n\n\n\n\n\n\n/usr/share/osg/voms-mapfile-default\n\n\nOSG\n\n\nMap VOs (default)\n\n\n\n\n\n\n\n\nManual Configuration\n\n\nThis section is intended for use as reference if you choose to forego configuring the LCMAPS VOMS plugin via osg-configure (i.e., if you prefer a configuration management system like \nAnsible\n or \nPuppet\n). Therefore, the following instructions serve as a replacement for \nthis section\n above.\n\n\nLCMAPS is configured in \n/etc/lcmaps.db\n. and since the VOMS plugin is a newer component, configuration for it may not be present in your existing \n/etc/lcmaps.db\n file.\n\n\n\n\n\n\nEnsure the following lines are present in the \"Module definitions\" section (the top section, before \nauthorize_only\n) of \n/etc/lcmaps.db\n:\n\n\ngridmapfile = \nlcmaps_localaccount.mod\n\n              \n-gridmap /etc/grid-security/grid-mapfile\n\nbanfile = \nlcmaps_ban_dn.mod\n\n          \n-banmapfile /etc/grid-security/ban-mapfile\n\nbanvomsfile = \nlcmaps_ban_fqan.mod\n\n              \n-banmapfile /etc/grid-security/ban-voms-mapfile\n\nvomsmapfile = \nlcmaps_voms_localaccount.mod\n\n              \n-gridmap /etc/grid-security/voms-mapfile\n\ndefaultmapfile = \nlcmaps_voms_localaccount2.mod\n\n                 \n-gridmap /usr/share/osg/voms-mapfile-default\n\n\nverifyproxynokey = \nlcmaps_verify_proxy2.mod\n\n          \n--allow-limited-proxy\n\n          \n--discard_private_key_absence\n\n          \n -certdir /etc/grid-security/certificates\n\n\n\n\n\n\n\n\n\n\nEdit the \nauthorize_only\n section so that it contains only the following uncommented lines:\n\n\nverifyproxynokey -\n banfile\nbanfile -\n banvomsfile | bad\nbanvomsfile -\n gridmapfile | bad\ngridmapfile -\n good | vomsmapfile\nvomsmapfile -\n good | defaultmapfile\ndefaultmapfile -\n good | bad\n\n\n\n\n\n\n\n\n\nEdit \n/etc/grid-security/gsi-authz.conf\n and ensure that it contains the following line with a newline at the end:\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout", 
            "title": "LCMAPS-VOMS authentication"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#installing-and-maintaining-the-lcmaps-voms-plugin", 
            "text": "", 
            "title": "Installing and Maintaining the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#about-this-guide", 
            "text": "LCMAPS is a software library for mapping grid certificates of incoming connections to specific Unix accounts. The LCMAPS VOMS plugin enables LCMAPS to make mapping decisions based on the VOMS attributes of grid certificates, e.g.,  /cms/Role=production/Capability=NULL . Starting in OSG 3.4, the LCMAPS VOMS plugin will replace GUMS and edg-mkgridmap as the authentication method at OSG sites.  The OSG provides a default set of mappings from VOMS attributes to Unix accounts. By configuring LCMAPS, you can override these mappings, including changing the Unix account that a VO is mapped to, banning based on VOMS attributes, banning a specific user, or adding a VO, VO group, VO role, and/or user that is not in the OSG's set of mappings.  Use this page to learn how to install and configure the LCMAPS VOMS plugin to authenticate users to access your resources on a per-VO basis.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#installing-the-lcmaps-voms-plugin", 
            "text": "To install the LCMAPS VOMS plugin, make sure that your host is up to date before installing the required packages:    Clean yum cache:  [root@client ~ ] $  yum clean all --enablerepo = *    Update software:  [root@client ~ ] $  yum update  This command will update  all  packages    Install  lcmaps , the default mapfile, and the configuration tools:  [root@server]#  yum install lcmaps vo-client-lcmaps-voms osg-configure-misc", 
            "title": "Installing the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#configuring-the-lcmaps-voms-plugin", 
            "text": "The following section describes the steps required to configure the LCMAPS VOMS plugin for authentication. If you are using OSG 3.3 packages, there are software-specific instructions that must be followed for  HTCondor-CE ,  GridFTP , and  XRootD . To check if you are running OSG 3.3, run the following command:  [root@server]#  rpm -q --queryformat = %{VERSION}\\n  osg-release  Additionally, there is  optional configuration  if you need to make changes to the default mappings.", 
            "title": "Configuring the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#enabling-the-lcmaps-voms-plugin", 
            "text": "To configure your host to use LCMAPS VOMS plugin authentication, edit  /etc/osg/config.d/10-misc.ini  and set the following options:  glexec_location   =   UNAVAILABLE  edit_lcmaps_db   =   True  authorization_method   =   vomsmap", 
            "title": "Enabling the LCMAPS VOMS plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#supporting-mapped-vos-and-users", 
            "text": "Unix accounts must exist for each VO, VO role, VO group, or user you choose to support in the  mapfiles :    Consult the default VO mappings in  /usr/share/osg/voms-mapfile-default  to determine the mapped Unix account names. Each of the mapfiles has the following format:  VO, VO role, VO group or user   Unix account     Create Unix accounts for each VO, VO role, VO group, and user that you wish to support   Edit  /etc/osg/config.d/30-gip.ini  and specify the supported VOs per  Subcluster or ResourceEntry section :   allowed_vos = VO1,VO2...", 
            "title": "Supporting mapped VOs and users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#applying-configuration-settings", 
            "text": "Making changes to the OSG configuration files in the  /etc/osg/config.d  directory does not apply those settings to software automatically. For the OSG settings, use the  osg-configure  tool to validate (to a limited extent) and apply the settings to the relevant software components. If instead you wish to manage the LCMAPS VOMS plugin configuration yourself, skip to the  manual configuration section .    Make all changes to  .ini  files in the  /etc/osg/config.d  directory.   Note  This document only describes the critical settings for the LCMAPS VOMS plugin and related software. You may need to configure other software that is installed on your host, too.     Validate the configuration settings:  [root@server]#  osg-configure -v    Once the validation command succeeds without errors, apply the configuration settings:  [root@server]#  osg-configure -c", 
            "title": "Applying configuration settings"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#optional-configuration", 
            "text": "The following subsections contain information on migration from  edg-mkgridmap , mapping or banning users by their certificates' Distinguished Names (DNs) or by their proxies' VOMS attributes. Any optional configuration is to be performed after the installation and configuration sections above.  For a table of the configuration files and their order of evaluation, consult the  reference section .   Migrating from edg-mkgridmap  Mapping VOs  Mapping users  Banning VOs  Banning users", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#migrating-from-edg-mkgridmap", 
            "text": "The program edg-mkgridmap (found in the package  edg-mkgridmap ), used for authentication on HTCondor-CE, GridFTP, and XRootD hosts, is no longer available starting in OSG 3.4. The LCMAPS VOMS plugin (package  lcmaps-plugins-voms ) now provides the same functionality. To migrate from edg-mkgridmap to the LCMAPS VOMS plugin, perform the following procedure:    Configure user DN mappings:    If you have a local grid mapfile (see  the EDG-mkgridmap docs ), replace the contents of  /etc/grid-security/grid-mapfile  with the contents of the local grid mapfile.    If you do not have a local grid mapfile, remove  /etc/grid-security/grid-mapfile .      If you are remaining on OSG 3.3, ensure that the you have set  export LLGT_VOMS_ENABLE_CREDENTIAL_CHECK=1  in the appropriate file and restart the service. If you have updated your host to OSG 3.4, skip to the next step.     If your host is a(n)...  Add the aforementioned line to...  And restart this service...      HTCondor-CE  /etc/sysconfig/condor-ce  condor-ce    GridFTP server  /etc/sysconfig/globus-gridftp-server  globus-gridftp-server       If you are converting an HTCondor-CE host, remove the HTCondor-CE  GRIDMAP  configuration. Otherwise, skip to the next step.    Find the location of this configuration using the following command:  [root@ce]#  condor_ce_config_val -v GRIDMAP    Delete the line that sets the  GRIDMAP  configuration variable   Reconfigure HTCondor-CE: [root@ce]#  condor_ce_reconfig      Remove edg-mkgridmap and related packages:  [root@ce]#  yum erase edg-mkgridmap   Warning  In the output from this command, yum should  not  list other packages than the one. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.3 (or 3.4) versions (they should have \".osg33\" or \".osg34\" in their versions), and try again.", 
            "title": "Migrating from edg-mkgridmap"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#mapping-vos", 
            "text": "/etc/grid-security/voms-mapfile  is used to map VOs, VO roles, or VO groups to Unix accounts based on their VOMS attributes. An example of the format of a  voms-mapfile  follows:  # map GLOW jobs in the chtc group to the  glow1  Unix account. /GLOW/chtc/*  glow1\n# map GLOW jobs with the htpc role to the  glow2  Unix account. /GLOW/Role=htpc/*  glow2\n# map other GLOW jobs to the  glow  Unix account. /GLOW/*  glow  Each non-commented line is a shell-style pattern which is compared against the user's VOMS attributes, and a Unix account that the user will be mapped to if the pattern matches.\nThe patterns are compared in the order they are listed in. Therefore, more general patterns should be placed later in the file.   Note  The Unix account must exist for the user to be mapped. If a VO's Unix account is missing, that VO will not be able to access your resources.", 
            "title": "Mapping VOs"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#mapping-users", 
            "text": "/etc/grid-security/grid-mapfile  is used to map specific users to Unix accounts based on their certificates' DNs. An example of the format of a  grid-mapfile  follows:  # map Matyas s FNAL DN to the  matyas  Unix account /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas  matyas   Note  The Unix account must exist for the user to be mapped. If a user's Unix account is missing, that user will not be able to access your resources.", 
            "title": "Mapping users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#banning-vos", 
            "text": "/etc/grid-security/ban-voms-mapfile  is used to ban an entire VO or a role withing a VO from accessing resources on your machine. An example of the format of a  ban-voms-mapfile  follows:  # ban CMS production jobs /cms/Role=production/*   Each non-commented line is a shell-style pattern which is compared against a user's VOMS attributes. If the pattern matches, that user will be unable to access your resources.   Warning  /etc/grid-security/ban-voms-mapfile   must  exist, even if you are not banning any VOs. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.", 
            "title": "Banning VOs"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#banning-users", 
            "text": "/etc/grid-security/ban-mapfile  is used to ban specific users from accessing your resources based on their certificates' DNs. An example of the format of a  ban-mapfile  follows:  # ban Matyas s FNAL DN /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas    Warning  /etc/grid-security/ban-mapfile   must  exist, even if you are not banning any users. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.", 
            "title": "Banning users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#validating-the-lcmaps-voms-plugin-vo-mappings", 
            "text": "To validate the LCMAPS VOMS plugin by itself, use the following procedure to test mapping your own cert to a user:   Verify your DN is  not  in  /etc/grid-security/grid-mapfile , or else it will generate a false positive  Verify your DN is  not  in  /etc/grid-security/ban-mapfile , or else it will generate a false negative   Install the  llrun  and  voms-clients  packages:  [root@host]#  yum install llrun voms-clients    As an unprivileged user, create a VOMS proxy (filling in  YOUR_VO  with a VO you are a member of):  [you@client]$  voms-proxy-init -voms  YOUR_VO     Verify that your credentials are mapped as expected:  [you@client]$  llrun -s -l  mode = pem,policy = authorize_only,db = /etc/lcmaps.db  \\ \n    -p/tmp/x509up_u ` id -u `     If you did not get correctly mapped, check your proxy's FQAN by running:  [you@client]$  voms-proxy-info -fqan  and make sure it matches one of the patterns in  /etc/grid-security/voms-mapfile  or  /usr/share/osg/voms-mapfile-default , and does not match any patterns in  /etc/grid-security/ban-voms-mapfile .", 
            "title": "Validating the LCMAPS VOMS plugin VO mappings"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#troubleshooting-the-lcmaps-voms-plugin", 
            "text": "LCMAPS logs to  journalctl  (EL7) or  /var/log/messages  (EL6) and the verbosity of the logging can be increased by setting the  LCMAPS_DEBUG_LEVEL  environment variable. You can also change the destination of the logging by setting the  LCMAPS_LOG_FILE  environment variable.    Use the table below to choose the appropriate file to edit:     If your host is a(n)...  Edit this file...      HTCondor-CE  /etc/sysconfig/condor-ce    GridFTP server  /etc/sysconfig/globus-gridftp-server     Add the following to the file chosen in the previous step:  export   LCMAPS_DEBUG_LEVEL = 5  # optional (uncomment the following line to output log messages to a file):  # export LCMAPS_LOG_FILE=/tmp/lcmaps.log     Use the table below to choose the appropriate service to restart:     If your host is a(n)...  Restart the following service...      HTCondor-CE  condor-ce    GridFTP server  globus-gridftp-server", 
            "title": "Troubleshooting the LCMAPS VOMS plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#troubleshooting-mapping-with-htcondor-ce", 
            "text": "HTCondor-CE caches auth lookups for 30 minutes by default. If you are testing changes to your various mapfiles with HTCondor-CE, you will need to disable this caching.  To do this, create a file in  /etc/condor-ce/config.d  called e.g.  99-disablegsicache.conf  with the following line:  GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION=0  and then restart  condor-ce .  Once you are satisfied that your mappings are working, you may remove this file and restart  condor-ce  in order to reduce the load on your CE caused by authentication.", 
            "title": "Troubleshooting mapping with HTCondor-CE"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#configuration-files", 
            "text": "The files are evaluated in the following order, with earlier files taking precedence over later ones:     File  Provider  Purpose      /etc/grid-security/ban-mapfile  Admin  Ban DNs    /etc/grid-security/ban-voms-mapfile  Admin  Ban VOs    /etc/grid-security/grid-mapfile  Admin  Map DNs    /etc/grid-security/voms-mapfile  Admin  Map VOs    /usr/share/osg/voms-mapfile-default  OSG  Map VOs (default)", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#manual-configuration", 
            "text": "This section is intended for use as reference if you choose to forego configuring the LCMAPS VOMS plugin via osg-configure (i.e., if you prefer a configuration management system like  Ansible  or  Puppet ). Therefore, the following instructions serve as a replacement for  this section  above.  LCMAPS is configured in  /etc/lcmaps.db . and since the VOMS plugin is a newer component, configuration for it may not be present in your existing  /etc/lcmaps.db  file.    Ensure the following lines are present in the \"Module definitions\" section (the top section, before  authorize_only ) of  /etc/lcmaps.db :  gridmapfile =  lcmaps_localaccount.mod \n               -gridmap /etc/grid-security/grid-mapfile \nbanfile =  lcmaps_ban_dn.mod \n           -banmapfile /etc/grid-security/ban-mapfile \nbanvomsfile =  lcmaps_ban_fqan.mod \n               -banmapfile /etc/grid-security/ban-voms-mapfile \nvomsmapfile =  lcmaps_voms_localaccount.mod \n               -gridmap /etc/grid-security/voms-mapfile \ndefaultmapfile =  lcmaps_voms_localaccount2.mod \n                  -gridmap /usr/share/osg/voms-mapfile-default \n\nverifyproxynokey =  lcmaps_verify_proxy2.mod \n           --allow-limited-proxy \n           --discard_private_key_absence \n            -certdir /etc/grid-security/certificates     Edit the  authorize_only  section so that it contains only the following uncommented lines:  verifyproxynokey -  banfile\nbanfile -  banvomsfile | bad\nbanvomsfile -  gridmapfile | bad\ngridmapfile -  good | vomsmapfile\nvomsmapfile -  good | defaultmapfile\ndefaultmapfile -  good | bad    Edit  /etc/grid-security/gsi-authz.conf  and ensure that it contains the following line with a newline at the end:  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout", 
            "title": "Manual Configuration"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/", 
            "text": "OSG Upcoming Software Repositories Installation Guide\n\n\nHere we describe the OSG Upcoming Software Repositories, their purpose, and how to use them.\n\n\nThis document is intended for site administrators.\n\n\nOverview and Purpose\n\n\nCertain sites have requested new versions of software that would be considered \"disruptive\" or \"experimental\" -- upgrading to them would likely require manual intervention before the site would come back up. We do not want sites to unwittingly upgrade to these versions. For the benefit of the sites that do want to upgrade, we want to provide the same assurance of quality and production-readiness that we provide to the software we currently ship now.\n\n\nDue to the relatively small number of such packages, a full fork of the OSG 3 distribution was not warranted. Instead, we have created a separate set of repositories that contain only the \"disruptive\" versions of the software.\n\n\nThese repositories have the same structure as our standard repositories. For example, there is an \nosg-upcoming-testing\n repository and an \nosg-upcoming\n repository.\n\n\nA full installation of our software stack is \nnot\n be possible using only the \nosg-upcoming\n repositories, since they contain a small subset of the software we ship. Both the main \nosg\n and the \nosg-upcoming\n repositories will need to be enabled for the installation to work. Because of this, interoperability will be maintained between the main \nosg\n and \nosg-upcoming\n.\n\n\nDepending on test results from sites, some packages in \nosg-upcoming\n may eventually end up in the main \nosg\n branch. The rest of the packages will eventually form the nucleus of the next fork of the software stack (e.g. \"OSG 3.5\").\n\n\nInstallation and Usage\n\n\nThe following directions must be followed to install software from the Upcoming repositories.\n\n\nFirst, install the standard OSG YUM repositories as per \nthe YUM repositories guide\n.\n\n\nEnabling Upcoming repositories\n\n\nYou should have a file called \nosg-upcoming.repo\n (el7) or \nosg-el6-upcoming.repo\n (el6) located in \n/etc/yum.repos.d\n. At the top, it should have a section looking like this (el7):\n\n\n[osg-upcoming]\n\n\nname\n=\nOSG Software for Enterprise Linux 7 - Upcoming - $basearch\n\n\n#baseurl=http://repo.grid.iu.edu/upcoming/el7/$basearch/release\n\n\nmirrorlist\n=\nhttp://repo.grid.iu.edu/mirror/upcoming/el7/$basearch/release\n\n\nfailovermethod\n=\npriority\n\n\npriority\n=\n98\n\n\nenabled\n=\n0\n\n\ngpgcheck\n=\n1\n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\nconsider_as_osg\n=\nyes\n\n\n\n\n\n\nEdit this file, and under the section \n[osg-upcoming]\n, change \nenabled=0\n to \nenabled=1\n. This will enable the production upcoming software repositories. Future YUM installs will bring in software from the upcoming repositories.\n\n\nTo get help, please use \nthis page\n.", 
            "title": "OSG upcoming software"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/#osg-upcoming-software-repositories-installation-guide", 
            "text": "Here we describe the OSG Upcoming Software Repositories, their purpose, and how to use them.  This document is intended for site administrators.", 
            "title": "OSG Upcoming Software Repositories Installation Guide"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/#overview-and-purpose", 
            "text": "Certain sites have requested new versions of software that would be considered \"disruptive\" or \"experimental\" -- upgrading to them would likely require manual intervention before the site would come back up. We do not want sites to unwittingly upgrade to these versions. For the benefit of the sites that do want to upgrade, we want to provide the same assurance of quality and production-readiness that we provide to the software we currently ship now.  Due to the relatively small number of such packages, a full fork of the OSG 3 distribution was not warranted. Instead, we have created a separate set of repositories that contain only the \"disruptive\" versions of the software.  These repositories have the same structure as our standard repositories. For example, there is an  osg-upcoming-testing  repository and an  osg-upcoming  repository.  A full installation of our software stack is  not  be possible using only the  osg-upcoming  repositories, since they contain a small subset of the software we ship. Both the main  osg  and the  osg-upcoming  repositories will need to be enabled for the installation to work. Because of this, interoperability will be maintained between the main  osg  and  osg-upcoming .  Depending on test results from sites, some packages in  osg-upcoming  may eventually end up in the main  osg  branch. The rest of the packages will eventually form the nucleus of the next fork of the software stack (e.g. \"OSG 3.5\").", 
            "title": "Overview and Purpose"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/#installation-and-usage", 
            "text": "The following directions must be followed to install software from the Upcoming repositories.  First, install the standard OSG YUM repositories as per  the YUM repositories guide .", 
            "title": "Installation and Usage"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/#enabling-upcoming-repositories", 
            "text": "You should have a file called  osg-upcoming.repo  (el7) or  osg-el6-upcoming.repo  (el6) located in  /etc/yum.repos.d . At the top, it should have a section looking like this (el7):  [osg-upcoming]  name = OSG Software for Enterprise Linux 7 - Upcoming - $basearch  #baseurl=http://repo.grid.iu.edu/upcoming/el7/$basearch/release  mirrorlist = http://repo.grid.iu.edu/mirror/upcoming/el7/$basearch/release  failovermethod = priority  priority = 98  enabled = 0  gpgcheck = 1  gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  consider_as_osg = yes   Edit this file, and under the section  [osg-upcoming] , change  enabled=0  to  enabled=1 . This will enable the production upcoming software repositories. Future YUM installs will bring in software from the upcoming repositories.  To get help, please use  this page .", 
            "title": "Enabling Upcoming repositories"
        }, 
        {
            "location": "/other/install-gwms-frontend/", 
            "text": "GlideinWMS VO Frontend Installation\n\n\nAbout This Document\n\n\nThis document describes how to install the Glidein Workflow Managment System (GlideinWMS) VO Frontend for use with the OSG glidein factory. This software is the minimum requirement for a VO to use glideinWMS.\n\n\nThis document assumes expertise with Condor and familiarity with the glideinWMS software. It \ndoes not\n cover anything but the simplest possible install. Please consult the \nGlidein WMS reference documentation\n for advanced topics, including non-\nroot\n, non-RPM-based installation.\n\n\nThis document covers three components of the GlideinWMS a VO needs to install:\n\n\n\n\nUser Pool Collectors\n: A set of \ncondor_collector\n processes. Pilots submitted by the factory will join to one of these collectors to form a Condor pool.\n\n\nUser Pool Schedd\n: A \ncondor_schedd\n. Users may submit Condor vanilla universe jobs to this schedd; it will run jobs in the Condor pool formed by the \nUser Pool Collectors\n.\n\n\nGlidein Frontend\n: The frontend will periodically query the \nUser Pool Schedd\n to determine the desired number of running job slots. If necessary, it will request the factory to launch additional pilots.\n\n\n\n\nThis guide covers installation of all three components on the same host: it is designed for small to medium VOs (see the Hardware Requirements below). Given a significant, large host, we have been able to scale the single-host install to 20,000 running jobs.\n\n\n\n\nRelease\n\n\nThis document reflects glideinWMS v3.2.17.\n\n\nHow to get Help?\n\n\nTo get assistance about the OSG software please use \nthis page\n.\n\n\nFor specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support \n\n\nTo request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to \n (see below).\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nA host to install the GlideinWMS Frontend (pristine node).\n\n\nOS is Red Hat Enterprise Linux 6, 7, and variants (see \ndetails\n). Currently most of our testing has been done on Scientific Linux 6.\n\n\nRoot access\n\n\n\n\nThe Glidein WMS VO Frontend has the following hardware requirements for a production host:\n\n\n\n\nCPU\n: Four cores, preferably no more than 2 years old.\n\n\nRAM\n: 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed.\n\n\nDisk\n: 30GB will be plenty sufficient for all the binaries, config and log files related to glideinWMS. As this will be an interactive submit host, plan enough disk space for your users' jobs. Depending on your workflow, this might require 2MB to 2GB per job in a workflow.\n\n\n\n\nUsers\n\n\nThe Glidein WMS Frontend installation will create the following users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nDefault uid\n\n\nComment\n\n\n\n\n\n\n\n\n\n\napache\n\n\n48\n\n\nRuns httpd to provide the monitoring page (installed via dependencies).\n\n\n\n\n\n\ncondor\n\n\nnone\n\n\nCondor user (installed via dependencies).\n\n\n\n\n\n\nfrontend\n\n\nnone\n\n\nThis user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins.\n\n\n\n\n\n\ngratia\n\n\nnone\n\n\nRuns the Gratia probes to collect accounting data (optional see \nthe Gratia section below\n)\n\n\n\n\n\n\n\n\nNote that if uid 48 is already taken but not used for the appropriate users, you will experience errors. \nDetails...\n\n\nCredentials and Proxies\n\n\nThe VO Frontend will use two credentials in its interactions with the the other glideinWMS services. At this time, these will be proxy files.\n\n\n\n\nthe \nVO Frontend proxy\n (used to authenticate with the other glideinWMS services).\n\n\none or more glideinWMS \npilot proxies\n (used/delegated to the factory services and submitted on the glideinWMS pilot jobs).\n\n\n\n\nThe \nVO Frontend proxy\n and the \n pilot proxy\n can be the same. By default, the VO Frontend will run as user \nfrontend\n (UID is machine dependent) so these proxies must be owned by the user \nfrontend\n.\n\n\nVO Frontend proxy\n\n\nThe use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the \nproxy configuration section\n. This can be a plain grid proxy (from \ngrid-proxy-init\n), no VO extensions are required.\n\n\nYou must notify the Factory operation of the DN of this proxy when you initially setup the frontend and each time the DN changes\n.\n\n\nPilot proxies\n\n\nThis proxy is used by the factory to submit the glideinWMS pilot jobs. Therefore, they must be authorized to access to the CEs (factory entry points) where jobs are submitted. There is no need to notify the Factory operation about the DN of this proxy (neither at the initial registration nor for subsequent changes). This second proxy has no special requirement or controls added by the factory but will probably require VO attributes because of the CEs: if you are able to use this proxy to submit jobs to the CEs where the Factory runs glideinWMS pilots for you, then the proxy is OK. You can test your proxy using \nglobusrun\n or HTCondor-G\n\n\nTo check the important information about a pem certificate you can use: \nopenssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout\n. You will need that to find out information for the configuration files and the request to the GlideinWMS factory.\n\n\nCertificates/Proxies configuration example\n\n\nThis document has a \nproxy configuration section\n that uses the host certificate/key and a user certificate to generate the required proxies.\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n            \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nHere\n are instructions to request a host certificate.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHTCondor port range\n\n\ntcp\n\n\nLOWPORT, HIGHPORT\n\n\nYES\n\n\n\n\ncontiguous range of ports\n\n\n\n\n\n\nGlideinWMS Frontend\n\n\ntcp\n\n\n9618 to 9660\n\n\nYES\n\n\n\n\nHTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs)\n\n\n\n\n\n\n\n\nThe VO frontend must have reliable network connectivity, be on the public internet (no NAT), and preferably with no firewalls. Each running pilot requires 5 outgoing TCP ports. Incoming TCP ports 9618 to 9660 must be open.\n\n\n\n\nFor example, 2000 running jobs require about 10,100 TCP connections. This will overwhelm many firewalls; if you are unfamiliar with your network topology, you may want to warn your network administrator.\n\n\n\n\nBefore the installation\n\n\nOnce all requirements are satisfied you must take a couple of actions before installing the Frontend:\n\n\n\n\nyou need all the data to connect to a GWMS Factory\n\n\nRemember to install HTCondor BEFORE installing the Frontend (\ninstructions are below\n)\n\n\n\n\nOSG Factory access\n\n\nBefore installing the Glidein WMS VO Frontend you need the information about a \nGlidein Factory\n that you can access:\n\n\n\n\n(recommended) OSG is managing a factory at UCSD and one at GOC and you can request access to them\n\n\nYou have another Glidein Factory that you can access\n\n\nYou \ninstall your own Glidein Factory\n\n\n\n\nTo request access to the OSG Glidein Factory at UCSD you have to send an email to \n providing:\n\n\n\n\nYour Name\n\n\nThe VO that is utilizing the VO Frontend\n\n\nThe DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the \nproxy configuration section\n)\n\n\nYou can propose a security name that will have to be confirmed/changed by the Factory managers (see below)\n\n\n\n\nA list of sites where you want to run:\n\n\n\n\n\n\nYour VO must be supported on those sites\n\n\n\n\nYou can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers\n\n\nYou can start with one single site\n\n\n\n\nIn the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend\n\n\n\n\nThe exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\".\n\n\nThe host of the Factory Collector: \ngfactory-1.t2.ucsd.edu\n\n\nThe DN os the factory, e.g. \n/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\n\n\nThe factory identity, e.g.: \ngfactory@gfactory-1.t2.ucsd.edu\n\n\nThe identity on the factory you will be mapped to. Something like: \nusername@gfactory-1.t2.ucsd.edu\n\n\nYour security name. A unique name, usually containing your VO name: \nMy_SecName\n\n\nA string to add in the main factory query_expr in the frontend configuration, e.g. \nstringListMember(\"\nVO\n\",GLIDEIN_Supported_VOs)\n. From there you get the correct name of the VO (above in this list).\n\n\n\n\nInstallation Procedure\n\n\nRefer to installation of the \nCA certificates\n\n\nInstall HTCondor\n\n\nMost required software is installed from the Frontend RPM installation. HTCondor is the only exception since there are \nmany different ways to install it\n, using the RPM system or not. You need to have HTCondor installed before installing the Glidein WMS Frontend. If yum cannot find a HTCondor RPM, it will install the dummy \nempty-condor\n RPM, assuming that you installed HTCondor using a tarball distribution.\n\n\nIf you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository:\n\n\n[root@client ~] #\n yum install condor.x86_64\n\n#\n If you have a \n32\n bit host use instead:\n\n[root@client ~] #\n yum install condor.i386\n\n\n\n\n\nSee \nthis HTCondor document\n for more information on the different options.\n\n\nDownload and install the VO Frontend RPM\n\n\nThe RPM is available in the OSG repository:\n\n\nInstall the RPM and dependencies (be prepared for a lot of dependencies).\n\n\n[root@client ~] #\n yum install glideinwms-vofrontend\n\n\n\n\n\nThis will install the current production release verified and tested by OSG with default condor configuration. This command will install the glideinwms vofrontend, condor, the OSG client, and all the required dependencies all on one node.\n\n\nIf you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows:\n\n\n\n\nyum install --enablerepo=osg-testing glideinwms-vofrontend\n: The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well.\n\n\nyum install --enablerepo=osg-contrib glideinwms-vofrontend\n: The most recent development series release, ie version 3 release. This has newer features such as cloud submission support, but is less tested.\n\n\n\n\nNote that these commands will install default condor configurations with all services on one node.\n\n\nAdvanced: Multi-node Installation\n\n\nFor advanced users requiring heavy usage on their submit node, you may want to consider splitting the usercollector, user submit, and vo frontend services.\n\n\nThis can be doing using the following three commands (on different machines):\n\n\n[root@client ~] #\n yum install glideinwms-vofrontend-standalone\n\n[root@client ~] #\n yum install glideinwms-usercollector\n\n[root@client ~] #\n yum install glideinwms-userschedd\n\n\n\n\n\nIn addition, you will need to perform the following steps:\n\n\n\n\nOn the vofrontend and userschedd, modify CONDOR_HOST to point to your usercollector. This is in \n/etc/condor/config.d/00_gwms_general.config\n. You can also override this value by placing it in a new config file. (For instance, \n/etc/condor/config.d/99_local_custom.config\n to avoid rpmsave/rpmnew conflicts on upgrades).\n\n\nIn \n/etc/condor/certs/condor_mapfile\n, you will need to all DNs for each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the \nglidecondor_addDN\n to add these values.\n\n\nIn the \n/etc/gwms-frontend/frontend.xml\n file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend xml are in the following sections.\n\n\n\n\nUpgrade Procedure\n\n\nIf you have a working installation of glideinwms-frontend you can just upgrade the frontend rpms and skip the most of the configuration procedure below. These general upgrade instructions apply when upgrading the glideinwms-frontend rpm within same major versions.\n\n\n%\nRED%# Update the glideinwms-vofrontend packages\n\n\n[root@client ~] #\n yum update glideinwms\n\\*\n\n\n%\nRED%# Update the scripts in the working directory to the latest one\n\n\n%\nRED%# For RHEL \n7\n, CentOS \n7\n, and SL7\n\n\n[root@client ~] #\n /usr/sbin/gwms-frontend upgrade\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\n[root@client ~] #\n service gwms-frontend upgrade\n\n%\nRED%# Restart HTCondor because the configuration may be different\n\n\n[root@client ~] #\n service condor restart\n\n\n\n\n\n\n\nNote\n\n\nThe \\*\n on the yum update is important.\n*\n\n\n\n\n\n\nWarning\n\n\nWhen you do a generic yum update that will update also condor, the upgrade may restore the personal condor config file that you have to remove with \nrm /etc/condor/config.d/00personal_condor.config\n\n\n\n\n\n\nNote\n\n\nWhen upgrading to GlideinWMS 3.2.7 the second schedd is removed from the default configuration. For a smooth transition:\n\n\n\n\nremove from \n/etc/gwms-frontend/frontend.xml\n the second schedd (the line containing \nschedd_jobs2@YOUR_HOST\n)\n\n\nreconfigure the frontend (\nservice gwms-frontend reconfig\n)\n\n\nrestart HTCondor (\nservice condor restart\n)\n\n\n\n\n\n\nUpgrading glideinwms-frontend from v2 series to v3 series\n\n\nDue to incompatibilities between the major versions, upgrade process involves certain steps. Following instructions apply when upgrading glideinwms-frontend from a v2 series (example: v2.7.x) to a v3 series (v3.2.x)\n\n\n\n\nUpdate the RPMs and backup configuration files\n\n\n\n\n%\nRED%# Stop the glideinwms-vofrontend service\n\n\n[root@client ~] #\n service gwms-frontend stop\n\n\n%\nRED%# Backup the v2.7.x configuration\n\n\n[root@client ~] #\n cp /var/lib/gwms-frontend/vofrontend/frontend.xml /var/lib/gwms-frontend/vofrontend/frontend-2.xml\n\n[root@client ~] #\n cp /etc/gwms-frontend/frontend.xml /etc/gwms-frontend/frontend-2.xml\n\n\n%\nRED%# Update the glideinwms-vofrontend packages from v2.7.x to v3.2.x\n\n\n[root@client ~] #\n yum update glideinwms\n\\*\n\n\n\n\n\n\n\n\nConvert v2.7.x configuration to v3.2.x configuration (only for RHEL 6, CentOS 6, and SL6. RHEL5 and drivative are not supported by v3.2.x, RHEL7 and derivative were not supported by v2.7.x)\n\n\n\n\n[root@client ~] #\n  /usr/lib/python2.6/site-packages/glideinwms/frontend/tools/convert_frontend_2to3.sh -i /var/lib/gwms-frontend/vofrontend/frontend-2.xml -o /var/lib/gwms-frontend/vofrontend/frontend.xml -s /usr/lib/python2.6/site-packages/glideinwms\n\n[root@client ~] #\n  /usr/lib/python2.6/site-packages/glideinwms/frontend/tools/convert_frontend_2to3.sh -i /etc/gwms-frontend/frontend-2.xml -o /etc/gwms-frontend/frontend.xml -s /usr/lib/python2.6/site-packages/glideinwms\n\n\n\n\n\n\n\nUpdate the scripts in the working directory\n\n\n\n\n%\nRED%# Update the scripts in the working directory to the latest one\n\n\n[root@client ~] #\n  service gwms-frontend upgrade\n\n\n\n\n\nConfiguration Procedure\n\n\nAfter installing the RPM, you need to configure the components of the glideinWMS VO Frontend:\n\n\n\n\nEdit Frontend configuration options\n\n\nEdit Condor configuration options\n\n\nCreate a Condor grid map file\n\n\nReconfigure and Start frontend\n\n\n\n\nConfiguring the Frontend\n\n\nThe VO Frontend configuration file is \n/etc/gwms-frontend/frontend.xml\n. The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted in red font. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference.\n\n\n\n\n\n\nThe VO you are affiliated with. This will identify those CEs that the glideinWMS pilot will be authorized to run on using the \npilot proxy\n described previously in the this \nsection\n. Sometimes the whole \nquery_expr\n is provided to you by the factory (see Factory access above):\n\n\nfactory query_expr=\n((stringListMember(\nVO\n, GLIDEIN_Supported_VOs)))\n\n\n\n\n\n\n\n\n\n\nFactory collector information. The \nusername\n that you are assigned by the factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production factory, you will have to change also \nDN\n, \nfactory_identity\n and \nnode\n attributes. (refer to the information provided to you by the factory operator):\n\n\ncollector DN=\n/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\n\n           comment=\nDefine factory collector globally for simplicity\n\n           factory_identity=\ngfactory@gfactory-1.t2.ucsd.edu\n\n           my_identity=\nusername@gfactory-1.t2.ucsd.edu\n\n           node=\ngfactory-1.t2.ucsd.edu\n/\n\n\n\n\n\n\n\n\n\n\nFrontend security information.\n\n\n\n\nThe \nclassad_proxy\n in the security entry is the location of the VO Frontend proxy described previously \nhere\n.\n\n\nThe \nproxy_DN\n is the DN of the \nclassad_proxy\n above.\n\n\nThe \nsecurity_name\n identifies this VO Frontend to the the Factory, It is provided by the factory operator.\n\n\nThe \nabsfname\n in the credential (or proxy in v 2.x) entry is the location of the glideinWMS \npilot\n proxy described in the requirements section \nhere\n. There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources). \nThe type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the factory. If there is no match, between these two attributes in one of the credentials and some entry in one of the factories, then this frontend cannot trigger glideins.\n\nBoth the \nclassad_proxy\n and \nabsfname\n files should be owned by \nfrontend\n user.\n# These lines are from the configuration of v 3.x\n\nsecurity\n \nclassad_proxy=\n/tmp/vo_proxy\n \nproxy_DN=\nDN of vo_proxy\n\n      \nproxy_selection_plugin=\nProxyAll\n\n      \nsecurity_name=\nThe security name, this is used by factory\n\n      \nsym_key=\naes_256_cbc\n\n      \ncredentials\n\n        \ncredential\n \nabsfname=\n/tmp/pilot_proxy\n \nsecurity_class=\nfrontend\n\n        \ntrust_domain=\nOSG\n \ntype=\ngrid_proxy\n/\n\n      \n/credentials\n\n\n/security\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe schedd information.\n\n\n\n\nThe \nDN\n of the \nVO Frontend Proxy\n described previously \nhere\n.\n\n\nThe \nfullname\n attribute is the fully qualified domain name of the host where you installed the VO Frontend (\nhostname --fqdn\n).\n\n\n\n\nA secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the frontend to service requests from multiple submit hosts.\n\n\n   \nschedds\n\n     \nschedd\n \nDN=\nCert DN used by the schedd at fullname:\n\n           \nfullname=\nHostname of the schedd\n/\n\n      \nschedd\n \nDN=\nCert DN used by the second Schedd at fullname:\n\n            \nfullname=\nschedd name@Hostname of second schedd\n/\n\n   \n/schedds\n\n\n\n\n\n\n\n\n\n\nThe User Collector information.\n\n\n\n\nThe \nDN\n of the \nVO Frontend Proxy\n described previously \nhere\n.\n\n\nThe \nnode\n attribute is the full hostname of the collectors (\nhostname --fqdn\n) and port\n\n\nThe \nsecondary\n attribute indicates whether the element is for the primary or secondary collectors (True/False).\n\n\n\n\nThe default Condor configuration of the VO Frontend starts multiple Collector processes on the host (\n/etc/condor/config.d/11_gwms_secondary_collectors.config\n). The \nDN\n and \nhostname\n on the first line are the hostname and the host certificate of the VO Frontend. The \nDN\n and \nhostname\n on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector ports can be defined as a range, e.g., 9620-9660).\n\n\n    \ncollector DN=\nDN of main collector\n\n           node=\nhostname.domain.tld:9618\n secondary=\nFalse\n/\n\n    \ncollector DN=\nDN of secondary collectors (usually same as DN in line above)\n\n           node=\nhostname.domain.tld:9620-9660\n secondary=\nTrue\n/\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths.     Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation):\n\n\n\n\nfrontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions)\n\n\nwork base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value)\n\n\n\n\n\n\nIf you have a different Factory\n\n\nThe configuration above points to the OSG production Factory. If you are using a different Factory, then you have to:\n\n\n\n\nreplace \ngfactory@gfactory-1.t2.ucsd.edu\n and \ngfactory-1.t2.ucsd.edu\n with the correct values for your factory. And control also that the name used for the frontend () matches.\n\n\nmake sure that the factory is advertising the attributes used in the factory query expression (\nquery_expr\n).\n\n\n\n\nConfiguring Condor\n\n\nThe condor configuration for the frontend is placed in \n/etc/condor/config.d\n.\n\n\n\n\n00_gwms_general.config\n\n\n01_gwms_collectors.config\n\n\n02_gwms_schedds.config\n\n\n03_gwms_local.config\n\n\n11_gwms_secondary_collectors.config\n\n\n90_gwms_dns.config\n\n\n\n\nGet rid of the pre-loaded condor default to avoid conflicts in the configuration.\n\n\n[root@client ~] #\n rm /etc/condor/config.d/00personal_condor.config\n\n\n\n\n\nFor most installations create a new file named \n/etc/condor/config.d/92_local_condor.config\n\n\nUsing other Condor RPMs, e.g. UW Madison HTCondor RPM\n\n\nThe above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using \nyum list condor\n. The version name should include \"osg\", e.g. \n7.8.6-3.osg.el5\n.\n\n\nIf you are using the UW Madison Condor RPMS, be aware of the following changes:\n\n\n\n\nThis Condor RPM uses a file \n/etc/condor/condor_config.local\n to add your local machine slot to the user pool.\n\n\nIf you want to disable this behavior (recommended), you should blank out that file or comment out the line in \n/etc/condor/condor_config\n for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to \n/etc/condor/config.d\n)\n\n\nNote that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the glideinwms RPMs, but please take note if you use this variable in your job submissions or other customizations.\n\n\n\n\nIn general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files:\n\n\n[root@client ~] #\n condor_config_val -config\n\nConfiguration source:\n\n\n    /etc/condor/condor_config\n\n\nLocal configuration sources:\n\n\n    /etc/condor/config.d/00_gwms_general.config\n\n\n    /etc/condor/config.d/01_gwms_collectors.config\n\n\n    /etc/condor/config.d/02_gwms_schedds.config\n\n\n    /etc/condor/config.d/03_gwms_local.config\n\n\n    /etc/condor/config.d/11_gwms_secondary_collectors.config\n\n\n    /etc/condor/config.d/90_gwms_dns.config\n\n\n%\nRED%/etc/condor/condor_config.local\n\n\n\n\n\n\nIf, like in the example above, the GlideinWMS configuration files are not the last ones in the list please verify that important configuration options have not been overridden by the other configuration files.\n\n\nVerify your Condor configuration\n\n\n\n\n\n\nThe glideinWMS configuration files in \n/etc/condor/config.d\n should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files.\n\n\n\n\n\n\nVerify the alll the expected HTCondor daemons are running:\n\n\n[root@client ~] #\n condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, COLLECTOR0\n\nCOLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12,\n\n\nCOLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23,\n\n\nCOLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34,\n\n\nCOLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40\n\n\nDefined in \n/etc/condor/config.d/11_gwms_secondary_collectors.config\n, line 193.\n\n\n\n\n\n\n\n\n\n\nIf you don't see all the collectors. shared port and the two schedd, then the configuration must be corrected. There should be \nno\n \nstartd\n daemons listed.\n\n\nCreate a Condor grid mapfile.\n\n\nThe Condor grid mapfile (\n/etc/condor/certs/condor_mapfile\n) is used for authentication between the glideinWMS pilot running on a remote worker node, and the local collector. Condor uses the mapfile to map certificates to pseudo-users on the local machine. It is important that you map the DN's of:\n\n\n\n\n\n\nEach schedd proxy\n: The \nDN\n of each schedd that the frontend talks to. Specified in the frontend.xml schedd element \nDN\n attribute:\n\n\nschedds\n\n  \nschedd\n \nDN=\n/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\n \nfullname=\nYOUR_HOST\n/\n\n  \nschedd\n \nDN=\n/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\n \nfullname=\nschedd_jobs2@YOUR_HOST\n/\n\n\n/schedds\n\n\n\n\n\n\n\n\n\n\nFrontend proxy\n: The DN of the proxy that the frontend uses to communicate with the other glideinWMS services. Specified in the frontend.xml security element \nproxy_DN\n attribute:\n\n\nsecurity classad_proxy=\n/tmp/vo_proxy\n proxy_DN=\nDN of vo_proxy\n ....\n\n\n\n\n\n\n\n\n\nEach pilot proxy\n The DN of \neach\n proxy that the frontend forwards to the factory to use with the glideinWMS pilots.  This allows the !glideinWMs pilot jobs to communicate with the User Collector. Specified in the frontend.xml proxy \nabsfname\n attribute (you need to specify the \nDN\n of each of those proxies:\n\n\nsecurity\n \n....\n\n\nproxies\n\n   \n proxy\n \nabsfname=\n/tmp/vo_proxy\n \n....\n\n   \n:\n\n\n/proxies\n\n\n\n\n\n\n\n\n\n\nBelow is an example mapfile, by default found in \n/etc/condor/certs/condor_mapfile\n. In this example there are lines for each of services mentioned above.\n\n\n\n\nNote\n\n\nThe \nexample_of_format\n entry as each DN should use this format for security purposes.\n\n\n\n\nGSI \nDN of schedd proxy\n schedd\nGSI \nDN of frontend proxy\n frontend\nGSI \nDN of pilot proxy\n$\n pilot_proxy\nGSI \n^\\/DC\\=org\\/DC\\=doegrids\\/OU\\=Services\\/CN\\=personal\\-submit\\-host2\\.mydomain\\.edu$\n \nexample_of_format\n\nGSI (.*) anonymous\nFS (.*) \\1\n\n\n\n\n\nRestart Condor\n\n\nAfter configuring condor, be sure to restart condor:\n\n\n[root@client ~] #\n service condor restart\n\n\n\n\n\nProxy Configuration\n\n\nThere are 2 types of (or purposes for) proxies required for the VO Frontend: 1 the \nVO Frontend proxy\n (used to authenticate with the other glideinWMS services) 1 one or more glideinWMS \npilot proxies\n (used/delegated to the factory services and submitted on the glideinWMS pilot jobs) The \nVO Frontend proxy\n and the \npilot proxy\n can be the same. By default, the VO Frontend will run as user \nfrontend\n (UID is machine dependent) so these proxies must be owned by the user \nfrontend\n.\n\n\nManual proxy renewal\n\n\nVO Frontend proxy\n\nThe VO Frontend Proxy is used for communicating with the other glideinWMS services (Factory, User Collector and Schedd/Submit services). Create the proxy using the glidenWMS VO Frontend Host (or Service) cert and change ownership to the frontend user.\n\n\n[root@client ~] #\n voms-proxy-init-valid \nhours_valid\n \n\\\n\n-cert /etc/grid-security/hostcert.pem \n\\\n\n-key /etc/grid-security/hostkey.pem \n\\\n\n-out \n/tmp/vofe_proxy\n\n\n[root@client ~] #\n chown frontend \n/tmp/vofe_proxy\n\n\n\n\n\n\nPilot proxy\n\nThe pilot proxy is used on the glideinWMS pilot jobs submitted to the CEs. Create the proxy using the \npilot certificate\n and change ownership to the frontend user.\n\n\n[root@client ~] #\n voms-proxy-init -valid \nhours_valid\n\\\n\n-voms \nvo\n\n\n-cert \npilot_cert\n \\\n\n\n-key \npilot_key\n \\\n\n\n-out \n/tmp/pilot_proxy\n\n\n[root@client ~] #\n chown frontend \n/tmp/pilot_proxy\n\n\n\n\n\n\n\n\nWarning\n\n\nProxies do expire.\n You can extend the validity by using a longer time interval, e.g. \n-valid 3000:0\n. This sequence of commands will need to be renewed when the proxy expires or the machine reboots (if /tmp is used only).\n\n\n\n\nMake sure that this location is specified correctly in the \nfrontend.xml\n described in the \nConfiguring the Frontend\n section.\n\n\nYou may want to automate the procedure above (or part of it) by writing a script and adding it to crontab.\n\n\nExample of automatic proxy renewal\n\n\nThis example (user provided) uses the script \nmake-proxy.sh\n attached to this document. You still need to do some prep-work but this can be done only once a year and the script will warn you with an email.\n\n\nPreparation for the \nVO Frontend proxy\n. You'll have to redo this each time the Host (or Service) certificate and key are renewed:\n\n\n\n\n\n\nCopy the Host (or Service) certificate and key\n\n\n[root@client ~] #\n cp /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem /var/lib/gwms-frontend/\n\n\n\n\n\n\n\n\n\nChange ownership and permission of the certificate and key\n\n\n[root@client ~] #\n chown frontend: /var/lib/gwms-frontend/host**.pem\n\n[root@client ~] #\n chmod \n0600\n /var/lib/gwms-frontend/host**.pem\n\n\n\n\n\n\n\n\n\nPreparation for the \n pilot proxy\n. You'll have to redo this for each new or renewed pilot cert.\n\n\n\n\n\n\nCreate the proxy using the pilot certificate/key (as the user/submitter)\n\n\n[root@client ~] #\n grid-proxy-init -valid \n8800\n:0 -out /tmp/tmp_proxy\n\n\n\n\n\n\n\n\n\nCopy the proxy to the correct name and change ownership and permissions (as root):\n\n\n[root@client ~] #\n cp /tmp/tmp_proxy /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy\n\n[root@client ~] #\n chown frontend: /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy\n\n[root@client ~] #\n chmod \n0600\n /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy\n\n[root@client ~] #\n rm /tmp/tmp_proxy\n\n\n\n\n\n\n\n\n\nConfigure the script for the \nVO Frontend proxy\n\n\n\n\n\n\nDownload the \nattached script\n (the latest one is \nHere on Github\n) and save it as \n/var/lib/gwms-frontend/make-frontend-proxy.sh\n, make sure that it is executable.\n\n\n\n\n\n\nEdit the VARIABLES section to look something like (replace your email, host name and the paths that are different in your setup - the comments in the script will help):\n\n\nSETUP_FILE=\n\nCERT_FILE=\n/var/lib/gwms-frontend/hostcert.pem\n\nKEY_FILE=\n/var/lib/gwms-frontend/hostkey.pem\n\nIN_NAME=\n/var/lib/gwms-frontend/frontend_base_proxy\n\nOUT_NAME=\n/tmp/vofe_proxy\n\nOWNER_EMAIL=\nyour@email_here\n\nPROXY_DESCRIPTION=\nVO Fronted on \nhostname\n\nVOMS_OPTION=\n\n\n\n\n\n\n\n\n\n\nConfigure the script for the \npilot proxy\n:\n\n\n\n\n\n\nDownload the \nattached script\n (the latest one is \nHere on Github\n) and save it as \n/var/lib/gwms-frontend/make-pilot-proxy.sh\n, make sure that it is executable.\n\n\n\n\n\n\nEdit the VARIABLES section to look something like (replace your email, host name and the paths that are different in your setup - the comments in the script will help):\n\n\nSETUP_FILE=\n\nCERT_FILE=\n\nKEY_FILE=\n\nIN_NAME=\n/var/lib/gwms-frontend/vofe_base_gi_delegated_proxy\n\nOUT_NAME=\n/tmp/vofe_gi_delegated_proxy\n\nOWNER_EMAIL=\nyour@email_here\n\nPROXY_DESCRIPTION=\nVO Fronted glidein delegated on \nhostname\n\nVOMS_OPTION=\nosg:/osg\n\n\n\n\n\n\n\n\n\n\nBefore adding the scripts to the crontab I'd recommend to test them manually once to make sure that there are no errors. As user \nfrontend\n run the scripts (you can also use \nsh -x\n to debug them):\n\n\n/var/lib/gwms-frontend/make-frontend-proxy.sh --no-voms-proxy /var/lib/gwms-frontend/make-pilot-proxy.sh\n\n\n\n\n\n\nAdd the scripts to the crontab of the user \nfrontend\n with \ncrontab -e\n:\n\n\n10 * * * * /var/lib/gwms-frontend/make-frontend-proxy.sh --no-voms-proxy\n10 * * * * /var/lib/gwms-frontend/make-pilot-proxy.sh\n\n\n\n\n\nAn additional script like \nmake-proxy-control.sh\n (the latest one is \nHere on Github\n) can be used for an independent verification of the proxies. If you like, download it, fix the variables and add it to the crontab like the other two.\n\n\nReconfigure and verify installation\n\n\n\n\nWarning\n\n\nIn order to use the frontend, first you must reconfigure and upgrade it.\n\n\n# For RHEL 6, CentOS 6, and SL6\n\n\n[\nroot\n@\nclient\n \n~\n]\n # \nservice\n \ngwms\n-\nfrontend\n \nreconfig\n\n\n[\nroot\n@\nclient\n \n~\n]\n # \nservice\n \ngwms\n-\nfrontend\n \nupgrade\n\n\n\n# For RHEL 7, CentOS 7, and SL7\n\n\n[\nroot\n@\nclient\n \n~\n]\n # \n/\nusr\n/\nsbin\n/\ngwms\n-\nfrontend\n \nreconfig\n\n\n[\nroot\n@\nclient\n \n~\n]\n # \n/\nusr\n/\nsbin\n/\ngwms\n-\nfrontend\n \nupgrade\n\n\n\n\n\n\n\n\nAfter this initial reconfiguring/upgrading, you can start the frontend:\n\n\n \n# For RHEL 6, CentOS 6, and SL6\n\n \n[\nroot\n@\nclient\n \n~\n]\n # \nservice\n \ngwms\n-\nfrontend\n \nstart\n\n \n# For RHEL 7, CentOS 7, and SL7\n\n \n[\nroot\n@\nclient\n \n~\n]\n # \nsystemctl\n \nstart\n \ngwms\n-\nfrontend\n\n\n\n\n\n\nAdding Gratia Accounting and a Local Monitoring Page on a Production Server\n\n\nYou must report to Gratia if you are running on OSG more than a few test jobs.\n\n\nProbeConfigGlideinWMS\n explains how to instal and configure the HTCondor Gratia probe. If you are on a Campus Grid without x509 certificates pay attention to the \nUsers without Certificates part\n in the Unusual Use Cases section.\n\n\nOptional Configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to \nthe section on service activation/deactivation\n.\n\n\n\n\nAllow users to specify where their jobs run\n\n\nCreating a group to test configuration changes\n\n\n\n\nAllow users to specify where their jobs run\n\n\nIn order to allow users to specify the sites at which their jobs want to run (or to test a specific site), a frontend can be configured to match on \nDESIRED_Sites\n or ignore it if not specified. Modify \n/etc/gwms-frontend/frontend.xml\n using the following instructions:\n\n\n\n\n\n\nIn the frontend's global \nmatch\n stanza, set the \nmatch_expr\n:\n\n\n((job.get(\nDESIRED_Sites\n,\nnosite\n)==\nnosite\n) or (glidein[\nattrs\n][\nGLIDEIN_Site\n] in job.get(\nDESIRED_Sites\n,\nnosite\n).split(\n,\n)))\n\n\n\n\n\n\n\n\n\n\nIn the same \nmatch\n stanza, set the \nstart_expr\n:\n\n\n(DESIRED_Sites=?=undefined || stringListMember(GLIDEIN_Site,DESIRED_Sites,\n,\n))\n\n\n\n\n\n\n\n\n\nAdd the \nDESIRED_Sites\n attribute to the match attributes list:\n\n\nmatch_attrs\n\n   \nmatch_attr\n \nname=\nDESIRED_Sites\n \ntype=\nstring\n/\n\n\n/match_attrs\n\n\n\n\n\n\n\n\n\n\nReconfigure the Frontend:\n\n\n[root@client ~] #\n /etc/init.d/gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\nCreating a group for testing configuration changes\n\n\nTo perform configuration changes without impacting production the recommended way is to create an ITB group in \n/etc/gwms-frontend/frontend.xml\n. This groupwould only match jobs that have the \n+is_itb=True\n ClassAd.\n\n\n\n\n\n\nCreate a \ngroup\n named itb.\n\n\n\n\n\n\nSet the group's \nstart_expr\n so that the group's glideins will only match user jobs with \n+is_itb=True\n:\n\n\nmatch match_expr=\nTrue\n start_expr=\n(is_itb)\n\n\n\n\n\n\n\n\n\n\nSet the \nfactory_query_expr\n so that this group only communicates with ITB factories:\n\n\nfactory query_expr=\nFactoryType=?=\nitb\n\n\n\n\n\n\n\n\n\n\nSet the group's \ncollector\n stanza to reference the ITB factory, replacing \nusername@gfactory-1.t2.ucsd.edu\n with your factory identity:\n\n\ncollector DN=\n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=glidein-itb.grid.iu.edu\n \\\n          factory_identity=\ngfactory@glidein-itb.grid.iu.edu\n \\\n          my_identity=\nusername@gfactory-1.t2.ucsd.edu\n \\\n          node=\nglidein-itb.grid.iu.edu\n/\n\n\n\n\n\n\n\n\n\n\nSet the job \nquery_expr\n so that only ITB jobs appear in \ncondor_q\n:\n\n\njob query_expr=\n(!isUndefined(is_itb) \n is_itb)\n\n\n\n\n\n\n\n\n\n\nReconfigure the Frontend:\n\n\n/etc/init.d/gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\nService Activation and Deactivation\n\n\nThe scripts updating your CA and CRLs plus three frontend services need to be running:\n\n\n\n\n\n\nYou need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:\n\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6, or OSG \n3\n _older_ than \n3\n.1.15\n\n\n[root@client ~]$\n /usr/sbin/fetch-crl   \n# This fetches the CRLs\n\n\n[root@client ~]$\n /sbin/service fetch-crl-boot start\n\n[root@client ~]$\n /sbin/service fetch-crl-cron start\n\n%\nRED% \n# For RHEL 7, CentOS 7, and SL7 \n\n\n[root@client ~]$\n /usr/sbin/fetch-crl   \n# This fetches the CRLs\n\n\n[root@client ~]$\n systemctl start fetch-crl-boot\n\n[root@client ~]$\n systemctl start fetch-crl-cron\n\n\n\n\n\n\n\n\n\nHTCondor, httpd, VO Frontend\n\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\n[root@client ~] #\n service condor start\n\n[root@client ~] #\n service httpd start\n\n[root@client ~] #\n service gwms-frontend start\n\n%\nRED%# For RHEL \n7\n, CentOS \n7\n, and SL7\n\n\n[root@client ~] #\n systemctl start condor\n\n[root@client ~] #\n systemctl start gwms-frontend\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nOnce you successfully start using the frontend service, each time you change the configuration or want to upgrade, you run the following command\n\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\n[root@client ~] #\n service gwms-frontend reconfig\n\n%\nRED%# And \nif\n you change also some code\n\n\n[root@client ~] #\n service gwms-frontend upgrade\n\n\n%\nRED%# But the situation is a bit more complicated in RHEL \n7\n, CentOS \n7\n, and SL7 due to systemd restrictions\n\n\n%\nGREEN%# For reconfig:\n\n\n%\nRED%A. when the frontend is running\n\n\n%\nRED%A.1 without any additional options\n\n\n[root@client ~] #\n /usr/sbin/gwms-frontend reconfig\n\n\nor\n\n\n[root@client ~] #\n systemctl reload gwms-frontend\n\n\n%\nRED%A.2 \nif\n you want to give additional options \n\n\nsystemctl stop gwms-frontend\n\n\n/usr/sbin/gwms-frontend reconfig \nand your options\n\n\nsystemctl start gwms-frontend\n\n\n\n%\nRED%B. when the frontend is NOT running \n\n\n[root@client ~] #\n /usr/sbin/gwms-frontend reconfig \n(\nand your options\n)\n\n\n\n%\nGREEN%# For upgrade:\n\n\n%\nRED%A. when the frontend is running \n\n\nsystemctl stop gwms-frontend\n\n\n/usr/sbin/gwms-frontend upgrade (\nand your options if any\n)\n\n\nsystemctl start gwms-frontend\n\n\n\n%\nRED%B. when the frontend is NOT running \n\n\n/usr/sbin/gwms-frontend upgrade (\nand your options if any\n)\n\n\n\n\n\n\n\n\nTo stop the frontend:\n\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\n[root@client ~] #\n service gwms-frontend stop\n\n%\nRED%# For RHEL \n7\n, CentOS \n7\n, and SL7\n\n\n[root@client ~] #\n systemctl stop gwms-frontend\n\n\n\n\n\nAnd you can stop also the other services if you are not using them independently form the frontend.\n\n\nValidation of Service Operation\n\n\nThe complete validation of the frontend is the submission of actual jobs. However, there are a few things that can be checked prior to submitting user jobs to Condor.\n\n\n\n\n\n\nVerify all Condor daemons are started.\n\n\n[user@client ~]$ condor_config_val -verbose DAEMON_LIST\nDAEMON_LIST: MASTER,  COLLECTOR, NEGOTIATOR,  SCHEDD, SHARED_PORT, SCHEDDJOBS2 COLLECTOR0 COLLECTOR1 COLLECTOR2\nCOLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11,\nCOLLECTOR12, COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20,\nCOLLECTOR21, COLLECTOR22, COLLECTOR23, COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29,\nCOLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38,\nCOLLECTOR39, COLLECTOR40\nDefined in \n/etc/condor/config.d/11_gwms_secondary_collectors.config\n, line 193.\n\n\n\n\n\nIf you don't see all the collectors and the two schedd, then the configuration must be corrected. There should be no startd daemons listed\n\n\n\n\n\n\nVerify all VO Frontend Condor services are communicating.\n\n\n[user@client ~]$ condor_status -any\nMyType               TargetType           Name\nglideresource        None                 MM_fermicloud026@gfactory_inst\nScheduler            None                 fermicloud020.fnal.gov\nDaemonMaster         None                 fermicloud020.fnal.gov\nNegotiator           None                 fermicloud020.fnal.gov\nCollector            None                 frontend_service@fermicloud020\nScheduler            None                 schedd_jobs2@fermicloud020.fnal\n\n\n\n\n\n\n\n\n\nTo see the details of the glidein resource use \ncondor_status -subsystem glideresource -l\n, including the GlideFactoryName.\n\n\n\n\n\n\nVerify that the Factory is seeing correctly the Frontend using \ncondor_status -pool \nFACTORY_HOST\n -any -constraint 'FrontendName==\n\"FRONTEND_NAME_FROM_CONFIG\"\n' -l\n, including the GlideFactoryName.\n\n\n\n\n\n\nGlidein WMS Job submission\n\n\nCondor submit file \nglidein-job.sub\n. This is a simple job printing the hostname of the host where the job is running:\n\n\n#file glidein-job.sub\nuniverse = vanilla\nexecutable = /bin/hostname\noutput = glidein/test.out\nerror = glidein/test.err\nrequirements = IS_GLIDEIN == True\nlog = glidein/test.log\nShouldTransferFiles = YES\n\nwhen_to_transfer_output = ON_EXIT\nqueue\n\n\n\n\n\nTo submit the job:\n\n\n[root@client ~] #\n condor_submit glidein-job.sub\n\n\n\n\n\nThen you can control the job like a normal condor job, e.g. to check the status of the job use \ncondor_q\n.\n\n\nMonitoring Web pages\n\n\nYou should be able to see the jobs also in the GWMS monitoring pages that are made available on the Web: \nhttp://gwms-frontend-host.domain/vofrontend/monitor/\n\n\nTroubleshooting\n\n\nFile Locations\n\n\n\n\n\n\n\n\nFile Description\n\n\nFile Location\n\n\n\n\n\n\n\n\n\n\nConfiguration file\n\n\n/etc/gwms-frontend/frontend.xml\n\n\n\n\n\n\nLogs\n\n\n/var/log/gwms-frontend/\n\n\n\n\n\n\nStartup script\n\n\n/etc/init.d/gwms-frontend\n\n\n\n\n\n\nWeb Directory\n\n\n/var/lib/gwms-frontend/web-area\n\n\n\n\n\n\nWeb Base\n\n\n/var/lib/gwms-frontend/web-base\n\n\n\n\n\n\nWeb configuration\n\n\n/etc/httpd/conf.d/gwms-frontend.conf\n\n\n\n\n\n\nWorking Directory\n\n\n/var/lib/gwms-frontend/vofrontend/\n\n\n\n\n\n\nLock files\n\n\n/etc/init.d/gwms-frontend/vofrontend/lock/frontend.lock /etc/init.d/gwms-frontend/vofrontend/group_*/lock/frontend.lock\n\n\n\n\n\n\nStatus files\n\n\n/var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n/var/lib/gwms-frontend\n is also the home directory of the \nfrontend\n user\n\n\n\n\nCertificates brief\n\n\nHere a short list of files to check when you change the certificates. Note that if you renew a proxy or certificate and the DN remains the same no configuration file needs to change, just put the renewed certificate/proxy in place.\n\n\n\n\n\n\n\n\nFile Description\n\n\nFile Location\n\n\n\n\n\n\n\n\n\n\nConfiguration file\n\n\n/etc/gwms-frontend/frontend.xml\n\n\n\n\n\n\nHTCondor certificates map\n\n\n/etc/condor/creds/condor_mapfile (1)\n\n\n\n\n\n\nHost certificate and key (2)\n\n\n/etc/grid-security/hostcert.pem            /etc/grid-security/hostkey.pem\n\n\n\n\n\n\nVO Frontend proxy (from host certificate)\n\n\n/tmp/vofe_proxy (3)\n\n\n\n\n\n\nPilot proxy\n\n\n/tmp/vofe_proxy (3)\n\n\n\n\n\n\n\n\n\n\n\n\nIf using HTCondor RPM installation, e.g. the one coming from OSG. If you have separate/multiple HTCondor hosts (schedds, collectors, negotiators, ..) you may have to check this file on all of them to make sure that the HTCondor authentication works correctly.\n\n\n\n\n\n\nUsed to create the VO Frontend proxy if following the \ninstructions above\n\n\n\n\n\n\nIf using the scripts described \nabove in this document\n\n\n\n\n\n\nRemember also that when you change DN:\n\n\n\n\nThe VO Frontend certificate DN must be communicated to the GWMS Factory (\nsee above\n)\n\n\nThe pilot proxy must be able to run jobs at the sites you are using, e.g. by being added to the correct VO in OSG (the Factory forwards the proxy and does not care about the DN)\n\n\n\n\nIncrease the log level and change rotation policies\n\n\nYou can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the \nprocess_log\n section of \n/etc/gwms-frontend/frontend.xml\n:\n\n\nlog_retention\n\n   \nprocess_logs\n\n       \nprocess_log extension=\nall\n max_days=\n7.0\n max_mbytes=\n100.0\n min_days=\n3.0\n msg_types=\nDEBUG,EXCEPTION,INFO,ERROR,ERR\n/\n\n\n\n\n\n\nYou can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files:\n\n\n\n\nmax_bytes is the max size of the log files\n\n\nmax_days it will be rotated.\n\n\ncompression specifies if rotated files are compressed\n\n\nbackup_count is the number of rotated log files kept\n\n\n\n\nFurther details are in the \nreference documentation\n.\n\n\nFrontend reconfig failing\n\n\nIf \nservice gwms-frontend reconfig\n fails at the end with an error like \"Writing back config file failed, Reconfiguring the frontend [FAILED]\", make sure that \n/etc/gwms-frontend/\n belongs to the \nfrontend\n user. It must be able to write to update the configuration file.\n\n\nFrontend failing to start\n\n\nIf the startup script of the frontend is failing, check the log file for errors (probably \n/var/log/gwms-frontend/frontend/frontend.\nTODAY\n.err.log\n and \n.debug.log\n).\n\n\nIf you find errors like \n\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\"\n and \n\"IOError: [Errno 9] Bad file descriptor\"\n you may have an empty status file (\n/var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml\n) that causes Glidein WMS Frontend not to start. The glideinFrontend crashes after a XML parsing exception visible in the log file (\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\").\n\n\nRemove the status file. Then start the frontend. The fronten will be fixed in future versions to handle this automatically.\n\n\nCertificates not there\n\n\nThe scripts should send an email warning if there are problems and they fail to generate the proxies. Anyway something could go wrong and you want to check manually. If you are using the scripts to generate automatically the proxies but the proxies are not there (in \n/tmp\n or wherever you expect them):\n\n\n\n\nmake sure that the scripts are there and configured with the correct values\n\n\nmake sure that the scripts are executable\n\n\nmake sure that the scripts are in \nfrontend\n 's crontab\n\n\nmake sure that the certificates (or master proxy) used to generate the proxies is not expired\n\n\n\n\nFailed authentication\n\n\nIf you get a failed authentication error (e.g. \"Failed to talk to factory_pool gfactory-1.t2.ucsd.edu...) then:\n\n\n\n\ncheck that you have the right x509 certificates mentioned in the security section of \n/etc/gwms-frontend/frontend.xml\n\n\nthe owner must be \nfrontend\n (user running the frontend)\n\n\nthe permission must be 600\n\n\nthey must be valid for more than one hour (2/300 hours), at least the non VO part\n\n\n\n\n\n\ncheck that the clock is synchronized (see HostTimeSetup)\n\n\n\n\nFrontend doesn't trust factory\n\n\nIf your frontend complains in the debug log:\n\n\ncode 256:[\nError: communication error\\n\n, \nAUTHENTICATE:1003:Failed to authenticate with any method\\n\n, \nAUTHENTICATE:1004:Failed to authenticate using GSI\\n\n, \nGSI:5006:Failed to authenticate because the subject \n/DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu\n is not currently trusted by you.  If it should be, add it to GSI_DAEMON_NAME in the condor_config, or use the environment variable override (check the manual).\\n\n, \nGSI:5004:Failed to gss_assist_gridmap /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu to a local user.\n\n\n\n\n\nA possible solution is to comment/remove the LOCAL_CONFIG_DIR in the file \n/var/lib/gwms-frontend/vofrontend/frontend.condor_config\n.\n\n\nNo security credentials match for factory pool ..., not advertising request\n\n\nYou may see a warning like \"No security credentials match for factory pool ..., not advertising request\", if the \ntrust_domain\n and \nauth_method\n of an entry in the Factory configuration is not matching any of the \ntrust_domain\n, \ntype\n couples in the credentials in the Frontend configuration. This causes the Frontend not to use some Factory entries (the ones not matching) and may end up without entries to send glideins to.\n\n\nTo fix the problem make sure that those attributes match as desired.\n\n\nJobs not running\n\n\nIf your jobs remain Idle\n\n\n\n\nCheck the frontend log files (see above)\n\n\nCheck the condor log files (\ncondor_config_val LOG\n will give you the correct log directory):\n\n\nSpecifically look the CollectorXXXLog files\n\n\n\n\n\n\n\n\nCommon causes of problems could be:\n\n\n\n\nx509 certificates\n\n\nmissing or expired or too short-lived proxy\n\n\nincorrect ownership or permission on the certificate/proxy file\n\n\nmissing certificates\n\n\n\n\n\n\nIf the frontend http server is down in the factory there will be errors like \"Failed to load file 'description.dbceCN.cfg' from '\nhttp://FRONTEND_HOST/vofrontend/stage\n'.\"\n\n\ncheck that the http server is running and you can reach the URL (\nhttp://FRONTEND_HOST/vofrontend/stage/description.dbceCN.cfg\n)\n\n\n\n\n\n\n\n\nAdvanced Configurations\n\n\n\n\nGlideinWMS Frontend on a Campus Grid\n\n\n\n\nReferences\n\n\nDefinitions:\n\n\n\n\nWhat is a \nVirtual Organisation\n\n\nIntroduction to the Grid for users/scientists\n\n\n\n\nDocuments about the Glidein-WMS system and the VO frontend:\n\n\n\n\nhttp://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/\n\n\nhttp://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/manual/\n\n\nHow to setup a Submit host flocking to the VO Frontend", 
            "title": "GlideinWMS Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#glideinwms-vo-frontend-installation", 
            "text": "", 
            "title": "GlideinWMS VO Frontend Installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#about-this-document", 
            "text": "This document describes how to install the Glidein Workflow Managment System (GlideinWMS) VO Frontend for use with the OSG glidein factory. This software is the minimum requirement for a VO to use glideinWMS.  This document assumes expertise with Condor and familiarity with the glideinWMS software. It  does not  cover anything but the simplest possible install. Please consult the  Glidein WMS reference documentation  for advanced topics, including non- root , non-RPM-based installation.  This document covers three components of the GlideinWMS a VO needs to install:   User Pool Collectors : A set of  condor_collector  processes. Pilots submitted by the factory will join to one of these collectors to form a Condor pool.  User Pool Schedd : A  condor_schedd . Users may submit Condor vanilla universe jobs to this schedd; it will run jobs in the Condor pool formed by the  User Pool Collectors .  Glidein Frontend : The frontend will periodically query the  User Pool Schedd  to determine the desired number of running job slots. If necessary, it will request the factory to launch additional pilots.   This guide covers installation of all three components on the same host: it is designed for small to medium VOs (see the Hardware Requirements below). Given a significant, large host, we have been able to scale the single-host install to 20,000 running jobs.", 
            "title": "About This Document"
        }, 
        {
            "location": "/other/install-gwms-frontend/#release", 
            "text": "This document reflects glideinWMS v3.2.17.", 
            "title": "Release"
        }, 
        {
            "location": "/other/install-gwms-frontend/#how-to-get-help", 
            "text": "To get assistance about the OSG software please use  this page .  For specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support   To request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to   (see below).", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/other/install-gwms-frontend/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/other/install-gwms-frontend/#host-and-os", 
            "text": "A host to install the GlideinWMS Frontend (pristine node).  OS is Red Hat Enterprise Linux 6, 7, and variants (see  details ). Currently most of our testing has been done on Scientific Linux 6.  Root access   The Glidein WMS VO Frontend has the following hardware requirements for a production host:   CPU : Four cores, preferably no more than 2 years old.  RAM : 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed.  Disk : 30GB will be plenty sufficient for all the binaries, config and log files related to glideinWMS. As this will be an interactive submit host, plan enough disk space for your users' jobs. Depending on your workflow, this might require 2MB to 2GB per job in a workflow.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/other/install-gwms-frontend/#users", 
            "text": "The Glidein WMS Frontend installation will create the following users unless they are already created.     User  Default uid  Comment      apache  48  Runs httpd to provide the monitoring page (installed via dependencies).    condor  none  Condor user (installed via dependencies).    frontend  none  This user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins.    gratia  none  Runs the Gratia probes to collect accounting data (optional see  the Gratia section below )     Note that if uid 48 is already taken but not used for the appropriate users, you will experience errors.  Details...", 
            "title": "Users"
        }, 
        {
            "location": "/other/install-gwms-frontend/#credentials-and-proxies", 
            "text": "The VO Frontend will use two credentials in its interactions with the the other glideinWMS services. At this time, these will be proxy files.   the  VO Frontend proxy  (used to authenticate with the other glideinWMS services).  one or more glideinWMS  pilot proxies  (used/delegated to the factory services and submitted on the glideinWMS pilot jobs).   The  VO Frontend proxy  and the   pilot proxy  can be the same. By default, the VO Frontend will run as user  frontend  (UID is machine dependent) so these proxies must be owned by the user  frontend .", 
            "title": "Credentials and Proxies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#vo-frontend-proxy", 
            "text": "The use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the  proxy configuration section . This can be a plain grid proxy (from  grid-proxy-init ), no VO extensions are required.  You must notify the Factory operation of the DN of this proxy when you initially setup the frontend and each time the DN changes .", 
            "title": "VO Frontend proxy"
        }, 
        {
            "location": "/other/install-gwms-frontend/#pilot-proxies", 
            "text": "This proxy is used by the factory to submit the glideinWMS pilot jobs. Therefore, they must be authorized to access to the CEs (factory entry points) where jobs are submitted. There is no need to notify the Factory operation about the DN of this proxy (neither at the initial registration nor for subsequent changes). This second proxy has no special requirement or controls added by the factory but will probably require VO attributes because of the CEs: if you are able to use this proxy to submit jobs to the CEs where the Factory runs glideinWMS pilots for you, then the proxy is OK. You can test your proxy using  globusrun  or HTCondor-G  To check the important information about a pem certificate you can use:  openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout . You will need that to find out information for the configuration files and the request to the GlideinWMS factory.", 
            "title": "Pilot proxies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificatesproxies-configuration-example", 
            "text": "This document has a  proxy configuration section  that uses the host certificate/key and a user certificate to generate the required proxies.     Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem              /etc/grid-security/hostkey.pem     Here  are instructions to request a host certificate.", 
            "title": "Certificates/Proxies configuration example"
        }, 
        {
            "location": "/other/install-gwms-frontend/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      HTCondor port range  tcp  LOWPORT, HIGHPORT  YES   contiguous range of ports    GlideinWMS Frontend  tcp  9618 to 9660  YES   HTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs)     The VO frontend must have reliable network connectivity, be on the public internet (no NAT), and preferably with no firewalls. Each running pilot requires 5 outgoing TCP ports. Incoming TCP ports 9618 to 9660 must be open.   For example, 2000 running jobs require about 10,100 TCP connections. This will overwhelm many firewalls; if you are unfamiliar with your network topology, you may want to warn your network administrator.", 
            "title": "Networking"
        }, 
        {
            "location": "/other/install-gwms-frontend/#before-the-installation", 
            "text": "Once all requirements are satisfied you must take a couple of actions before installing the Frontend:   you need all the data to connect to a GWMS Factory  Remember to install HTCondor BEFORE installing the Frontend ( instructions are below )", 
            "title": "Before the installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#osg-factory-access", 
            "text": "Before installing the Glidein WMS VO Frontend you need the information about a  Glidein Factory  that you can access:   (recommended) OSG is managing a factory at UCSD and one at GOC and you can request access to them  You have another Glidein Factory that you can access  You  install your own Glidein Factory   To request access to the OSG Glidein Factory at UCSD you have to send an email to   providing:   Your Name  The VO that is utilizing the VO Frontend  The DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the  proxy configuration section )  You can propose a security name that will have to be confirmed/changed by the Factory managers (see below)   A list of sites where you want to run:    Your VO must be supported on those sites   You can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers  You can start with one single site   In the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend   The exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\".  The host of the Factory Collector:  gfactory-1.t2.ucsd.edu  The DN os the factory, e.g.  /DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu  The factory identity, e.g.:  gfactory@gfactory-1.t2.ucsd.edu  The identity on the factory you will be mapped to. Something like:  username@gfactory-1.t2.ucsd.edu  Your security name. A unique name, usually containing your VO name:  My_SecName  A string to add in the main factory query_expr in the frontend configuration, e.g.  stringListMember(\" VO \",GLIDEIN_Supported_VOs) . From there you get the correct name of the VO (above in this list).", 
            "title": "OSG Factory access"
        }, 
        {
            "location": "/other/install-gwms-frontend/#installation-procedure", 
            "text": "Refer to installation of the  CA certificates", 
            "title": "Installation Procedure"
        }, 
        {
            "location": "/other/install-gwms-frontend/#install-htcondor", 
            "text": "Most required software is installed from the Frontend RPM installation. HTCondor is the only exception since there are  many different ways to install it , using the RPM system or not. You need to have HTCondor installed before installing the Glidein WMS Frontend. If yum cannot find a HTCondor RPM, it will install the dummy  empty-condor  RPM, assuming that you installed HTCondor using a tarball distribution.  If you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository:  [root@client ~] #  yum install condor.x86_64 #  If you have a  32  bit host use instead: [root@client ~] #  yum install condor.i386  See  this HTCondor document  for more information on the different options.", 
            "title": "Install HTCondor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#download-and-install-the-vo-frontend-rpm", 
            "text": "The RPM is available in the OSG repository:  Install the RPM and dependencies (be prepared for a lot of dependencies).  [root@client ~] #  yum install glideinwms-vofrontend  This will install the current production release verified and tested by OSG with default condor configuration. This command will install the glideinwms vofrontend, condor, the OSG client, and all the required dependencies all on one node.  If you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows:   yum install --enablerepo=osg-testing glideinwms-vofrontend : The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well.  yum install --enablerepo=osg-contrib glideinwms-vofrontend : The most recent development series release, ie version 3 release. This has newer features such as cloud submission support, but is less tested.   Note that these commands will install default condor configurations with all services on one node.", 
            "title": "Download and install the VO Frontend RPM"
        }, 
        {
            "location": "/other/install-gwms-frontend/#advanced-multi-node-installation", 
            "text": "For advanced users requiring heavy usage on their submit node, you may want to consider splitting the usercollector, user submit, and vo frontend services.  This can be doing using the following three commands (on different machines):  [root@client ~] #  yum install glideinwms-vofrontend-standalone [root@client ~] #  yum install glideinwms-usercollector [root@client ~] #  yum install glideinwms-userschedd  In addition, you will need to perform the following steps:   On the vofrontend and userschedd, modify CONDOR_HOST to point to your usercollector. This is in  /etc/condor/config.d/00_gwms_general.config . You can also override this value by placing it in a new config file. (For instance,  /etc/condor/config.d/99_local_custom.config  to avoid rpmsave/rpmnew conflicts on upgrades).  In  /etc/condor/certs/condor_mapfile , you will need to all DNs for each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the  glidecondor_addDN  to add these values.  In the  /etc/gwms-frontend/frontend.xml  file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend xml are in the following sections.", 
            "title": "Advanced: Multi-node Installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#upgrade-procedure", 
            "text": "If you have a working installation of glideinwms-frontend you can just upgrade the frontend rpms and skip the most of the configuration procedure below. These general upgrade instructions apply when upgrading the glideinwms-frontend rpm within same major versions.  % RED%# Update the glideinwms-vofrontend packages  [root@client ~] #  yum update glideinwms \\*  % RED%# Update the scripts in the working directory to the latest one  % RED%# For RHEL  7 , CentOS  7 , and SL7  [root@client ~] #  /usr/sbin/gwms-frontend upgrade % RED%# For RHEL  6 , CentOS  6 , and SL6  [root@client ~] #  service gwms-frontend upgrade % RED%# Restart HTCondor because the configuration may be different  [root@client ~] #  service condor restart   Note  The \\*  on the yum update is important. *    Warning  When you do a generic yum update that will update also condor, the upgrade may restore the personal condor config file that you have to remove with  rm /etc/condor/config.d/00personal_condor.config    Note  When upgrading to GlideinWMS 3.2.7 the second schedd is removed from the default configuration. For a smooth transition:   remove from  /etc/gwms-frontend/frontend.xml  the second schedd (the line containing  schedd_jobs2@YOUR_HOST )  reconfigure the frontend ( service gwms-frontend reconfig )  restart HTCondor ( service condor restart )", 
            "title": "Upgrade Procedure"
        }, 
        {
            "location": "/other/install-gwms-frontend/#upgrading-glideinwms-frontend-from-v2-series-to-v3-series", 
            "text": "Due to incompatibilities between the major versions, upgrade process involves certain steps. Following instructions apply when upgrading glideinwms-frontend from a v2 series (example: v2.7.x) to a v3 series (v3.2.x)   Update the RPMs and backup configuration files   % RED%# Stop the glideinwms-vofrontend service  [root@client ~] #  service gwms-frontend stop % RED%# Backup the v2.7.x configuration  [root@client ~] #  cp /var/lib/gwms-frontend/vofrontend/frontend.xml /var/lib/gwms-frontend/vofrontend/frontend-2.xml [root@client ~] #  cp /etc/gwms-frontend/frontend.xml /etc/gwms-frontend/frontend-2.xml % RED%# Update the glideinwms-vofrontend packages from v2.7.x to v3.2.x  [root@client ~] #  yum update glideinwms \\*    Convert v2.7.x configuration to v3.2.x configuration (only for RHEL 6, CentOS 6, and SL6. RHEL5 and drivative are not supported by v3.2.x, RHEL7 and derivative were not supported by v2.7.x)   [root@client ~] #   /usr/lib/python2.6/site-packages/glideinwms/frontend/tools/convert_frontend_2to3.sh -i /var/lib/gwms-frontend/vofrontend/frontend-2.xml -o /var/lib/gwms-frontend/vofrontend/frontend.xml -s /usr/lib/python2.6/site-packages/glideinwms [root@client ~] #   /usr/lib/python2.6/site-packages/glideinwms/frontend/tools/convert_frontend_2to3.sh -i /etc/gwms-frontend/frontend-2.xml -o /etc/gwms-frontend/frontend.xml -s /usr/lib/python2.6/site-packages/glideinwms   Update the scripts in the working directory   % RED%# Update the scripts in the working directory to the latest one  [root@client ~] #   service gwms-frontend upgrade", 
            "title": "Upgrading glideinwms-frontend from v2 series to v3 series"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuration-procedure", 
            "text": "After installing the RPM, you need to configure the components of the glideinWMS VO Frontend:   Edit Frontend configuration options  Edit Condor configuration options  Create a Condor grid map file  Reconfigure and Start frontend", 
            "title": "Configuration Procedure"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuring-the-frontend", 
            "text": "The VO Frontend configuration file is  /etc/gwms-frontend/frontend.xml . The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted in red font. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference.    The VO you are affiliated with. This will identify those CEs that the glideinWMS pilot will be authorized to run on using the  pilot proxy  described previously in the this  section . Sometimes the whole  query_expr  is provided to you by the factory (see Factory access above):  factory query_expr= ((stringListMember( VO , GLIDEIN_Supported_VOs)))     Factory collector information. The  username  that you are assigned by the factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production factory, you will have to change also  DN ,  factory_identity  and  node  attributes. (refer to the information provided to you by the factory operator):  collector DN= /DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu \n           comment= Define factory collector globally for simplicity \n           factory_identity= gfactory@gfactory-1.t2.ucsd.edu \n           my_identity= username@gfactory-1.t2.ucsd.edu \n           node= gfactory-1.t2.ucsd.edu /     Frontend security information.   The  classad_proxy  in the security entry is the location of the VO Frontend proxy described previously  here .  The  proxy_DN  is the DN of the  classad_proxy  above.  The  security_name  identifies this VO Frontend to the the Factory, It is provided by the factory operator.  The  absfname  in the credential (or proxy in v 2.x) entry is the location of the glideinWMS  pilot  proxy described in the requirements section  here . There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources).  The type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the factory. If there is no match, between these two attributes in one of the credentials and some entry in one of the factories, then this frontend cannot trigger glideins. \nBoth the  classad_proxy  and  absfname  files should be owned by  frontend  user. # These lines are from the configuration of v 3.x security   classad_proxy= /tmp/vo_proxy   proxy_DN= DN of vo_proxy \n       proxy_selection_plugin= ProxyAll \n       security_name= The security name, this is used by factory \n       sym_key= aes_256_cbc \n       credentials \n         credential   absfname= /tmp/pilot_proxy   security_class= frontend \n         trust_domain= OSG   type= grid_proxy / \n       /credentials  /security       The schedd information.   The  DN  of the  VO Frontend Proxy  described previously  here .  The  fullname  attribute is the fully qualified domain name of the host where you installed the VO Frontend ( hostname --fqdn ).   A secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the frontend to service requests from multiple submit hosts.      schedds \n      schedd   DN= Cert DN used by the schedd at fullname: \n            fullname= Hostname of the schedd / \n       schedd   DN= Cert DN used by the second Schedd at fullname: \n             fullname= schedd name@Hostname of second schedd / \n    /schedds     The User Collector information.   The  DN  of the  VO Frontend Proxy  described previously  here .  The  node  attribute is the full hostname of the collectors ( hostname --fqdn ) and port  The  secondary  attribute indicates whether the element is for the primary or secondary collectors (True/False).   The default Condor configuration of the VO Frontend starts multiple Collector processes on the host ( /etc/condor/config.d/11_gwms_secondary_collectors.config ). The  DN  and  hostname  on the first line are the hostname and the host certificate of the VO Frontend. The  DN  and  hostname  on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector ports can be defined as a range, e.g., 9620-9660).       collector DN= DN of main collector \n           node= hostname.domain.tld:9618  secondary= False / \n     collector DN= DN of secondary collectors (usually same as DN in line above) \n           node= hostname.domain.tld:9620-9660  secondary= True /      Warning  The Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths.     Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation):   frontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions)  work base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value)", 
            "title": "Configuring the Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#if-you-have-a-different-factory", 
            "text": "The configuration above points to the OSG production Factory. If you are using a different Factory, then you have to:   replace  gfactory@gfactory-1.t2.ucsd.edu  and  gfactory-1.t2.ucsd.edu  with the correct values for your factory. And control also that the name used for the frontend () matches.  make sure that the factory is advertising the attributes used in the factory query expression ( query_expr ).", 
            "title": "If you have a different Factory"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuring-condor", 
            "text": "The condor configuration for the frontend is placed in  /etc/condor/config.d .   00_gwms_general.config  01_gwms_collectors.config  02_gwms_schedds.config  03_gwms_local.config  11_gwms_secondary_collectors.config  90_gwms_dns.config   Get rid of the pre-loaded condor default to avoid conflicts in the configuration.  [root@client ~] #  rm /etc/condor/config.d/00personal_condor.config  For most installations create a new file named  /etc/condor/config.d/92_local_condor.config", 
            "title": "Configuring Condor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#using-other-condor-rpms-eg-uw-madison-htcondor-rpm", 
            "text": "The above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using  yum list condor . The version name should include \"osg\", e.g.  7.8.6-3.osg.el5 .  If you are using the UW Madison Condor RPMS, be aware of the following changes:   This Condor RPM uses a file  /etc/condor/condor_config.local  to add your local machine slot to the user pool.  If you want to disable this behavior (recommended), you should blank out that file or comment out the line in  /etc/condor/condor_config  for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to  /etc/condor/config.d )  Note that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the glideinwms RPMs, but please take note if you use this variable in your job submissions or other customizations.   In general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files:  [root@client ~] #  condor_config_val -config Configuration source:      /etc/condor/condor_config  Local configuration sources:      /etc/condor/config.d/00_gwms_general.config      /etc/condor/config.d/01_gwms_collectors.config      /etc/condor/config.d/02_gwms_schedds.config      /etc/condor/config.d/03_gwms_local.config      /etc/condor/config.d/11_gwms_secondary_collectors.config      /etc/condor/config.d/90_gwms_dns.config  % RED%/etc/condor/condor_config.local   If, like in the example above, the GlideinWMS configuration files are not the last ones in the list please verify that important configuration options have not been overridden by the other configuration files.", 
            "title": "Using other Condor RPMs, e.g. UW Madison HTCondor RPM"
        }, 
        {
            "location": "/other/install-gwms-frontend/#verify-your-condor-configuration", 
            "text": "The glideinWMS configuration files in  /etc/condor/config.d  should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files.    Verify the alll the expected HTCondor daemons are running:  [root@client ~] #  condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, COLLECTOR0 COLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12,  COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23,  COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34,  COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40  Defined in  /etc/condor/config.d/11_gwms_secondary_collectors.config , line 193.     If you don't see all the collectors. shared port and the two schedd, then the configuration must be corrected. There should be  no   startd  daemons listed.", 
            "title": "Verify your Condor configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#create-a-condor-grid-mapfile", 
            "text": "The Condor grid mapfile ( /etc/condor/certs/condor_mapfile ) is used for authentication between the glideinWMS pilot running on a remote worker node, and the local collector. Condor uses the mapfile to map certificates to pseudo-users on the local machine. It is important that you map the DN's of:    Each schedd proxy : The  DN  of each schedd that the frontend talks to. Specified in the frontend.xml schedd element  DN  attribute:  schedds \n   schedd   DN= /DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST   fullname= YOUR_HOST / \n   schedd   DN= /DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST   fullname= schedd_jobs2@YOUR_HOST /  /schedds     Frontend proxy : The DN of the proxy that the frontend uses to communicate with the other glideinWMS services. Specified in the frontend.xml security element  proxy_DN  attribute:  security classad_proxy= /tmp/vo_proxy  proxy_DN= DN of vo_proxy  ....    Each pilot proxy  The DN of  each  proxy that the frontend forwards to the factory to use with the glideinWMS pilots.  This allows the !glideinWMs pilot jobs to communicate with the User Collector. Specified in the frontend.xml proxy  absfname  attribute (you need to specify the  DN  of each of those proxies:  security   ....  proxies \n     proxy   absfname= /tmp/vo_proxy   .... \n    :  /proxies     Below is an example mapfile, by default found in  /etc/condor/certs/condor_mapfile . In this example there are lines for each of services mentioned above.   Note  The  example_of_format  entry as each DN should use this format for security purposes.   GSI  DN of schedd proxy  schedd\nGSI  DN of frontend proxy  frontend\nGSI  DN of pilot proxy $  pilot_proxy\nGSI  ^\\/DC\\=org\\/DC\\=doegrids\\/OU\\=Services\\/CN\\=personal\\-submit\\-host2\\.mydomain\\.edu$   example_of_format \nGSI (.*) anonymous\nFS (.*) \\1", 
            "title": "Create a Condor grid mapfile."
        }, 
        {
            "location": "/other/install-gwms-frontend/#restart-condor", 
            "text": "After configuring condor, be sure to restart condor:  [root@client ~] #  service condor restart", 
            "title": "Restart Condor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#proxy-configuration", 
            "text": "There are 2 types of (or purposes for) proxies required for the VO Frontend: 1 the  VO Frontend proxy  (used to authenticate with the other glideinWMS services) 1 one or more glideinWMS  pilot proxies  (used/delegated to the factory services and submitted on the glideinWMS pilot jobs) The  VO Frontend proxy  and the  pilot proxy  can be the same. By default, the VO Frontend will run as user  frontend  (UID is machine dependent) so these proxies must be owned by the user  frontend .", 
            "title": "Proxy Configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#manual-proxy-renewal", 
            "text": "VO Frontend proxy \nThe VO Frontend Proxy is used for communicating with the other glideinWMS services (Factory, User Collector and Schedd/Submit services). Create the proxy using the glidenWMS VO Frontend Host (or Service) cert and change ownership to the frontend user.  [root@client ~] #  voms-proxy-init-valid  hours_valid   \\ \n-cert /etc/grid-security/hostcert.pem  \\ \n-key /etc/grid-security/hostkey.pem  \\ \n-out  /tmp/vofe_proxy  [root@client ~] #  chown frontend  /tmp/vofe_proxy   Pilot proxy \nThe pilot proxy is used on the glideinWMS pilot jobs submitted to the CEs. Create the proxy using the  pilot certificate  and change ownership to the frontend user.  [root@client ~] #  voms-proxy-init -valid  hours_valid \\ \n-voms  vo  -cert  pilot_cert  \\  -key  pilot_key  \\  -out  /tmp/pilot_proxy  [root@client ~] #  chown frontend  /tmp/pilot_proxy    Warning  Proxies do expire.  You can extend the validity by using a longer time interval, e.g.  -valid 3000:0 . This sequence of commands will need to be renewed when the proxy expires or the machine reboots (if /tmp is used only).   Make sure that this location is specified correctly in the  frontend.xml  described in the  Configuring the Frontend  section.  You may want to automate the procedure above (or part of it) by writing a script and adding it to crontab.", 
            "title": "Manual proxy renewal"
        }, 
        {
            "location": "/other/install-gwms-frontend/#example-of-automatic-proxy-renewal", 
            "text": "This example (user provided) uses the script  make-proxy.sh  attached to this document. You still need to do some prep-work but this can be done only once a year and the script will warn you with an email.  Preparation for the  VO Frontend proxy . You'll have to redo this each time the Host (or Service) certificate and key are renewed:    Copy the Host (or Service) certificate and key  [root@client ~] #  cp /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem /var/lib/gwms-frontend/    Change ownership and permission of the certificate and key  [root@client ~] #  chown frontend: /var/lib/gwms-frontend/host**.pem [root@client ~] #  chmod  0600  /var/lib/gwms-frontend/host**.pem    Preparation for the   pilot proxy . You'll have to redo this for each new or renewed pilot cert.    Create the proxy using the pilot certificate/key (as the user/submitter)  [root@client ~] #  grid-proxy-init -valid  8800 :0 -out /tmp/tmp_proxy    Copy the proxy to the correct name and change ownership and permissions (as root):  [root@client ~] #  cp /tmp/tmp_proxy /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy [root@client ~] #  chown frontend: /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy [root@client ~] #  chmod  0600  /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy [root@client ~] #  rm /tmp/tmp_proxy    Configure the script for the  VO Frontend proxy    Download the  attached script  (the latest one is  Here on Github ) and save it as  /var/lib/gwms-frontend/make-frontend-proxy.sh , make sure that it is executable.    Edit the VARIABLES section to look something like (replace your email, host name and the paths that are different in your setup - the comments in the script will help):  SETUP_FILE= \nCERT_FILE= /var/lib/gwms-frontend/hostcert.pem \nKEY_FILE= /var/lib/gwms-frontend/hostkey.pem \nIN_NAME= /var/lib/gwms-frontend/frontend_base_proxy \nOUT_NAME= /tmp/vofe_proxy \nOWNER_EMAIL= your@email_here \nPROXY_DESCRIPTION= VO Fronted on  hostname \nVOMS_OPTION=     Configure the script for the  pilot proxy :    Download the  attached script  (the latest one is  Here on Github ) and save it as  /var/lib/gwms-frontend/make-pilot-proxy.sh , make sure that it is executable.    Edit the VARIABLES section to look something like (replace your email, host name and the paths that are different in your setup - the comments in the script will help):  SETUP_FILE= \nCERT_FILE= \nKEY_FILE= \nIN_NAME= /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy \nOUT_NAME= /tmp/vofe_gi_delegated_proxy \nOWNER_EMAIL= your@email_here \nPROXY_DESCRIPTION= VO Fronted glidein delegated on  hostname \nVOMS_OPTION= osg:/osg     Before adding the scripts to the crontab I'd recommend to test them manually once to make sure that there are no errors. As user  frontend  run the scripts (you can also use  sh -x  to debug them):  /var/lib/gwms-frontend/make-frontend-proxy.sh --no-voms-proxy /var/lib/gwms-frontend/make-pilot-proxy.sh   Add the scripts to the crontab of the user  frontend  with  crontab -e :  10 * * * * /var/lib/gwms-frontend/make-frontend-proxy.sh --no-voms-proxy\n10 * * * * /var/lib/gwms-frontend/make-pilot-proxy.sh  An additional script like  make-proxy-control.sh  (the latest one is  Here on Github ) can be used for an independent verification of the proxies. If you like, download it, fix the variables and add it to the crontab like the other two.", 
            "title": "Example of automatic proxy renewal"
        }, 
        {
            "location": "/other/install-gwms-frontend/#reconfigure-and-verify-installation", 
            "text": "Warning  In order to use the frontend, first you must reconfigure and upgrade it.  # For RHEL 6, CentOS 6, and SL6  [ root @ client   ~ ]  #  service   gwms - frontend   reconfig  [ root @ client   ~ ]  #  service   gwms - frontend   upgrade  # For RHEL 7, CentOS 7, and SL7  [ root @ client   ~ ]  #  / usr / sbin / gwms - frontend   reconfig  [ root @ client   ~ ]  #  / usr / sbin / gwms - frontend   upgrade    After this initial reconfiguring/upgrading, you can start the frontend:    # For RHEL 6, CentOS 6, and SL6 \n  [ root @ client   ~ ]  #  service   gwms - frontend   start \n  # For RHEL 7, CentOS 7, and SL7 \n  [ root @ client   ~ ]  #  systemctl   start   gwms - frontend", 
            "title": "Reconfigure and verify installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#adding-gratia-accounting-and-a-local-monitoring-page-on-a-production-server", 
            "text": "You must report to Gratia if you are running on OSG more than a few test jobs.  ProbeConfigGlideinWMS  explains how to instal and configure the HTCondor Gratia probe. If you are on a Campus Grid without x509 certificates pay attention to the  Users without Certificates part  in the Unusual Use Cases section.", 
            "title": "Adding Gratia Accounting and a Local Monitoring Page on a Production Server"
        }, 
        {
            "location": "/other/install-gwms-frontend/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to  the section on service activation/deactivation .   Allow users to specify where their jobs run  Creating a group to test configuration changes", 
            "title": "Optional Configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#allow-users-to-specify-where-their-jobs-run", 
            "text": "In order to allow users to specify the sites at which their jobs want to run (or to test a specific site), a frontend can be configured to match on  DESIRED_Sites  or ignore it if not specified. Modify  /etc/gwms-frontend/frontend.xml  using the following instructions:    In the frontend's global  match  stanza, set the  match_expr :  ((job.get( DESIRED_Sites , nosite )== nosite ) or (glidein[ attrs ][ GLIDEIN_Site ] in job.get( DESIRED_Sites , nosite ).split( , )))     In the same  match  stanza, set the  start_expr :  (DESIRED_Sites=?=undefined || stringListMember(GLIDEIN_Site,DESIRED_Sites, , ))    Add the  DESIRED_Sites  attribute to the match attributes list:  match_attrs \n    match_attr   name= DESIRED_Sites   type= string /  /match_attrs     Reconfigure the Frontend:  [root@client ~] #  /etc/init.d/gwms-frontend reconfig", 
            "title": "Allow users to specify where their jobs run"
        }, 
        {
            "location": "/other/install-gwms-frontend/#creating-a-group-for-testing-configuration-changes", 
            "text": "To perform configuration changes without impacting production the recommended way is to create an ITB group in  /etc/gwms-frontend/frontend.xml . This groupwould only match jobs that have the  +is_itb=True  ClassAd.    Create a  group  named itb.    Set the group's  start_expr  so that the group's glideins will only match user jobs with  +is_itb=True :  match match_expr= True  start_expr= (is_itb)     Set the  factory_query_expr  so that this group only communicates with ITB factories:  factory query_expr= FactoryType=?= itb     Set the group's  collector  stanza to reference the ITB factory, replacing  username@gfactory-1.t2.ucsd.edu  with your factory identity:  collector DN= /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=glidein-itb.grid.iu.edu  \\\n          factory_identity= gfactory@glidein-itb.grid.iu.edu  \\\n          my_identity= username@gfactory-1.t2.ucsd.edu  \\\n          node= glidein-itb.grid.iu.edu /     Set the job  query_expr  so that only ITB jobs appear in  condor_q :  job query_expr= (!isUndefined(is_itb)   is_itb)     Reconfigure the Frontend:  /etc/init.d/gwms-frontend reconfig", 
            "title": "Creating a group for testing configuration changes"
        }, 
        {
            "location": "/other/install-gwms-frontend/#service-activation-and-deactivation", 
            "text": "The scripts updating your CA and CRLs plus three frontend services need to be running:    You need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:  % RED%# For RHEL  6 , CentOS  6 , and SL6, or OSG  3  _older_ than  3 .1.15  [root@client ~]$  /usr/sbin/fetch-crl    # This fetches the CRLs  [root@client ~]$  /sbin/service fetch-crl-boot start [root@client ~]$  /sbin/service fetch-crl-cron start % RED%  # For RHEL 7, CentOS 7, and SL7   [root@client ~]$  /usr/sbin/fetch-crl    # This fetches the CRLs  [root@client ~]$  systemctl start fetch-crl-boot [root@client ~]$  systemctl start fetch-crl-cron    HTCondor, httpd, VO Frontend  % RED%# For RHEL  6 , CentOS  6 , and SL6  [root@client ~] #  service condor start [root@client ~] #  service httpd start [root@client ~] #  service gwms-frontend start % RED%# For RHEL  7 , CentOS  7 , and SL7  [root@client ~] #  systemctl start condor [root@client ~] #  systemctl start gwms-frontend     Note  Once you successfully start using the frontend service, each time you change the configuration or want to upgrade, you run the following command  % RED%# For RHEL  6 , CentOS  6 , and SL6  [root@client ~] #  service gwms-frontend reconfig % RED%# And  if  you change also some code  [root@client ~] #  service gwms-frontend upgrade % RED%# But the situation is a bit more complicated in RHEL  7 , CentOS  7 , and SL7 due to systemd restrictions  % GREEN%# For reconfig:  % RED%A. when the frontend is running  % RED%A.1 without any additional options  [root@client ~] #  /usr/sbin/gwms-frontend reconfig  or  [root@client ~] #  systemctl reload gwms-frontend % RED%A.2  if  you want to give additional options   systemctl stop gwms-frontend  /usr/sbin/gwms-frontend reconfig  and your options  systemctl start gwms-frontend  % RED%B. when the frontend is NOT running   [root@client ~] #  /usr/sbin/gwms-frontend reconfig  ( and your options )  % GREEN%# For upgrade:  % RED%A. when the frontend is running   systemctl stop gwms-frontend  /usr/sbin/gwms-frontend upgrade ( and your options if any )  systemctl start gwms-frontend  % RED%B. when the frontend is NOT running   /usr/sbin/gwms-frontend upgrade ( and your options if any )    To stop the frontend:  % RED%# For RHEL  6 , CentOS  6 , and SL6  [root@client ~] #  service gwms-frontend stop % RED%# For RHEL  7 , CentOS  7 , and SL7  [root@client ~] #  systemctl stop gwms-frontend  And you can stop also the other services if you are not using them independently form the frontend.", 
            "title": "Service Activation and Deactivation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#validation-of-service-operation", 
            "text": "The complete validation of the frontend is the submission of actual jobs. However, there are a few things that can be checked prior to submitting user jobs to Condor.    Verify all Condor daemons are started.  [user@client ~]$ condor_config_val -verbose DAEMON_LIST\nDAEMON_LIST: MASTER,  COLLECTOR, NEGOTIATOR,  SCHEDD, SHARED_PORT, SCHEDDJOBS2 COLLECTOR0 COLLECTOR1 COLLECTOR2\nCOLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11,\nCOLLECTOR12, COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20,\nCOLLECTOR21, COLLECTOR22, COLLECTOR23, COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29,\nCOLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38,\nCOLLECTOR39, COLLECTOR40\nDefined in  /etc/condor/config.d/11_gwms_secondary_collectors.config , line 193.  If you don't see all the collectors and the two schedd, then the configuration must be corrected. There should be no startd daemons listed    Verify all VO Frontend Condor services are communicating.  [user@client ~]$ condor_status -any\nMyType               TargetType           Name\nglideresource        None                 MM_fermicloud026@gfactory_inst\nScheduler            None                 fermicloud020.fnal.gov\nDaemonMaster         None                 fermicloud020.fnal.gov\nNegotiator           None                 fermicloud020.fnal.gov\nCollector            None                 frontend_service@fermicloud020\nScheduler            None                 schedd_jobs2@fermicloud020.fnal    To see the details of the glidein resource use  condor_status -subsystem glideresource -l , including the GlideFactoryName.    Verify that the Factory is seeing correctly the Frontend using  condor_status -pool  FACTORY_HOST  -any -constraint 'FrontendName== \"FRONTEND_NAME_FROM_CONFIG\" ' -l , including the GlideFactoryName.", 
            "title": "Validation of Service Operation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#glidein-wms-job-submission", 
            "text": "Condor submit file  glidein-job.sub . This is a simple job printing the hostname of the host where the job is running:  #file glidein-job.sub\nuniverse = vanilla\nexecutable = /bin/hostname\noutput = glidein/test.out\nerror = glidein/test.err\nrequirements = IS_GLIDEIN == True\nlog = glidein/test.log\nShouldTransferFiles = YES\n\nwhen_to_transfer_output = ON_EXIT\nqueue  To submit the job:  [root@client ~] #  condor_submit glidein-job.sub  Then you can control the job like a normal condor job, e.g. to check the status of the job use  condor_q .", 
            "title": "Glidein WMS Job submission"
        }, 
        {
            "location": "/other/install-gwms-frontend/#monitoring-web-pages", 
            "text": "You should be able to see the jobs also in the GWMS monitoring pages that are made available on the Web:  http://gwms-frontend-host.domain/vofrontend/monitor/", 
            "title": "Monitoring Web pages"
        }, 
        {
            "location": "/other/install-gwms-frontend/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/other/install-gwms-frontend/#file-locations", 
            "text": "File Description  File Location      Configuration file  /etc/gwms-frontend/frontend.xml    Logs  /var/log/gwms-frontend/    Startup script  /etc/init.d/gwms-frontend    Web Directory  /var/lib/gwms-frontend/web-area    Web Base  /var/lib/gwms-frontend/web-base    Web configuration  /etc/httpd/conf.d/gwms-frontend.conf    Working Directory  /var/lib/gwms-frontend/vofrontend/    Lock files  /etc/init.d/gwms-frontend/vofrontend/lock/frontend.lock /etc/init.d/gwms-frontend/vofrontend/group_*/lock/frontend.lock    Status files  /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml      Note  /var/lib/gwms-frontend  is also the home directory of the  frontend  user", 
            "title": "File Locations"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificates-brief", 
            "text": "Here a short list of files to check when you change the certificates. Note that if you renew a proxy or certificate and the DN remains the same no configuration file needs to change, just put the renewed certificate/proxy in place.     File Description  File Location      Configuration file  /etc/gwms-frontend/frontend.xml    HTCondor certificates map  /etc/condor/creds/condor_mapfile (1)    Host certificate and key (2)  /etc/grid-security/hostcert.pem            /etc/grid-security/hostkey.pem    VO Frontend proxy (from host certificate)  /tmp/vofe_proxy (3)    Pilot proxy  /tmp/vofe_proxy (3)       If using HTCondor RPM installation, e.g. the one coming from OSG. If you have separate/multiple HTCondor hosts (schedds, collectors, negotiators, ..) you may have to check this file on all of them to make sure that the HTCondor authentication works correctly.    Used to create the VO Frontend proxy if following the  instructions above    If using the scripts described  above in this document    Remember also that when you change DN:   The VO Frontend certificate DN must be communicated to the GWMS Factory ( see above )  The pilot proxy must be able to run jobs at the sites you are using, e.g. by being added to the correct VO in OSG (the Factory forwards the proxy and does not care about the DN)", 
            "title": "Certificates brief"
        }, 
        {
            "location": "/other/install-gwms-frontend/#increase-the-log-level-and-change-rotation-policies", 
            "text": "You can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the  process_log  section of  /etc/gwms-frontend/frontend.xml :  log_retention \n    process_logs \n        process_log extension= all  max_days= 7.0  max_mbytes= 100.0  min_days= 3.0  msg_types= DEBUG,EXCEPTION,INFO,ERROR,ERR /   You can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files:   max_bytes is the max size of the log files  max_days it will be rotated.  compression specifies if rotated files are compressed  backup_count is the number of rotated log files kept   Further details are in the  reference documentation .", 
            "title": "Increase the log level and change rotation policies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-reconfig-failing", 
            "text": "If  service gwms-frontend reconfig  fails at the end with an error like \"Writing back config file failed, Reconfiguring the frontend [FAILED]\", make sure that  /etc/gwms-frontend/  belongs to the  frontend  user. It must be able to write to update the configuration file.", 
            "title": "Frontend reconfig failing"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-failing-to-start", 
            "text": "If the startup script of the frontend is failing, check the log file for errors (probably  /var/log/gwms-frontend/frontend/frontend. TODAY .err.log  and  .debug.log ).  If you find errors like  \"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\"  and  \"IOError: [Errno 9] Bad file descriptor\"  you may have an empty status file ( /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml ) that causes Glidein WMS Frontend not to start. The glideinFrontend crashes after a XML parsing exception visible in the log file (\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\").  Remove the status file. Then start the frontend. The fronten will be fixed in future versions to handle this automatically.", 
            "title": "Frontend failing to start"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificates-not-there", 
            "text": "The scripts should send an email warning if there are problems and they fail to generate the proxies. Anyway something could go wrong and you want to check manually. If you are using the scripts to generate automatically the proxies but the proxies are not there (in  /tmp  or wherever you expect them):   make sure that the scripts are there and configured with the correct values  make sure that the scripts are executable  make sure that the scripts are in  frontend  's crontab  make sure that the certificates (or master proxy) used to generate the proxies is not expired", 
            "title": "Certificates not there"
        }, 
        {
            "location": "/other/install-gwms-frontend/#failed-authentication", 
            "text": "If you get a failed authentication error (e.g. \"Failed to talk to factory_pool gfactory-1.t2.ucsd.edu...) then:   check that you have the right x509 certificates mentioned in the security section of  /etc/gwms-frontend/frontend.xml  the owner must be  frontend  (user running the frontend)  the permission must be 600  they must be valid for more than one hour (2/300 hours), at least the non VO part    check that the clock is synchronized (see HostTimeSetup)", 
            "title": "Failed authentication"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-doesnt-trust-factory", 
            "text": "If your frontend complains in the debug log:  code 256:[ Error: communication error\\n ,  AUTHENTICATE:1003:Failed to authenticate with any method\\n ,  AUTHENTICATE:1004:Failed to authenticate using GSI\\n ,  GSI:5006:Failed to authenticate because the subject  /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu  is not currently trusted by you.  If it should be, add it to GSI_DAEMON_NAME in the condor_config, or use the environment variable override (check the manual).\\n ,  GSI:5004:Failed to gss_assist_gridmap /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu to a local user.  A possible solution is to comment/remove the LOCAL_CONFIG_DIR in the file  /var/lib/gwms-frontend/vofrontend/frontend.condor_config .", 
            "title": "Frontend doesn't trust factory"
        }, 
        {
            "location": "/other/install-gwms-frontend/#no-security-credentials-match-for-factory-pool-not-advertising-request", 
            "text": "You may see a warning like \"No security credentials match for factory pool ..., not advertising request\", if the  trust_domain  and  auth_method  of an entry in the Factory configuration is not matching any of the  trust_domain ,  type  couples in the credentials in the Frontend configuration. This causes the Frontend not to use some Factory entries (the ones not matching) and may end up without entries to send glideins to.  To fix the problem make sure that those attributes match as desired.", 
            "title": "No security credentials match for factory pool ..., not advertising request"
        }, 
        {
            "location": "/other/install-gwms-frontend/#jobs-not-running", 
            "text": "If your jobs remain Idle   Check the frontend log files (see above)  Check the condor log files ( condor_config_val LOG  will give you the correct log directory):  Specifically look the CollectorXXXLog files     Common causes of problems could be:   x509 certificates  missing or expired or too short-lived proxy  incorrect ownership or permission on the certificate/proxy file  missing certificates    If the frontend http server is down in the factory there will be errors like \"Failed to load file 'description.dbceCN.cfg' from ' http://FRONTEND_HOST/vofrontend/stage '.\"  check that the http server is running and you can reach the URL ( http://FRONTEND_HOST/vofrontend/stage/description.dbceCN.cfg )", 
            "title": "Jobs not running"
        }, 
        {
            "location": "/other/install-gwms-frontend/#advanced-configurations", 
            "text": "GlideinWMS Frontend on a Campus Grid", 
            "title": "Advanced Configurations"
        }, 
        {
            "location": "/other/install-gwms-frontend/#references", 
            "text": "Definitions:   What is a  Virtual Organisation  Introduction to the Grid for users/scientists   Documents about the Glidein-WMS system and the VO frontend:   http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/  http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/manual/  How to setup a Submit host flocking to the VO Frontend", 
            "title": "References"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/", 
            "text": "Install a CVMFS Stratum 1\n\n\nAbout this Document\n\n\nThis document describes how to install a CVMFS Stratum 1. There are many different variations on how to do that, but this document focuses on the configuration of the OSG GOC Stratum 1 oasis-replica.opensciencegrid.org. It is applicable to other Stratum 1s as well, very likely with modifications (some of which are suggested in the document below).\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are cvmfs and cvmfs-server \n= 2.2.1.\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nSupported operating system\n\n\nRoot access\n\n\nSELinux disabled\n\n\nAdequate disk space for the repositories that will be served, at \n/srv/cvmfs\n. Do not use xfs as the filesystem type on operating systems older than RHEL7, because it has been demonstrated to perform poorly for CVMFS repositories; instead use ext3 or ext4. About 10GB should be reserved for apache and squid logs under /var/log on a production server, although they normally will not get that large. A Stratum 1 that is also a repository server should have at least 5GB available at \n/var/cache\n.\n\n\n\n\nUsers and Groups\n\n\nIf your machine is also going to be a repository server like the OSG GOC, the installation will create one user unless it already exists:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\n\n\n\n\n\n\nA repository server installation will also create a cvmfs group and default the cvmfs user to that group.\n\n\nIn addition, if the fuse rpm is not for some reason already installed, installing the repository server packages will also install fuse and that will create another group:\n\n\n\n\n\n\n\n\nGroup\n\n\nComment\n\n\nGroup members\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\nnone\n\n\n\n\n\n\nfuse\n\n\nFUSE service account\n\n\ncvmfs\n\n\n\n\n\n\n\n\nInstall Instructions\n\n\nAll CVMFS Stratum 1s require cvmfs-server software and apache (httpd). It is highly recommended to also install frontier-squid and frontier-awstats on the same machine to be able to easily join the WLCG \nMRTG\n and \nawstats\n monitoring systems. The recommended configuration for frontier-squid below does not do any caching, it is just for monitoring.\n\n\nInstalling cvmfs-server and httpd\n\n\nThe OSG GOC Stratum 1 has to function as a repository server in addition to serving repository replications; most Stratum 1s serve only replications. Serving repositories is done quite differently on Redhat EL5 and EL6 systems. Instructions are also provided for how to install cvmfs-server on Stratum 1s that do not have to be repository servers, which is the same on both versions of operating systems. Choose one of the following three sections.\n\n\nInstalling a CVMFS repository server\n\n\nInstalling CVMFS repository server on RHEL5\n\n\nRedhat EL5-based systems that are not Scientific Linux-based should first build the \nSL5 aufs source rpm\n and install the resulting rpm in their own yum repository.\n\n\nFollow these instructions to install:\n\n\nroot@host #\n rpm -i http://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs/EL/5/x86_64/cvmfs-release-2-4.el5.noarch.rpm\n\nroot@host #\n rpm -Uvh http://dl.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm\n\nroot@host #\n yum -y install aufs cvmfs-server.x86_64 cvmfs.x86_64 mod_wsgi\n\n\n\n\n\nInstalling CVMFS repository server on RHEL6\n\n\nRedhat EL6-based systems running a CVMFS repository server have to get their kernel from CERN: \n\n\nroot@host #\n rpm -i http://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs-release-latest.noarch.rpm\n\nroot@host #\n yum -y install --enablerepo\n=\ncernvm-kernel kernel aufs2-util cvmfs-server.x86_64 cvmfs.x86_64 mod_wsgi\n\nroot@host #\n reboot\n\n\n\n\n\nCVMFS repository server and RHEL7\n\n\nRHEL7.2 cannot be reliably used as a repository server, because of bugs in the union filesystem OverlayFS. The bugs are fixed in RHEL7.3.\n\n\nRedhat EL7.3-based systems running a CVMFS repository server is the simplest method: \n\n\nroot@host #\n rpm -i http://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs-release-latest.noarch.rpm\n\nroot@host #\n yum -y install cvmfs-server.x86_64 cvmfs.x86_64 mod_wsgi\n\n\n\n\n\nInstalling CVMFS stratum 1 code without repository server code\n\n\nIf you're not installing for the OSG GOC or otherwise want to support serving repositories on the same machine as a Stratum 1, use this section.\n\n\nOn Redhat EL5, first do these commands:\n\n\nroot@host #\n curl -O https://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs/EL/5/x86_64/cvmfs-release-2-4.el5.noarch.rpm\n\nroot@host #\n rpm -i cvmfs-release-2-4.el5.noarch.rpm \n\nroot@host #\n curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n\nroot@host #\n rpm -Uvh epel-release-latest-5.noarch.rpm\n\n\n\n\n\nOn Redhat EL6, first do these commands:\n\n\nroot@host #\n rpm -i https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\n\n\n\n\n\nOn Redhat EL7, first do these commands:\n\n\nroot@host #\n rpm -i https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\n\n\n\n\n\nThen on any type of system do this command:\n\n\nroot@host #\n yum -y install cvmfs-server.x86_64 cvmfs-config mod_wsgi\n\n\n\n\n\nInstalling frontier-squid and frontier-awstats\n\n\nDo these commands to install frontier-squid and frontier-awstats:\n\n\nroot@host #\n rpm -i http://frontier.cern.ch/dist/rpms/RPMS/noarch/frontier-release-1.1-1.noarch.rpm\n\nroot@host #\n yum -y install frontier-awstats\n\n\n\n\n\nConfiguring\n\n\nConfiguring the system\n\n\nIncrease the default number of open file descriptors:\n\n\nroot@host #\n \necho\n -e \n\\*\\t\\t-\\tnofile\\t\\t16384\n \n/etc/security/limits.conf \n\nroot@host #\n \nulimit\n -n \n16384\n\n\n\n\n\n\nIn order for this to apply also interactively when logging in over ssh, the option \nUsePAM\n has to be set to \nyes\n in \n/etc/ssh/sshd_config\n.\n\n\nConfiguring cron\n\n\nFirst, create the log directory: \n\n\nroot@host #\n mkdir -p /var/log/cvmfs\n\n\n\n\n\nPut the following in \n/etc/cron.d/cvmfs\n:\n\n\n0,15,30,45 * * * * root test -d /srv/cvmfs || exit;cvmfs_server snapshot -ai \n0 9 * * * root find /srv/cvmfs/\\*.**/data/txn -name \n**.*\n -mtime +2 2\n/dev/null|xargs rm -f\n\n\n\n\n\nAlso put the following in \n/etc/logrotate.d/cvmfs\n:\n\n\n/var/log/cvmfs/\\*.log { weekly missingok notifempty }\n\n\n\n\n\nConfiguring apache\n\n\nIf you are installing frontier-squid, create \n/etc/httpd/conf.d/cvmfs.conf\n and put the following lines into it:\n\n\nListen 8080 KeepAlive On\n\n\n\n\n\nIf you are not installing frontier-squid, instead put the following lines into that file:\n\n\nListen 8000 KeepAlive On\n\n\n\n\n\nIf you will be serving opensciencegrid.org repositories, you have to allow for old client configurations that access repositories without the domain name added. For that reason, you will need to remove each \n/etc/httpd/conf.d/cvmfs.\nrepositoryname\n.conf\n that adding a replica creates (this is included in the \nadd_osg_repository script\n), and instead add the following to \n/etc/httpd/conf.d/cvmfs.conf\n:\n\n\nRewriteEngine On \nRewriteRule ^/cvmfs/(\\[^./\\]**)/(.**)$ /cvmfs/$1.opensciencegrid.org/$2 \nRewriteRule ^/cvmfs/(\\[^/\\]+)/api/(.**)$ /cvmfs/$1/api/$2 \\[PT\\] \nRewriteRule ^/cvmfs/(.**)$ /srv/cvmfs/$1 \n\nDirectory\n \n/srv/cvmfs\n \n  Options -MultiViews +FollowSymLinks -Indexes \n  AllowOverride All \n  Order allow,deny \n  Allow from all\n\n  EnableMMAP Off EnableSendFile Off\n\n  \nFilesMatch\n \n^\\.cvmfs\n ForceType application/x-cvmfs \n/FilesMatch\n\n\n  Header unset \n  Last-Modified \n  FileETag None\n\n  ExpiresActive On \n  ExpiresDefault \naccess plus 3 days\n \n  ExpiresByType text/html \naccess plus 15 minutes\n \n  ExpiresByType application/x-cvmfs \naccess plus 2 minutes\n \n\n/Directory\n\n\nWSGIDaemonProcess cvmfs-api processes=2 display-name=%{GROUP} \\\\ python-path=/usr/share/cvmfs-server/webapi \nWSGIProcessGroup cvmfs-api \nWSGISocketPrefix /var/run/wsgi \nWSGIScriptAliasMatch /cvmfs/(\\[^/\\]+)/api /var/www/wsgi-scripts/cvmfs-api.wsgi/$1 \n\n\n\n\n\nOn EL7-based systems (apache httpd 2.4 and later) replace the \"Order allow, deny\" and \"Allow from all\" to \"Require all granted\".\n\n\nIf you will be serving cern.ch repositories, it has the same problem; replace opensciencegrid.org above with cern.ch. If you need to serve both opensciencegrid.org and cern.ch contact Dave Dykstra to discuss the options.\n\n\nThen enable apache:\n\n\nroot@host #\n chkconfig httpd on \n\nroot@host #\n service httpd start\n\n\n\n\n\nConfiguring frontier-squid\n\n\nPut the following in \n/etc/squid/customize.sh\n after the existing comment header:\n\n\nawk\n \n--\nfile\n \n`\ndirname\n \n$\n0\n`\n/\ncustomhelps\n.\nawk\n \n--\nsource\n \n{\n\n\n\n# cache only api calls \n\n\ninsertline(\n^http_access deny all\n, \nacl CVMFSAPI urlpath_regex ^/cvmfs/\\[^/\\]\\*/api/\n) insertline(\n^http_access deny all\n, \ncache deny CVMFSAPI\n)\n\n\n\n# port 80 is also supported, through an iptables redirect \n\n\nsetoption(\nhttp_port\n, \n8000 accel defaultsite=localhost:8080 no-vhost\n) setoption(\ncache_peer\n, \nlocalhost parent 8080 0 no-query originserver\n)\n\n\n\n# allow incoming http accesses from anywhere \\# all requests will be forwarded to the originserver \n\n\ncommentout(\nhttp_access allow NET_LOCAL\n) insertline(\n^http_access deny all\n, \nhttp_access allow all\n)\n\n\n\n# do not let squid cache DNS entries more than 5 minutes \n\n\nsetoption(\npositive_dns_ttl\n, \n5 minutes\n)\n\n\n\n# set shutdown_lifetime to 0 to avoid giving new connections error \\# codes, which get cached upstream \n\n\nsetoption(\nshutdown_lifetime\n, \n0 seconds\n)\n\n\n\n# turn off collapsed_forwarding to prevent slow clients from slowing down faster ones\n\n\nsetoption(\ncollapsed_forwarding\n, \noff\n)\n\n\n\nprint }\n\n\n\n\n\n\nOn an RHEL7 system, make sure that iptables-services is installed and enabled:\n\n\nroot@host #\n yum -y install iptables-services \n\nroot@host #\n systemctl \nenable\n iptables\n\n\n\n\n\nForward port 80 to port 8000 (first command is for external, second command for localhost):\n\n\nroot@host #\n iptables -t nat -A PREROUTING -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n \n\nroot@host #\n iptables -t nat -A OUTPUT -o lo -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n \n\nroot@host #\n service iptables save\n\n\n\n\n\nOn RHEL7 also set up the the same port forwarding for IPv6 (unfortunately it is not supported on RHEL6):\n\n\nroot@host #\n ip6tables -t nat -A PREROUTING -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nroot@host #\n ip6tables -t nat -A OUTPUT -o lo -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nroot@host #\n service ip6tables save\n\n\n\n\n\nEnable frontier-squid:\n\n\nroot@host #\n chkconfig frontier-squid on\n\nroot@host #\n service frontier-squid start\n\n\n\n\n\nNote: the above configuration is for a single squid thread, which is fine for 1Gbit/s and possibly 2Gbit/s, but if higher bandwidth is needed, see the \ninstructions for running multiple squid workers\n.\n\n\nVerifying\n\n\nIn order to verify that everything is installed correctly, create a repository replica. The repository chosen for the instructions below is one from egi.eu because it is very small, but you can use another one if you prefer.\n\n\nAdding an example repository\n\n\nThe OSG GOC Stratum 1 should add a repository replica using the \nadd_osg_repository\n script from the oasis-2 rpm. Instructions for installing that are elsewhere but you can also \ndownload add_osg_repository\n. This script assumes that the oasis.opensciencegrid.org replica repository was first created, so this instruction creates it but does not download the first snapshot because that would take a lot of space and time. Use these commands to create the oasis replica and to create and download the example replica:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://oasis.opensciencegrid.org:8000/cvmfs/oasis.opensciencegrid.org\n /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub \n\nroot@host #\n add_osg_repository http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu\n\n\n\n\n\nIt's a good idea for other Stratum 1s to make their own scripts for adding repository replicas, because there's always two or three commands to run, and it's easy to forget the commands after the first one. The first command is generically this:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu\n /etc/cvmfs/keys/egi.eu/egi.eu.pub\n\n\n\n\n\nHowever, non-GOC OSG Stratum 1s (that is, at BNL and FNAL), for the sake of fulfilling an OSG security requirement, need to instead read from the OSG GOC machine with this as their first command:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://oasis-replica.opensciencegrid.org:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub:/etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub\n\n\n\n\n\nThe second command for Stratum 1s that have the httpd configuration as described above in the \nConfiguring apache section\n is this:\n\n\nroot@host #\n rm -f /etc/httpd/conf.d/cvmfs.config-egi.egi.eu.conf\n\n\n\n\n\nThen the next command is this:\n\n\nroot@host #\n cvmfs_server snapshot config-egi.egi.eu\n\n\n\n\n\nWith large repositories that can take a very long time, but with small repositories it should be very quick and not show any errors.\n\n\nVerifying that the replica is being served\n\n\nNow to verify that the replication is working, do the following commands:\n\n\nroot@host #\n wget -qdO- http://localhost:8000/cvmfs/config-egi.egi.eu/.cvmfspublished%7Ccat -v\n\nroot@host #\n wget -qdO- http://localhost:80/cvmfs/config-egi.egi.eu/.cvmfspublished%7Ccat -v\n\n\n\n\n\nBoth commands should show a short file including gibberish at the end which is the signature.\n\n\nIt is a good idea to familiarize yourself with the log entries at \n/var/log/httpd/access_log\n and also, if you have installed frontier-squid, at \n/var/log/squid/access.log\n. Also, at least 15 minutes after the snapshot is finished, check the log \n/var/log/cvmfs\n to see that it tried to get an update and got no errors.\n\n\nSetting up monitoring\n\n\nIf you installed frontier-squid and frontier-awstats, there is a little more to do to configure monitoring.\n\n\nFirst, make sure that your firewall accepts UDP queries from the monitoring server at CERN. Details are on \nthe frontier-squid instructions\n. Next, choose any random password and put it in \n/etc/awstats/password-file\n. Then tell Dave Dykstra the fully qualified domain name of your machine and the password you chose, and he'll set up the monitoring servers.", 
            "title": "Install a CVMFS Stratum 1"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#install-a-cvmfs-stratum-1", 
            "text": "", 
            "title": "Install a CVMFS Stratum 1"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#about-this-document", 
            "text": "This document describes how to install a CVMFS Stratum 1. There are many different variations on how to do that, but this document focuses on the configuration of the OSG GOC Stratum 1 oasis-replica.opensciencegrid.org. It is applicable to other Stratum 1s as well, very likely with modifications (some of which are suggested in the document below).", 
            "title": "About this Document"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#applicable-versions", 
            "text": "The applicable software versions for this document are cvmfs and cvmfs-server  = 2.2.1.", 
            "title": "Applicable versions"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#host-and-os", 
            "text": "Supported operating system  Root access  SELinux disabled  Adequate disk space for the repositories that will be served, at  /srv/cvmfs . Do not use xfs as the filesystem type on operating systems older than RHEL7, because it has been demonstrated to perform poorly for CVMFS repositories; instead use ext3 or ext4. About 10GB should be reserved for apache and squid logs under /var/log on a production server, although they normally will not get that large. A Stratum 1 that is also a repository server should have at least 5GB available at  /var/cache .", 
            "title": "Host and OS"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#users-and-groups", 
            "text": "If your machine is also going to be a repository server like the OSG GOC, the installation will create one user unless it already exists:     User  Comment      cvmfs  CernVM-FS service account     A repository server installation will also create a cvmfs group and default the cvmfs user to that group.  In addition, if the fuse rpm is not for some reason already installed, installing the repository server packages will also install fuse and that will create another group:     Group  Comment  Group members      cvmfs  CernVM-FS service account  none    fuse  FUSE service account  cvmfs", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#install-instructions", 
            "text": "All CVMFS Stratum 1s require cvmfs-server software and apache (httpd). It is highly recommended to also install frontier-squid and frontier-awstats on the same machine to be able to easily join the WLCG  MRTG  and  awstats  monitoring systems. The recommended configuration for frontier-squid below does not do any caching, it is just for monitoring.", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-cvmfs-server-and-httpd", 
            "text": "The OSG GOC Stratum 1 has to function as a repository server in addition to serving repository replications; most Stratum 1s serve only replications. Serving repositories is done quite differently on Redhat EL5 and EL6 systems. Instructions are also provided for how to install cvmfs-server on Stratum 1s that do not have to be repository servers, which is the same on both versions of operating systems. Choose one of the following three sections.", 
            "title": "Installing cvmfs-server and httpd"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-a-cvmfs-repository-server", 
            "text": "", 
            "title": "Installing a CVMFS repository server"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-cvmfs-repository-server-on-rhel5", 
            "text": "Redhat EL5-based systems that are not Scientific Linux-based should first build the  SL5 aufs source rpm  and install the resulting rpm in their own yum repository.  Follow these instructions to install:  root@host #  rpm -i http://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs/EL/5/x86_64/cvmfs-release-2-4.el5.noarch.rpm root@host #  rpm -Uvh http://dl.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm root@host #  yum -y install aufs cvmfs-server.x86_64 cvmfs.x86_64 mod_wsgi", 
            "title": "Installing CVMFS repository server on RHEL5"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-cvmfs-repository-server-on-rhel6", 
            "text": "Redhat EL6-based systems running a CVMFS repository server have to get their kernel from CERN:   root@host #  rpm -i http://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs-release-latest.noarch.rpm root@host #  yum -y install --enablerepo = cernvm-kernel kernel aufs2-util cvmfs-server.x86_64 cvmfs.x86_64 mod_wsgi root@host #  reboot", 
            "title": "Installing CVMFS repository server on RHEL6"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#cvmfs-repository-server-and-rhel7", 
            "text": "RHEL7.2 cannot be reliably used as a repository server, because of bugs in the union filesystem OverlayFS. The bugs are fixed in RHEL7.3.  Redhat EL7.3-based systems running a CVMFS repository server is the simplest method:   root@host #  rpm -i http://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs-release-latest.noarch.rpm root@host #  yum -y install cvmfs-server.x86_64 cvmfs.x86_64 mod_wsgi", 
            "title": "CVMFS repository server and RHEL7"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-cvmfs-stratum-1-code-without-repository-server-code", 
            "text": "If you're not installing for the OSG GOC or otherwise want to support serving repositories on the same machine as a Stratum 1, use this section.  On Redhat EL5, first do these commands:  root@host #  curl -O https://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs/EL/5/x86_64/cvmfs-release-2-4.el5.noarch.rpm root@host #  rpm -i cvmfs-release-2-4.el5.noarch.rpm  root@host #  curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm root@host #  rpm -Uvh epel-release-latest-5.noarch.rpm  On Redhat EL6, first do these commands:  root@host #  rpm -i https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm  On Redhat EL7, first do these commands:  root@host #  rpm -i https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm  Then on any type of system do this command:  root@host #  yum -y install cvmfs-server.x86_64 cvmfs-config mod_wsgi", 
            "title": "Installing CVMFS stratum 1 code without repository server code"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-frontier-squid-and-frontier-awstats", 
            "text": "Do these commands to install frontier-squid and frontier-awstats:  root@host #  rpm -i http://frontier.cern.ch/dist/rpms/RPMS/noarch/frontier-release-1.1-1.noarch.rpm root@host #  yum -y install frontier-awstats", 
            "title": "Installing frontier-squid and frontier-awstats"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring", 
            "text": "", 
            "title": "Configuring"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-the-system", 
            "text": "Increase the default number of open file descriptors:  root@host #   echo  -e  \\*\\t\\t-\\tnofile\\t\\t16384   /etc/security/limits.conf  root@host #   ulimit  -n  16384   In order for this to apply also interactively when logging in over ssh, the option  UsePAM  has to be set to  yes  in  /etc/ssh/sshd_config .", 
            "title": "Configuring the system"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-cron", 
            "text": "First, create the log directory:   root@host #  mkdir -p /var/log/cvmfs  Put the following in  /etc/cron.d/cvmfs :  0,15,30,45 * * * * root test -d /srv/cvmfs || exit;cvmfs_server snapshot -ai \n0 9 * * * root find /srv/cvmfs/\\*.**/data/txn -name  **.*  -mtime +2 2 /dev/null|xargs rm -f  Also put the following in  /etc/logrotate.d/cvmfs :  /var/log/cvmfs/\\*.log { weekly missingok notifempty }", 
            "title": "Configuring cron"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-apache", 
            "text": "If you are installing frontier-squid, create  /etc/httpd/conf.d/cvmfs.conf  and put the following lines into it:  Listen 8080 KeepAlive On  If you are not installing frontier-squid, instead put the following lines into that file:  Listen 8000 KeepAlive On  If you will be serving opensciencegrid.org repositories, you have to allow for old client configurations that access repositories without the domain name added. For that reason, you will need to remove each  /etc/httpd/conf.d/cvmfs. repositoryname .conf  that adding a replica creates (this is included in the  add_osg_repository script ), and instead add the following to  /etc/httpd/conf.d/cvmfs.conf :  RewriteEngine On \nRewriteRule ^/cvmfs/(\\[^./\\]**)/(.**)$ /cvmfs/$1.opensciencegrid.org/$2 \nRewriteRule ^/cvmfs/(\\[^/\\]+)/api/(.**)$ /cvmfs/$1/api/$2 \\[PT\\] \nRewriteRule ^/cvmfs/(.**)$ /srv/cvmfs/$1  Directory   /srv/cvmfs  \n  Options -MultiViews +FollowSymLinks -Indexes \n  AllowOverride All \n  Order allow,deny \n  Allow from all\n\n  EnableMMAP Off EnableSendFile Off\n\n   FilesMatch   ^\\.cvmfs  ForceType application/x-cvmfs  /FilesMatch \n\n  Header unset \n  Last-Modified \n  FileETag None\n\n  ExpiresActive On \n  ExpiresDefault  access plus 3 days  \n  ExpiresByType text/html  access plus 15 minutes  \n  ExpiresByType application/x-cvmfs  access plus 2 minutes   /Directory \n\nWSGIDaemonProcess cvmfs-api processes=2 display-name=%{GROUP} \\\\ python-path=/usr/share/cvmfs-server/webapi \nWSGIProcessGroup cvmfs-api \nWSGISocketPrefix /var/run/wsgi \nWSGIScriptAliasMatch /cvmfs/(\\[^/\\]+)/api /var/www/wsgi-scripts/cvmfs-api.wsgi/$1   On EL7-based systems (apache httpd 2.4 and later) replace the \"Order allow, deny\" and \"Allow from all\" to \"Require all granted\".  If you will be serving cern.ch repositories, it has the same problem; replace opensciencegrid.org above with cern.ch. If you need to serve both opensciencegrid.org and cern.ch contact Dave Dykstra to discuss the options.  Then enable apache:  root@host #  chkconfig httpd on  root@host #  service httpd start", 
            "title": "Configuring apache"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-frontier-squid", 
            "text": "Put the following in  /etc/squid/customize.sh  after the existing comment header:  awk   -- file   ` dirname   $ 0 ` / customhelps . awk   -- source   {  # cache only api calls   insertline( ^http_access deny all ,  acl CVMFSAPI urlpath_regex ^/cvmfs/\\[^/\\]\\*/api/ ) insertline( ^http_access deny all ,  cache deny CVMFSAPI )  # port 80 is also supported, through an iptables redirect   setoption( http_port ,  8000 accel defaultsite=localhost:8080 no-vhost ) setoption( cache_peer ,  localhost parent 8080 0 no-query originserver )  # allow incoming http accesses from anywhere \\# all requests will be forwarded to the originserver   commentout( http_access allow NET_LOCAL ) insertline( ^http_access deny all ,  http_access allow all )  # do not let squid cache DNS entries more than 5 minutes   setoption( positive_dns_ttl ,  5 minutes )  # set shutdown_lifetime to 0 to avoid giving new connections error \\# codes, which get cached upstream   setoption( shutdown_lifetime ,  0 seconds )  # turn off collapsed_forwarding to prevent slow clients from slowing down faster ones  setoption( collapsed_forwarding ,  off )  print }   On an RHEL7 system, make sure that iptables-services is installed and enabled:  root@host #  yum -y install iptables-services  root@host #  systemctl  enable  iptables  Forward port 80 to port 8000 (first command is for external, second command for localhost):  root@host #  iptables -t nat -A PREROUTING -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000   root@host #  iptables -t nat -A OUTPUT -o lo -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000   root@host #  service iptables save  On RHEL7 also set up the the same port forwarding for IPv6 (unfortunately it is not supported on RHEL6):  root@host #  ip6tables -t nat -A PREROUTING -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000  root@host #  ip6tables -t nat -A OUTPUT -o lo -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000  root@host #  service ip6tables save  Enable frontier-squid:  root@host #  chkconfig frontier-squid on root@host #  service frontier-squid start  Note: the above configuration is for a single squid thread, which is fine for 1Gbit/s and possibly 2Gbit/s, but if higher bandwidth is needed, see the  instructions for running multiple squid workers .", 
            "title": "Configuring frontier-squid"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#verifying", 
            "text": "In order to verify that everything is installed correctly, create a repository replica. The repository chosen for the instructions below is one from egi.eu because it is very small, but you can use another one if you prefer.", 
            "title": "Verifying"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#adding-an-example-repository", 
            "text": "The OSG GOC Stratum 1 should add a repository replica using the  add_osg_repository  script from the oasis-2 rpm. Instructions for installing that are elsewhere but you can also  download add_osg_repository . This script assumes that the oasis.opensciencegrid.org replica repository was first created, so this instruction creates it but does not download the first snapshot because that would take a lot of space and time. Use these commands to create the oasis replica and to create and download the example replica:  root@host #  cvmfs_server add-replica -o root http://oasis.opensciencegrid.org:8000/cvmfs/oasis.opensciencegrid.org  /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub  root@host #  add_osg_repository http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu  It's a good idea for other Stratum 1s to make their own scripts for adding repository replicas, because there's always two or three commands to run, and it's easy to forget the commands after the first one. The first command is generically this:  root@host #  cvmfs_server add-replica -o root http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu  /etc/cvmfs/keys/egi.eu/egi.eu.pub  However, non-GOC OSG Stratum 1s (that is, at BNL and FNAL), for the sake of fulfilling an OSG security requirement, need to instead read from the OSG GOC machine with this as their first command:  root@host #  cvmfs_server add-replica -o root http://oasis-replica.opensciencegrid.org:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub:/etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub  The second command for Stratum 1s that have the httpd configuration as described above in the  Configuring apache section  is this:  root@host #  rm -f /etc/httpd/conf.d/cvmfs.config-egi.egi.eu.conf  Then the next command is this:  root@host #  cvmfs_server snapshot config-egi.egi.eu  With large repositories that can take a very long time, but with small repositories it should be very quick and not show any errors.", 
            "title": "Adding an example repository"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#verifying-that-the-replica-is-being-served", 
            "text": "Now to verify that the replication is working, do the following commands:  root@host #  wget -qdO- http://localhost:8000/cvmfs/config-egi.egi.eu/.cvmfspublished%7Ccat -v root@host #  wget -qdO- http://localhost:80/cvmfs/config-egi.egi.eu/.cvmfspublished%7Ccat -v  Both commands should show a short file including gibberish at the end which is the signature.  It is a good idea to familiarize yourself with the log entries at  /var/log/httpd/access_log  and also, if you have installed frontier-squid, at  /var/log/squid/access.log . Also, at least 15 minutes after the snapshot is finished, check the log  /var/log/cvmfs  to see that it tried to get an update and got no errors.", 
            "title": "Verifying that the replica is being served"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#setting-up-monitoring", 
            "text": "If you installed frontier-squid and frontier-awstats, there is a little more to do to configure monitoring.  First, make sure that your firewall accepts UDP queries from the monitoring server at CERN. Details are on  the frontier-squid instructions . Next, choose any random password and put it in  /etc/awstats/password-file . Then tell Dave Dykstra the fully qualified domain name of your machine and the password you chose, and he'll set up the monitoring servers.", 
            "title": "Setting up monitoring"
        }, 
        {
            "location": "/monitoring/rsv-overview/", 
            "text": "Resource and Service Validation (RSV) Service\n\n\nRSV Description\n\n\nThe Resource and Service Validation (RSV) software provides a scalable and easy to maintain resource/service monitoring infrastructure for an OSG site admin.\n\n\nThe components of RSV are:\n\n\n\n\nRSV Client:\n The client allows OSG site administrators to run tests against their CEs/SEs. This provides a set of metrics to test the resource, Condor-Cron for scheduling, and a Gratia infrastructure for collecting and storing the results.\n\n\nThe RSV Client runs metrics at scheduled time intervals. It produces a simple webpage of local RSV results for a site administrator's viewing. It also has the capability to upload those probe results to a central collector (addressed next).\n\n\n\n\n\n\nRSV Collector/Server:\n The server collects results from any number of RSV clients and stores them in a database. You can view these central results on \nthe MyOSG-based RSV current status page\n. Other RSV-based choices are available in the \nResource Group menu\n within MyOSG.\n\n\nPeriodic Availability Reports:\n The availability of all active registered OSG resources and the services running on each of those resources is calculated using the results received for \ncritical metrics\n. Once a day, these availability numbers are \npublished online\n (More information: \nOutline of reports\n\n\nRSV-SAM Transport:\n The WLCG RSV-SAM Transport infrastructure pushes out RSV results (for resources that are flagged to be part of the WLCG Interoperability agreement) from the GOC collector to WLCG's Service Availability Monitoring (SAM) system. More information on \nviewing these results is available here\n.\n\n\nMyOSG and OIM Links:\n RSV picks up resource information, WLCG interoperability information, etc. from a MyOSG resource group summary listing, which is in turn based on the \nOSG Information Management (OIM) (topology) system\n (Requires registration). Resource \nmaintenance scheduled on OIM\n, are forwarded to WLCG SAM, if applicable.\n\n\n\n\n\n\n\n\nInstalling and configuring RSV\n\n\n\n\nInstalling RSV\n - For installing RSV and simple configuration\n\n\nConfiguring RSV\n - Advanced configuration instructions\n\n\nManaging RSV via rsv-control\n - rsv-control is a tool to manage RSV\n\n\n\n\nRSV Troubleshooting\n\n\nTo get assistance, use the \nhelp procedure\n.\n\n\nRSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:\n\n\nroot@host#\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\n\n\nNote\n\n\nIf you are getting assistance via the trouble ticket system, you must add a \n.txt\n extension to the tarball so it can be uploaded:\n\n\nroot@host#\n mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt\n\n\n\n\n\n\n\nResending failed Gratia records\n\n\nIf RSV fails to send Gratia records, it will save a copy of the output into \n/var/spool/rsv/failed-gratia-scripts\n.\nYou will be notified if files are in this directory on your HTML status page.\n\n\nIf files appear here, you can attempt to determine why by looking at this log file: \n/var/log/rsv/consumers/gratia-consumer.output\n.\n(This file is rotated, so the error message may no longer be present.)\n\n\nUsually this error is spurious - there may have been a problem with the central collector being unavailable, or there may have been a network problem.\nThe first step to fix this problem is to try to resend these files.\nTo do so, move them back into the \ngratia\n directory and they will be resent the next time the gratia-script-consumer runs (about every 5 minutes):\n\n\nroot@host#\n mv /var/spool/rsv/failed-gratia-records/* /var/spool/rsv/gratia-consumer/\n\n\n\n\n\nIf that does not solve your problem, follow the \nhelp procedure\n.", 
            "title": "RSV Overview"
        }, 
        {
            "location": "/monitoring/rsv-overview/#resource-and-service-validation-rsv-service", 
            "text": "", 
            "title": "Resource and Service Validation (RSV) Service"
        }, 
        {
            "location": "/monitoring/rsv-overview/#rsv-description", 
            "text": "The Resource and Service Validation (RSV) software provides a scalable and easy to maintain resource/service monitoring infrastructure for an OSG site admin.  The components of RSV are:   RSV Client:  The client allows OSG site administrators to run tests against their CEs/SEs. This provides a set of metrics to test the resource, Condor-Cron for scheduling, and a Gratia infrastructure for collecting and storing the results.  The RSV Client runs metrics at scheduled time intervals. It produces a simple webpage of local RSV results for a site administrator's viewing. It also has the capability to upload those probe results to a central collector (addressed next).    RSV Collector/Server:  The server collects results from any number of RSV clients and stores them in a database. You can view these central results on  the MyOSG-based RSV current status page . Other RSV-based choices are available in the  Resource Group menu  within MyOSG.  Periodic Availability Reports:  The availability of all active registered OSG resources and the services running on each of those resources is calculated using the results received for  critical metrics . Once a day, these availability numbers are  published online  (More information:  Outline of reports  RSV-SAM Transport:  The WLCG RSV-SAM Transport infrastructure pushes out RSV results (for resources that are flagged to be part of the WLCG Interoperability agreement) from the GOC collector to WLCG's Service Availability Monitoring (SAM) system. More information on  viewing these results is available here .  MyOSG and OIM Links:  RSV picks up resource information, WLCG interoperability information, etc. from a MyOSG resource group summary listing, which is in turn based on the  OSG Information Management (OIM) (topology) system  (Requires registration). Resource  maintenance scheduled on OIM , are forwarded to WLCG SAM, if applicable.", 
            "title": "RSV Description"
        }, 
        {
            "location": "/monitoring/rsv-overview/#installing-and-configuring-rsv", 
            "text": "Installing RSV  - For installing RSV and simple configuration  Configuring RSV  - Advanced configuration instructions  Managing RSV via rsv-control  - rsv-control is a tool to manage RSV", 
            "title": "Installing and configuring RSV"
        }, 
        {
            "location": "/monitoring/rsv-overview/#rsv-troubleshooting", 
            "text": "To get assistance, use the  help procedure .  RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:  root@host#  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)    Note  If you are getting assistance via the trouble ticket system, you must add a  .txt  extension to the tarball so it can be uploaded:  root@host#  mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt", 
            "title": "RSV Troubleshooting"
        }, 
        {
            "location": "/monitoring/rsv-overview/#resending-failed-gratia-records", 
            "text": "If RSV fails to send Gratia records, it will save a copy of the output into  /var/spool/rsv/failed-gratia-scripts .\nYou will be notified if files are in this directory on your HTML status page.  If files appear here, you can attempt to determine why by looking at this log file:  /var/log/rsv/consumers/gratia-consumer.output .\n(This file is rotated, so the error message may no longer be present.)  Usually this error is spurious - there may have been a problem with the central collector being unavailable, or there may have been a network problem.\nThe first step to fix this problem is to try to resend these files.\nTo do so, move them back into the  gratia  directory and they will be resent the next time the gratia-script-consumer runs (about every 5 minutes):  root@host#  mv /var/spool/rsv/failed-gratia-records/* /var/spool/rsv/gratia-consumer/  If that does not solve your problem, follow the  help procedure .", 
            "title": "Resending failed Gratia records"
        }, 
        {
            "location": "/monitoring/install-rsv/", 
            "text": "Installing, Configuring, Using, and Troubleshooting RSV\n\n\nAbout This Guide\n\n\nThe Resource and Service Validation (RSV) software helps a site administrator verify that certain site resources and services are working as expected. OSG recommends that sites install and run RSV, but it is optional; further, each site selects which specific tests (called \nprobes\n) to run.\n\n\nUse this page to learn more about RSV in general, and how to install, configure, run, test, and troubleshoot RSV from the OSG software repositories. For documentation on specific probes or on how to write your own probes, please check the \nReference section\n.\n\n\nIntroduction to RSV\n\n\nThe Resource and Service Validation (RSV) software provides OSG site administrators a scalable and easy-to-maintain resource and service monitoring infrastructure. The components of RSV are:\n\n\n\n\nRSV Client.\n The client tools allow a site administrator to run tests against their site by providing a set of tests (which can run on the same or other hosts within a site), HTCondor-Cron for scheduling, and tools for collecting and storing the results (using Gratia). The client package is not installed by default and may be installed on a CE or other host. Generally, you configure the RSV client to run tests at scheduled time intervals and then it makes results available on a local website. Also, the client can upload test results to a central collector (see next item).\n\n\nRSV Collector/Server.\n The central OSG RSV Collector accepts and stores results from RSV clients throughout OSG, which can be viewed in \nMyOSG\n, on the \u201cCurrent RSV Status\u201d page and under the \u201cResource Group\u201d menu.\n\n\nPeriodic Availability Reports.\n The availability of all active registered OSG resources and the services running on each of those resources is calculated using the results received for \ncritical metrics\n. Once a day, these availability numbers are \npublished online\n (More information: \nOutline of reports\n).\n\n\nRSV-SAM Transport.\n The WLCG RSV-SAM Transport infrastructure pushes out RSV results, for resources that are flagged to be part of the WLCG Interoperability agreement, from the GOC collector to WLCG's Service Availability Monitoring (SAM) system. More information on viewing these results is \navailable here\n.\n\n\nMyOSG and OIM Links.\n RSV picks up resource information, WLCG interoperability information, etc., from a MyOSG resource group summary listing, which is in turn based on the \nOSG Information Management (OIM) (topology) system\n (Requires registration). Resource \nmaintenance scheduled on OIM\n are forwarded to WLCG SAM, if applicable.\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux user IDs \nrsv\n and \ncndrcron\n\n\nService certificate:\n The RSV service requires a service certificate (\n/etc/grid-security/rsv/rsvcert.pem\n) and matching key (\n/etc/grid-security/rsv/rsvkey.pem\n)\n\n\nNetwork ports:\n To view results, port 80 must accept incoming requests; outbound connectivity to tested services must work, too\n\n\nHost choice:\n Install RSV on your site CE unless you have specific reasons (e.g., performance) for installing on a separate host\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the RSV host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling RSV\n\n\nAn installation of RSV at a site consists of the RSV client software, the Apache web server, parts of HTCondor (for its cron-like scheduling capabilities), and various other small tools. To simplify installation, OSG provides a convenience RPM that installs all required software with a single command.\n\n\n\n\n\n\nConsider updating your local cache of Yum repository data and your existing RPM packages:\n\n\n[root@client ~] #\n yum clean all --enablerepo\n=\n\\*\n\n\n[root@client ~] #\n yum update\n\n\n\n\n\n\n\nNote\n\n\nThe \nupdate\n command will update \nall\n packages on your system.\n\n\n\n\n\n\n\n\nIf you have installed HTCondor already but not by RPM, install a special empty RPM to make RSV happy:\n\n\n[root@client ~] #\n yum install empty-condor --enablerepo\n=\nosg-empty\n\n\n\n\n\n\n\n\n\nInstall RSV and related software:\n\n\n[root@client ~] #\n yum install rsv\n\n\n\n\n\n\n\n\n\nConfiguring RSV\n\n\nAfter installation, there are some one-time configuration steps to tell RSV how to operate at your site.\n\n\n\n\n\n\nEdit \n/etc/osg/config.d/30-rsv.ini\n and follow the instructions in the file. There are detailed comments for each setting. In the simplest case \u2014 to monitor only your CE \u2014 set the \nhtcondor_ce_hosts\n variable to the fully qualified hostname of your CE.\n\n\n\n\n\n\nIf you have installed HTCondor already but not by RPM, specify the location of the Condor installation in \n30-rsv.ini\n in the \ncondor_location\n setting. If an HTCondor RPM is installed, you do not need to set \ncondor_location\n.\n\n\n\n\n\n\nComplete the configuration using the \nosg-configure\n tool:\n\n\n[root@client ~] #\n osg-configure -v\n\n[root@client ~] #\n osg-configure -c\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small or typical site. If you do not need any of the following special configurations, skip to \nthe section on using RSV\n.\n\n\nGenerally speaking, read the \nConfigureRsv\n page for more advanced configuration options.\n\n\nConfiguring RSV to run probes using a remote server\n\n\nRSV monitors systems by running probes, which can run on the RSV host itself (the default case), via a separate batch system like HTCondor, or via a remote batch system using a Globus gatekeeper and its job manager. The last two options both can count those jobs and report them to, for example, Gratia.\n\n\nIn this case, remember to:\n\n\n\n\nAdd the RSV user \nrsv\n on all the systems where the probes may run, and\n\n\nMap the RSV service certificate to the user you intend to use for RSV. This should be a local user used exclusively for RSV and not belonging to an institutional VO to avoid for the RSV probes to be accounted as regular VO jobs in Gratia. This can be done in \nGUMS\n or \nusing a grid-mapfile-local\n (if you use a grid-mapfile). \nMapServiceCertToRsvUser\n explains how to configure GUMS or the grid-mapfile. Also see the \nCE installation document\n for more information.\n\n\n\n\nConfiguring the RSV web server to use HTTPS instead of HTTP\n\n\nIf you would like your local RSV web server to use HTTPS instead of the default HTTP (for compatibility or security reasons), complete the steps below. This procedure assumes that you already have an HTTP service certificate (or a copy of the host certificate) in \n/etc/grid-security/http/\n. If not, omit the \nSSLCertificate*\n modifications below, and your web server will start with its own, self-signed certificate.\n\n\n\n\n\n\nInstall \nmod_ssl\n:\n\n\n[root@client ~] #\n yum install mod_ssl\n\n\n\n\n\n\n\n\n\nMake an alternate set of HTTP service certificate files:\n\n\n[root@client ~] #\n cp -p /etc/grid-security/http/httpcert.pem /etc/grid-security/http/httpcert2.pem\n\n[root@client ~] #\n cp -p /etc/grid-security/http/httpkey.pem /etc/grid-security/http/httpkey2.pem\n\n[root@client ~] #\n chown apache:apache /etc/grid-security/http/http*2.pem\n\n\n\n\n\n\n\n\n\nBack up existing Apache configuration files:\n\n\n[root@client ~] #\n cp -p /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.orig\n\n[root@client ~] #\n cp -p /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.orig\n\n\n\n\n\n\n\n\n\nChange the default port for HTTP connections to 8000 by editing \n/etc/httpd/conf/httpd.conf\n\n\nListen 8000\n\n\n\n\n\n\n\n\n\nSet up HTTPS access by editing \n/etc/httpd/conf.d/ssl.conf\n:\n\n\nListen 8443\n\nVirtualHost _default_:8443\n\nSSLCertificateFile /etc/grid-security/http/httpcert2.pem\nSSLCertificateKeyFile /etc/grid-security/http/httpkey2.pem\n\n\n\n\n\nAfter these changes, when you start the Apache service, it will listening on ports \n8000\n (for HTTP) and \n8443\n (for HTTPS), rather than the default port \n80\n (for HTTP only).\n\n\n\n\nWarning\n\n\nif you make the changes above, you must restart the Apache server after each CA certificate update to pick up the changes.\n\n\n\n\n\n\n\n\nUsing RSV\n\n\nManaging RSV and associated services\n\n\nIn addition to the RSV service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n\n\n\n\n\n\nApache\n\n\nhttpd\n\n\n\n\n\n\n\n\nHTCondor-Cron\n\n\ncondor-cron\n\n\n\n\n\n\n\n\nRSV\n\n\nrsv\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\n\n\n\n\nStop a service\n\n\nservice \nSERVICE-NAME\n stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\n\n\n\n\n\n\nRunning RSV manually\n\n\nNormally, the HTCondor-Cron scheduler runs RSV periodically. However, you can run RSV probes manually at any time:\n\n\n[root@client ~] #\n rsv-control --run --all-enabled\n\n\n\n\n\nIf successful, results will be available from your local RSV web server (e.g., \nhttp://localhost/rsv\n) and, if enabled (which is the default) on \nMyOSG\n.\n\n\nYou can also run the metrics individually or pass special parameters as explained in the \nrsv-control document\n.\n\n\nTroubleshooting RSV\n\n\nYou can find more information on troubleshooting RSV in the \nrsv-control documentation\n and in \nTroubleshootRSV\n.\n\n\nImportant file locations\n\n\nLogs and configuration:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric log files\n\n\n/var/log/rsv/metrics\n\n\n\n\n\n\n\n\nConsumer log files\n\n\n/var/log/rsv/consumers\n\n\n\n\n\n\n\n\nHTML files\n\n\n/usr/share/rsv/www/\n\n\nAvailable at \nhttp://your.host.example.com/rsv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nInitial configuration\n\n\n/etc/osg/config.d/30-rsv.ini\n\n\nRead by \nosg-configure\n\n\n\n\n\n\nRSV configuration\n\n\n/etc/rsv\n\n\nGenerally files in this directory should not be edited directly. Use \nosg-configure\n instead.\n\n\n\n\n\n\nMetric configuration\n\n\n/etc/rsv/metrics/HOSTNAME/METRICNAME.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\n\n\nTo find the metrics and the other files in RSV you can use also the RPM commands: \nrpm -ql rsv-metrics\n and \nrpm -ql rsv\n.\n\n\nGetting more information from rsv-control\n\n\nThe first step to getting more information is to run rsv-control with more verbosity. Use the \n--verbose\n (\n-v\n) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:\n\n\n\n\n0 = print nothing\n\n\n1 = print warnings and errors along with usual output of command being run (1 is the default level)\n\n\n2 = adds informational messages\n\n\n3 = full debugging output\n\n\n\n\nFor example, here is the output when running a metric with -v2.\n\n\n\n  \nShow detailed ouput\n\n   \n\n\n   [root@fermicloud016 condor]# rsv-control -r org.osg.general.osg-version -v 2 -u osg-edu.cs.wisc.edu\n\n\n   INFO: Reading configuration file /etc/rsv/rsv.conf\n\n\n   INFO: Reading configuration file /etc/rsv/consumers.conf\n\n\n   INFO: Validating configuration:\n\n\n   INFO: Validating user:\n\n\nINFO:     Invoked as root.  Switching to \nrsv\n user (uid: 100 - gid: 102)\n\n\nINFO: Registered consumers: html-consumer, gratia-consumer\n\n\nINFO: Loading config file \n/etc/rsv/meta/metrics/org.osg.general.osg-version.meta\n\n\nINFO: Loading config file \n/etc/rsv/metrics/org.osg.general.osg-version.conf\n\n\nINFO: Optional config file \n/etc/rsv/metrics/osg-edu.cs.wisc.edu/org.osg.general.osg-version.conf\n does not exist\n\n\nINFO: Checking proxy:\n\n\nINFO:     Using service certificate proxy\n\n\nINFO: Running command with timeout (1200 seconds):\n\n\n        /usr/bin/openssl x509 -in /tmp/rsvproxy -noout -enddate -checkend 21600\n\n\nINFO: Exit code of job: 0\n\n\nINFO:     Service certificate valid for at least 6 hours.\n\n\nINFO: Pinging host osg-edu.cs.wisc.edu:\n\n\nINFO: Running command with timeout (1200 seconds):\n\n\n        /bin/ping -W 3 -c 1 osg-edu.cs.wisc.edu\n\n\nINFO: Exit code of job: 0\n\n\nINFO:     Ping successful\n\n\n\nRunning metric org.osg.general.osg-version:\n\n\n\nINFO: Executing job remotely using Condor-G\n\n\nINFO: Setting up job environment:\n\n\nINFO:     No environment setup declared\n\n\nINFO: Condor-G working directory: /var/tmp/rsv/condor_g-JiQthF\n\n\nINFO: Forming arguments:\n\n\nINFO:     Arguments: \n\n\nINFO: List of files to transfer: /usr/libexec/rsv/probes/RSVMetric.pm\n\n\nINFO: Condor submission: Submitting job(s).\n\n\n1 job(s) submitted to cluster 2.\n\n\nINFO: Trimming data to 10000 bytes because details-data-trim-length is set\n\n\nINFO: Creating record for html-consumer consumer at \n/var/spool/rsv/html-consumer/org.osg.general.osg-version.7rgLfn\n\n\nINFO: Creating record for gratia-consumer consumer at \n/var/spool/rsv/gratia-consumer/org.osg.general.osg-version.-qelnL\n\n\nINFO: Result:\n\n\n\nmetricName: org.osg.general.osg-version\n\n\nmetricType: status\n\n\ntimestamp: 2012-01-25 16:12:40 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: fermicloud016.fnal.gov\n\n\nsummaryData: OK\n\n\ndetailsData: OSG 1.2.26\n\n\n\nEOT\n\n\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use \nthis page\n.\n\n\n[root@client ~] #\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\nReference\n\n\nHere are some other RSV documents that might be helpful:\n\n\n\n\nA longer \nintroduction to RSV\n\n\nThe RSV architecture\n\n\nRSV storage probes\n\n\n\n\nUsers\n\n\nThe RSV installation will create two users unless they are already created. The users are created when the \nrsv\n and \ncondor-cron\n packages are installed.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nrsv\n\n\nRuns the RSV tests; the RSV certificate (below) will need to be owned by this user\n\n\n\n\n\n\ncndrcron\n\n\nRuns the Condor Cron processes to schedule the running of the tests\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nif you pre-create the RSV user, it should have a working shell. That is, it shouldn't have a default shell of \n/sbin/nologin\n.\n\n\n\n\n\n\nWarning\n\n\nIf you manage your \n/etc/passwd\n file with configuration management software such as Puppet, CFEngine or 411, make sure the UID and GID in \n/etc/condor-cron/config.d/condor_ids\n matches the UID and GID of the \ncndrcron\n user and group in \n/etc/passwd\n. If it does not, create a file named \n/etc/condor-cron/config.d/condor_ids_override\n with the contents:\n\n\n\n\nCONDOR_IDS=UID.GID\n\n\n\n\n\nwhere \nUID\n and \nGID\n are the UID and GID of the \ncndrcron\n user and group.\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nRSV service certificate\n\n\nrsv\n\n\n/etc/grid-security/rsv/rsvcert.pem\n\n\n\n\n\n\n\n\n\n\n/etc/grid-security/rsv/rsvkey.pem\n\n\n\n\n\n\n\n\nEnsure an RSV service certificate is installed in \n/etc/grid-security/rsv/\n and the certificate files are owned by the \nrsv\n user. Adjust the permissions if necessary (cert needs to be readable by all, key needs to be readable by nobody but owner).\n\n\nYou may need another certificate owned by \napache\n if you'd like an authenticated web server; see \nConfiguring the RSV web server to use HTTPS instead of HTTP\n above.\n\n\nSee \ninstructions\n to request a service certificate.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHTTP\n\n\ntcp\n\n\n80\n\n\nYES\n\n\n\n\nRSV runs an HTTP server (Apache) that publishes a page with the RSV testing results\n\n\n\n\n\n\nHTTP\n\n\ntcp\n\n\n80\n\n\n\n\nYES\n\n\nRSV pushes testing results to the OSG Gratia Collectors at opensciencegrid.org\n\n\n\n\n\n\nvarious\n\n\nvarious\n\n\nvarious\n\n\n\n\nYES\n\n\nAllow outbound network connection to all services that you want to test\n\n\n\n\n\n\n\n\nOr, if you'd rather have your RSV web page appear as \nhttps\n://...:8443/rsv/\n like it used to in OSG 1.2, the first column above would be \nHTTPS\n / \ntcp\n / \n8443\n. See \nabove\n for how to configure this.", 
            "title": "Install RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#installing-configuring-using-and-troubleshooting-rsv", 
            "text": "", 
            "title": "Installing, Configuring, Using, and Troubleshooting RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#about-this-guide", 
            "text": "The Resource and Service Validation (RSV) software helps a site administrator verify that certain site resources and services are working as expected. OSG recommends that sites install and run RSV, but it is optional; further, each site selects which specific tests (called  probes ) to run.  Use this page to learn more about RSV in general, and how to install, configure, run, test, and troubleshoot RSV from the OSG software repositories. For documentation on specific probes or on how to write your own probes, please check the  Reference section .", 
            "title": "About This Guide"
        }, 
        {
            "location": "/monitoring/install-rsv/#introduction-to-rsv", 
            "text": "The Resource and Service Validation (RSV) software provides OSG site administrators a scalable and easy-to-maintain resource and service monitoring infrastructure. The components of RSV are:   RSV Client.  The client tools allow a site administrator to run tests against their site by providing a set of tests (which can run on the same or other hosts within a site), HTCondor-Cron for scheduling, and tools for collecting and storing the results (using Gratia). The client package is not installed by default and may be installed on a CE or other host. Generally, you configure the RSV client to run tests at scheduled time intervals and then it makes results available on a local website. Also, the client can upload test results to a central collector (see next item).  RSV Collector/Server.  The central OSG RSV Collector accepts and stores results from RSV clients throughout OSG, which can be viewed in  MyOSG , on the \u201cCurrent RSV Status\u201d page and under the \u201cResource Group\u201d menu.  Periodic Availability Reports.  The availability of all active registered OSG resources and the services running on each of those resources is calculated using the results received for  critical metrics . Once a day, these availability numbers are  published online  (More information:  Outline of reports ).  RSV-SAM Transport.  The WLCG RSV-SAM Transport infrastructure pushes out RSV results, for resources that are flagged to be part of the WLCG Interoperability agreement, from the GOC collector to WLCG's Service Availability Monitoring (SAM) system. More information on viewing these results is  available here .  MyOSG and OIM Links.  RSV picks up resource information, WLCG interoperability information, etc., from a MyOSG resource group summary listing, which is in turn based on the  OSG Information Management (OIM) (topology) system  (Requires registration). Resource  maintenance scheduled on OIM  are forwarded to WLCG SAM, if applicable.", 
            "title": "Introduction to RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux user IDs  rsv  and  cndrcron  Service certificate:  The RSV service requires a service certificate ( /etc/grid-security/rsv/rsvcert.pem ) and matching key ( /etc/grid-security/rsv/rsvkey.pem )  Network ports:  To view results, port 80 must accept incoming requests; outbound connectivity to tested services must work, too  Host choice:  Install RSV on your site CE unless you have specific reasons (e.g., performance) for installing on a separate host   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the RSV host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/monitoring/install-rsv/#installing-rsv", 
            "text": "An installation of RSV at a site consists of the RSV client software, the Apache web server, parts of HTCondor (for its cron-like scheduling capabilities), and various other small tools. To simplify installation, OSG provides a convenience RPM that installs all required software with a single command.    Consider updating your local cache of Yum repository data and your existing RPM packages:  [root@client ~] #  yum clean all --enablerepo = \\*  [root@client ~] #  yum update   Note  The  update  command will update  all  packages on your system.     If you have installed HTCondor already but not by RPM, install a special empty RPM to make RSV happy:  [root@client ~] #  yum install empty-condor --enablerepo = osg-empty    Install RSV and related software:  [root@client ~] #  yum install rsv", 
            "title": "Installing RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-rsv", 
            "text": "After installation, there are some one-time configuration steps to tell RSV how to operate at your site.    Edit  /etc/osg/config.d/30-rsv.ini  and follow the instructions in the file. There are detailed comments for each setting. In the simplest case \u2014 to monitor only your CE \u2014 set the  htcondor_ce_hosts  variable to the fully qualified hostname of your CE.    If you have installed HTCondor already but not by RPM, specify the location of the Condor installation in  30-rsv.ini  in the  condor_location  setting. If an HTCondor RPM is installed, you do not need to set  condor_location .    Complete the configuration using the  osg-configure  tool:  [root@client ~] #  osg-configure -v [root@client ~] #  osg-configure -c", 
            "title": "Configuring RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small or typical site. If you do not need any of the following special configurations, skip to  the section on using RSV .  Generally speaking, read the  ConfigureRsv  page for more advanced configuration options.", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-rsv-to-run-probes-using-a-remote-server", 
            "text": "RSV monitors systems by running probes, which can run on the RSV host itself (the default case), via a separate batch system like HTCondor, or via a remote batch system using a Globus gatekeeper and its job manager. The last two options both can count those jobs and report them to, for example, Gratia.  In this case, remember to:   Add the RSV user  rsv  on all the systems where the probes may run, and  Map the RSV service certificate to the user you intend to use for RSV. This should be a local user used exclusively for RSV and not belonging to an institutional VO to avoid for the RSV probes to be accounted as regular VO jobs in Gratia. This can be done in  GUMS  or  using a grid-mapfile-local  (if you use a grid-mapfile).  MapServiceCertToRsvUser  explains how to configure GUMS or the grid-mapfile. Also see the  CE installation document  for more information.", 
            "title": "Configuring RSV to run probes using a remote server"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-the-rsv-web-server-to-use-https-instead-of-http", 
            "text": "If you would like your local RSV web server to use HTTPS instead of the default HTTP (for compatibility or security reasons), complete the steps below. This procedure assumes that you already have an HTTP service certificate (or a copy of the host certificate) in  /etc/grid-security/http/ . If not, omit the  SSLCertificate*  modifications below, and your web server will start with its own, self-signed certificate.    Install  mod_ssl :  [root@client ~] #  yum install mod_ssl    Make an alternate set of HTTP service certificate files:  [root@client ~] #  cp -p /etc/grid-security/http/httpcert.pem /etc/grid-security/http/httpcert2.pem [root@client ~] #  cp -p /etc/grid-security/http/httpkey.pem /etc/grid-security/http/httpkey2.pem [root@client ~] #  chown apache:apache /etc/grid-security/http/http*2.pem    Back up existing Apache configuration files:  [root@client ~] #  cp -p /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.orig [root@client ~] #  cp -p /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.orig    Change the default port for HTTP connections to 8000 by editing  /etc/httpd/conf/httpd.conf  Listen 8000    Set up HTTPS access by editing  /etc/httpd/conf.d/ssl.conf :  Listen 8443 VirtualHost _default_:8443 \nSSLCertificateFile /etc/grid-security/http/httpcert2.pem\nSSLCertificateKeyFile /etc/grid-security/http/httpkey2.pem  After these changes, when you start the Apache service, it will listening on ports  8000  (for HTTP) and  8443  (for HTTPS), rather than the default port  80  (for HTTP only).   Warning  if you make the changes above, you must restart the Apache server after each CA certificate update to pick up the changes.", 
            "title": "Configuring the RSV web server to use HTTPS instead of HTTP"
        }, 
        {
            "location": "/monitoring/install-rsv/#using-rsv", 
            "text": "", 
            "title": "Using RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#managing-rsv-and-associated-services", 
            "text": "In addition to the RSV service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation    Apache  httpd     HTCondor-Cron  condor-cron     RSV  rsv      Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service  SERVICE-NAME  start    Stop a service  service  SERVICE-NAME  stop    Enable a service to start during boot  chkconfig  SERVICE-NAME  on    Disable a service from starting during boot  chkconfig  SERVICE-NAME  off", 
            "title": "Managing RSV and associated services"
        }, 
        {
            "location": "/monitoring/install-rsv/#running-rsv-manually", 
            "text": "Normally, the HTCondor-Cron scheduler runs RSV periodically. However, you can run RSV probes manually at any time:  [root@client ~] #  rsv-control --run --all-enabled  If successful, results will be available from your local RSV web server (e.g.,  http://localhost/rsv ) and, if enabled (which is the default) on  MyOSG .  You can also run the metrics individually or pass special parameters as explained in the  rsv-control document .", 
            "title": "Running RSV manually"
        }, 
        {
            "location": "/monitoring/install-rsv/#troubleshooting-rsv", 
            "text": "You can find more information on troubleshooting RSV in the  rsv-control documentation  and in  TroubleshootRSV .", 
            "title": "Troubleshooting RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#important-file-locations", 
            "text": "Logs and configuration:     File Description  Location  Comment      Metric log files  /var/log/rsv/metrics     Consumer log files  /var/log/rsv/consumers     HTML files  /usr/share/rsv/www/  Available at  http://your.host.example.com/rsv        File Description  Location  Comment      Initial configuration  /etc/osg/config.d/30-rsv.ini  Read by  osg-configure    RSV configuration  /etc/rsv  Generally files in this directory should not be edited directly. Use  osg-configure  instead.    Metric configuration  /etc/rsv/metrics/HOSTNAME/METRICNAME.conf  To change arguments and environment     To find the metrics and the other files in RSV you can use also the RPM commands:  rpm -ql rsv-metrics  and  rpm -ql rsv .", 
            "title": "Important file locations"
        }, 
        {
            "location": "/monitoring/install-rsv/#getting-more-information-from-rsv-control", 
            "text": "The first step to getting more information is to run rsv-control with more verbosity. Use the  --verbose  ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:   0 = print nothing  1 = print warnings and errors along with usual output of command being run (1 is the default level)  2 = adds informational messages  3 = full debugging output   For example, here is the output when running a metric with -v2.  \n   Show detailed ouput \n        [root@fermicloud016 condor]# rsv-control -r org.osg.general.osg-version -v 2 -u osg-edu.cs.wisc.edu     INFO: Reading configuration file /etc/rsv/rsv.conf     INFO: Reading configuration file /etc/rsv/consumers.conf     INFO: Validating configuration:     INFO: Validating user:  INFO:     Invoked as root.  Switching to  rsv  user (uid: 100 - gid: 102)  INFO: Registered consumers: html-consumer, gratia-consumer  INFO: Loading config file  /etc/rsv/meta/metrics/org.osg.general.osg-version.meta  INFO: Loading config file  /etc/rsv/metrics/org.osg.general.osg-version.conf  INFO: Optional config file  /etc/rsv/metrics/osg-edu.cs.wisc.edu/org.osg.general.osg-version.conf  does not exist  INFO: Checking proxy:  INFO:     Using service certificate proxy  INFO: Running command with timeout (1200 seconds):          /usr/bin/openssl x509 -in /tmp/rsvproxy -noout -enddate -checkend 21600  INFO: Exit code of job: 0  INFO:     Service certificate valid for at least 6 hours.  INFO: Pinging host osg-edu.cs.wisc.edu:  INFO: Running command with timeout (1200 seconds):          /bin/ping -W 3 -c 1 osg-edu.cs.wisc.edu  INFO: Exit code of job: 0  INFO:     Ping successful  Running metric org.osg.general.osg-version:  INFO: Executing job remotely using Condor-G  INFO: Setting up job environment:  INFO:     No environment setup declared  INFO: Condor-G working directory: /var/tmp/rsv/condor_g-JiQthF  INFO: Forming arguments:  INFO:     Arguments:   INFO: List of files to transfer: /usr/libexec/rsv/probes/RSVMetric.pm  INFO: Condor submission: Submitting job(s).  1 job(s) submitted to cluster 2.  INFO: Trimming data to 10000 bytes because details-data-trim-length is set  INFO: Creating record for html-consumer consumer at  /var/spool/rsv/html-consumer/org.osg.general.osg-version.7rgLfn  INFO: Creating record for gratia-consumer consumer at  /var/spool/rsv/gratia-consumer/org.osg.general.osg-version.-qelnL  INFO: Result:  metricName: org.osg.general.osg-version  metricType: status  timestamp: 2012-01-25 16:12:40 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: fermicloud016.fnal.gov  summaryData: OK  detailsData: OSG 1.2.26  EOT", 
            "title": "Getting more information from rsv-control"
        }, 
        {
            "location": "/monitoring/install-rsv/#getting-help", 
            "text": "To get assistance, please use  this page .  [root@client ~] #  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)", 
            "title": "Getting Help"
        }, 
        {
            "location": "/monitoring/install-rsv/#reference", 
            "text": "Here are some other RSV documents that might be helpful:   A longer  introduction to RSV  The RSV architecture  RSV storage probes", 
            "title": "Reference"
        }, 
        {
            "location": "/monitoring/install-rsv/#users", 
            "text": "The RSV installation will create two users unless they are already created. The users are created when the  rsv  and  condor-cron  packages are installed.     User  Comment      rsv  Runs the RSV tests; the RSV certificate (below) will need to be owned by this user    cndrcron  Runs the Condor Cron processes to schedule the running of the tests      Note  if you pre-create the RSV user, it should have a working shell. That is, it shouldn't have a default shell of  /sbin/nologin .    Warning  If you manage your  /etc/passwd  file with configuration management software such as Puppet, CFEngine or 411, make sure the UID and GID in  /etc/condor-cron/config.d/condor_ids  matches the UID and GID of the  cndrcron  user and group in  /etc/passwd . If it does not, create a file named  /etc/condor-cron/config.d/condor_ids_override  with the contents:   CONDOR_IDS=UID.GID  where  UID  and  GID  are the UID and GID of the  cndrcron  user and group.", 
            "title": "Users"
        }, 
        {
            "location": "/monitoring/install-rsv/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      RSV service certificate  rsv  /etc/grid-security/rsv/rsvcert.pem      /etc/grid-security/rsv/rsvkey.pem     Ensure an RSV service certificate is installed in  /etc/grid-security/rsv/  and the certificate files are owned by the  rsv  user. Adjust the permissions if necessary (cert needs to be readable by all, key needs to be readable by nobody but owner).  You may need another certificate owned by  apache  if you'd like an authenticated web server; see  Configuring the RSV web server to use HTTPS instead of HTTP  above.  See  instructions  to request a service certificate.", 
            "title": "Certificates"
        }, 
        {
            "location": "/monitoring/install-rsv/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      HTTP  tcp  80  YES   RSV runs an HTTP server (Apache) that publishes a page with the RSV testing results    HTTP  tcp  80   YES  RSV pushes testing results to the OSG Gratia Collectors at opensciencegrid.org    various  various  various   YES  Allow outbound network connection to all services that you want to test     Or, if you'd rather have your RSV web page appear as  https ://...:8443/rsv/  like it used to in OSG 1.2, the first column above would be  HTTPS  /  tcp  /  8443 . See  above  for how to configure this.", 
            "title": "Networking"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/", 
            "text": "Advanced RSV Configuration\n\n\nAbout This Document\n\n\nMost site administrators will be able to configure RSV by editing \n/etc/osg/config.d/30-rsv.ini\n and running osg-configure as described in \nthe RSV installation document\n.  This document provides instructions for configuration beyond what osg-configure is able to do.\n\n\nConfiguring metrics\n\n\nIf you need to change the behavior of a metric you can edit the metric configuration files. These replace the spec files from previous versions of RSV.\n\n\n\n\n/etc/rsv/metrics\n - changes made to conf file in this directory named after a metric will affect the metric when run against all hosts\n\n\n/etc/rsv/metrics/\nHOST\n - changes made to conf files in this directory (named as the host FQDN) will affect the metric when run against the specific host\n\n\n\n\nThe configuration files are in INI format and have two sections:\n\n\n\n\na first one named after the metric with execution options\n\n\na second one with the name including the \"args\" keyword, including parameters sent to the probe at invokation\n\n\n\n\nChanging the times a metric runs\n\n\nTo change the time a metric runs set the \ncron-interval\n setting in the metric's conf file. Use \nman 5 crontab\n for a description of the format. For example, to change the \norg.osg.general.ping-host\n to run at a different time:\n\n\n[org.osg.general.ping-host]\n\n\ncron-interval\n \n=\n \n45 * * * *\n\n\n\n[org.osg.general.ping-host args]\n\n\n#ping-count =\n\n\n#ping-timeout =\n\n\n\n\n\n\n\n\nNote\n\n\nBe sure to put the \ncron-interval\n setting in the \n[org.osg.general.ping-host]\n section, and not the \n[org.osg.general.ping-host args]\n section! The purpose of the \"args\" section is described in the \"passing extra parameters to a metric\" section below.\n\n\n\n\nAfter modifying the cron time of a metric you must restart RSV for the change to take effect.\n\n\nTo see what times each of the metrics is running you can use \nrsv-control\n as follows:\n\n\nroot@host#\n rsv-control -l --cron-times\n\n\nMetrics enabled for host: osg-edu.cs.wisc.edu:10443 | Cron times\n\n\n----------------------------------------------------+--------------------\n\n\norg.osg.srm.srmcp-readwrite                         | 28 * * * *\n\n\norg.osg.srm.srmping                                 | 13,33,53 * * * *\n\n\n...\n\n\n\n\n\n\nPassing extra parameters to a metric\n\n\nAny \nkey=value\n pairs in the \"args\" section of the metric's \nconf\n file will be turned into command line parameters to the probe. For example, for this file:\n\n\n[org.osg.certificates.cacert-expiry args]\n\n\nwarning-hours\n \n=\n \n6\n\n\nerror-hours\n \n=\n \n12\n\n\n\n\n\n\nThis would lead to the probe getting called with the command-line parameters \n--warning-hours 6 --error-hours 12\n.\n\n\nConfigure consumers\n\n\nThere is a configuration file common to all consumers: \n/etc/rsv/consumers.conf\n. It is a file in INI format and possible entries are:\n\n\n\n\n\n\n\n\nSetting\n\n\nValues\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nconsumers\n\n\nComma-separated list of consumers to be enabled\n\n\n\n\n\n\ntimestamp\n\n\nlocal\n\n\nIf this is set to local, a record with a local timestamp will be supplied to the consumer. If this is set to any other value, or is not set, a record with the GMT will be created.\n\n\n\n\n\n\n\n\nEach consumer has a configuration file in \n/etc/rsv/consumers\n named after it. This allows to specify command lines and environment for the consumers. Some consumers may have also their own configuration file, usually in \n/etc/rsv/\n. Below is an example for the Nagios consumer.\n\n\nSending RSV records to Nagios\n\n\n\n\nEdit your \n/etc/rsv/rsv-nagios.conf\n file and fill in the appropriate information. The path of the configuration file is specified in \n/etc/rsv/consumers/nagios-consumer.conf\n.\n\n\n\n\nIf your Nagios config file contains password information you will want to lock down the permissions. Here is a suggested way to do this (replace \nrsvuser\n with the group of your RSV user (\nrsvuser\n by default)):\n\n\nroot@host#\n chown root:\nrsvuser\n /etc/rsv/rsv-nagios.conf\n\nroot@host#\n chmod \n0440\n /etc/rsv/rsv-nagios.conf\n\n\n\n\n\n\n\n\n\nIn the configuration file at \n/etc/rsv/consumers/nagios-consumer.conf\n, check the following two settings:\n\n\n\n\nMake sure that the path to your config file is correct. It may be referencing a directory \nconfig\n instead of \netc\n\n\nIf you want to use \nrsv2nsca\n add the string \"--send-nsca\" to the \nargs\n line.\n\n\n\n\n\n\n\n\nEnable and start the Nagios consumer by editing \nconsumers.conf\n or by using \nrsv-control\n as follows:\n\n\nroot@host#\n rsv-control --enable nagios-consumer\n\n\n\n\n\nThe Nagios consumer will be started the next time that you start RSV. If you are already running RSV you can turn on the Nagios consumer immediately by running:\n\n\nroot@host#\n rsv-control --on nagios-consumer\n\n\n\n\n\n\n\n\n\nTo verify that the Nagios consumer is running you can run \nrsv-control -j\n.\n\n\n\n\nThe log information for the Nagios consumer can be found in these files:\n\n\n/var/log/rsv/consumers/nagios-consumer.log\n\n\n/var/log/rsv/consumers/nagios-consumer.out\n\n\n/var/log/rsv/consumers/nagios-consumer.err\n\n\n\n\n\n\n\n\nGeneral RSV configuration options\n\n\nYou can configure the RSV framework using \n/etc/rsv/rsv.conf\n. It is a file in INI format and possible entries are:\n\n\n\n\n\n\n\n\nSetting\n\n\nValues\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nuser\n\n\nusername\n\n\nThe UNIX username that owns RSV. This is mandatory\n\n\n\n\n\n\nservice-cert\n\n\npath\n\n\nAbsolute path to the service certificate file. If this is set service-key and service-proxy must also be set.\n\n\n\n\n\n\nservice-key\n\n\npath\n\n\nAbsolute path to the service key file. This must be used with service-cert.\n\n\n\n\n\n\nservice-proxy\n\n\npath\n\n\nAbsolute path where the service proxy will be generated. This must be used with service-cert.\n\n\n\n\n\n\nproxy-file\n\n\npath\n\n\nAlternative to service-cert. The absolute path where the user proxy file is located. This will not be auto-regenerated.\n\n\n\n\n\n\ndetails-data-trim-length\n\n\ninteger\n\n\nThe number of bytes to trim the detailsData section to. If set to 0 no trimming will occur.\n\n\n\n\n\n\njob-timeout\n\n\ninteger\n\n\nTime in seconds before a metric is killed. A metric that times out will return a CRITICAL status.\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nImportant files locations\n\n\nConfiguration files:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nRSV configuration directory\n\n\n/etc/rsv\n\n\n\n\n\n\n\n\nRSV configuration\n\n\n/etc/rsv/rsv.conf\n\n\nRSV framework configuration\n\n\n\n\n\n\nConsumers configuration in RSV\n\n\n/etc/rsv/consumers.conf\n\n\nSelect the consumers and change generic options\n\n\n\n\n\n\nConsumers configuration\n\n\n/etc/rsv/consumers/\nCONSUMERNAME\n\n\nTo change arguments and environment\n\n\n\n\n\n\nGeneric metrics configuration\n\n\n/etc/rsv/metrics/\nMETRICNAME\n.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\nHost specific metrics configuration\n\n\n/etc/rsv/metrics/\nHOSTNAME\n/\nMETRICNAME\n.conf\n\n\nTo change arguments and environment when running on HOSTNAME\n\n\n\n\n\n\n\n\nOther files:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric log files\n\n\n/var/log/rsv/metrics\n\n\n\n\n\n\n\n\nConsumer log files\n\n\n/var/log/rsv/consumers\n\n\n\n\n\n\n\n\nInitial configuration\n\n\n/etc/osg/config.d/30-rsv.ini\n\n\nRead by \nosg-configure\n\n\n\n\n\n\nWeb files output\n\n\n/usr/share/rsv/www/\n\n\n\n\n\n\n\n\n\n\nTo find the metrics and the other files in RSV you can use also the RPM commands: \nrpm -ql rsv-metrics\n and \nrpm -ql rsv\n.", 
            "title": "Advanced RSV Configuration"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#advanced-rsv-configuration", 
            "text": "", 
            "title": "Advanced RSV Configuration"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#about-this-document", 
            "text": "Most site administrators will be able to configure RSV by editing  /etc/osg/config.d/30-rsv.ini  and running osg-configure as described in  the RSV installation document .  This document provides instructions for configuration beyond what osg-configure is able to do.", 
            "title": "About This Document"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#configuring-metrics", 
            "text": "If you need to change the behavior of a metric you can edit the metric configuration files. These replace the spec files from previous versions of RSV.   /etc/rsv/metrics  - changes made to conf file in this directory named after a metric will affect the metric when run against all hosts  /etc/rsv/metrics/ HOST  - changes made to conf files in this directory (named as the host FQDN) will affect the metric when run against the specific host   The configuration files are in INI format and have two sections:   a first one named after the metric with execution options  a second one with the name including the \"args\" keyword, including parameters sent to the probe at invokation", 
            "title": "Configuring metrics"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#changing-the-times-a-metric-runs", 
            "text": "To change the time a metric runs set the  cron-interval  setting in the metric's conf file. Use  man 5 crontab  for a description of the format. For example, to change the  org.osg.general.ping-host  to run at a different time:  [org.osg.general.ping-host]  cron-interval   =   45 * * * *  [org.osg.general.ping-host args]  #ping-count =  #ping-timeout =    Note  Be sure to put the  cron-interval  setting in the  [org.osg.general.ping-host]  section, and not the  [org.osg.general.ping-host args]  section! The purpose of the \"args\" section is described in the \"passing extra parameters to a metric\" section below.   After modifying the cron time of a metric you must restart RSV for the change to take effect.  To see what times each of the metrics is running you can use  rsv-control  as follows:  root@host#  rsv-control -l --cron-times Metrics enabled for host: osg-edu.cs.wisc.edu:10443 | Cron times  ----------------------------------------------------+--------------------  org.osg.srm.srmcp-readwrite                         | 28 * * * *  org.osg.srm.srmping                                 | 13,33,53 * * * *  ...", 
            "title": "Changing the times a metric runs"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#passing-extra-parameters-to-a-metric", 
            "text": "Any  key=value  pairs in the \"args\" section of the metric's  conf  file will be turned into command line parameters to the probe. For example, for this file:  [org.osg.certificates.cacert-expiry args]  warning-hours   =   6  error-hours   =   12   This would lead to the probe getting called with the command-line parameters  --warning-hours 6 --error-hours 12 .", 
            "title": "Passing extra parameters to a metric"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#configure-consumers", 
            "text": "There is a configuration file common to all consumers:  /etc/rsv/consumers.conf . It is a file in INI format and possible entries are:     Setting  Values  Details      enabled  consumers  Comma-separated list of consumers to be enabled    timestamp  local  If this is set to local, a record with a local timestamp will be supplied to the consumer. If this is set to any other value, or is not set, a record with the GMT will be created.     Each consumer has a configuration file in  /etc/rsv/consumers  named after it. This allows to specify command lines and environment for the consumers. Some consumers may have also their own configuration file, usually in  /etc/rsv/ . Below is an example for the Nagios consumer.", 
            "title": "Configure consumers"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#sending-rsv-records-to-nagios", 
            "text": "Edit your  /etc/rsv/rsv-nagios.conf  file and fill in the appropriate information. The path of the configuration file is specified in  /etc/rsv/consumers/nagios-consumer.conf .   If your Nagios config file contains password information you will want to lock down the permissions. Here is a suggested way to do this (replace  rsvuser  with the group of your RSV user ( rsvuser  by default)):  root@host#  chown root: rsvuser  /etc/rsv/rsv-nagios.conf root@host#  chmod  0440  /etc/rsv/rsv-nagios.conf    In the configuration file at  /etc/rsv/consumers/nagios-consumer.conf , check the following two settings:   Make sure that the path to your config file is correct. It may be referencing a directory  config  instead of  etc  If you want to use  rsv2nsca  add the string \"--send-nsca\" to the  args  line.     Enable and start the Nagios consumer by editing  consumers.conf  or by using  rsv-control  as follows:  root@host#  rsv-control --enable nagios-consumer  The Nagios consumer will be started the next time that you start RSV. If you are already running RSV you can turn on the Nagios consumer immediately by running:  root@host#  rsv-control --on nagios-consumer    To verify that the Nagios consumer is running you can run  rsv-control -j .   The log information for the Nagios consumer can be found in these files:  /var/log/rsv/consumers/nagios-consumer.log  /var/log/rsv/consumers/nagios-consumer.out  /var/log/rsv/consumers/nagios-consumer.err", 
            "title": "Sending RSV records to Nagios"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#general-rsv-configuration-options", 
            "text": "You can configure the RSV framework using  /etc/rsv/rsv.conf . It is a file in INI format and possible entries are:     Setting  Values  Details      user  username  The UNIX username that owns RSV. This is mandatory    service-cert  path  Absolute path to the service certificate file. If this is set service-key and service-proxy must also be set.    service-key  path  Absolute path to the service key file. This must be used with service-cert.    service-proxy  path  Absolute path where the service proxy will be generated. This must be used with service-cert.    proxy-file  path  Alternative to service-cert. The absolute path where the user proxy file is located. This will not be auto-regenerated.    details-data-trim-length  integer  The number of bytes to trim the detailsData section to. If set to 0 no trimming will occur.    job-timeout  integer  Time in seconds before a metric is killed. A metric that times out will return a CRITICAL status.", 
            "title": "General RSV configuration options"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#important-files-locations", 
            "text": "Configuration files:     File Description  Location  Comment      RSV configuration directory  /etc/rsv     RSV configuration  /etc/rsv/rsv.conf  RSV framework configuration    Consumers configuration in RSV  /etc/rsv/consumers.conf  Select the consumers and change generic options    Consumers configuration  /etc/rsv/consumers/ CONSUMERNAME  To change arguments and environment    Generic metrics configuration  /etc/rsv/metrics/ METRICNAME .conf  To change arguments and environment    Host specific metrics configuration  /etc/rsv/metrics/ HOSTNAME / METRICNAME .conf  To change arguments and environment when running on HOSTNAME     Other files:     File Description  Location  Comment      Metric log files  /var/log/rsv/metrics     Consumer log files  /var/log/rsv/consumers     Initial configuration  /etc/osg/config.d/30-rsv.ini  Read by  osg-configure    Web files output  /usr/share/rsv/www/      To find the metrics and the other files in RSV you can use also the RPM commands:  rpm -ql rsv-metrics  and  rpm -ql rsv .", 
            "title": "Important files locations"
        }, 
        {
            "location": "/monitoring/rsv-control/", 
            "text": "Using rsv-control\n\n\nOverview\n\n\nThis document is for System Administrators. It details the usage of the \nrsv-control\n command for enabling, disabling, testing and running RSV probes.\n\n\nrsv-control\n provides an interface to many RSV tasks. \nrsv-control\n can view RSV jobs, run metrics, enable or disable metrics and consumers, and allow advanced configuration.\n\n\n\n\nWarning\n\n\nrsv-control\n can be used to configure RSV as described here and in \nthe advanced configuration document\n. Most site admins will be able to configure RSV by editing \n/etc/osg/config.d/30-rsv.ini\n and running \nosg-configure\n as described in the \ninstallation doc\n.\n\n\n\n\nUsing \nrsv-control\n to configure is for advanced RSV use including enabling non-default metrics. Admins who don't use \nrsv-control\n for configuration can still use it to view their RSV jobs, run RSV tests, and help debug RSV problems. Anyone can view the jobs, but you must be root or the RSV user (\nrsv\n by default) to execute other commands, e.g. run, enable and disable probes, or to turn RSV on and off.\n\n\nViewing RSV jobs\n\n\nrsv-control provides two different views: viewing the \ndesired\n state and viewing the current \nactual\n state.\n\n\n\n\nDesired = what metrics and consumers will start the next time RSV is started\n\n\nActual = what metrics and consumers are currently running\n\n\n\n\nDesired state\n\n\nTo view the desired state, use the \n--list\n (\n-l\n for short) flag. This will create one table for each host showing the metrics that are enabled to run against that host.\n\n\nroot@host#\n rsv-control --list\n\n\nMetrics enabled for host: osgitb1.nhn.ou.edu              | Service\n\n\n----------------------------------------------------------+--------------------\n\n\norg.osg.batch.jobmanager-default-status                   | OSG-CE\n\n\norg.osg.batch.jobmanagers-available                       | OSG-CE\n\n\norg.osg.certificates.cacert-expiry                        | OSG-CE\n\n\norg.osg.certificates.crl-expiry                           | OSG-CE\n\n\norg.osg.general.osg-directories-CE-permissions            | OSG-CE\n\n\norg.osg.general.osg-version                               | OSG-CE\n\n\norg.osg.general.ping-host                                 | OSG-CE\n\n\norg.osg.general.vdt-version                               | OSG-CE\n\n\norg.osg.general.vo-supported                              | OSG-CE\n\n\norg.osg.globus.gram-authentication                        | OSG-CE\n\n\norg.osg.globus.gridftp-simple                             | OSG-GridFTP\n\n\norg.osg.gratia.condor                                     | OSG-CE\n\n\norg.osg.gratia.metric                                     | OSG-CE\n\n\n\n\nMetrics enabled for host: osg-edu.cs.wisc.edu:10443       | Service\n\n\n----------------------------------------------------------+--------------------\n\n\norg.osg.srm.srmcp-readwrite                               | OSG-SRM\n\n\norg.osg.srm.srmping                                       | OSG-SRM\n\n\n\n\n\n\nOther options:\n\n\n\n\nTo view all installed metrics use the \n--all\n (\n-a\n) flag along with \n--list\n. This will print an extra table showing metrics that are disabled on all hosts.\n\n\nIf you are having problems with the output being truncated, try the \n--wide\n (\n-w\n) flag.\n\n\n\n\nActual state\n\n\nTo view the current, running state of RSV jobs, use the \n--job-list\n flag (\n-j\n for short). This will show all metrics and consumers running in RSV. (It queries the underlying Condor Cron system that we use to run the metrics).\n\n\nroot@host#\n rsv-control --job-list\n\n\nHostname: osg-edu.cs.wisc.edu\n\n\n     ID OWNER      ST NEXT RUN TIME   METRIC\n\n\n  154.0 rsvuser    I  11-19 12:15     org.osg.certificates.cacert-expiry\n\n\n  155.0 rsvuser    R  11-19 11:23     org.osg.gratia.metric\n\n\n  156.0 rsvuser    I  11-19 18:47     org.osg.general.vdt-version\n\n\n  157.0 rsvuser    I  11-19 12:30     org.osg.certificates.crl-expiry\n\n\n  158.0 rsvuser    I  11-19 11:31     org.osg.globus.gram-authentication\n\n\n  159.0 rsvuser    I  11-19 11:41     org.osg.general.osg-version\n\n\n  160.0 rsvuser    R  11-19 11:25     org.osg.batch.jobmanager-default-status\n\n\n  161.0 rsvuser    I  11-20 04:59     org.osg.batch.jobmanagers-available\n\n\n  162.0 rsvuser    I  11-19 11:37     org.osg.general.osg-directories-CE-permissions\n\n\n  163.0 rsvuser    I  11-19 12:08     org.osg.globus.gridftp-simple\n\n\n  164.0 rsvuser    I  11-19 12:09     org.osg.gratia.condor\n\n\n  165.0 rsvuser    R  11-19 11:27     org.osg.general.ping-host\n\n\n  166.0 rsvuser    I  11-19 18:47     org.osg.general.vo-supported\n\n\n\nHostname: osg-edu.cs.wisc.edu:10443\n\n\n     ID OWNER      ST NEXT RUN TIME   METRIC\n\n\n  113.0 rsvuser    I  11-19 11:33     org.osg.srm.srmping\n\n\n  114.0 rsvuser    R  11-19 11:28     org.osg.srm.srmcp-readwrite\n\n\n\n     ID OWNER      ST CONSUMER\n\n\n  198.0 rsvuser    R  html-consumer\n\n\n  199.0 rsvuser    R  gratia-consumer\n\n\n\n\n\n\nThe ST field indicates the current job status:\n\n\n\n\nR = the metric is currently running\n\n\nI = the metric is idle and will be run at the next scheduled interval\n\n\nAny other letter may indicate a problem\n\n\nConsumers will always appear to be running even though they will only run once every five minutes.\n\n\n\n\nRunning a metric\n\n\nrsv-control\n can be used to run metrics one time against a host. This can be useful for:\n\n\n\n\nupdating the status of a metric that had a problem instead of waiting until the next scheduled run time\n\n\ntesting a metric against a host before deciding whether to enable it\n\n\n\n\nNote that \nthe record for each run will be published to all active consumers\n. That is, it will be published to Gratia or will show up on your local web page, if you have those enabled.\n\n\nSimplest test\n\n\nUse the \n--run\n (\n-r\n) flag. You must also provide the \n--host\n flag. The syntax is:\n\n\nrsv-control --run --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nwhere \nMETRIC\n is the full metric name (e.g. \norg.osg.general.osg-version\n). You can get the metric names from the \n--list\n output.\n\n\nroot@host#\n rsv-control --run \n\\\n\n    --host osg-edu.cs.wisc.edu org.osg.general.osg-version\n\n\nRunning metric org.osg.general.osg-version:\n\n\n\nmetricName: org.osg.general.osg-version\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 11:40:19 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: OSG 1.2.15\n\n\nEOT\n\n\n\n\n\n\nNote the \nmetricStatus\n in the example above: that's where you can see if it was successful or not. In this case, it was successful, because it printed OK.\n\n\nYou may run multiple metrics against a single host by specifying multiple metrics to \nrsv-control\n.\n\n\nIn order to run metrics against multiple hosts you must run \nrsv-control\n multiple times, once for each host.\n\n\nRunning all enabled metrics\n\n\nWhen RSV is first installed it can take up to a day for each enabled metric to run once. A new option is provided to force each metric to run immediately, for all hosts. Use the \n--all-enabled\n flag along with \n--run\n. With this option it is not necessary to specify a host - all enabled metrics for all configured hosts will be run (in fact, if you do specify a host it will be ignored).\n\n\nroot@host#\n rsv-control -r --all-enabled\n\n\nRunning metric org.osg.certificates.cacert-expiry (1 of 15)\n\n\n\nmetricName: org.osg.certificates.cacert-expiry\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 13:44:08 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: Security Probe Version: 1.1\n\n\nOK: CAs are in sync with OSG distribution\n\n\nEOT\n\n\n\n\n...\n\n\n\n\n\n\nPassing extra configuration\n\n\nIf you want to pass extra configuration when running a metric without editing its configuration file you can make an INI-formatted file and pass it on the command line. For example, you can make a file like this for the \norg.osg.srm.srmclient-ping\n metric (tmp-srm.ini):\n\n\n[org.osg.srm.srmclient-ping args]\n\n\nsrm-destination-dir\n=\n/srmcache/~\n\n\nsrm-webservice-path\n=\nsrm/v2/server\n\n\n\n\n\n\nThen use the \n--extra-config-file\n parameter and pass the path to the INI file:\n\n\nroot@host#\n rsv-control -r --extra-config-file tmp-srm.ini \n\\\n\n    --host osg-edu.cs.wisc.edu:10443 org.osg.srm.srmclient-ping\n\n\nRunning metric org.osg.srm.srmclient-ping:\n\n\n\nmetricName: org.osg.srm.srmclient-ping\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 14:12:35 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-SRM\n\n\nserviceURI: osg-edu.cs.wisc.edu:10443\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: SRM server running on osg-edu.cs.wisc.edu is alive and responding to the srmping command.\n\n\n.  Details: Storage Resource Manager (SRM) Client version 2.1.5-16\n\n\nCopyright (c) 2002-2009 Fermi National Accelerator Laboratory\n\n\n\n...\n\n\n\n\n\n\nEnabling and disabling metrics and consumers\n\n\nMetrics and consumers can be enabled or disabled by \nrsv-control\n using the \n--enable\n and \n--disable\n flags. Note that \"enable\" and \"disable\" are desired states (this is similar to \nosg-control\n). After enabling a metric you should turn it on if you want it to be running immediately. After disabling a metric that is running, you should still turn it off (a message will print after each of these actions to remind you of this behavior).\n\n\nEnabling\n\n\nThe syntax for enabling metrics looks similar to the syntax for running metrics:\n\n\nrsv-control --enable --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nYou must provide a host to enable the metric against (in order to enable a metric on multiple hosts you must run \nrsv-control\n once per host).\n\n\nroot@host#\n rsv-control --enable \n\\\n\n    --host osg-edu.cs.wisc.edu org.osg.gip.consistency\n\nEnabling metric \norg.osg.gip.consistency\n for host \nosg-edu.cs.wisc.edu\n\n\n\nOne or more metrics have been enabled and will be started the next time RSV is started.  To turn them on immediately run \nrsv-control --on\n.\n\n\n\n\n\n\nConsumers do not run against a specific host, they process records for all hosts. When enabling consumers a host is not required (if a host is passed it will be ignored).\n\n\nroot@host#\n rsv-control --enable nagios-consumer\n\nEnabling consumer nagios-consumer\n\n\n\n\n\n\nDisabling\n\n\nThe syntax for disabling metrics looks similar to the syntax for running metrics:\n\n\nrsv-control --disable --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nYou must provide a host to disable the metric against (in order to disable a metric on multiple hosts you must run \nrsv-control\n once per host).\n\n\nroot@host#\n rsv-control --disable \n\\\n\n    --host vdt-itb.cs.wisc.edu org.osg.local.containercert-expiry\n\nDisabling metric \norg.osg.local.containercert-expiry\n for host \nvdt-itb.cs.wisc.edu\n\n\n\nOne or more metrics have been disabled and will not start the next time RSV is started.  You may still need to turn them off if they are currently running.\n\n\n\n\n\n\nConsumers do not run against a specific host, they process records for all hosts. When disabling consumers a host is not required (if a host is passed it will be ignored).\n\n\nroot@host#\n rsv-control --disable html-consumer gratia-consumer\n\nDisabling consumer html-consumer\n\n\nDisabling consumer gratia-consumer\n\n\n   Consumer already disabled\n\n\n\n\n\n\nMetrics and consumers can both be listed in the same disable command.\n\n\nTroubleshooting\n\n\nGetting more information from rsv-control\n\n\nThe first step to getting more information is to run \nrsv-control\n with more verbosity. Use the \n--verbose\n (\n-v\n) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:\n\n\n\n\n0 = print nothing\n\n\n1 = print warnings and errors along with usual output of command being run (1 is the default level)\n\n\n2 = adds informational messages\n\n\n3 = full debugging output\n\n\n\n\nUsing the RSV verify tool\n\n\nThe \n--verify\n flag will run some basic checks for your RSV installation:\n\n\nroot@host#\n rsv-control --verify\n\nTesting if Condor-Cron is running...\n\n\nOK\n\n\n\nTesting if metrics are running...\n\n\nOK (98 running metrics)\n\n\n\nTesting if consumers are running...\n\n\nOK (1 running consumers)\n\n\n\nChecking which consumers are configured...\n\n\nThe following consumers are enabled: html-consumer\n\n\nWARNING: The gratia-consumer is not enabled.  This indicates that your\n\n\n         resource is not reporting to OSG.\n\n\n\n\n\n\nThis tool is still under development and it does only basic checks, but it is a good first step when debugging issues.\n\n\nRunning the RSV profiler\n\n\nRSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:\n\n\nroot@host#\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\n\n\nNote\n\n\nIf you are getting assistance via the trouble ticket system, you must add a \n.txt\n extension to the tarball so it can be uploaded.\n\n\nroot@host#\n mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt", 
            "title": "Manage RSV via rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#using-rsv-control", 
            "text": "", 
            "title": "Using rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#overview", 
            "text": "This document is for System Administrators. It details the usage of the  rsv-control  command for enabling, disabling, testing and running RSV probes.  rsv-control  provides an interface to many RSV tasks.  rsv-control  can view RSV jobs, run metrics, enable or disable metrics and consumers, and allow advanced configuration.   Warning  rsv-control  can be used to configure RSV as described here and in  the advanced configuration document . Most site admins will be able to configure RSV by editing  /etc/osg/config.d/30-rsv.ini  and running  osg-configure  as described in the  installation doc .   Using  rsv-control  to configure is for advanced RSV use including enabling non-default metrics. Admins who don't use  rsv-control  for configuration can still use it to view their RSV jobs, run RSV tests, and help debug RSV problems. Anyone can view the jobs, but you must be root or the RSV user ( rsv  by default) to execute other commands, e.g. run, enable and disable probes, or to turn RSV on and off.", 
            "title": "Overview"
        }, 
        {
            "location": "/monitoring/rsv-control/#viewing-rsv-jobs", 
            "text": "rsv-control provides two different views: viewing the  desired  state and viewing the current  actual  state.   Desired = what metrics and consumers will start the next time RSV is started  Actual = what metrics and consumers are currently running", 
            "title": "Viewing RSV jobs"
        }, 
        {
            "location": "/monitoring/rsv-control/#desired-state", 
            "text": "To view the desired state, use the  --list  ( -l  for short) flag. This will create one table for each host showing the metrics that are enabled to run against that host.  root@host#  rsv-control --list Metrics enabled for host: osgitb1.nhn.ou.edu              | Service  ----------------------------------------------------------+--------------------  org.osg.batch.jobmanager-default-status                   | OSG-CE  org.osg.batch.jobmanagers-available                       | OSG-CE  org.osg.certificates.cacert-expiry                        | OSG-CE  org.osg.certificates.crl-expiry                           | OSG-CE  org.osg.general.osg-directories-CE-permissions            | OSG-CE  org.osg.general.osg-version                               | OSG-CE  org.osg.general.ping-host                                 | OSG-CE  org.osg.general.vdt-version                               | OSG-CE  org.osg.general.vo-supported                              | OSG-CE  org.osg.globus.gram-authentication                        | OSG-CE  org.osg.globus.gridftp-simple                             | OSG-GridFTP  org.osg.gratia.condor                                     | OSG-CE  org.osg.gratia.metric                                     | OSG-CE  Metrics enabled for host: osg-edu.cs.wisc.edu:10443       | Service  ----------------------------------------------------------+--------------------  org.osg.srm.srmcp-readwrite                               | OSG-SRM  org.osg.srm.srmping                                       | OSG-SRM   Other options:   To view all installed metrics use the  --all  ( -a ) flag along with  --list . This will print an extra table showing metrics that are disabled on all hosts.  If you are having problems with the output being truncated, try the  --wide  ( -w ) flag.", 
            "title": "Desired state"
        }, 
        {
            "location": "/monitoring/rsv-control/#actual-state", 
            "text": "To view the current, running state of RSV jobs, use the  --job-list  flag ( -j  for short). This will show all metrics and consumers running in RSV. (It queries the underlying Condor Cron system that we use to run the metrics).  root@host#  rsv-control --job-list Hostname: osg-edu.cs.wisc.edu       ID OWNER      ST NEXT RUN TIME   METRIC    154.0 rsvuser    I  11-19 12:15     org.osg.certificates.cacert-expiry    155.0 rsvuser    R  11-19 11:23     org.osg.gratia.metric    156.0 rsvuser    I  11-19 18:47     org.osg.general.vdt-version    157.0 rsvuser    I  11-19 12:30     org.osg.certificates.crl-expiry    158.0 rsvuser    I  11-19 11:31     org.osg.globus.gram-authentication    159.0 rsvuser    I  11-19 11:41     org.osg.general.osg-version    160.0 rsvuser    R  11-19 11:25     org.osg.batch.jobmanager-default-status    161.0 rsvuser    I  11-20 04:59     org.osg.batch.jobmanagers-available    162.0 rsvuser    I  11-19 11:37     org.osg.general.osg-directories-CE-permissions    163.0 rsvuser    I  11-19 12:08     org.osg.globus.gridftp-simple    164.0 rsvuser    I  11-19 12:09     org.osg.gratia.condor    165.0 rsvuser    R  11-19 11:27     org.osg.general.ping-host    166.0 rsvuser    I  11-19 18:47     org.osg.general.vo-supported  Hostname: osg-edu.cs.wisc.edu:10443       ID OWNER      ST NEXT RUN TIME   METRIC    113.0 rsvuser    I  11-19 11:33     org.osg.srm.srmping    114.0 rsvuser    R  11-19 11:28     org.osg.srm.srmcp-readwrite       ID OWNER      ST CONSUMER    198.0 rsvuser    R  html-consumer    199.0 rsvuser    R  gratia-consumer   The ST field indicates the current job status:   R = the metric is currently running  I = the metric is idle and will be run at the next scheduled interval  Any other letter may indicate a problem  Consumers will always appear to be running even though they will only run once every five minutes.", 
            "title": "Actual state"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-a-metric", 
            "text": "rsv-control  can be used to run metrics one time against a host. This can be useful for:   updating the status of a metric that had a problem instead of waiting until the next scheduled run time  testing a metric against a host before deciding whether to enable it   Note that  the record for each run will be published to all active consumers . That is, it will be published to Gratia or will show up on your local web page, if you have those enabled.", 
            "title": "Running a metric"
        }, 
        {
            "location": "/monitoring/rsv-control/#simplest-test", 
            "text": "Use the  --run  ( -r ) flag. You must also provide the  --host  flag. The syntax is:  rsv-control --run --host  HOST   METRIC  [  METRIC2  ...]  where  METRIC  is the full metric name (e.g.  org.osg.general.osg-version ). You can get the metric names from the  --list  output.  root@host#  rsv-control --run  \\ \n    --host osg-edu.cs.wisc.edu org.osg.general.osg-version Running metric org.osg.general.osg-version:  metricName: org.osg.general.osg-version  metricType: status  timestamp: 2010-11-19 11:40:19 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: OSG 1.2.15  EOT   Note the  metricStatus  in the example above: that's where you can see if it was successful or not. In this case, it was successful, because it printed OK.  You may run multiple metrics against a single host by specifying multiple metrics to  rsv-control .  In order to run metrics against multiple hosts you must run  rsv-control  multiple times, once for each host.", 
            "title": "Simplest test"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-all-enabled-metrics", 
            "text": "When RSV is first installed it can take up to a day for each enabled metric to run once. A new option is provided to force each metric to run immediately, for all hosts. Use the  --all-enabled  flag along with  --run . With this option it is not necessary to specify a host - all enabled metrics for all configured hosts will be run (in fact, if you do specify a host it will be ignored).  root@host#  rsv-control -r --all-enabled Running metric org.osg.certificates.cacert-expiry (1 of 15)  metricName: org.osg.certificates.cacert-expiry  metricType: status  timestamp: 2010-11-19 13:44:08 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: Security Probe Version: 1.1  OK: CAs are in sync with OSG distribution  EOT  ...", 
            "title": "Running all enabled metrics"
        }, 
        {
            "location": "/monitoring/rsv-control/#passing-extra-configuration", 
            "text": "If you want to pass extra configuration when running a metric without editing its configuration file you can make an INI-formatted file and pass it on the command line. For example, you can make a file like this for the  org.osg.srm.srmclient-ping  metric (tmp-srm.ini):  [org.osg.srm.srmclient-ping args]  srm-destination-dir = /srmcache/~  srm-webservice-path = srm/v2/server   Then use the  --extra-config-file  parameter and pass the path to the INI file:  root@host#  rsv-control -r --extra-config-file tmp-srm.ini  \\ \n    --host osg-edu.cs.wisc.edu:10443 org.osg.srm.srmclient-ping Running metric org.osg.srm.srmclient-ping:  metricName: org.osg.srm.srmclient-ping  metricType: status  timestamp: 2010-11-19 14:12:35 CST  metricStatus: OK  serviceType: OSG-SRM  serviceURI: osg-edu.cs.wisc.edu:10443  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: SRM server running on osg-edu.cs.wisc.edu is alive and responding to the srmping command.  .  Details: Storage Resource Manager (SRM) Client version 2.1.5-16  Copyright (c) 2002-2009 Fermi National Accelerator Laboratory  ...", 
            "title": "Passing extra configuration"
        }, 
        {
            "location": "/monitoring/rsv-control/#enabling-and-disabling-metrics-and-consumers", 
            "text": "Metrics and consumers can be enabled or disabled by  rsv-control  using the  --enable  and  --disable  flags. Note that \"enable\" and \"disable\" are desired states (this is similar to  osg-control ). After enabling a metric you should turn it on if you want it to be running immediately. After disabling a metric that is running, you should still turn it off (a message will print after each of these actions to remind you of this behavior).", 
            "title": "Enabling and disabling metrics and consumers"
        }, 
        {
            "location": "/monitoring/rsv-control/#enabling", 
            "text": "The syntax for enabling metrics looks similar to the syntax for running metrics:  rsv-control --enable --host  HOST   METRIC  [  METRIC2  ...]  You must provide a host to enable the metric against (in order to enable a metric on multiple hosts you must run  rsv-control  once per host).  root@host#  rsv-control --enable  \\ \n    --host osg-edu.cs.wisc.edu org.osg.gip.consistency Enabling metric  org.osg.gip.consistency  for host  osg-edu.cs.wisc.edu  One or more metrics have been enabled and will be started the next time RSV is started.  To turn them on immediately run  rsv-control --on .   Consumers do not run against a specific host, they process records for all hosts. When enabling consumers a host is not required (if a host is passed it will be ignored).  root@host#  rsv-control --enable nagios-consumer Enabling consumer nagios-consumer", 
            "title": "Enabling"
        }, 
        {
            "location": "/monitoring/rsv-control/#disabling", 
            "text": "The syntax for disabling metrics looks similar to the syntax for running metrics:  rsv-control --disable --host  HOST   METRIC  [  METRIC2  ...]  You must provide a host to disable the metric against (in order to disable a metric on multiple hosts you must run  rsv-control  once per host).  root@host#  rsv-control --disable  \\ \n    --host vdt-itb.cs.wisc.edu org.osg.local.containercert-expiry Disabling metric  org.osg.local.containercert-expiry  for host  vdt-itb.cs.wisc.edu  One or more metrics have been disabled and will not start the next time RSV is started.  You may still need to turn them off if they are currently running.   Consumers do not run against a specific host, they process records for all hosts. When disabling consumers a host is not required (if a host is passed it will be ignored).  root@host#  rsv-control --disable html-consumer gratia-consumer Disabling consumer html-consumer  Disabling consumer gratia-consumer     Consumer already disabled   Metrics and consumers can both be listed in the same disable command.", 
            "title": "Disabling"
        }, 
        {
            "location": "/monitoring/rsv-control/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/monitoring/rsv-control/#getting-more-information-from-rsv-control", 
            "text": "The first step to getting more information is to run  rsv-control  with more verbosity. Use the  --verbose  ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:   0 = print nothing  1 = print warnings and errors along with usual output of command being run (1 is the default level)  2 = adds informational messages  3 = full debugging output", 
            "title": "Getting more information from rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#using-the-rsv-verify-tool", 
            "text": "The  --verify  flag will run some basic checks for your RSV installation:  root@host#  rsv-control --verify Testing if Condor-Cron is running...  OK  Testing if metrics are running...  OK (98 running metrics)  Testing if consumers are running...  OK (1 running consumers)  Checking which consumers are configured...  The following consumers are enabled: html-consumer  WARNING: The gratia-consumer is not enabled.  This indicates that your           resource is not reporting to OSG.   This tool is still under development and it does only basic checks, but it is a good first step when debugging issues.", 
            "title": "Using the RSV verify tool"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-the-rsv-profiler", 
            "text": "RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:  root@host#  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)    Note  If you are getting assistance via the trouble ticket system, you must add a  .txt  extension to the tarball so it can be uploaded.  root@host#  mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt", 
            "title": "Running the RSV profiler"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/", 
            "text": "Installing and Using the RSV GlideinWMS Tester\n\n\nAbout This Guide\n\n\nThe RSV GlideinWMS Tester (or \nTester\n, in this document) is a tool that a VO front-end administrator can use to test remote sites for the ability to run the VO\u2019s jobs. It is particularly useful when setting up a VO for the first time or when changing the sites at which a VO\u2019s jobs can run. For a site to pass the test, it must successfully run a simple test job via the normal GlideinWMS mechanisms, in much the same way as a real VO job.\n\n\nUse this page to learn how to install, configure, and use the Tester for your VO front-end.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nSoftware:\n You must have \na GlideinWMS Front-end\n installed\n\n\nConfiguration:\n The GlideinWMS Front-end must be configured (a) \nto have at least one group that matches pilots to sites using DESIRED_SITES\n, and (b) \nto support the is_itb user job attribute\n\n\nHost choice:\n The Tester should be installed on its own host; a small Virtual Machine (VM) is ideal\n\n\nService certificate:\n The Tester requires a host certificate at \n/etc/grid-security/hostcert.pem\n and an accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nNetwork ports:\n Test jobs must be able to contact the tester using the HTCondor Shared Port on port 9615 (TCP), and you must be able to contact a web server on port 80 (TCP) to view test results.\n\n\n\n\nInstalling the Tester\n\n\nThe Tester software takes advantage of several other OSG software components, so the installation will also include OSG\u2019s site validation system (RSV), HTCondor, and the GlideinWMS pilot submission software.\n\n\n[root@client ~] #\n yum install rsv-gwms-tester\n\n\n\n\n\nConfiguring the Tester\n\n\nBefore you use the Tester, there are some one-time configuration steps to complete, one set on your GlideinWMS Front-end Central Manager host and one set on the Tester host.\n\n\nConfiguring the GlideinWMS Front-end Central Manager\n\n\nComplete these steps \non your GlideinWMS Front-end Central Manager host\n:\n\n\n\n\n\n\nAuthorize the Tester host to connect to your Central Manager:\n\n\n[root@client ~] #\n glidecondor_addDN -allow-others -daemon \nCOMMENT\n \nTESTER_DN\n condor\n\n\n\n\n\nWhere \nCOMMENT\n is a human-readable label for the Tester host (e.g., \u201cRSV GWMS Tester at myhost\u201d), and \nTESTER_DN\n is the Distinguished Name (DN) of the host certificate of your Tester host. Most likely, you will need to quote both of these values to protect them from the shell. For example:\n\n\n[root@client ~] #\n glidecondor_addDN -allow-others -daemon \nRSV GWMS Tester on Fermicloud\n \n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov\n condor\n\n\n\n\n\n\n\n\n\nRestart HTCondor to apply the changes\n\n\nOn \nEL\u00a06\n systems:\n\n\n[root@client ~] #\n service condor restart\n\n\n\n\n\nOn \nEL\u00a07\n systems:\n\n\n[root@client ~] #\n systemctl restart condor\n\n\n\n\n\n\n\n\n\nAdd the new Tester to your GlideinWMS front-end configuration.\n   Edit the file \n/etc/gwms-frontend/frontend.xml\n and add a line as follows within the \nschedds\n element\n\n\nschedd DN=\nTESTER_DN\n fullname=\nTESTER_HOSTNAME\n\n\n\n\n\n\nWhere \nTESTER_DN\n is the Distinguished Name (DN) of the host certificate of your Tester host (as above), and \nTESTER_HOSTNAME\n is the fully qualified hostname of the Tester host. For example:\n\n\nschedd DN=\n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov\n fullname=\nfermicloud357.fnal.gov\n\n\n\n\n\n\nReconfigure your GlideinWMS front-end to apply the changes:\n\n\n[root@client ~] #\n service gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\nConfiguring the Tester host\n\n\nComplete the following steps \non your Tester host\n:\n\n\n\n\n\n\nConfigure the Tester for the VOs that your Front-end supports\n\n\nEdit the file \n/etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf\n. The \nconstraint\n line is an HTCondor ClassAd expression containing one \nstringListMember\n function per VO that your Front-end supports. If there is more than one VO, the function invocations are joined by the \u201clogical or\u201d operator, \n||\n. Edit the \nconstraint\n line for your Front-end.\n\n\nFor example, for a single VO named \nFoo\n, the line would be:\n\n\nconstraint = stringListMember(\nFoo\n, GLIDEIN_Supported_VOs)\n\n\n\n\n\nFor two VOs named \nFoo\n and \nBar\n, the line would be:\n\n\nconstraint = stringListMember(\nFoo\n, GLIDEIN_Supported_VOs) || stringListMember(\nBar\n, GLIDEIN_Supported_VOs)\n\n\n\n\n\nDo not change the other settings in this file, unless you have clear and specific reasons to do so.\n\n\n\n\n\n\nAuthorize the central manager of your Front-end to connect to the tester host:\n\n\n[root@client ~] #\n glidecondor_addDN -allow-others -daemon \nCOMMENT\n \nCENTRAL_MGR\n condor\n\n\n\n\n\nWhere \nCOMMENT\n is a human-readable identifier for the Central Manager, and \nCENTRAL_MGR\n is the Distinguished Name (DN) of the host certificate of your GlideinWMS Front-end\u2019s Central Manager host. Most likely, you will need to quote both of these values to protect them from the shell. For example:\n\n\n[root@client ~] #\n glidecondor_addDN -allow-others -daemon \nUCSD central manager DN\n \n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=osg-ligo-1.t2.ucsd.edu\n condor\n\n\n\n\n\n\n\n\n\nConfigure the special HTCondor-RSV instance with your host IP address.\n\n\nCreate the file \n/etc/condor/config.d/98_public_interface.config\n with this content:\n\n\nNETWORK_INTERFACE = \nADDRESS\n\nCONDOR_HOST = \nCENTRAL_MGR\n\n\n\n\n\n\nWhere \nADDRESS\n is the IP address of your Tester host, and \nCENTRAL_MGR\n is the hostname of your GlideinWMS Front-end Central Manager.\n\n\n\n\n\n\nEnable the Tester\u2019s RSV probe:\n\n\n[root@client ~] #\n rsv-control --enable org.osg.local-gfactory-site-querying-local --host localhost\n\n\n\n\n\n\n\n\n\nUsing the Tester\n\n\nThere are at least two aspects of using the Tester:\n\n\n\n\nManaging the services that are associated with the Tester software\n\n\nViewing results from the Tester\n\n\n\n\nManaging Tester services\n\n\nBecause the Tester is built on other OSG software, there are a number of services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nApache HTTP Server\n\n\nhttpd\n\n\nWeb server for results\n\n\n\n\n\n\nHTCondor-Cron\n\n\ncondor-cron\n\n\ncron-like jobs in HTCondor\n\n\n\n\n\n\nRSV\n\n\nrsv\n\n\nOSG site validator\n\n\n\n\n\n\n\n\nViewing Tester results\n\n\nOnce the Tester RSV probe is enabled and active, and the services listed above have been started, there are two kinds of RSV probes that run periodically:\n\n\n\n\nOne probe asks the GlideinWMS factory for the up-to-date list of sites supported by your VO(s)\u00a0\u2014 runs every 30 minutes\n\n\nOne probe submits and monitors one test job to each site supported by your VO(s)\u00a0\u2014 run every 60 minutes\n\n\n\n\nYou can view the latest results of both probe types on an RSV results web page, or you can manually run the first probe to see the full list of sites.\n\n\nViewing RSV results online\n\n\nTo see the latest results, access \nhttps://\nHOSTNAME\n/rsv/\n (where \nHOSTNAME\n is the name of your Tester host).\n\n\n\n\nThere should be one result row per site supported by your VO(s), using the \u201corg.osg.general.dummy-vanilla-probe\u201d probe (aka \nmetric\n)\n\n\nThere should be exactly one result row for the probe that fetches the list of sites, which is the \u201corg.osg.local-gfactory-site-querying-local\u201d probe (aka \nmetric\n)\n\n\nThere is a legend for the background colors at the end of the page\n\n\n\n\nIdeally, each site supported by your VO(s) should be shown with a green background, which indicates that a Tester job ran at that site recently and successfully. There may be transient failures but if you notice a site in the failed state over multiple days, contact OSG Factory Operations (\n) about the failing site, including a link to your Tester RSV results page.\n\n\nTo see detailed information from each probe, click on the probe name in the Metric column.\n\n\nTo see the list of sites that are supported by your VO(s) and are being tested, click the \u201corg.osg.local-gfactory-site-querying-local\u201d link at the bottom of the list of probes. You can also run the probe manually, as described next.\n\n\nListing supported sites manually\n\n\nTo manually run the probe that fetches the list of sites supported by your VO(s), run the following command on your Tester host:\n\n\n[root@client ~] #\n rsv-control --run org.osg.local-gfactory-site-querying-local --host localhost\n\n\n\n\n\nThe probe produces many lines of output, some of which are just about the probe execution itself. But look for lines like this:\n\n\nMSG: Updating configuration for host \nUCSD\n\n\n\n\n\n\nThe highlighted name is the site name, and there should be one such line per site supported by your VO(s).\n\n\nTroubleshooting RSV-GWMS-Tester\n\n\nYou can find more information on troubleshooting in the \nRSV troubleshooting section\n\n\nLogs and configuration:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nCondor Cron log files\n\n\n/var/log/condor-cron\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric configuration\n\n\n/etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n\n\n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.", 
            "title": "RSV GlideinWMS Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#installing-and-using-the-rsv-glideinwms-tester", 
            "text": "", 
            "title": "Installing and Using the RSV GlideinWMS Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#about-this-guide", 
            "text": "The RSV GlideinWMS Tester (or  Tester , in this document) is a tool that a VO front-end administrator can use to test remote sites for the ability to run the VO\u2019s jobs. It is particularly useful when setting up a VO for the first time or when changing the sites at which a VO\u2019s jobs can run. For a site to pass the test, it must successfully run a simple test job via the normal GlideinWMS mechanisms, in much the same way as a real VO job.  Use this page to learn how to install, configure, and use the Tester for your VO front-end.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   Software:  You must have  a GlideinWMS Front-end  installed  Configuration:  The GlideinWMS Front-end must be configured (a)  to have at least one group that matches pilots to sites using DESIRED_SITES , and (b)  to support the is_itb user job attribute  Host choice:  The Tester should be installed on its own host; a small Virtual Machine (VM) is ideal  Service certificate:  The Tester requires a host certificate at  /etc/grid-security/hostcert.pem  and an accompanying key at  /etc/grid-security/hostkey.pem  Network ports:  Test jobs must be able to contact the tester using the HTCondor Shared Port on port 9615 (TCP), and you must be able to contact a web server on port 80 (TCP) to view test results.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#installing-the-tester", 
            "text": "The Tester software takes advantage of several other OSG software components, so the installation will also include OSG\u2019s site validation system (RSV), HTCondor, and the GlideinWMS pilot submission software.  [root@client ~] #  yum install rsv-gwms-tester", 
            "title": "Installing the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-tester", 
            "text": "Before you use the Tester, there are some one-time configuration steps to complete, one set on your GlideinWMS Front-end Central Manager host and one set on the Tester host.", 
            "title": "Configuring the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-glideinwms-front-end-central-manager", 
            "text": "Complete these steps  on your GlideinWMS Front-end Central Manager host :    Authorize the Tester host to connect to your Central Manager:  [root@client ~] #  glidecondor_addDN -allow-others -daemon  COMMENT   TESTER_DN  condor  Where  COMMENT  is a human-readable label for the Tester host (e.g., \u201cRSV GWMS Tester at myhost\u201d), and  TESTER_DN  is the Distinguished Name (DN) of the host certificate of your Tester host. Most likely, you will need to quote both of these values to protect them from the shell. For example:  [root@client ~] #  glidecondor_addDN -allow-others -daemon  RSV GWMS Tester on Fermicloud   /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov  condor    Restart HTCondor to apply the changes  On  EL\u00a06  systems:  [root@client ~] #  service condor restart  On  EL\u00a07  systems:  [root@client ~] #  systemctl restart condor    Add the new Tester to your GlideinWMS front-end configuration.\n   Edit the file  /etc/gwms-frontend/frontend.xml  and add a line as follows within the  schedds  element  schedd DN= TESTER_DN  fullname= TESTER_HOSTNAME   Where  TESTER_DN  is the Distinguished Name (DN) of the host certificate of your Tester host (as above), and  TESTER_HOSTNAME  is the fully qualified hostname of the Tester host. For example:  schedd DN= /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov  fullname= fermicloud357.fnal.gov   Reconfigure your GlideinWMS front-end to apply the changes:  [root@client ~] #  service gwms-frontend reconfig", 
            "title": "Configuring the GlideinWMS Front-end Central Manager"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-tester-host", 
            "text": "Complete the following steps  on your Tester host :    Configure the Tester for the VOs that your Front-end supports  Edit the file  /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf . The  constraint  line is an HTCondor ClassAd expression containing one  stringListMember  function per VO that your Front-end supports. If there is more than one VO, the function invocations are joined by the \u201clogical or\u201d operator,  || . Edit the  constraint  line for your Front-end.  For example, for a single VO named  Foo , the line would be:  constraint = stringListMember( Foo , GLIDEIN_Supported_VOs)  For two VOs named  Foo  and  Bar , the line would be:  constraint = stringListMember( Foo , GLIDEIN_Supported_VOs) || stringListMember( Bar , GLIDEIN_Supported_VOs)  Do not change the other settings in this file, unless you have clear and specific reasons to do so.    Authorize the central manager of your Front-end to connect to the tester host:  [root@client ~] #  glidecondor_addDN -allow-others -daemon  COMMENT   CENTRAL_MGR  condor  Where  COMMENT  is a human-readable identifier for the Central Manager, and  CENTRAL_MGR  is the Distinguished Name (DN) of the host certificate of your GlideinWMS Front-end\u2019s Central Manager host. Most likely, you will need to quote both of these values to protect them from the shell. For example:  [root@client ~] #  glidecondor_addDN -allow-others -daemon  UCSD central manager DN   /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=osg-ligo-1.t2.ucsd.edu  condor    Configure the special HTCondor-RSV instance with your host IP address.  Create the file  /etc/condor/config.d/98_public_interface.config  with this content:  NETWORK_INTERFACE =  ADDRESS \nCONDOR_HOST =  CENTRAL_MGR   Where  ADDRESS  is the IP address of your Tester host, and  CENTRAL_MGR  is the hostname of your GlideinWMS Front-end Central Manager.    Enable the Tester\u2019s RSV probe:  [root@client ~] #  rsv-control --enable org.osg.local-gfactory-site-querying-local --host localhost", 
            "title": "Configuring the Tester host"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#using-the-tester", 
            "text": "There are at least two aspects of using the Tester:   Managing the services that are associated with the Tester software  Viewing results from the Tester", 
            "title": "Using the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#managing-tester-services", 
            "text": "Because the Tester is built on other OSG software, there are a number of services in your installation. The specific services are:     Software  Service name  Notes      Apache HTTP Server  httpd  Web server for results    HTCondor-Cron  condor-cron  cron-like jobs in HTCondor    RSV  rsv  OSG site validator", 
            "title": "Managing Tester services"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#viewing-tester-results", 
            "text": "Once the Tester RSV probe is enabled and active, and the services listed above have been started, there are two kinds of RSV probes that run periodically:   One probe asks the GlideinWMS factory for the up-to-date list of sites supported by your VO(s)\u00a0\u2014 runs every 30 minutes  One probe submits and monitors one test job to each site supported by your VO(s)\u00a0\u2014 run every 60 minutes   You can view the latest results of both probe types on an RSV results web page, or you can manually run the first probe to see the full list of sites.", 
            "title": "Viewing Tester results"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#viewing-rsv-results-online", 
            "text": "To see the latest results, access  https:// HOSTNAME /rsv/  (where  HOSTNAME  is the name of your Tester host).   There should be one result row per site supported by your VO(s), using the \u201corg.osg.general.dummy-vanilla-probe\u201d probe (aka  metric )  There should be exactly one result row for the probe that fetches the list of sites, which is the \u201corg.osg.local-gfactory-site-querying-local\u201d probe (aka  metric )  There is a legend for the background colors at the end of the page   Ideally, each site supported by your VO(s) should be shown with a green background, which indicates that a Tester job ran at that site recently and successfully. There may be transient failures but if you notice a site in the failed state over multiple days, contact OSG Factory Operations ( ) about the failing site, including a link to your Tester RSV results page.  To see detailed information from each probe, click on the probe name in the Metric column.  To see the list of sites that are supported by your VO(s) and are being tested, click the \u201corg.osg.local-gfactory-site-querying-local\u201d link at the bottom of the list of probes. You can also run the probe manually, as described next.", 
            "title": "Viewing RSV results online"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#listing-supported-sites-manually", 
            "text": "To manually run the probe that fetches the list of sites supported by your VO(s), run the following command on your Tester host:  [root@client ~] #  rsv-control --run org.osg.local-gfactory-site-querying-local --host localhost  The probe produces many lines of output, some of which are just about the probe execution itself. But look for lines like this:  MSG: Updating configuration for host  UCSD   The highlighted name is the site name, and there should be one such line per site supported by your VO(s).", 
            "title": "Listing supported sites manually"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#troubleshooting-rsv-gwms-tester", 
            "text": "You can find more information on troubleshooting in the  RSV troubleshooting section  Logs and configuration:     File Description  Location  Comment      Condor Cron log files  /var/log/condor-cron         File Description  Location  Comment      Metric configuration  /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf  To change arguments and environment", 
            "title": "Troubleshooting RSV-GWMS-Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem    Host key  root  /etc/grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Certificates"
        }, 
        {
            "location": "/release/release_series/", 
            "text": "OSG Release Series\n\n\nAn OSG release series is a sequence of OSG software releases that are intended to provide a painless upgrade path. For example, the 3.2 release series contains OSG software 3.2.0, 3.2.1, 3.2.2, and so forth. A release series corresponds to a set of Yum software repositories, including ones for development, testing, and production use. The Yum repositories for one release series are completely distinct from the repositories for a different release series, even though they share many common packages.  A particular release within a series is a snapshot of packages and their exact versions at one point in time. When you install software from a release series, say 3.2, you get the most current versions of software packages within that series, regardless of the current release version.\n\n\nWhen a new series is released, it is an opportunity for the OSG Technology area to add major new software packages, make substantial updates to existing packages, and remove obsolete packages. When a new series is initially released, most packages are identical to the previous release, but two adjacent series will diverge over time.\n\n\nOur goal is, within a series, that one may upgrade their OSG services via \nyum update\n cleanly and without any necessary config file changes or excessive downtime.\n\n\nOSG Release Series\n\n\nSince the start of the RPM-based OSG software stack, we have offered the following release series:\n\n\n\n\n\n\nOSG 3.1\n started in April 2012, and was end-of-lifed in April 2015. While the files have not been removed, it is strongly recommended that it not be installed anymore. Historically, there were 3.0.x releases as well, but there was no separate release series for 3.0 and 3.1; we simply went from 3.0.10 to 3.1.0 in the same repositories.\n\n\n\n\n\n\nOSG 3.2\n started in November 2013, and was end-of-lifed in August 2016. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between it and 3.1 were the introduction of glideinWMS 3.2, HTCondor 8.0, and Hadoop/HDFS 2.0; also the gLite CE Monitor system was dropped in favor of osg-info-services.\n\n\n\n\n\n\nOSG 3.3\n started in August 2015 and is still supported today.  End-of-support is scheduled for June 2018; sites are encouraged to investigate the upgrade to OSG 3.4. The main differences between 3.3 and 3.2 are the dropping of EL5 support, the addition of EL7 support, and the dropping of Globus GRAM support.\n\n\n\n\n\n\nOSG 3.4\n stared June 2017. The main differences between it and 3.3 are the removal of edg-mkgridmap, GUMS, BeStMan, and VOMS Admin Server packages.\n\n\n\n\n\n\nOSG Upcoming\n\n\nThere is one more OSG Series called \"upcoming\" which contains major updates planned for a future release series. The yum repositories for upcoming (\nosg-upcoming\n and \nosg-upcoming-testing\n) are available from all OSG 3.x series, and individual packages can be installed from Upcoming without needing to update entirely to a new series. Note, however, that packages in the \"upcoming\" repositories are tested against the most recent OSG series.  As of the time of writing, \nosg-upcoming\n is meant to work with OSG 3.4.\n\n\nInstalling an OSG Release Series\n\n\nSee the \nyum repositories document\n for instructions on installing the OSG repositories.\n\n\nUpdating from OSG 3.1, 3.2, 3.3 to 3.3 or 3.4\n\n\n\n\n\n\nIf you have an existing installation based on OSG 3.1, 3.2, or 3.3 (which will be referred to as the \nold series\n), and want to upgrade to 3.3 or 3.4 (the \nnew series\n), we recommend the following procedure:\n\n\nFirst, remove the old series yum repositories:\n\n\n[root@client ~] #\n rpm -e osg-release\n\n\n\n\n\nThis step ensures that any local modifications to \n*.repo\n files will not prevent installing the new series repos. Any modified \n*.repo\n files should appear under \n/etc/yum.repos.d/\n with the \n*.rpmsave\n extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the \n*.rpmsave\n files to the new \n*.repo\n files.\n\n\n\n\n\n\nInstall the OSG repositories:\n\n\n[root@client ~] #\n rpm -Uvh \nURL\n\n\n\n\n\n\nwhere \nURL\n is one of the following:\n\n\n\n\n\n\n\n\nSeries\n\n\nEL5 URL (for RHEL5, CentOS5, or SL5)\n\n\nEL6 URL (for RHEL6, CentOS6, or SL6)\n\n\nEL7 URL (for RHEL7, CentOS7, or SL7)\n\n\n\n\n\n\n\n\n\n\nOSG 3.1\n (unsupported)\n\n\nhttp://repo.grid.iu.edu/osg/3.1/osg-3.1-el5-release-latest.rpm\n\n\nhttp://repo.grid.iu.edu/osg/3.1/osg-3.1-el6-release-latest.rpm\n\n\nN/A\n\n\n\n\n\n\nOSG 3.2\n (unsupported)\n\n\nhttp://repo.grid.iu.edu/osg/3.2/osg-3.2-el5-release-latest.rpm\n\n\nhttp://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\n\n\nN/A\n\n\n\n\n\n\nOSG 3.3\n\n\nN/A\n\n\nhttp://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm\n\n\nhttp://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\n\n\n\n\nOSG 3.4\n\n\nN/A\n\n\nhttp://repo.grid.iu.edu/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n\nhttp://repo.grid.iu.edu/osg/3.4/osg-3.4-el7-release-latest.rpm\n\n\n\n\n\n\n\n\n\n\n\n\nClean yum cache:\n\n\n[root@client ~] #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\n[root@client ~] #\n yum update\n\n\n\n\n\n\n\n\n\nThis command will update \nall\n packages on your system.\n\n\nTroubleshooting\n If you are not having the expected result or having problems with Yum please see the \nYum troubleshooting guide\n\n\nMigrating from edg-mkgridmap to LCMAPS VOMS Plugin\n\n\nAfter following the update instructions above, perform the migration process documented \nhere\n.\n\n\nUpdating from Frontier Squid 2.7 to Frontier Squid 3.5 (upgrading from OSG 3.3)\n\n\nThe program \nfrontier-squid\n received a major version upgrade (versions 2.7 to 3.5) between OSG 3.3 and OSG 3.4. Follow the \nupstream upgrade documentation\n when transitioning your squid server to OSG 3.4.\n\n\nUninstalling BeStMan2 from the Storage Element (upgrading to OSG 3.4)\n\n\nThe program BeStMan2 is no longer available in OSG 3.4 and its functionality has been replaced by \nload-balanced GridFTP\n. To update your storage element to OSG 3.4, you must perform the following procedure:\n\n\n\n\n\n\nEnsure that OSG BeStMan packages are installed:\n\n\n[root@client ~] #\n rpm -q osg-se-bestman\n\n\n\n\n\n\n\n\n\nStop the \nbestman2\n service:\n\n\n[root@client ~] #\n service bestman2 stop\n\n\n\n\n\n\n\n\n\nUninstall the software:\n\n\n[root@client ~] #\n yum erase bestman2-tester-libs bestman2-common-libs \n\\\n\n                            bestman2-server-libs bestman2-server-dep-libs \n\\\n\n                            bestman2-client-libs bestman2-tester bestman2-client \n\\\n\n                            bestman2-server osg-se-bestman\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those nine. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 versions, and try again.\n\n\n\n\nAfter successfully removing BeStMan2, continue updating your host to OSG 3.4 by following the instructions above.\n\n\nUninstalling OSG Info Services from the Compute Element (upgrading from OSG 3.3 or 3.2)\n\n\nThe program OSG Info Services is no longer required on OSG 3.3, and is no longer available starting in OSG 3.4. This is because the service that OSG Info Services reported to, named BDII, has been retired and is no longer functional.\n\n\nTo cleanly uninstall OSG Info Services from your CE, perform the following procedure (after following the main update instructions above):\n\n\n\n\n\n\nEnsure that you are using a sufficiently new version of the \nosg-ce\n metapackages:\n\n\n[root@client ~] #\n rpm -q osg-ce\n\n\n\n\n\nshould be at least 3.3-12 (OSG 3.3) or 3.4-1 (OSG 3.4).  If not, update them:\n\n\n[root@client ~] #\n yum update osg-ce\n\n\n\n\n\n\n\n\n\nStop the \nosg-info-services\n service:\n\n\n[root@client ~] #\n service osg-info-services stop\n\n\n\n\n\n\n\n\n\nUninstall the software:\n\n\n[root@client ~] #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those two. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 (or 3.4) versions, and try again.\n\n\n\n\nUninstalling CEMon from the Compute Element (upgrading from OSG 3.1)\n\n\nThe program CEMon (found in the package \nglite-ce-monitor\n) is no longer available starting in OSG 3.2. Its functionality is no longer required because the service that CEMon reported to has been retired and is no longer functional.\n\n\nTo cleanly uninstall CEMon from your CE, perform the following procedure (after following the main update instructions above):\n\n\n\n\n\n\nEnsure that you are using a sufficiently new version of the \nosg-ce\n metapackages:\n\n\n[root@client ~] #\n rpm -q osg-ce\n\n\n\n\n\nshould be at least 3.3-12 (OSG 3.3) or 3.4-1 (OSG 3.4). If not, update them:\n\n\n[root@client ~] #\n yum update osg-ce\n\n\n\n\n\n\n\n\n\nIf there is a CEMon configuration file at \n/etc/osg/config.d/30-cemon.ini\n, remove it.\n\n\n\n\nRemove CEMon and related packages:\n[root@client ~] #\n yum erase glite-ce-monitor glite-ce-osg-ce-plugin osg-configure-cemon\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those three. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.3 (or 3.4) versions (they should have \n.osg33\n or \n.osg34\n in their versions), and try again.\n\n\n\n\nReferences\n\n\n\n\nYUM Repositories\n\n\nBasic use of Yum\n\n\nBest practices in using Yum", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-release-series", 
            "text": "An OSG release series is a sequence of OSG software releases that are intended to provide a painless upgrade path. For example, the 3.2 release series contains OSG software 3.2.0, 3.2.1, 3.2.2, and so forth. A release series corresponds to a set of Yum software repositories, including ones for development, testing, and production use. The Yum repositories for one release series are completely distinct from the repositories for a different release series, even though they share many common packages.  A particular release within a series is a snapshot of packages and their exact versions at one point in time. When you install software from a release series, say 3.2, you get the most current versions of software packages within that series, regardless of the current release version.  When a new series is released, it is an opportunity for the OSG Technology area to add major new software packages, make substantial updates to existing packages, and remove obsolete packages. When a new series is initially released, most packages are identical to the previous release, but two adjacent series will diverge over time.  Our goal is, within a series, that one may upgrade their OSG services via  yum update  cleanly and without any necessary config file changes or excessive downtime.", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-release-series_1", 
            "text": "Since the start of the RPM-based OSG software stack, we have offered the following release series:    OSG 3.1  started in April 2012, and was end-of-lifed in April 2015. While the files have not been removed, it is strongly recommended that it not be installed anymore. Historically, there were 3.0.x releases as well, but there was no separate release series for 3.0 and 3.1; we simply went from 3.0.10 to 3.1.0 in the same repositories.    OSG 3.2  started in November 2013, and was end-of-lifed in August 2016. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between it and 3.1 were the introduction of glideinWMS 3.2, HTCondor 8.0, and Hadoop/HDFS 2.0; also the gLite CE Monitor system was dropped in favor of osg-info-services.    OSG 3.3  started in August 2015 and is still supported today.  End-of-support is scheduled for June 2018; sites are encouraged to investigate the upgrade to OSG 3.4. The main differences between 3.3 and 3.2 are the dropping of EL5 support, the addition of EL7 support, and the dropping of Globus GRAM support.    OSG 3.4  stared June 2017. The main differences between it and 3.3 are the removal of edg-mkgridmap, GUMS, BeStMan, and VOMS Admin Server packages.", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-upcoming", 
            "text": "There is one more OSG Series called \"upcoming\" which contains major updates planned for a future release series. The yum repositories for upcoming ( osg-upcoming  and  osg-upcoming-testing ) are available from all OSG 3.x series, and individual packages can be installed from Upcoming without needing to update entirely to a new series. Note, however, that packages in the \"upcoming\" repositories are tested against the most recent OSG series.  As of the time of writing,  osg-upcoming  is meant to work with OSG 3.4.", 
            "title": "OSG Upcoming"
        }, 
        {
            "location": "/release/release_series/#installing-an-osg-release-series", 
            "text": "See the  yum repositories document  for instructions on installing the OSG repositories.", 
            "title": "Installing an OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#updating-from-osg-31-32-33-to-33-or-34", 
            "text": "If you have an existing installation based on OSG 3.1, 3.2, or 3.3 (which will be referred to as the  old series ), and want to upgrade to 3.3 or 3.4 (the  new series ), we recommend the following procedure:  First, remove the old series yum repositories:  [root@client ~] #  rpm -e osg-release  This step ensures that any local modifications to  *.repo  files will not prevent installing the new series repos. Any modified  *.repo  files should appear under  /etc/yum.repos.d/  with the  *.rpmsave  extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the  *.rpmsave  files to the new  *.repo  files.    Install the OSG repositories:  [root@client ~] #  rpm -Uvh  URL   where  URL  is one of the following:     Series  EL5 URL (for RHEL5, CentOS5, or SL5)  EL6 URL (for RHEL6, CentOS6, or SL6)  EL7 URL (for RHEL7, CentOS7, or SL7)      OSG 3.1  (unsupported)  http://repo.grid.iu.edu/osg/3.1/osg-3.1-el5-release-latest.rpm  http://repo.grid.iu.edu/osg/3.1/osg-3.1-el6-release-latest.rpm  N/A    OSG 3.2  (unsupported)  http://repo.grid.iu.edu/osg/3.2/osg-3.2-el5-release-latest.rpm  http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm  N/A    OSG 3.3  N/A  http://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm  http://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm    OSG 3.4  N/A  http://repo.grid.iu.edu/osg/3.4/osg-3.4-el6-release-latest.rpm  http://repo.grid.iu.edu/osg/3.4/osg-3.4-el7-release-latest.rpm       Clean yum cache:  [root@client ~] #  yum clean all --enablerepo = *    Update software:  [root@client ~] #  yum update    This command will update  all  packages on your system.  Troubleshooting  If you are not having the expected result or having problems with Yum please see the  Yum troubleshooting guide", 
            "title": "Updating from OSG 3.1, 3.2, 3.3 to 3.3 or 3.4"
        }, 
        {
            "location": "/release/release_series/#migrating-from-edg-mkgridmap-to-lcmaps-voms-plugin", 
            "text": "After following the update instructions above, perform the migration process documented  here .", 
            "title": "Migrating from edg-mkgridmap to LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/release/release_series/#updating-from-frontier-squid-27-to-frontier-squid-35-upgrading-from-osg-33", 
            "text": "The program  frontier-squid  received a major version upgrade (versions 2.7 to 3.5) between OSG 3.3 and OSG 3.4. Follow the  upstream upgrade documentation  when transitioning your squid server to OSG 3.4.", 
            "title": "Updating from Frontier Squid 2.7 to Frontier Squid 3.5 (upgrading from OSG 3.3)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-bestman2-from-the-storage-element-upgrading-to-osg-34", 
            "text": "The program BeStMan2 is no longer available in OSG 3.4 and its functionality has been replaced by  load-balanced GridFTP . To update your storage element to OSG 3.4, you must perform the following procedure:    Ensure that OSG BeStMan packages are installed:  [root@client ~] #  rpm -q osg-se-bestman    Stop the  bestman2  service:  [root@client ~] #  service bestman2 stop    Uninstall the software:  [root@client ~] #  yum erase bestman2-tester-libs bestman2-common-libs  \\ \n                            bestman2-server-libs bestman2-server-dep-libs  \\ \n                            bestman2-client-libs bestman2-tester bestman2-client  \\ \n                            bestman2-server osg-se-bestman     Note  In the output from this command, yum should  not  list other packages than those nine. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 versions, and try again.   After successfully removing BeStMan2, continue updating your host to OSG 3.4 by following the instructions above.", 
            "title": "Uninstalling BeStMan2 from the Storage Element (upgrading to OSG 3.4)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-osg-info-services-from-the-compute-element-upgrading-from-osg-33-or-32", 
            "text": "The program OSG Info Services is no longer required on OSG 3.3, and is no longer available starting in OSG 3.4. This is because the service that OSG Info Services reported to, named BDII, has been retired and is no longer functional.  To cleanly uninstall OSG Info Services from your CE, perform the following procedure (after following the main update instructions above):    Ensure that you are using a sufficiently new version of the  osg-ce  metapackages:  [root@client ~] #  rpm -q osg-ce  should be at least 3.3-12 (OSG 3.3) or 3.4-1 (OSG 3.4).  If not, update them:  [root@client ~] #  yum update osg-ce    Stop the  osg-info-services  service:  [root@client ~] #  service osg-info-services stop    Uninstall the software:  [root@client ~] #  yum erase gip osg-info-services     Note  In the output from this command, yum should  not  list other packages than those two. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 (or 3.4) versions, and try again.", 
            "title": "Uninstalling OSG Info Services from the Compute Element (upgrading from OSG 3.3 or 3.2)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-cemon-from-the-compute-element-upgrading-from-osg-31", 
            "text": "The program CEMon (found in the package  glite-ce-monitor ) is no longer available starting in OSG 3.2. Its functionality is no longer required because the service that CEMon reported to has been retired and is no longer functional.  To cleanly uninstall CEMon from your CE, perform the following procedure (after following the main update instructions above):    Ensure that you are using a sufficiently new version of the  osg-ce  metapackages:  [root@client ~] #  rpm -q osg-ce  should be at least 3.3-12 (OSG 3.3) or 3.4-1 (OSG 3.4). If not, update them:  [root@client ~] #  yum update osg-ce    If there is a CEMon configuration file at  /etc/osg/config.d/30-cemon.ini , remove it.   Remove CEMon and related packages: [root@client ~] #  yum erase glite-ce-monitor glite-ce-osg-ce-plugin osg-configure-cemon     Note  In the output from this command, yum should  not  list other packages than those three. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.3 (or 3.4) versions (they should have  .osg33  or  .osg34  in their versions), and try again.", 
            "title": "Uninstalling CEMon from the Compute Element (upgrading from OSG 3.1)"
        }, 
        {
            "location": "/release/release_series/#references", 
            "text": "YUM Repositories  Basic use of Yum  Best practices in using Yum", 
            "title": "References"
        }, 
        {
            "location": "/release/supported_platforms/", 
            "text": "OSG Software Supported Operating Systems\n\n\nThe OSG Software 3.3 and 3.4 release series are supported on Red Hat Enterprise 6 and 7 for 64-bit (RHEL 6 and RHEL 7) and 32-bit (RHEL 6 only) Intel architectures.\n\n\nOSG also supports select rebuilds of RHEL.  Specifically:\n\n\n\n\nCentOS 6\n\n\nCentOS 7\n\n\nRed Hat Enterprise Linux 6\n\n\nRed Hat Enterprise Linux 7\n\n\nScientific Linux 6\n\n\nScientific Linux 7\n\n\n\n\nThe OSG Software 3.1 and 3.2 release series also supported Red Hat Enterprise Linux 5, but neither 3.1 nor 3.2 are actively supported.", 
            "title": "Supported Platforms"
        }, 
        {
            "location": "/release/supported_platforms/#osg-software-supported-operating-systems", 
            "text": "The OSG Software 3.3 and 3.4 release series are supported on Red Hat Enterprise 6 and 7 for 64-bit (RHEL 6 and RHEL 7) and 32-bit (RHEL 6 only) Intel architectures.  OSG also supports select rebuilds of RHEL.  Specifically:   CentOS 6  CentOS 7  Red Hat Enterprise Linux 6  Red Hat Enterprise Linux 7  Scientific Linux 6  Scientific Linux 7   The OSG Software 3.1 and 3.2 release series also supported Red Hat Enterprise Linux 5, but neither 3.1 nor 3.2 are actively supported.", 
            "title": "OSG Software Supported Operating Systems"
        }, 
        {
            "location": "/release/yum-basics/", 
            "text": "Basics of using yum and RPM\n\n\nAbout This Document\n\n\nThis document introduces package management tools that help you install, update, and remove packages. OSG uses RPMs (the Red Hat Packaging Manager) to package its software. While RPM is the packaging format, \nyum\n is the command you will use to do the installation. For example, \nyum\n will resolve and download the dependencies for the package you want to install; \nrpm\n will simply complain if you want to install a package that does not have all its dependencies installed.\n\n\nInstallation\n\n\nInstallation is done with the \nyum install\n command. Each of the individual installation guide shows you the correct command to use to do an installation. Here is an example installation with all of the output from yum.\n\n\n[root@client ~] #\n sudo yum install osg-ca-certs\n\nLoaded plugins: kernel-module, priorities\n\n\nepel                                                                                         | 3.7 kB     00:00     \n\n\nepel/primary_db                                                                              | 3.8 MB     00:00     \n\n\nfermi-base                                                                                   | 2.1 kB     00:00     \n\n\nfermi-base/primary_db                                                                        |  48 kB     00:00     \n\n\nfermi-security                                                                               | 1.9 kB     00:00     \n\n\nfermi-security/primary_db                                                                    | 1.7 MB     00:00     \n\n\nosg                                                                                          | 1.9 kB     00:00     \n\n\nosg/primary_db                                                                               |  65 kB     00:00     \n\n\nsl-base                                                                                      | 2.1 kB     00:00     \n\n\nsl-base/primary_db                                                                           | 2.0 MB     00:00     \n\n\n957 packages excluded due to repository priority protections\n\n\nSetting up Install Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package osg-ca-certs.noarch 0:1.23-1 set to be updated\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                         Arch                      Version                     Repository              Size\n\n\n====================================================================================================================\n\n\nInstalling:\n\n\n osg-ca-certs                    noarch                    1.23-1                      osg                    450 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nInstall       1 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\n\nTotal download size: 450 k\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\nosg-ca-certs-1.23-1.noarch.rpm                                                               | 450 kB     00:00     \n\n\nwarning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603\n\n\nosg/gpgkey                                                                                   | 1.7 kB     00:00     \n\n\nImporting GPG key 0x824B8603 \nOSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\nIs this ok [y/N]: y\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Installing     : osg-ca-certs                                                                                 1/1 \n\n\n\nInstalled:\n\n\n  osg-ca-certs.noarch 0:1.23-1                                                                                      \n\n\n\nComplete!\n\n\n\n\n\n\nPlease Note\n: When you first install a package from the OSG repository, you will be prompted to import the GPG key. We use this key to sign our RPMs as a security measure. You should double-check the key id (above it is 824B8603) with the \ninformation on our signed RPMs\n. If it doesn't match, there is a problem somewhere and you should report it to the OSG via goc@opensciencegrid.org.\n\n\nVerifying Packages and Installations\n\n\nYou can check if an RPM has been modified. For instance, to check to see if any files have been modified in the \nosg-ca-certs\n RPM you just installed:\n\n\n[user@client ~] $\n rpm --verify osg-ca-certs\n\n\n\n\n\nThe lack of any output means there were no problems. If you would like to see all the files for which there are no problems, you can do:\n\n\n[user@client ~] $\n rpm --verify -v osg-ca-certs\n\n........    /etc/grid-security/certificates\n\n\n........    /etc/grid-security/certificates/0119347c.0\n\n\n........    /etc/grid-security/certificates/0119347c.namespaces\n\n\n........    /etc/grid-security/certificates/0119347c.signing_policy\n\n\n... etc ...\n\n\n\n\n\n\nEach dot indicates a specific check that was made and passed. If someone had modified a file, you might see this:\n\n\n[user@client ~] $\n rpm --verify osg-ca-certs\n\n..5....T    /etc/grid-security/certificates/ffc3d59b.0\n\n\n\n\n\n\nThis means the files MD5 checksum has changed (so the contents have changed) and the timestamp is different. The complete set of changes you might see (copied from the \nrpm\n man page) are:\n\n\n\n\n\n\n\n\nLetter\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nS\n\n\nfile Size differs\n\n\n\n\n\n\nM\n\n\nMode differs (includes permissions and file type)\n\n\n\n\n\n\n5\n\n\nMD5 sum differs\n\n\n\n\n\n\nD\n\n\nDevice major/minor number mismatch\n\n\n\n\n\n\nL\n\n\nreadLink(2) path mismatch\n\n\n\n\n\n\nU\n\n\nUser ownership differs\n\n\n\n\n\n\nG\n\n\nGroup ownership differs\n\n\n\n\n\n\nT\n\n\nmTime differs\n\n\n\n\n\n\n\n\nIf you don't care about some of those changes, you can tell rpm to ignore them. For instance, to ignore changes in the modification time:\n\n\n[user@client ~] $\n rpm --verify --nomtime osg-ca-certs\n\n..5.....    /etc/grid-security/certificates/ffc3d59b.0\n\n\n\n\n\n\nUnderstanding a package\n\n\nIf you want to know what package a file belongs to, you can ask rpm. For instance, if you're curious what package contains the \nsrm-ls\n command, you can do:\n\n\n#\n \n1\n. Find the exact path\n\n[user@client ~] $\n which srm-ls\n\n/usr/bin/srm-ls\n\n\n\n#\n \n2\n. Ask rpm what package it is part of:\n\n[user@client ~] $\n rpm -q --file /usr/bin/srm-ls\n\nbestman2-client-2.2.0-14.osg.el5.noarch\n\n\n\n\n\n\nIf you want to know what other things are in a package--perhaps the other available tools or configuration files--you can do that as well:\n\n\n[user@client ~] $\n rpm -ql bestman2-client\n\n/etc/bestman2/conf/bestman2.rc\n\n\n/etc/bestman2/conf/bestman2.rc.samples\n\n\n/etc/bestman2/conf/srmclient.conf\n\n\n/etc/sysconfig/bestman2\n\n\n/usr/bin/srm-copy\n\n\n/usr/bin/srm-copy-status\n\n\n/usr/bin/srm-extendfilelifetime\n\n\n/usr/bin/srm-ls\n\n\n/usr/bin/srm-ls-status\n\n\n... output trimmed ...\n\n\n\n\n\n\nWhat else does a package install?\n\n\nSometimes you need to understand what other software is installed by a package. This can be particularly useful for understanding \nmeta-packages\n, which are packages such as the \nosg-wn-client\n (worker node client) that contain nothing by themselves but only depend on other RPMs. To do this, use the \n--requires\n option to rpm. For example, you can see that the worker node client (as of OSG 3.1.8 in early September, 2012) will install \ncurl\n, \nuberftp\n, \nlcg-utils\n, and a dozen or so other packages.\n\n\n[user@client ~] $\n rpm -q --requires osg-wn-client\n\n/usr/bin/curl  \n\n\n/usr/bin/dccp  \n\n\n/usr/bin/ldapsearch  \n\n\n/usr/bin/uberftp  \n\n\n/usr/bin/wget  \n\n\nbestman2-client  \n\n\nconfig(osg-wn-client) = 3.0.0-16.osg.el5\n\n\ndcache-srmclient  \n\n\ndcap-tunnel-gsi  \n\n\nedg-gridftp-client  \n\n\nfetch-crl  \n\n\nglite-fts-client  \n\n\nglobus-gass-copy-progs  \n\n\ngrid-certificates  \n\n\njava-1.6.0-sun-compat  \n\n\nlcg-utils  \n\n\nlfc-client  \n\n\nlfc-python  \n\n\nmyproxy  \n\n\nosg-system-profiler  \n\n\nosg-version  \n\n\nrpmlib(CompressedFileNames) \n= 3.0.4-1\n\n\nrpmlib(PayloadFilesHavePrefix) \n= 4.0-1\n\n\nvo-client\n\n\n\n\n\n\nFinding RPM Packages\n\n\nIt is normally best to read the OSG documentation to decide which packages to install because it may not be obvious what the correct packages to install are. That said, you can use yum to find out all sort of things. For instance, you can list packages that begin with \"voms\":\n\n\n[user@client ~] $\n yum list \nvoms*\n\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\nAvailable Packages\n\n\nvoms.i386                                                    2.0.6-3.osg                                        osg \n\n\nvoms.x86_64                                                  2.0.6-3.osg                                        osg \n\n\nvoms-admin-client.x86_64                                     2.0.16-1                                           osg \n\n\nvoms-admin-server.noarch                                     2.6.1-9                                            osg \n\n\nvoms-clients.x86_64                                          2.0.6-3.osg                                        osg \n\n\nvoms-compat.i386                                             1.9.19.2-6.osg                                     osg \n\n\nvoms-compat.x86_64                                           1.9.19.2-6.osg                                     osg \n\n\nvoms-devel.i386                                              2.0.6-3.osg                                        osg \n\n\nvoms-devel.x86_64                                            2.0.6-3.osg                                        osg \n\n\nvoms-doc.x86_64                                              2.0.6-3.osg                                        osg \n\n\nvoms-mysql-plugin.x86_64                                     3.1.5.1-1.el5                                      epel\n\n\nvoms-server.x86_64                                           2.0.6-3.osg                                        osg \n\n\nvomsjapi.x86_64                                              2.0.6-3.osg                                        osg \n\n\nvomsjapi-javadoc.x86_64                                      2.0.6-3.osg                                        osg\n\n\n\n\n\n\nIf you want to search for packages that contain VOMS anywhere in the name or description, you can use \nyum search\n:\n\n\n[user@client ~] $\n yum search voms\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n================================================== Matched: voms ===================================================\n\n\nosg-voms.noarch : OSG VOMS\n\n\nperl-VOMS-Lite.noarch : Perl extension for VOMS Attribute certificate creation\n\n\nperl-voms-server.noarch : Perl extension for VOMS Attribute certificate creation\n\n\nphp-voms-admin.noarch : Web based interface to control VOMS parameters written in PHP\n\n\nvoms.i386 : Virtual Organization Membership Service\n\n\nvoms.x86_64 : Virtual Organization Membership Service\n\n\n... etc ...\n\n\n\n\n\n\nOne last example, if you want to know what RPM would give you the \nvoms-proxy-init\n command, you can ask \nyum\n. The \n*\n indicates that you don't know the full pathname of \nvoms-proxy-init\n.\n\n\n[user@client ~] $\n yum whatprovides \n*voms-proxy-init\n\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\nvoms-clients-2.0.6-3.osg.x86_64 : Virtual Organization Membership Service Clients\n\n\nRepo        : osg\n\n\nMatched from:\n\n\nFilename    : /usr/bin/voms-proxy-init\n\n\n\n\n\n\nRemoving Packages\n\n\nTo remove a single RPM, you can use \nyum remove\n. Not only will it uninstall the RPM you requested, but it will uninstall anything that depends on it. For example, if I previously installed the \nvoms-clients\n package, I also installed another package it depends on called \nvoms\n. If I remove \nvoms\n, yum will also remove \nvoms-clients\n:\n\n\n[user@client ~] $\n sudo yum remove voms\n\nLoaded plugins: kernel-module, priorities\n\n\nSetting up Remove Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package voms.x86_64 0:2.0.6-3.osg set to be erased\n\n\n--\n Processing Dependency: libvomsapi.so.1()(64bit) for package: voms-clients\n\n\n--\n Processing Dependency: voms = 2.0.6-3.osg for package: voms-clients\n\n\n--\n Running transaction check\n\n\n---\n Package voms-clients.x86_64 0:2.0.6-3.osg set to be erased\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                      Arch                   Version                        Repository                 Size\n\n\n====================================================================================================================\n\n\nRemoving:\n\n\n voms                         x86_64                 2.0.6-3.osg                    installed                 407 k\n\n\nRemoving for dependencies:\n\n\n voms-clients                 x86_64                 2.0.6-3.osg                    installed                 373 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nRemove        2 Package(s)\n\n\nReinstall     0 Package(s)\n\n\nDowngrade     0 Package(s)\n\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Erasing        : voms                                                                                         1/2 \n\n\n  Erasing        : voms-clients                                                                                 2/2 \n\n\n\nRemoved:\n\n\n  voms.x86_64 0:2.0.6-3.osg                                                                                         \n\n\n\nDependency Removed:\n\n\n  voms-clients.x86_64 0:2.0.6-3.osg                                                                                 \n\n\n\nComplete!\n\n\n\n\n\n\nUpgrading Packages\n\n\nYou can check for updates with \nyum check-update\n. For example:\n\n\n[root@client ~] #\n yum check-update\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n\nkernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security\n\n\nObsoleting Packages\n\n\nocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel          \n\n\n    ocsinventory-client.noarch                           0.9.9-10                                     installed     \n\n\n\n\n\n\nYou can do the update with \nyum update\n. Note that in this case we got more than was listed due to dependencies that needed to be resolved:\n\n\n[root@client ~] #\n yum update\n\n957 packages excluded due to repository priority protections\n\n\nSetting up Update Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package kernel.x86_64 0:2.6.18-274.3.1.el5 set to be installed\n\n\n---\n Package ocsinventory-agent.noarch 0:1.1.2.1-1.el5 set to be updated\n\n\n--\n Processing Dependency: perl(Crypt::SSLeay) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Proc::Daemon) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: monitor-edid for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Net::IP) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: nmap for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Net::SSLeay) for package: ocsinventory-agent\n\n\n--\n Running transaction check\n\n\n---\n Package monitor-edid.x86_64 0:2.5-1.el5.1 set to be updated\n\n\n---\n Package nmap.x86_64 2:4.11-1.1 set to be updated\n\n\n---\n Package perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 set to be updated\n\n\n---\n Package perl-Net-IP.noarch 0:1.25-2.fc6 set to be updated\n\n\n---\n Package perl-Net-SSLeay.x86_64 0:1.30-4.fc6 set to be updated\n\n\n---\n Package perl-Proc-Daemon.noarch 0:0.03-1.el5 set to be updated\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n--\n Running transaction check\n\n\n---\n Package kernel.x86_64 0:2.6.18-238.1.1.el5 set to be erased\n\n\n--\n Finished Dependency Resolution\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                        Arch               Version                         Repository                  Size\n\n\n====================================================================================================================\n\n\nInstalling:\n\n\n kernel                         x86_64             2.6.18-274.3.1.el5              fermi-security              21 M\n\n\n ocsinventory-agent             noarch             1.1.2.1-1.el5                   epel                       156 k\n\n\n     replacing  ocsinventory-client.noarch 0.9.9-10\n\n\n\nRemoving:\n\n\n kernel                         x86_64             2.6.18-238.1.1.el5              installed                   93 M\n\n\nInstalling for dependencies:\n\n\n monitor-edid                   x86_64             2.5-1.el5.1                     epel                        82 k\n\n\n nmap                           x86_64             2:4.11-1.1                      sl-base                    680 k\n\n\n perl-Crypt-SSLeay              x86_64             0.51-11.el5                     sl-base                     45 k\n\n\n perl-Net-IP                    noarch             1.25-2.fc6                      sl-base                     31 k\n\n\n perl-Net-SSLeay                x86_64             1.30-4.fc6                      sl-base                    192 k\n\n\n perl-Proc-Daemon               noarch             0.03-1.el5                      epel                       9.4 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nInstall       8 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\nRemove        1 Package(s)\n\n\nReinstall     0 Package(s)\n\n\nDowngrade     0 Package(s)\n\n\n\nTotal download size: 22 M\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\n(1/8): perl-Proc-Daemon-0.03-1.el5.noarch.rpm                                                | 9.4 kB     00:00     \n\n\n(2/8): perl-Net-IP-1.25-2.fc6.noarch.rpm                                                     |  31 kB     00:00     \n\n\n(3/8): perl-Crypt-SSLeay-0.51-11.el5.x86_64.rpm                                              |  45 kB     00:00     \n\n\n(4/8): monitor-edid-2.5-1.el5.1.x86_64.rpm                                                   |  82 kB     00:00     \n\n\n(5/8): ocsinventory-agent-1.1.2.1-1.el5.noarch.rpm                                           | 156 kB     00:00     \n\n\n(6/8): perl-Net-SSLeay-1.30-4.fc6.x86_64.rpm                                                 | 192 kB     00:00     \n\n\n(7/8): nmap-4.11-1.1.x86_64.rpm                                                              | 680 kB     00:00     \n\n\n(8/8): kernel-2.6.18-274.3.1.el5.x86_64.rpm                                                  |  21 MB     00:00     \n\n\n--------------------------------------------------------------------------------------------------------------------\n\n\nTotal                                                                               3.5 MB/s |  22 MB     00:06     \n\n\nwarning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 217521f6\n\n\nepel/gpgkey                                                                                  | 1.7 kB     00:00     \n\n\nImporting GPG key 0x217521F6 \nFedora EPEL \nepel@fedoraproject.org\n from /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL\n\n\nIs this ok [y/N]: y\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Installing     : perl-Net-SSLeay                                                                             1/10 \n\n\n  Installing     : nmap                                                                                        2/10 \n\n\n  Installing     : monitor-edid                                                                                3/10 \n\n\n  Installing     : perl-Crypt-SSLeay                                                                           4/10 \n\n\n  Installing     : perl-Net-IP                                                                                 5/10 \n\n\n  Installing     : perl-Proc-Daemon                                                                            6/10 \n\n\n  Installing     : kernel                                                                                      7/10 \n\n\n  Installing     : ocsinventory-agent                                                                          8/10 \n\n\nule, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n\nkernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security\n\n\nObsoleting Packages\n\n\nocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel          \n\n\n    ocsinventory-client.noarch                           0.9.9-10                                     installed     \n\n\n  Erasing        : ocsinventory-client                                                                         9/10 \n\n\nwarning: /etc/ocsinventory-client/ocsinv.conf saved as /etc/ocsinventory-client/ocsinv.conf.rpmsave\n\n\n  Cleanup        : kernel                                                                                     10/10 \n\n\n\nRemoved:\n\n\n  kernel.x86_64 0:2.6.18-238.1.1.el5                                                                                \n\n\n\nInstalled:\n\n\n  kernel.x86_64 0:2.6.18-274.3.1.el5                    ocsinventory-agent.noarch 0:1.1.2.1-1.el5                   \n\n\n\nDependency Installed:\n\n\n  monitor-edid.x86_64 0:2.5-1.el5.1   nmap.x86_64 2:4.11-1.1                perl-Crypt-SSLeay.x86_64 0:0.51-11.el5  \n\n\n  perl-Net-IP.noarch 0:1.25-2.fc6     perl-Net-SSLeay.x86_64 0:1.30-4.fc6   perl-Proc-Daemon.noarch 0:0.03-1.el5    \n\n\n\nReplaced:\n\n\n  ocsinventory-client.noarch 0:0.9.9-10                                                                             \n\n\n\nComplete!\n\n\n\n\n\n\nAdvanced topic: Only geting OSG updates\n\n\nIf you only want to get updates from the OSG repository and \nno other\n repositories, you can tell yum to do that with the following command:\n\n\n[root@client ~] #\n yum --disablerepo\n=\n* --enablerepo\n=\nosg update\n\n\n\n\n\nAdvanced topic: Getting debugging information for installed software\n\n\nIf you run into a problem with our software and have a hankering to debug it directly (or perhaps we need to ask you for some help), you can install so-called \"debuginfo\" packages. These packages will provide debugging symbols and source code so that you can do things like run \ngdb\n or \npstack\n to get information about a program.\n\n\nInstalling the debuginfo package requires three steps.\n\n\n\n\n\n\nEnable the installation of debuginfo packages. This only needs to be done once. Edit the yum repo file, usually \n/etc/yum.repos.d/osg.repo\n to enable the separate debuginfo repository. Near the bottom of the file, you'll see the \nosg-debug\n repo: \n\n\n[osg-debug]\n\n\n\nname\n=\nOSG Software for Enterprise Linux 5 - $basearch - Debug\n\n\nbaseurl\n=\nhttp://repo.grid.iu.edu/osg-release/$basearch/debu\n\n\nfailovermethod\n=\npriority \n\n\npriority\n=\n98 \n\n\nenabled\n=\n1\n\n\ngpgcheck\n=\n1 \n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\nMake sure that \"enabled\" is set to 1.\n\n\n\n\n\n\nFigure out which package installed the program you want to debug. One way to figure it out is to ask RPM. For example, if you want to debug grid-proxy-init:\n\n\n[user@client ~] $\n rpm -qf \n`\nwhich grid-proxy-init\n`\n\n\nglobus-proxy-utils-5.0-5.osg.x86_64\n\n\n\n\n\n\n\n\n\n\nInstall the debugging information for that package. Continuing this example: \n\n\n[root@client ~] #\n debuginfo-install globus-proxy-utils\n\n...\n\n\n=================================================================================================================================\n\n\n Package                                      Arch                   Version                     Repository                 Size\n\n\n=================================================================================================================================\n\n\nInstalling:\n\n\n globus-proxy-utils-debuginfo                 x86_64                 5.0-5.osg                   osg-debug                  61 k\n\n\n\nTransaction Summary\n\n\n=================================================================================================================================\n\n\nInstall       1 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\n\nTotal download size: 61 k\n\n\nIs this ok [y/N]: y\n\n\n...\n\n\nInstalled:\n\n\n  globus-proxy-utils-debuginfo.x86_64 0:5.0-5.osg\n\n\n\n\n\n\nThis last step will select the right package name, then use \nyum\n to install it.\n\n\n\n\n\n\nTroubleshooting\n\n\nYum not finding packages\n\n\nIf you is not finding some packages, e.g.:\n\n\nError Downloading Packages:\n  packageXYZ: failure: packageXYZ.rpm from osg: [Errno 256] No more mirrors to try.\n\n\n\n\n\nthen you can try cleaning up Yum's cache: \n\n\n[root@client ~] #\n yum clean all --enablerpeo\n=\n*\n\n\n\n\n\nto make an even more thorough job you can follow also add:\n\n\n[root@client ~] #\n yum clean expire-cache --enablerepo\n=\n*\n\n\n\n\n\n\n\nNote\n\n\nyum clean\n cleans only enabled repositories. If you want to also clean any (temporarily) disabled repositories you need to use \n--enablerepo=\u2019*\u2019\n option.\n\n\n\n\nYum complaining about missing keys\n\n\nIf yum is complaining you can re-import the keys in your distribution: \n\n\n[root@client ~] #\n rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY*\n\n\n\n\n\nReferences\n\n\n\n\nThe main yum web site\n\n\nA good description of the commands for RPM and yum can be found at \nLearn Linux 101: RPM and YUM Package Management\n.", 
            "title": "Yum Basics"
        }, 
        {
            "location": "/release/yum-basics/#basics-of-using-yum-and-rpm", 
            "text": "", 
            "title": "Basics of using yum and RPM"
        }, 
        {
            "location": "/release/yum-basics/#about-this-document", 
            "text": "This document introduces package management tools that help you install, update, and remove packages. OSG uses RPMs (the Red Hat Packaging Manager) to package its software. While RPM is the packaging format,  yum  is the command you will use to do the installation. For example,  yum  will resolve and download the dependencies for the package you want to install;  rpm  will simply complain if you want to install a package that does not have all its dependencies installed.", 
            "title": "About This Document"
        }, 
        {
            "location": "/release/yum-basics/#installation", 
            "text": "Installation is done with the  yum install  command. Each of the individual installation guide shows you the correct command to use to do an installation. Here is an example installation with all of the output from yum.  [root@client ~] #  sudo yum install osg-ca-certs Loaded plugins: kernel-module, priorities  epel                                                                                         | 3.7 kB     00:00       epel/primary_db                                                                              | 3.8 MB     00:00       fermi-base                                                                                   | 2.1 kB     00:00       fermi-base/primary_db                                                                        |  48 kB     00:00       fermi-security                                                                               | 1.9 kB     00:00       fermi-security/primary_db                                                                    | 1.7 MB     00:00       osg                                                                                          | 1.9 kB     00:00       osg/primary_db                                                                               |  65 kB     00:00       sl-base                                                                                      | 2.1 kB     00:00       sl-base/primary_db                                                                           | 2.0 MB     00:00       957 packages excluded due to repository priority protections  Setting up Install Process  Resolving Dependencies  --  Running transaction check  ---  Package osg-ca-certs.noarch 0:1.23-1 set to be updated  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  Dependencies Resolved  ====================================================================================================================   Package                         Arch                      Version                     Repository              Size  ====================================================================================================================  Installing:   osg-ca-certs                    noarch                    1.23-1                      osg                    450 k  Transaction Summary  ====================================================================================================================  Install       1 Package(s)  Upgrade       0 Package(s)  Total download size: 450 k  Is this ok [y/N]: y  Downloading Packages:  osg-ca-certs-1.23-1.noarch.rpm                                                               | 450 kB     00:00       warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603  osg/gpgkey                                                                                   | 1.7 kB     00:00       Importing GPG key 0x824B8603  OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org  from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  Is this ok [y/N]: y  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Installing     : osg-ca-certs                                                                                 1/1   Installed:    osg-ca-certs.noarch 0:1.23-1                                                                                        Complete!   Please Note : When you first install a package from the OSG repository, you will be prompted to import the GPG key. We use this key to sign our RPMs as a security measure. You should double-check the key id (above it is 824B8603) with the  information on our signed RPMs . If it doesn't match, there is a problem somewhere and you should report it to the OSG via goc@opensciencegrid.org.", 
            "title": "Installation"
        }, 
        {
            "location": "/release/yum-basics/#verifying-packages-and-installations", 
            "text": "You can check if an RPM has been modified. For instance, to check to see if any files have been modified in the  osg-ca-certs  RPM you just installed:  [user@client ~] $  rpm --verify osg-ca-certs  The lack of any output means there were no problems. If you would like to see all the files for which there are no problems, you can do:  [user@client ~] $  rpm --verify -v osg-ca-certs ........    /etc/grid-security/certificates  ........    /etc/grid-security/certificates/0119347c.0  ........    /etc/grid-security/certificates/0119347c.namespaces  ........    /etc/grid-security/certificates/0119347c.signing_policy  ... etc ...   Each dot indicates a specific check that was made and passed. If someone had modified a file, you might see this:  [user@client ~] $  rpm --verify osg-ca-certs ..5....T    /etc/grid-security/certificates/ffc3d59b.0   This means the files MD5 checksum has changed (so the contents have changed) and the timestamp is different. The complete set of changes you might see (copied from the  rpm  man page) are:     Letter  Meaning      S  file Size differs    M  Mode differs (includes permissions and file type)    5  MD5 sum differs    D  Device major/minor number mismatch    L  readLink(2) path mismatch    U  User ownership differs    G  Group ownership differs    T  mTime differs     If you don't care about some of those changes, you can tell rpm to ignore them. For instance, to ignore changes in the modification time:  [user@client ~] $  rpm --verify --nomtime osg-ca-certs ..5.....    /etc/grid-security/certificates/ffc3d59b.0", 
            "title": "Verifying Packages and Installations"
        }, 
        {
            "location": "/release/yum-basics/#understanding-a-package", 
            "text": "If you want to know what package a file belongs to, you can ask rpm. For instance, if you're curious what package contains the  srm-ls  command, you can do:  #   1 . Find the exact path [user@client ~] $  which srm-ls /usr/bin/srm-ls  #   2 . Ask rpm what package it is part of: [user@client ~] $  rpm -q --file /usr/bin/srm-ls bestman2-client-2.2.0-14.osg.el5.noarch   If you want to know what other things are in a package--perhaps the other available tools or configuration files--you can do that as well:  [user@client ~] $  rpm -ql bestman2-client /etc/bestman2/conf/bestman2.rc  /etc/bestman2/conf/bestman2.rc.samples  /etc/bestman2/conf/srmclient.conf  /etc/sysconfig/bestman2  /usr/bin/srm-copy  /usr/bin/srm-copy-status  /usr/bin/srm-extendfilelifetime  /usr/bin/srm-ls  /usr/bin/srm-ls-status  ... output trimmed ...", 
            "title": "Understanding a package"
        }, 
        {
            "location": "/release/yum-basics/#what-else-does-a-package-install", 
            "text": "Sometimes you need to understand what other software is installed by a package. This can be particularly useful for understanding  meta-packages , which are packages such as the  osg-wn-client  (worker node client) that contain nothing by themselves but only depend on other RPMs. To do this, use the  --requires  option to rpm. For example, you can see that the worker node client (as of OSG 3.1.8 in early September, 2012) will install  curl ,  uberftp ,  lcg-utils , and a dozen or so other packages.  [user@client ~] $  rpm -q --requires osg-wn-client /usr/bin/curl    /usr/bin/dccp    /usr/bin/ldapsearch    /usr/bin/uberftp    /usr/bin/wget    bestman2-client    config(osg-wn-client) = 3.0.0-16.osg.el5  dcache-srmclient    dcap-tunnel-gsi    edg-gridftp-client    fetch-crl    glite-fts-client    globus-gass-copy-progs    grid-certificates    java-1.6.0-sun-compat    lcg-utils    lfc-client    lfc-python    myproxy    osg-system-profiler    osg-version    rpmlib(CompressedFileNames)  = 3.0.4-1  rpmlib(PayloadFilesHavePrefix)  = 4.0-1  vo-client", 
            "title": "What else does a package install?"
        }, 
        {
            "location": "/release/yum-basics/#finding-rpm-packages", 
            "text": "It is normally best to read the OSG documentation to decide which packages to install because it may not be obvious what the correct packages to install are. That said, you can use yum to find out all sort of things. For instance, you can list packages that begin with \"voms\":  [user@client ~] $  yum list  voms*  Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  Available Packages  voms.i386                                                    2.0.6-3.osg                                        osg   voms.x86_64                                                  2.0.6-3.osg                                        osg   voms-admin-client.x86_64                                     2.0.16-1                                           osg   voms-admin-server.noarch                                     2.6.1-9                                            osg   voms-clients.x86_64                                          2.0.6-3.osg                                        osg   voms-compat.i386                                             1.9.19.2-6.osg                                     osg   voms-compat.x86_64                                           1.9.19.2-6.osg                                     osg   voms-devel.i386                                              2.0.6-3.osg                                        osg   voms-devel.x86_64                                            2.0.6-3.osg                                        osg   voms-doc.x86_64                                              2.0.6-3.osg                                        osg   voms-mysql-plugin.x86_64                                     3.1.5.1-1.el5                                      epel  voms-server.x86_64                                           2.0.6-3.osg                                        osg   vomsjapi.x86_64                                              2.0.6-3.osg                                        osg   vomsjapi-javadoc.x86_64                                      2.0.6-3.osg                                        osg   If you want to search for packages that contain VOMS anywhere in the name or description, you can use  yum search :  [user@client ~] $  yum search voms Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  ================================================== Matched: voms ===================================================  osg-voms.noarch : OSG VOMS  perl-VOMS-Lite.noarch : Perl extension for VOMS Attribute certificate creation  perl-voms-server.noarch : Perl extension for VOMS Attribute certificate creation  php-voms-admin.noarch : Web based interface to control VOMS parameters written in PHP  voms.i386 : Virtual Organization Membership Service  voms.x86_64 : Virtual Organization Membership Service  ... etc ...   One last example, if you want to know what RPM would give you the  voms-proxy-init  command, you can ask  yum . The  *  indicates that you don't know the full pathname of  voms-proxy-init .  [user@client ~] $  yum whatprovides  *voms-proxy-init  Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  voms-clients-2.0.6-3.osg.x86_64 : Virtual Organization Membership Service Clients  Repo        : osg  Matched from:  Filename    : /usr/bin/voms-proxy-init", 
            "title": "Finding RPM Packages"
        }, 
        {
            "location": "/release/yum-basics/#removing-packages", 
            "text": "To remove a single RPM, you can use  yum remove . Not only will it uninstall the RPM you requested, but it will uninstall anything that depends on it. For example, if I previously installed the  voms-clients  package, I also installed another package it depends on called  voms . If I remove  voms , yum will also remove  voms-clients :  [user@client ~] $  sudo yum remove voms Loaded plugins: kernel-module, priorities  Setting up Remove Process  Resolving Dependencies  --  Running transaction check  ---  Package voms.x86_64 0:2.0.6-3.osg set to be erased  --  Processing Dependency: libvomsapi.so.1()(64bit) for package: voms-clients  --  Processing Dependency: voms = 2.0.6-3.osg for package: voms-clients  --  Running transaction check  ---  Package voms-clients.x86_64 0:2.0.6-3.osg set to be erased  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  Dependencies Resolved  ====================================================================================================================   Package                      Arch                   Version                        Repository                 Size  ====================================================================================================================  Removing:   voms                         x86_64                 2.0.6-3.osg                    installed                 407 k  Removing for dependencies:   voms-clients                 x86_64                 2.0.6-3.osg                    installed                 373 k  Transaction Summary  ====================================================================================================================  Remove        2 Package(s)  Reinstall     0 Package(s)  Downgrade     0 Package(s)  Is this ok [y/N]: y  Downloading Packages:  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Erasing        : voms                                                                                         1/2     Erasing        : voms-clients                                                                                 2/2   Removed:    voms.x86_64 0:2.0.6-3.osg                                                                                           Dependency Removed:    voms-clients.x86_64 0:2.0.6-3.osg                                                                                   Complete!", 
            "title": "Removing Packages"
        }, 
        {
            "location": "/release/yum-basics/#upgrading-packages", 
            "text": "You can check for updates with  yum check-update . For example:  [root@client ~] #  yum check-update Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  kernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security  Obsoleting Packages  ocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel                ocsinventory-client.noarch                           0.9.9-10                                     installed        You can do the update with  yum update . Note that in this case we got more than was listed due to dependencies that needed to be resolved:  [root@client ~] #  yum update 957 packages excluded due to repository priority protections  Setting up Update Process  Resolving Dependencies  --  Running transaction check  ---  Package kernel.x86_64 0:2.6.18-274.3.1.el5 set to be installed  ---  Package ocsinventory-agent.noarch 0:1.1.2.1-1.el5 set to be updated  --  Processing Dependency: perl(Crypt::SSLeay) for package: ocsinventory-agent  --  Processing Dependency: perl(Proc::Daemon) for package: ocsinventory-agent  --  Processing Dependency: monitor-edid for package: ocsinventory-agent  --  Processing Dependency: perl(Net::IP) for package: ocsinventory-agent  --  Processing Dependency: nmap for package: ocsinventory-agent  --  Processing Dependency: perl(Net::SSLeay) for package: ocsinventory-agent  --  Running transaction check  ---  Package monitor-edid.x86_64 0:2.5-1.el5.1 set to be updated  ---  Package nmap.x86_64 2:4.11-1.1 set to be updated  ---  Package perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 set to be updated  ---  Package perl-Net-IP.noarch 0:1.25-2.fc6 set to be updated  ---  Package perl-Net-SSLeay.x86_64 0:1.30-4.fc6 set to be updated  ---  Package perl-Proc-Daemon.noarch 0:0.03-1.el5 set to be updated  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  --  Running transaction check  ---  Package kernel.x86_64 0:2.6.18-238.1.1.el5 set to be erased  --  Finished Dependency Resolution  Dependencies Resolved  ====================================================================================================================   Package                        Arch               Version                         Repository                  Size  ====================================================================================================================  Installing:   kernel                         x86_64             2.6.18-274.3.1.el5              fermi-security              21 M   ocsinventory-agent             noarch             1.1.2.1-1.el5                   epel                       156 k       replacing  ocsinventory-client.noarch 0.9.9-10  Removing:   kernel                         x86_64             2.6.18-238.1.1.el5              installed                   93 M  Installing for dependencies:   monitor-edid                   x86_64             2.5-1.el5.1                     epel                        82 k   nmap                           x86_64             2:4.11-1.1                      sl-base                    680 k   perl-Crypt-SSLeay              x86_64             0.51-11.el5                     sl-base                     45 k   perl-Net-IP                    noarch             1.25-2.fc6                      sl-base                     31 k   perl-Net-SSLeay                x86_64             1.30-4.fc6                      sl-base                    192 k   perl-Proc-Daemon               noarch             0.03-1.el5                      epel                       9.4 k  Transaction Summary  ====================================================================================================================  Install       8 Package(s)  Upgrade       0 Package(s)  Remove        1 Package(s)  Reinstall     0 Package(s)  Downgrade     0 Package(s)  Total download size: 22 M  Is this ok [y/N]: y  Downloading Packages:  (1/8): perl-Proc-Daemon-0.03-1.el5.noarch.rpm                                                | 9.4 kB     00:00       (2/8): perl-Net-IP-1.25-2.fc6.noarch.rpm                                                     |  31 kB     00:00       (3/8): perl-Crypt-SSLeay-0.51-11.el5.x86_64.rpm                                              |  45 kB     00:00       (4/8): monitor-edid-2.5-1.el5.1.x86_64.rpm                                                   |  82 kB     00:00       (5/8): ocsinventory-agent-1.1.2.1-1.el5.noarch.rpm                                           | 156 kB     00:00       (6/8): perl-Net-SSLeay-1.30-4.fc6.x86_64.rpm                                                 | 192 kB     00:00       (7/8): nmap-4.11-1.1.x86_64.rpm                                                              | 680 kB     00:00       (8/8): kernel-2.6.18-274.3.1.el5.x86_64.rpm                                                  |  21 MB     00:00       --------------------------------------------------------------------------------------------------------------------  Total                                                                               3.5 MB/s |  22 MB     00:06       warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 217521f6  epel/gpgkey                                                                                  | 1.7 kB     00:00       Importing GPG key 0x217521F6  Fedora EPEL  epel@fedoraproject.org  from /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL  Is this ok [y/N]: y  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Installing     : perl-Net-SSLeay                                                                             1/10     Installing     : nmap                                                                                        2/10     Installing     : monitor-edid                                                                                3/10     Installing     : perl-Crypt-SSLeay                                                                           4/10     Installing     : perl-Net-IP                                                                                 5/10     Installing     : perl-Proc-Daemon                                                                            6/10     Installing     : kernel                                                                                      7/10     Installing     : ocsinventory-agent                                                                          8/10   ule, priorities  957 packages excluded due to repository priority protections  kernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security  Obsoleting Packages  ocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel                ocsinventory-client.noarch                           0.9.9-10                                     installed         Erasing        : ocsinventory-client                                                                         9/10   warning: /etc/ocsinventory-client/ocsinv.conf saved as /etc/ocsinventory-client/ocsinv.conf.rpmsave    Cleanup        : kernel                                                                                     10/10   Removed:    kernel.x86_64 0:2.6.18-238.1.1.el5                                                                                  Installed:    kernel.x86_64 0:2.6.18-274.3.1.el5                    ocsinventory-agent.noarch 0:1.1.2.1-1.el5                     Dependency Installed:    monitor-edid.x86_64 0:2.5-1.el5.1   nmap.x86_64 2:4.11-1.1                perl-Crypt-SSLeay.x86_64 0:0.51-11.el5      perl-Net-IP.noarch 0:1.25-2.fc6     perl-Net-SSLeay.x86_64 0:1.30-4.fc6   perl-Proc-Daemon.noarch 0:0.03-1.el5      Replaced:    ocsinventory-client.noarch 0:0.9.9-10                                                                               Complete!", 
            "title": "Upgrading Packages"
        }, 
        {
            "location": "/release/yum-basics/#advanced-topic-only-geting-osg-updates", 
            "text": "If you only want to get updates from the OSG repository and  no other  repositories, you can tell yum to do that with the following command:  [root@client ~] #  yum --disablerepo = * --enablerepo = osg update", 
            "title": "Advanced topic: Only geting OSG updates"
        }, 
        {
            "location": "/release/yum-basics/#advanced-topic-getting-debugging-information-for-installed-software", 
            "text": "If you run into a problem with our software and have a hankering to debug it directly (or perhaps we need to ask you for some help), you can install so-called \"debuginfo\" packages. These packages will provide debugging symbols and source code so that you can do things like run  gdb  or  pstack  to get information about a program.  Installing the debuginfo package requires three steps.    Enable the installation of debuginfo packages. This only needs to be done once. Edit the yum repo file, usually  /etc/yum.repos.d/osg.repo  to enable the separate debuginfo repository. Near the bottom of the file, you'll see the  osg-debug  repo:   [osg-debug]  name = OSG Software for Enterprise Linux 5 - $basearch - Debug  baseurl = http://repo.grid.iu.edu/osg-release/$basearch/debu  failovermethod = priority   priority = 98   enabled = 1  gpgcheck = 1   gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG   Make sure that \"enabled\" is set to 1.    Figure out which package installed the program you want to debug. One way to figure it out is to ask RPM. For example, if you want to debug grid-proxy-init:  [user@client ~] $  rpm -qf  ` which grid-proxy-init `  globus-proxy-utils-5.0-5.osg.x86_64     Install the debugging information for that package. Continuing this example:   [root@client ~] #  debuginfo-install globus-proxy-utils ...  =================================================================================================================================   Package                                      Arch                   Version                     Repository                 Size  =================================================================================================================================  Installing:   globus-proxy-utils-debuginfo                 x86_64                 5.0-5.osg                   osg-debug                  61 k  Transaction Summary  =================================================================================================================================  Install       1 Package(s)  Upgrade       0 Package(s)  Total download size: 61 k  Is this ok [y/N]: y  ...  Installed:    globus-proxy-utils-debuginfo.x86_64 0:5.0-5.osg   This last step will select the right package name, then use  yum  to install it.", 
            "title": "Advanced topic: Getting debugging information for installed software"
        }, 
        {
            "location": "/release/yum-basics/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/release/yum-basics/#yum-not-finding-packages", 
            "text": "If you is not finding some packages, e.g.:  Error Downloading Packages:\n  packageXYZ: failure: packageXYZ.rpm from osg: [Errno 256] No more mirrors to try.  then you can try cleaning up Yum's cache:   [root@client ~] #  yum clean all --enablerpeo = *  to make an even more thorough job you can follow also add:  [root@client ~] #  yum clean expire-cache --enablerepo = *   Note  yum clean  cleans only enabled repositories. If you want to also clean any (temporarily) disabled repositories you need to use  --enablerepo=\u2019*\u2019  option.", 
            "title": "Yum not finding packages"
        }, 
        {
            "location": "/release/yum-basics/#yum-complaining-about-missing-keys", 
            "text": "If yum is complaining you can re-import the keys in your distribution:   [root@client ~] #  rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY*", 
            "title": "Yum complaining about missing keys"
        }, 
        {
            "location": "/release/yum-basics/#references", 
            "text": "The main yum web site  A good description of the commands for RPM and yum can be found at  Learn Linux 101: RPM and YUM Package Management .", 
            "title": "References"
        }, 
        {
            "location": "/release/signing/", 
            "text": "OSG Release Signing Information\n\n\nVerifying OSG's RPMs\n\n\nWe use a GPG key to sign our software packages. Normally \nyum\n and \nrpm\n transparently use the GPG signatures to verify the packages have not been corrupted and were created by us. You get our GPG public key when you install the \nosg-release\n RPM.\n\n\nIf you wish to verify one of our RPMs manually, you can run:\n\n\n$\n rpm --checksig -v \nNAME.RPM\n\n\n\n\n\n\nFor example:\n\n\n$\n rpm --checksig -v globus-core-8.0-2.osg.x86_64.rpm \n\nglobus-core-8.0-2.osg.x86_64.rpm:\n\n\n    Header V3 DSA signature: OK, key ID 824b8603\n\n\n    Header SHA1 digest: OK (2b5af4348c548c27f10e2e47e1ec80500c4f85d7)\n\n\n    MD5 digest: OK (d11503a229a1a0e02262034efe0f7e46)\n\n\n    V3 DSA signature: OK, key ID 824b8603\n\n\n\n\n\n\nThe OSG Packaging Signing Key\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\n/etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\nDownload\n\n\nGitHub\n\n\n\n\n\n\nDownload\n\n\nUW-Madison\n\n\n\n\n\n\nFingerprint\n\n\n6459 !D9D2 AAA9 AB67 A251  FB44 2110 !B1C8 824B 8603\n\n\n\n\n\n\nKey ID\n\n\n824b8603\n\n\n\n\n\n\n\n\nYou can see the fingerprint for yourself:\n\n\n$\n gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\npub  1024D/824B8603 2011-09-15 OSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n\n\n      Key fingerprint = 6459 D9D2 AAA9 AB67 A251  FB44 2110 B1C8 824B 8603\n\n\nsub  2048g/28E5857C 2011-09-15", 
            "title": "RPM Signing"
        }, 
        {
            "location": "/release/signing/#osg-release-signing-information", 
            "text": "", 
            "title": "OSG Release Signing Information"
        }, 
        {
            "location": "/release/signing/#verifying-osgs-rpms", 
            "text": "We use a GPG key to sign our software packages. Normally  yum  and  rpm  transparently use the GPG signatures to verify the packages have not been corrupted and were created by us. You get our GPG public key when you install the  osg-release  RPM.  If you wish to verify one of our RPMs manually, you can run:  $  rpm --checksig -v  NAME.RPM   For example:  $  rpm --checksig -v globus-core-8.0-2.osg.x86_64.rpm  globus-core-8.0-2.osg.x86_64.rpm:      Header V3 DSA signature: OK, key ID 824b8603      Header SHA1 digest: OK (2b5af4348c548c27f10e2e47e1ec80500c4f85d7)      MD5 digest: OK (d11503a229a1a0e02262034efe0f7e46)      V3 DSA signature: OK, key ID 824b8603", 
            "title": "Verifying OSG's RPMs"
        }, 
        {
            "location": "/release/signing/#the-osg-packaging-signing-key", 
            "text": "Location  /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG    Download  GitHub    Download  UW-Madison    Fingerprint  6459 !D9D2 AAA9 AB67 A251  FB44 2110 !B1C8 824B 8603    Key ID  824b8603     You can see the fingerprint for yourself:  $  gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG pub  1024D/824B8603 2011-09-15 OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org        Key fingerprint = 6459 D9D2 AAA9 AB67 A251  FB44 2110 B1C8 824B 8603  sub  2048g/28E5857C 2011-09-15", 
            "title": "The OSG Packaging Signing Key"
        }, 
        {
            "location": "/release/contrib/", 
            "text": "Contrib software\n\n\nWhat is the contrib software repository?\n\n\nIn addition to our regular software repository, we also have a \ncontrib\n (short for \"contributed\") software repository. This is software that is does not go through the same software testing and release processes as the official OSG Software release, but may be useful to you.\n\n\nCaveats\n\n\nYou should be aware:\n\n\n\n\nThe software in the contrib repository may have been more lightly tested than software in our official release. In particular, we do not test all of the software in the contrib repository together, and there is no guarantee all the software in compatible or even that it works.\n\n\nIf you have problems with the software in the contrib repository, you can ask us for help, but you may be on your own. This software is not officially supported and we may not have time to help you with it.\n\n\n\n\nDo you want more software in the repository?\n\n\nWe're happy to work with you to add more software to the contrib repository. If you provide the RPM packaging (preferably a spec file, so we can build it), it will get into the repository than if we have to do the packaging ourselves.\n\n\nWhat software is in the contrib repository?\n\n\nThe definitive list of software in the contrib repository is the repository itself. You can browse:\n\n\n\n\nThe OSG 3.4 EL6 contrib software repository\n\n\nThe OSG 3.4 EL7 contrib software repository\n\n\n\n\nThe software includes:\n\n\n\n\ngsh:\n Gsh is the \"grid shell\". It acts like a simple shell, something like bash, but with fewer features. All of the commands are run with globus-job-run against a remote Compute Element. This can be a very convenient way to debug a Compute Element.\n\n\nglideinwms\n: The \nosg-contrib\n repository contains the development series for the GlideinWMS workload management system. This system submits condor-based pilots to grid sites to create a dynamic worker node pool for user jobs. Contained in the RPMs are packages for frontend and factory installations. This repository contains the development series that has newer features still in wider testing. At the time of writing, this series is the version 3 with new features for submission to clouds and to TeraGrid sites using corral frontends. For more stable installations, use the \nosg-release\n or \nosg-testing\n repositories.\n\n\nxrootd-cmstfc\n: This package is for a plugin for xrootd storage system to provide a static rule-based logical to physical mapping. This is primarily used by the CMS experiment.\n\n\nxrootd-status-probe\n: This package is a simple nagios probe for xrootd that verifies the first kilobyte of a multi-GB file.\n\n\ncms-xrootd\n: Meta-packages containing sample configuration files for joining the CMS xrootd infrastructure.\n\n\n\n\nInstalling from the contrib repository\n\n\nTo install software, you need to enable the osg-cotrib repository. The easiest way to do that is on case-by-case basis in the \nyum\n command-line. For example, to install \ngsh\n:\n\n\n[root@client ~] $\n yum install --enablerepo\n=\nosg-contrib gsh", 
            "title": "Contrib Software"
        }, 
        {
            "location": "/release/contrib/#contrib-software", 
            "text": "", 
            "title": "Contrib software"
        }, 
        {
            "location": "/release/contrib/#what-is-the-contrib-software-repository", 
            "text": "In addition to our regular software repository, we also have a  contrib  (short for \"contributed\") software repository. This is software that is does not go through the same software testing and release processes as the official OSG Software release, but may be useful to you.", 
            "title": "What is the contrib software repository?"
        }, 
        {
            "location": "/release/contrib/#caveats", 
            "text": "You should be aware:   The software in the contrib repository may have been more lightly tested than software in our official release. In particular, we do not test all of the software in the contrib repository together, and there is no guarantee all the software in compatible or even that it works.  If you have problems with the software in the contrib repository, you can ask us for help, but you may be on your own. This software is not officially supported and we may not have time to help you with it.", 
            "title": "Caveats"
        }, 
        {
            "location": "/release/contrib/#do-you-want-more-software-in-the-repository", 
            "text": "We're happy to work with you to add more software to the contrib repository. If you provide the RPM packaging (preferably a spec file, so we can build it), it will get into the repository than if we have to do the packaging ourselves.", 
            "title": "Do you want more software in the repository?"
        }, 
        {
            "location": "/release/contrib/#what-software-is-in-the-contrib-repository", 
            "text": "The definitive list of software in the contrib repository is the repository itself. You can browse:   The OSG 3.4 EL6 contrib software repository  The OSG 3.4 EL7 contrib software repository   The software includes:   gsh:  Gsh is the \"grid shell\". It acts like a simple shell, something like bash, but with fewer features. All of the commands are run with globus-job-run against a remote Compute Element. This can be a very convenient way to debug a Compute Element.  glideinwms : The  osg-contrib  repository contains the development series for the GlideinWMS workload management system. This system submits condor-based pilots to grid sites to create a dynamic worker node pool for user jobs. Contained in the RPMs are packages for frontend and factory installations. This repository contains the development series that has newer features still in wider testing. At the time of writing, this series is the version 3 with new features for submission to clouds and to TeraGrid sites using corral frontends. For more stable installations, use the  osg-release  or  osg-testing  repositories.  xrootd-cmstfc : This package is for a plugin for xrootd storage system to provide a static rule-based logical to physical mapping. This is primarily used by the CMS experiment.  xrootd-status-probe : This package is a simple nagios probe for xrootd that verifies the first kilobyte of a multi-GB file.  cms-xrootd : Meta-packages containing sample configuration files for joining the CMS xrootd infrastructure.", 
            "title": "What software is in the contrib repository?"
        }, 
        {
            "location": "/release/contrib/#installing-from-the-contrib-repository", 
            "text": "To install software, you need to enable the osg-cotrib repository. The easiest way to do that is on case-by-case basis in the  yum  command-line. For example, to install  gsh :  [root@client ~] $  yum install --enablerepo = osg-contrib gsh", 
            "title": "Installing from the contrib repository"
        }, 
        {
            "location": "/common/pki-cli/", 
            "text": "OSG PKI Command Line Clients\n\n\nOverview\n\n\nThe OSG PKI Command Line Clients provide a command-line interface for requesting and issuing host certificates from the OSG PKI. They complement the \nOIM Web Interface\n.\n\n\nPrerequisites\n\n\nIf you have not already done so, you need to \nconfigure the OSG software repositories\n.\n\n\nInstallation\n\n\nThe command-line scripts have been packaged as an RPM and are available from the OSG repositories.\n\n\nTo install the RPM, run:\n\n\n[root@client ~] #\n yum install osg-pki-tools\n\n\n\n\n\nUsage\n\n\nConfiguration Files\n\n\nThis configuration file contains information divided into two sections for testing and production.  Configuration variables include:\n\n\n\n\nRequest URL\n\n\nApprove URL\n\n\nRetrieve URL\n\n\nHost URL\n\n\n\n\nThese parameters are used as input to the script depending upon the mode of execution of the script (test or OIM). The command-line utilities check for configuration files in the following order:\n\n\n\n\n$HOME/.osg-pki/OSG_PKI.ini\n\n\n./pki-clients.ini\n\n\n/etc/pki-clients.ini\n\n\n\n\nosg-cert-request\n\n\nSends a request for a host certificate.\n\n\nThis script generates a private key and submits a request for a certificate to the OSG PKI. The request will be approved by an appropriate Grid Admin. You will receive an email when this approval has been completed containing directions on how to run \nosg-cert-retreive\n to retrieve the certificate. It works in two modes:\n\n\n\n\nCSR is provided by the user: the CSR provided is sent to the OIM.\n\n\nCSR is not provided by the user: the script generates a private key for the user. Writes it to default key file name or the one specified by \n-o\n.\n\n\n\n\nThis script:\n\n\n\n\nGenerates a new host private key and CSR (the only important part of CSR is \nCN=\nHOSTNAME\n component).\n\n\nSaves the host private key to disk (as specified by the user).\n\n\nAuthenticates to OIM and posts the CSR as a request to OIM.\n\n\nReturns the request ID to the user.\n\n\n\n\nIf the user provides the CSR, then this script would just send the same CSR to OIM.\n\n\nInputs:\n\n\n\n\nfully-qualified hostname\n\n\nfilename to store private key (optional; default is \n./hostkey.pem\n).\n\n\npath to user's certificate (optional: default is path specified by \n$X509_USER_CERT\n environment variable, then \n~/.globus/usercert.pem\n).\n\n\npath to user's private key (optional: default is path specified by \n$X509_USER_KEY\n environment variable, then \n~/.globus/userkey.pem\n).\n\n\nPassphrase for user's private key (via non-echoing prompt).\n\n\nUser needs to provide VO name if the requested hostname has multiple VO's assigned.\n\n\n\n\nOutputs:\n\n\n\n\nPrivate key, to filename specified by \n-o\n or \n./hostkey.pem\n by default.\n\n\nRequest Id, to \nstdout\n.\n\n\n\n\n[root@client ~] #\n osg-cert-request --help\n\nUsage: osg-cert-request [options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -c CSR, --csr=CSR     Specify CSR name (default = gennew.csr)\n\n\n  -o OUTPUT KEYFILE, --outkeyfile=OUTPUT KEYFILE\n\n\n                        Specify the output filename for the retrieved user certificate.\n\n\n                        Default is ./hostkey.pem\n\n\n  -v VO name, --vo=VO name\n\n\n                        Specify the VO for the host request\n\n\n  -y CC LIST, --cc=CC LIST\n\n\n                        Specify the CC list(the email id\ns to be CCed).\n\n\n                        Separate values by \n,\n\n\n  -m COMMENT, --comment=COMMENT\n\n\n                        The comment to be added to the request\n\n\n  -H CN, --hostname=CN  Specify a hostname for CSR (FQDN)\n\n\n  -a HOSTNAME, --altname=HOSTNAME\n\n\n                        Specify an alternative hostname for the CSR (FQDN). May be used more than once\n\n\n  -e EMAIL, --email=EMAIL\n\n\n                        Email address to receive certificate\n\n\n  -n NAME, --name=NAME  Name of user receiving certificate\n\n\n  -p PHONE, --phone=PHONE\n\n\n                        Phone number of user receiving certificate\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -T, --test            Run in test mode\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY\n\n\n                        Write the output files to this directory\n\n\n  -V, --version         Print version information and exit\n\n\n\n\n\n\nExamples.\n\n\nOSG generates the key pair for the request.\n\n\n[root@client ~] #\n osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n \nYour Name\n -p \n9999999999\n -y \nxyz@domain.com,abc@domain.com\n -m \nThis is my comment\n\n\n\n\n\n\nIf you want to request a service certificate, you need to escape backslash for service name inside CN like following.\n\n\n[root@client ~] #\n osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n \nYour Name\n -p \n9999999999\n -y \nrsv\\/xyz@domain.com\n -m \nThis is my comment\n\n\n\n\n\n\nYou can create your CSR on your target hosts using tools such as \nopenssl\n.\n\n\n[root@client ~] #\n \numask\n \n077\n;\n openssl req -new -newkey rsa:2048 -nodes -keyout hostkey.pem -subj \n/CN=osg-ce.example.edu\n -out csr.pem\n\n\n\n\n\nNote that the DN will be overriden by the OSG PKI except for the CN component.\n\n\nSubmitting the request:\n\n\n[root@client ~] #\n osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n \nYour Name\n -p \n9999999999\n -y \nxyz@domain.com,abc@domain.com\n -m \nThis is my comment\n --csr csr.pem\n\n\n\n\n\nosg-cert-retrieve\n\n\nRetrieve a certificate (host or user) from OIM given a request Id. Typically you will run this script after submitting a request with \nosg-cert-request\n and receiving an email telling you your certificate has been approved.\n\n\nYou can also use this script to retrieve other certificates that have been previously issued (assuming you know their request ID number).\n\n\nSince certificates are public, no authentication of the user is required.\n\n\nThis script:\n\n\n\n\nAccepts a request Id from the user\n\n\nConnects to OIM and requests the certificate identified by the request ID\n\n\nWrites the certificate to disk (as specified by the user)\n\n\n\n\nInputs:\n\n\n\n\nRequest ID\n\n\nFilename to store certificate (optional: default is \n./hostcert.pem\n).\n\n\n\n\nOutputs:\n\n\n\n\nHost certificate as PEM, to filename specified or \n./hostcert.pem\n.\n\n\n\n\n[root@client ~] #\n osg-cert-retrieve --help\n\nUsage: osg-cert-retrieve [options] \nRequest ID\n\n\nUsage: osg-cert-retrieve -h/--help [for detailed explanations of options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -o ID, --certfile=ID  Specify the output filename for the retrieved user\n\n\n                        certificate . Default is ./hostcert.pem\n\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY\n\n\n                        Write the output files to this directory\n\n\n  -V, --version         Print version information and exit\n\n\n  -i, --id              Specify ID# of certificate to be retrieved\n\n\n                        [deprecated]\n\n\n\n\n\n\nExample:\n\n\n[root@client ~] #\n osg-cert-retrieve -i \n555\n\n\n\n\n\n\nosg-gridadmin-cert-request\n\n\nRequest and retrieve multliple host certificates from OIM. Authenticates to OIM and is only for use by Grid Admins for certificates they are authorized to approve. This script is only supported with all hosts being in the same domain (so we ensure they go to the same Grid Admin). The certificates are stored with the format of \nhostname-requestid.pem\n (i.e. the id generated from the request for the certificate). The key is stored as \nhostname-serial-key.pem\n.\n\n\nThis script does the following in the process of acquiring certificates for the hostnames specified:\n\n\n\n\nReads a list of fully-qualified hostnames from a file specified by the user.\n\n\nFor each hostname:\n\n\nGenerates a new private key and CSR.\n\n\nOnly important part of CSR is \nCN=\nHOSTNAME\n component.\n\n\nWrites the private key to a file with filename: \nPREFIX\n/\nHOSTNAME\n-key.pem\n.\n\n\nPrompts the user for their private key pass phrase (the pass phrase is cached so user is not re-prompted).\n\n\nAuthenticates to OIM and posts the CSRs as a single request to OIM.\n\n\nRequest ID is returned and subsequently used.\n\n\nAuthenticates to OIM and approves the request.\n\n\nWaits one minute for request to be processed by OIM.\n\n\nConnects to OIM and attempts to retrieve certificates.\n\n\nWrites out any certificates it retrieves with filename of \nPREFIX\n/\nHOSTNAME\n-\nID\n.pem\n.\n\n\nIf all certificates have been retrieved, exits loop.\n\n\nWait 5 seconds and proceeds with the next repeat.\n\n\n\n\n\n\n\n\nInputs:\n\n\n\n\nfilename of list of hostnames.\n\n\nprefix path in which to write private keys and certificates (default: \n.\n.)\n\n\npath to user's certificate (optional: default is path specified by \n$X509_USER_CERT\n environment variable, then \n~/.globus/usercert.pem\n).\n\n\npath to user's private key (optional, default is path specified by \n$X509_USER_KEY\n environment variable, then \n~/.globus/userkey.pem\n).\n\n\nPassphrase for user's private key via non-echoing prompt.\n\n\n\n\nOutputs:\n\n\n\n\nN\n host certificates in PEM format.\n\n\nN\n private keys in PEM format.\n\n\n\n\n[root@client ~] #\n osg-gridadmin-cert-request --help\n\nUsage: osg-gridadmin-cert-request [options] arg\n\n\nUsage: osg-gridadmin-cert-request -h/--help [for detailed explanations of options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -k PKEY, --pkey=PKEY  Specify Requestor\ns private key (PEM Format). If not\n\n\n                        specifiedwill take the value of X509_USER_KEY or\n\n\n                        $\nHOME/.globus/userkey.pem\n\n  -c CERT, --cert=CERT  Specify Requestor\ns certificate (PEM Format). If not\n\n\n                        specified, will take the value of X509_USER_CERT or\n\n\n                        $\nHOME/.globus/usercert.pem\n\n  -a HOSTNAME, --altname=HOSTNAME\n\n\n                        Specify an alternative hostname for CSR (FQDN). May be\n\n\n                        used more than once and if specified, -f/--hostfile\n\n\n                        will be ignored\n\n\n  -v VO name, --vo=VO name\n\n\n                        Specify the VO for the host request\n\n\n  -y CC List, --cc=CC List\n\n\n                        Specify the CC list(the email id\ns to be\n\n\n                        CCed).Separate values by \n,\n\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY\n\n\n                        Write the output files to this directory\n\n\n  -V, --version         Print version information and exit\n\n\n\n  Hostname Options:\n\n\n    Use either of these options. Specify hostname as a single hostname\n\n\n    using -H/--hostname or specify from a file using -f/--hostfile.\n\n\n\n    -H HOSTNAME, --hostname=HOSTNAME\n\n\n                        Specify the hostname or service/hostname for which you\n\n\n                        want to request the certificate for. If specified,\n\n\n                        -f/--hostfile will be ignored\n\n\n    -f HOSTFILE, --hostfile=HOSTFILE\n\n\n                        Filename with one host (hostname or service/hostname\n\n\n                        and its optional,alternative hostnames, separated by\n\n\n                        spaces) per line\n\n\n\n\n\n\nExamples:\n\n\n[root@client ~] #\n osg-gridadmin-cert-request -f filename -k privatekeyfile -c certificatefile\n\n\n\n\n\n[root@client ~] #\n osg-gridadmin-cert-request -H hostname.domain.com -k privatekeyfile -c certificatefile\n\n\n\n\n\nosg-user-cert-renew\n\n\nSends a request for renewing a user certificate and if the certificate can be renewed, fetches and writes the renewed certificate.\n\n\nThe script generates request for renewing user certificate to the OIM, and if the certificate is renewed, it fetches the renewed user certificate. The user is authenticated before making such a request. If the user certificate is renewed, the user gets email notification regarding the same and the renewed certificate is saved by the name of the existing certificate suffixed by \n-renewed.pem\n (e.g. when we renew \nmy-cert.pem\n, the renewed certificate is named \nmy-cert-renewed.pem\n).\n\n\nInputs:\n\n\n\n\npath to user's certificate (optional: default is path specified by \n$X509_USER_CERT\n environment variable, then \n~/.globus/usercert.pem\n).\n\n\npath to user's private key (optional: default is path specified by \n$X509_USER_KEY\n environment variable, then \n~/.globus/userkey.pem\n).\n\n\nPassphrase for user's private key via non-echoing prompt.\n\n\nUser needs to provide VO name if the requested hostname has multiple VO's assigned\n\n\n\n\nOutputs:\n\n\n\n\nOn Renewal, the renewed certificate is stored with the filename of the older user certificate suffixed with \n-renewed.pem\n.  The certificate name for renewed certificate is sent to \nstdout\n.\n\n\n\n\n\n\nNote\n\n\nIf the retrieval of user certificate fails for some reason, the user can download the renewed certificate from OIM web interface using the following URL (where \nREQID\n is the request ID number for the user certificate on OIM):\n\n\nhttps://oim.opensciencegrid.org/oim/certificateuser?id=\nREQID\n\n\n\n\n\n\n\n\n[root@client ~] #\n osg-user-cert-renew --help\n\nUsage: osg-user-cert-renew [options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -k PKEY, --pkey=PKEY  Specify Requestor\ns private key (PEM Format). If not\n\n\n                        specified  will take the value of X509_USER_KEY or\n\n\n                        $\nHOME/.globus/userkey.pem\n\n  -c CERT, --cert=CERT  Specify Requestor\ns certificate (PEM Format).  If not\n\n\n                        specified will take the value of X509_USER_CERT or\n\n\n                        $\nHOME/.globus/usercert.pem\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY\n\n\n                        Write the output files to this directory\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -V, --version         Print version information and exit\n\n\n\n\n\n\nExample\n\n\n[root@client ~] #\n osg-user-cert-renew -k userkey.pem -c usercert.pem\n\n\n\n\n\nosg-user-cert-revoke\n\n\nRevoke a user certificate from OIM given a request ID or certificate ID. Usually the script is run when a user wants to revoke user certificate.\n\n\nFor revoking user certificate, user authentication is done and if the user is authorized to revoke the user certificate, the certificate is immediately revoked and an email notification is sent informing the user that the user certificate is revoked.\n\n\nThe script:\n\n\n\n\nAccepts a Request ID or Certificate ID from the user.\n\n\nAuthenticates user and connects to OIM to revoke the user certificate identified by the request ID or the certificate ID.\n\n\n\n\nInputs:\n\n\n\n\nPrivate key for the user requesting host certificate revocation.\n\n\nUser certificate for the user requesting host certificate revocation.\n\n\nMessage for requesting the user certificate revocation.\n\n\nRequest ID OR the certificate ID for the user certificate to be revoked.\n\n\n\n\nOutputs:\n\n\n\n\nMessage if the revocation was successful along with the host cert request ID/certificate ID on \nstdout\n.\n\n\nError message if the revocation was unsuccessful on \nstdout\n.\n\n\n\n\nIf the user's private key and certificate are not provided, the script takes the private key and user certificate from the \n~/.globus\n folder using the default names (\nuserkey.pem\n and \nusercert.pem\n, respectively)\n\n\n[root@client ~] #\n osg-user-cert-revoke --help\n\nUsage: osg-user-cert-revoke [options] \nRequest ID\n \nmessage\n\n\nUsage: osg-user-cert-revoke -h/--help [for detailed explanations of options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -n, --certid          Treat the ID argument as the serial ID# for the\n\n\n                        certificate to be revoked\n\n\n  -u, --user            Certificate to be revoked is a user certificate.\n\n\n                        Redundant when using `osg-user-cert-revoke`.\n\n\n  -k PKEY, --pkey=PKEY  Specify Requestor\ns private key (PEM Format). If not\n\n\n                        specified, this takes the value of X509_USER_KEY or\n\n\n                        $\nHOME/.globus/userkey.pem\n\n  -c CERT, --cert=CERT  Specify Requestor\ns certificate (PEM Format). If not\n\n\n                        specified, this takes the value of X509_USER_CERT or\n\n\n                        $\nHOME/.globus/usercert.pem\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -V, --version         Print version information and exit\n\n\n  -m REASON, --message=REASON\n\n\n                        Specify the reason for certificate revocation\n\n\n                        [deprecated]\n\n\n  -i, --id              Specify ID# of certificate to be retrieved\n\n\n                        [deprecated]\n\n\n\n\n\n\nExample:\n\n\n[root@client ~] #\n osg-user-cert-revoke -i \n999\n -m \nTesting user cert revocation\n -k privatekeyfile -c usercertfile\n\n\n\n\n\nosg-cert-revoke\n\n\nRevoke a host certificate from OIM given a request ID. Usually the script is run when a user wants to revoke host/service certificate.\n\n\nFor revoking host certificate, user authentication is done and if the user is authorized to revoke the host certificate, the certificate is immediately revoked and an email notification is sent informing the user that the host certificate is revoked.\n\n\nThe script:\n\n\n\n\nAccepts a request ID from the user.\n\n\nAuthenticates user and connects to OIM to revoke the host certificate identified by the request ID.\n\n\n\n\nInputs:\n\n\n\n\nPrivate key for the user requesting host certificate revocation.\n\n\ncertificate for the user requesting host certificate revocation.\n\n\nMessage for requesting the host certificate revocation.\n\n\nRequest ID for the host certificate to be revoked.\n\n\n\n\nOutputs:\n\n\n\n\nMessage if the revocation was successful along with the host cert request ID on \nstdout\n.\n\n\nError message if the revocation was unsuccessful on \nstdout\n.\n\n\n\n\nIf the user's private key and certificate are not provided, the script takes the private key and user certificate from the \n~/.globus\n folder using the default names (\nuserkey.pem\n and \nusercert.pem\n, respectively).\n\n\n[root@client ~] #\n osg-user-cert-revoke --help\n\nUsage: osg-cert-revoke [options] \nRequest ID\n \nmessage\n\n\nUsage: osg-cert-revoke -h/--help [for detailed explanations of options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -n, --certid          Treat the ID argument as the serial ID# for the\n\n\n                        certificate to be revoked\n\n\n  -u, --user            Certificate to be revoked is a user certificate.\n\n\n                        Redundant when using `osg-user-cert-revoke`.\n\n\n  -k PKEY, --pkey=PKEY  Specify Requestor\ns private key (PEM Format). If not\n\n\n                        specified, this takes the value of X509_USER_KEY or\n\n\n                        $\nHOME/.globus/userkey.pem\n\n  -c CERT, --cert=CERT  Specify Requestor\ns certificate (PEM Format). If not\n\n\n                        specified, this takes the value of X509_USER_CERT or\n\n\n                        $\nHOME/.globus/usercert.pem\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -V, --version         Print version information and exit\n\n\n  -m REASON, --message=REASON\n\n\n                        Specify the reason for certificate revocation\n\n\n                        [deprecated]\n\n\n  -i, --id              Specify ID# of certificate to be retrieved\n\n\n                        [deprecated]\n\n\n\n\n\n\nExample:\n\n\n[root@client ~] #\n osg-cert-revoke -i \n999\n -m \nTesting host cert revocation\n -k privatekeyfile -c usercertfile\n\n\n\n\n\nTest Mode\n\n\nThe scripts have two modes of execution.\n\n\nIn the normal mode of execution, the script connects to the production server and generated certificates are from default OSG CA.\n\n\nIf the user provides a \n-T\n parameter on the command-line, the scripts connect to the OIM-ITB server and any generated certificates are issued by the OSG test CAs. This mode is intended for testing and training. The resulting certificates are not usable in a production environment.\n\n\nCurrent Limitations and Bugs\n\n\n\n\nNote that Common Names (CNs) are limited to 64 characters. This is a limitation of OpenSSL and the PKI standard. For details see \nOSGPKI-252\n.", 
            "title": "Certificate Tools"
        }, 
        {
            "location": "/common/pki-cli/#osg-pki-command-line-clients", 
            "text": "", 
            "title": "OSG PKI Command Line Clients"
        }, 
        {
            "location": "/common/pki-cli/#overview", 
            "text": "The OSG PKI Command Line Clients provide a command-line interface for requesting and issuing host certificates from the OSG PKI. They complement the  OIM Web Interface .", 
            "title": "Overview"
        }, 
        {
            "location": "/common/pki-cli/#prerequisites", 
            "text": "If you have not already done so, you need to  configure the OSG software repositories .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/common/pki-cli/#installation", 
            "text": "The command-line scripts have been packaged as an RPM and are available from the OSG repositories.  To install the RPM, run:  [root@client ~] #  yum install osg-pki-tools", 
            "title": "Installation"
        }, 
        {
            "location": "/common/pki-cli/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/common/pki-cli/#configuration-files", 
            "text": "This configuration file contains information divided into two sections for testing and production.  Configuration variables include:   Request URL  Approve URL  Retrieve URL  Host URL   These parameters are used as input to the script depending upon the mode of execution of the script (test or OIM). The command-line utilities check for configuration files in the following order:   $HOME/.osg-pki/OSG_PKI.ini  ./pki-clients.ini  /etc/pki-clients.ini", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/common/pki-cli/#osg-cert-request", 
            "text": "Sends a request for a host certificate.  This script generates a private key and submits a request for a certificate to the OSG PKI. The request will be approved by an appropriate Grid Admin. You will receive an email when this approval has been completed containing directions on how to run  osg-cert-retreive  to retrieve the certificate. It works in two modes:   CSR is provided by the user: the CSR provided is sent to the OIM.  CSR is not provided by the user: the script generates a private key for the user. Writes it to default key file name or the one specified by  -o .   This script:   Generates a new host private key and CSR (the only important part of CSR is  CN= HOSTNAME  component).  Saves the host private key to disk (as specified by the user).  Authenticates to OIM and posts the CSR as a request to OIM.  Returns the request ID to the user.   If the user provides the CSR, then this script would just send the same CSR to OIM.  Inputs:   fully-qualified hostname  filename to store private key (optional; default is  ./hostkey.pem ).  path to user's certificate (optional: default is path specified by  $X509_USER_CERT  environment variable, then  ~/.globus/usercert.pem ).  path to user's private key (optional: default is path specified by  $X509_USER_KEY  environment variable, then  ~/.globus/userkey.pem ).  Passphrase for user's private key (via non-echoing prompt).  User needs to provide VO name if the requested hostname has multiple VO's assigned.   Outputs:   Private key, to filename specified by  -o  or  ./hostkey.pem  by default.  Request Id, to  stdout .   [root@client ~] #  osg-cert-request --help Usage: osg-cert-request [options]  Options:    -h, --help            show this help message and exit    -c CSR, --csr=CSR     Specify CSR name (default = gennew.csr)    -o OUTPUT KEYFILE, --outkeyfile=OUTPUT KEYFILE                          Specify the output filename for the retrieved user certificate.                          Default is ./hostkey.pem    -v VO name, --vo=VO name                          Specify the VO for the host request    -y CC LIST, --cc=CC LIST                          Specify the CC list(the email id s to be CCed).                          Separate values by  ,    -m COMMENT, --comment=COMMENT                          The comment to be added to the request    -H CN, --hostname=CN  Specify a hostname for CSR (FQDN)    -a HOSTNAME, --altname=HOSTNAME                          Specify an alternative hostname for the CSR (FQDN). May be used more than once    -e EMAIL, --email=EMAIL                          Email address to receive certificate    -n NAME, --name=NAME  Name of user receiving certificate    -p PHONE, --phone=PHONE                          Phone number of user receiving certificate    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -T, --test            Run in test mode    -q, --quiet           don t print status messages to stdout    -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY                          Write the output files to this directory    -V, --version         Print version information and exit", 
            "title": "osg-cert-request"
        }, 
        {
            "location": "/common/pki-cli/#examples", 
            "text": "OSG generates the key pair for the request.  [root@client ~] #  osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n  Your Name  -p  9999999999  -y  xyz@domain.com,abc@domain.com  -m  This is my comment   If you want to request a service certificate, you need to escape backslash for service name inside CN like following.  [root@client ~] #  osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n  Your Name  -p  9999999999  -y  rsv\\/xyz@domain.com  -m  This is my comment   You can create your CSR on your target hosts using tools such as  openssl .  [root@client ~] #   umask   077 ;  openssl req -new -newkey rsa:2048 -nodes -keyout hostkey.pem -subj  /CN=osg-ce.example.edu  -out csr.pem  Note that the DN will be overriden by the OSG PKI except for the CN component.  Submitting the request:  [root@client ~] #  osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n  Your Name  -p  9999999999  -y  xyz@domain.com,abc@domain.com  -m  This is my comment  --csr csr.pem", 
            "title": "Examples."
        }, 
        {
            "location": "/common/pki-cli/#osg-cert-retrieve", 
            "text": "Retrieve a certificate (host or user) from OIM given a request Id. Typically you will run this script after submitting a request with  osg-cert-request  and receiving an email telling you your certificate has been approved.  You can also use this script to retrieve other certificates that have been previously issued (assuming you know their request ID number).  Since certificates are public, no authentication of the user is required.  This script:   Accepts a request Id from the user  Connects to OIM and requests the certificate identified by the request ID  Writes the certificate to disk (as specified by the user)   Inputs:   Request ID  Filename to store certificate (optional: default is  ./hostcert.pem ).   Outputs:   Host certificate as PEM, to filename specified or  ./hostcert.pem .   [root@client ~] #  osg-cert-retrieve --help Usage: osg-cert-retrieve [options]  Request ID  Usage: osg-cert-retrieve -h/--help [for detailed explanations of options]  Options:    -h, --help            show this help message and exit    -o ID, --certfile=ID  Specify the output filename for the retrieved user                          certificate . Default is ./hostcert.pem    -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -q, --quiet           don t print status messages to stdout    -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY                          Write the output files to this directory    -V, --version         Print version information and exit    -i, --id              Specify ID# of certificate to be retrieved                          [deprecated]   Example:  [root@client ~] #  osg-cert-retrieve -i  555", 
            "title": "osg-cert-retrieve"
        }, 
        {
            "location": "/common/pki-cli/#osg-gridadmin-cert-request", 
            "text": "Request and retrieve multliple host certificates from OIM. Authenticates to OIM and is only for use by Grid Admins for certificates they are authorized to approve. This script is only supported with all hosts being in the same domain (so we ensure they go to the same Grid Admin). The certificates are stored with the format of  hostname-requestid.pem  (i.e. the id generated from the request for the certificate). The key is stored as  hostname-serial-key.pem .  This script does the following in the process of acquiring certificates for the hostnames specified:   Reads a list of fully-qualified hostnames from a file specified by the user.  For each hostname:  Generates a new private key and CSR.  Only important part of CSR is  CN= HOSTNAME  component.  Writes the private key to a file with filename:  PREFIX / HOSTNAME -key.pem .  Prompts the user for their private key pass phrase (the pass phrase is cached so user is not re-prompted).  Authenticates to OIM and posts the CSRs as a single request to OIM.  Request ID is returned and subsequently used.  Authenticates to OIM and approves the request.  Waits one minute for request to be processed by OIM.  Connects to OIM and attempts to retrieve certificates.  Writes out any certificates it retrieves with filename of  PREFIX / HOSTNAME - ID .pem .  If all certificates have been retrieved, exits loop.  Wait 5 seconds and proceeds with the next repeat.     Inputs:   filename of list of hostnames.  prefix path in which to write private keys and certificates (default:  . .)  path to user's certificate (optional: default is path specified by  $X509_USER_CERT  environment variable, then  ~/.globus/usercert.pem ).  path to user's private key (optional, default is path specified by  $X509_USER_KEY  environment variable, then  ~/.globus/userkey.pem ).  Passphrase for user's private key via non-echoing prompt.   Outputs:   N  host certificates in PEM format.  N  private keys in PEM format.   [root@client ~] #  osg-gridadmin-cert-request --help Usage: osg-gridadmin-cert-request [options] arg  Usage: osg-gridadmin-cert-request -h/--help [for detailed explanations of options]  Options:    -h, --help            show this help message and exit    -k PKEY, --pkey=PKEY  Specify Requestor s private key (PEM Format). If not                          specifiedwill take the value of X509_USER_KEY or                          $ HOME/.globus/userkey.pem   -c CERT, --cert=CERT  Specify Requestor s certificate (PEM Format). If not                          specified, will take the value of X509_USER_CERT or                          $ HOME/.globus/usercert.pem   -a HOSTNAME, --altname=HOSTNAME                          Specify an alternative hostname for CSR (FQDN). May be                          used more than once and if specified, -f/--hostfile                          will be ignored    -v VO name, --vo=VO name                          Specify the VO for the host request    -y CC List, --cc=CC List                          Specify the CC list(the email id s to be                          CCed).Separate values by  ,    -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -q, --quiet           don t print status messages to stdout    -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY                          Write the output files to this directory    -V, --version         Print version information and exit    Hostname Options:      Use either of these options. Specify hostname as a single hostname      using -H/--hostname or specify from a file using -f/--hostfile.      -H HOSTNAME, --hostname=HOSTNAME                          Specify the hostname or service/hostname for which you                          want to request the certificate for. If specified,                          -f/--hostfile will be ignored      -f HOSTFILE, --hostfile=HOSTFILE                          Filename with one host (hostname or service/hostname                          and its optional,alternative hostnames, separated by                          spaces) per line   Examples:  [root@client ~] #  osg-gridadmin-cert-request -f filename -k privatekeyfile -c certificatefile  [root@client ~] #  osg-gridadmin-cert-request -H hostname.domain.com -k privatekeyfile -c certificatefile", 
            "title": "osg-gridadmin-cert-request"
        }, 
        {
            "location": "/common/pki-cli/#osg-user-cert-renew", 
            "text": "Sends a request for renewing a user certificate and if the certificate can be renewed, fetches and writes the renewed certificate.  The script generates request for renewing user certificate to the OIM, and if the certificate is renewed, it fetches the renewed user certificate. The user is authenticated before making such a request. If the user certificate is renewed, the user gets email notification regarding the same and the renewed certificate is saved by the name of the existing certificate suffixed by  -renewed.pem  (e.g. when we renew  my-cert.pem , the renewed certificate is named  my-cert-renewed.pem ).  Inputs:   path to user's certificate (optional: default is path specified by  $X509_USER_CERT  environment variable, then  ~/.globus/usercert.pem ).  path to user's private key (optional: default is path specified by  $X509_USER_KEY  environment variable, then  ~/.globus/userkey.pem ).  Passphrase for user's private key via non-echoing prompt.  User needs to provide VO name if the requested hostname has multiple VO's assigned   Outputs:   On Renewal, the renewed certificate is stored with the filename of the older user certificate suffixed with  -renewed.pem .  The certificate name for renewed certificate is sent to  stdout .    Note  If the retrieval of user certificate fails for some reason, the user can download the renewed certificate from OIM web interface using the following URL (where  REQID  is the request ID number for the user certificate on OIM):  https://oim.opensciencegrid.org/oim/certificateuser?id= REQID    [root@client ~] #  osg-user-cert-renew --help Usage: osg-user-cert-renew [options]  Options:    -h, --help            show this help message and exit    -k PKEY, --pkey=PKEY  Specify Requestor s private key (PEM Format). If not                          specified  will take the value of X509_USER_KEY or                          $ HOME/.globus/userkey.pem   -c CERT, --cert=CERT  Specify Requestor s certificate (PEM Format).  If not                          specified will take the value of X509_USER_CERT or                          $ HOME/.globus/usercert.pem   -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY                          Write the output files to this directory    -q, --quiet           don t print status messages to stdout    -V, --version         Print version information and exit   Example  [root@client ~] #  osg-user-cert-renew -k userkey.pem -c usercert.pem", 
            "title": "osg-user-cert-renew"
        }, 
        {
            "location": "/common/pki-cli/#osg-user-cert-revoke", 
            "text": "Revoke a user certificate from OIM given a request ID or certificate ID. Usually the script is run when a user wants to revoke user certificate.  For revoking user certificate, user authentication is done and if the user is authorized to revoke the user certificate, the certificate is immediately revoked and an email notification is sent informing the user that the user certificate is revoked.  The script:   Accepts a Request ID or Certificate ID from the user.  Authenticates user and connects to OIM to revoke the user certificate identified by the request ID or the certificate ID.   Inputs:   Private key for the user requesting host certificate revocation.  User certificate for the user requesting host certificate revocation.  Message for requesting the user certificate revocation.  Request ID OR the certificate ID for the user certificate to be revoked.   Outputs:   Message if the revocation was successful along with the host cert request ID/certificate ID on  stdout .  Error message if the revocation was unsuccessful on  stdout .   If the user's private key and certificate are not provided, the script takes the private key and user certificate from the  ~/.globus  folder using the default names ( userkey.pem  and  usercert.pem , respectively)  [root@client ~] #  osg-user-cert-revoke --help Usage: osg-user-cert-revoke [options]  Request ID   message  Usage: osg-user-cert-revoke -h/--help [for detailed explanations of options]  Options:    -h, --help            show this help message and exit    -n, --certid          Treat the ID argument as the serial ID# for the                          certificate to be revoked    -u, --user            Certificate to be revoked is a user certificate.                          Redundant when using `osg-user-cert-revoke`.    -k PKEY, --pkey=PKEY  Specify Requestor s private key (PEM Format). If not                          specified, this takes the value of X509_USER_KEY or                          $ HOME/.globus/userkey.pem   -c CERT, --cert=CERT  Specify Requestor s certificate (PEM Format). If not                          specified, this takes the value of X509_USER_CERT or                          $ HOME/.globus/usercert.pem   -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -q, --quiet           don t print status messages to stdout    -V, --version         Print version information and exit    -m REASON, --message=REASON                          Specify the reason for certificate revocation                          [deprecated]    -i, --id              Specify ID# of certificate to be retrieved                          [deprecated]   Example:  [root@client ~] #  osg-user-cert-revoke -i  999  -m  Testing user cert revocation  -k privatekeyfile -c usercertfile", 
            "title": "osg-user-cert-revoke"
        }, 
        {
            "location": "/common/pki-cli/#osg-cert-revoke", 
            "text": "Revoke a host certificate from OIM given a request ID. Usually the script is run when a user wants to revoke host/service certificate.  For revoking host certificate, user authentication is done and if the user is authorized to revoke the host certificate, the certificate is immediately revoked and an email notification is sent informing the user that the host certificate is revoked.  The script:   Accepts a request ID from the user.  Authenticates user and connects to OIM to revoke the host certificate identified by the request ID.   Inputs:   Private key for the user requesting host certificate revocation.  certificate for the user requesting host certificate revocation.  Message for requesting the host certificate revocation.  Request ID for the host certificate to be revoked.   Outputs:   Message if the revocation was successful along with the host cert request ID on  stdout .  Error message if the revocation was unsuccessful on  stdout .   If the user's private key and certificate are not provided, the script takes the private key and user certificate from the  ~/.globus  folder using the default names ( userkey.pem  and  usercert.pem , respectively).  [root@client ~] #  osg-user-cert-revoke --help Usage: osg-cert-revoke [options]  Request ID   message  Usage: osg-cert-revoke -h/--help [for detailed explanations of options]  Options:    -h, --help            show this help message and exit    -n, --certid          Treat the ID argument as the serial ID# for the                          certificate to be revoked    -u, --user            Certificate to be revoked is a user certificate.                          Redundant when using `osg-user-cert-revoke`.    -k PKEY, --pkey=PKEY  Specify Requestor s private key (PEM Format). If not                          specified, this takes the value of X509_USER_KEY or                          $ HOME/.globus/userkey.pem   -c CERT, --cert=CERT  Specify Requestor s certificate (PEM Format). If not                          specified, this takes the value of X509_USER_CERT or                          $ HOME/.globus/usercert.pem   -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -q, --quiet           don t print status messages to stdout    -V, --version         Print version information and exit    -m REASON, --message=REASON                          Specify the reason for certificate revocation                          [deprecated]    -i, --id              Specify ID# of certificate to be retrieved                          [deprecated]   Example:  [root@client ~] #  osg-cert-revoke -i  999  -m  Testing host cert revocation  -k privatekeyfile -c usercertfile", 
            "title": "osg-cert-revoke"
        }, 
        {
            "location": "/common/pki-cli/#test-mode", 
            "text": "The scripts have two modes of execution.  In the normal mode of execution, the script connects to the production server and generated certificates are from default OSG CA.  If the user provides a  -T  parameter on the command-line, the scripts connect to the OIM-ITB server and any generated certificates are issued by the OSG test CAs. This mode is intended for testing and training. The resulting certificates are not usable in a production environment.", 
            "title": "Test Mode"
        }, 
        {
            "location": "/common/pki-cli/#current-limitations-and-bugs", 
            "text": "Note that Common Names (CNs) are limited to 64 characters. This is a limitation of OpenSSL and the PKI standard. For details see  OSGPKI-252 .", 
            "title": "Current Limitations and Bugs"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/", 
            "text": "Configuration with OSG-Configure\n\n\n\n\nAbout this document\n\n\nInvocation and script usage\n\n\nSyntax and layout\n\n\nConfiguration sections\n\n\nJob managers (batch systems):\n\n\nBosco\n\n\nCondor\n\n\nLSF\n\n\nPBS\n\n\nSGE\n\n\nSlurm\n\n\n\n\n\n\nMonitoring/reporting:\n\n\nGratia\n\n\nInfo Services\n\n\nRSV\n\n\nSubcluster / Resource Entry\n\n\n\n\n\n\nGateway\n\n\nLocal Settings\n\n\nMisc Services\n\n\nSite Information\n\n\nSquid\n\n\nStorage\n\n\n\n\n\n\n\n\nAbout this document\n\n\nOSG-Configure and the INI files in \n/etc/osg/config.d\n allow a high level configuration of OSG services.\nThis document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software.\n\n\nThis page gives an overview of the options for each of the sections of the configuration files that \nosg-configure\n uses.\n\n\nInvocation and script usage\n\n\nThe \nosg-configure\n script is used to process the INI files and apply changes to the system.\n\nosg-configure\n must be run as root.\n\n\nThe typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes.\n\n\nTo verify the config files, run:\n\n\n[root@server] osg-configure -v\n\n\n\n\n\n\nOSG-Configure will list any errors in your configuration, usually including the section and option where the problem is.\nPotential problems are:\n\n\n\n\nRequired option not filled in\n\n\nInvalid value\n\n\nSyntax error\n\n\nInconsistencies between options\n\n\n\n\nTo apply changes, run:\n\n\n[root@server] osg-configure -c\n\n\n\n\n\n\nIf your INI files do not change, then re-running \nosg-configure -c\n will result in the same configuration as when you ran it the last time.\nThis allows you to experiment with your settings without having to worry about messing up your system.\n\n\nOSG-Configure is split up into modules. Normally, all modules are run when calling \nosg-configure\n.\nHowever, it is possible to run specific modules separately.\nTo see a list of modules, including whether they can be run separately, run:\n\n\n[root@server] osg-configure -l\n\n\n\n\n\n\nIf the module can be run separately, specify it with the \n-m \nMODULE\n option:\n\n\n[root@server] osg-configure -c -m \nMODULE\n\n\n\n\n\n\nOptions may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses.\nYou may query the final value of an option via one of these methods:\n\n\n[root@server] osg-configure -o \nOPTION\n\n\n[root@server] osg-configure -o \nSECTION\n.\nOPTION\n\n\n\n\n\n\nLogs are written to \n/var/log/osg/osg-configure.log\n.\nIf something goes wrong, specify the \n-d\n flag to add more verbose output to \nosg-configure.log\n.\n\n\nThe rest of this document will detail what to specify in the INI files.\n\n\nConventions\n\n\nIn the tables below:\n\n\n\n\nMandatory options for a section are given in \nbold\n type. Sometime the default value may be OK and no edit required, but the variable has to be in the file.\n\n\nOptions that are not found in the default ini file are in \nitalics\n.\n\n\n\n\nSyntax and layout\n\n\nThe configuration files used by \nosg-configure\n are the one supported by Python's \nSafeConfigParser\n, similar in format to the \nINI configuration file\n used by MS Windows:\n\n\n\n\nConfig files are separated into sections, specified by a section name in square brackets (e.g. \n[Section 1]\n)\n\n\nOptions should be set using \nname = value\n pairs\n\n\nLines that begin with \n;\n or \n#\n are comments\n\n\nLong lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in \nemail RFC 822\n)\n\n\nVariable substitutions are supported -- \nsee below\n\n\n\n\nosg-configure\n reads and uses all of the files in \n/etc/osg/config.d\n that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called \n[PBS]\n can be given in \n20-pbs.ini\n as well as \n99-local-site-settings.ini\n.\n\n\nEach of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g. \n99-local-site-settings.ini\n) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated.\n\n\nVariable substitution\n\n\nThe osg-configure parser allows variables to be defined and used in the configuration file:\nany option set in a given section can be used as a variable in that section.  Assuming that you have set an option with the name \nmyoption\n in the section, you can substitute the value of that option elsewhere in the section by referring to it as \n%(myoption)s\n.\n\n\n\n\nNote\n\n\nThe trailing \ns\n is required. Also, option names cannot have a variable subsitution in them.\n\n\n\n\nSpecial Settings\n\n\nIf a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible.\n\n\nIgnore setting\n\n\nThe \nenabled\n option, specifying whether a service is enabled or not, is a boolean but also accepts \nIgnore\n as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using \nFalse\n (or the \n%(disabled)s\n variable), because using \nFalse\n results in the service associated with the section being disabled. \nosg-configure\n will not change the configuration of the service if the \nenabled\n is set to \nIgnore\n.\n\n\nThis is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the \nIgnore\n setting so that \nosg-configure\n does not alter the service's configuration and status.\n\n\nConfiguration sections\n\n\nThe OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g. \n[Section 1]\n). The configuration is split in multiple files and options form one section can be in more than one files.\n\n\nThe following sections give an overview of the options for each of the sections of the configuration files that \nosg-configure\n uses.\n\n\nBosco\n\n\nThis section is contained in \n/etc/osg/config.d/20-bosco.ini\n which is provided by the \nosg-configure-bosco\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Bosco jobmanager is being used or not.\n\n\n\n\n\n\nusers\n\n\nString\n\n\nA comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here.\n\n\n\n\n\n\nendpoint\n\n\nString\n\n\nThe remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of \n, exactly as you would use to ssh into the remote cluster.\n\n\n\n\n\n\nbatch\n\n\nString\n\n\nThe type of scheduler installed on the remote cluster.\n\n\n\n\n\n\nssh_key\n\n\nString\n\n\nThe location of the ssh key, as created above.\n\n\n\n\n\n\n\n\nCondor\n\n\nThis section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-condor.ini\n which is provided by the \nosg-configure-condor\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Condor jobmanager is being used or not.\n\n\n\n\n\n\ncondor_location\n\n\nString\n\n\nThis should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the \nosg-configure\n script will try to get this from the CONDOR_LOCATION environment variable if available otherwise it will use \n/usr\n which works for the RPM installation.\n\n\n\n\n\n\ncondor_config\n\n\nString\n\n\nThis should be set to be path where the condor_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the \nosg-configure\n script will try to get this from the CONDOR_CONFIG environment variable if available otherwise it will use \n/etc/condor/condor_config\n, the default for the RPM installation.\n\n\n\n\n\n\n\n\nLSF\n\n\nThis section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-lsf.ini\n which is provided by the \nosg-configure-lsf\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the LSF jobmanager is being used or not.\n\n\n\n\n\n\nlsf_location\n\n\nString\n\n\nThis should be set to be directory where lsf is installed\n\n\n\n\n\n\n\n\nPBS\n\n\nThis section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-pbs.ini\n which is provided by the \nosg-configure-pbs\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the PBS jobmanager is being used or not.\n\n\n\n\n\n\npbs_location\n\n\nString\n\n\nThis should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs_location/bin.\n\n\n\n\n\n\naccounting_log_directory\n\n\nString\n\n\nThis setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting.\n\n\n\n\n\n\npbs_server\n\n\nString\n\n\nThis setting is optional and should point to your PBS server node if it is different from your OSG CE\n\n\n\n\n\n\n\n\nSGE\n\n\nThis section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-sge.ini\n which is provided by the \nosg-configure-sge\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the SGE jobmanager is being used or not.\n\n\n\n\n\n\nsge_root\n\n\nString\n\n\nThis should be set to be directory where sge is installed (e.g. same as \n$SGE_ROOT\n variable).\n\n\n\n\n\n\nsge_cell\n\n\nString\n\n\nThe sge_cell setting should be set to the value of $SGE_CELL for your SGE install.\n\n\n\n\n\n\ndefault_queue\n\n\nString\n\n\nThis setting determines queue that jobs should be placed in if the job description does not specify a queue.\n\n\n\n\n\n\navailable_queues\n\n\nString\n\n\nThis setting indicates which queues are available on the cluster and should be used for validation when \nvalidate_queues\n is set.\n\n\n\n\n\n\nvalidate_queues\n\n\nString\n\n\nThis setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nvalidate_queues\n:\n\nIf \navailable_queues\n is set, that list of queues will be used for\nvalidation, otherwise SGE will be queried for available queues.\n\n\n\n\nSlurm\n\n\nThis section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-slurm.ini\n which is provided by the \nosg-configure-slurm\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Slurm jobmanager is being used or not.\n\n\n\n\n\n\nslurm_location\n\n\nString\n\n\nThis should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm_location/bin.\n\n\n\n\n\n\ndb_host\n\n\nString\n\n\nHostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_port\n\n\nString\n\n\nPort of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_user\n\n\nString\n\n\nUsername used to access the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_pass\n\n\nString\n\n\nThe location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_name\n\n\nString\n\n\nName of the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\nslurm_cluster\n\n\nString\n\n\nThe name of the Slurm cluster\n\n\n\n\n\n\n\n\nGratia\n\n\nThis section configures Gratia. If \nprobes\n is set to \nUNAVAILABLE\n, then \nosg-configure\n will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes, \n%(osg-jobmanager-gratia)s\n, \n%(osg-gridftp-gratia)s\n, \n%(osg-metric-gratia)s\n, \n%(itb-jobmanager-gratia)s\n, \n%(itb-gridftp-gratia)s\n, \n%(itb-metric-gratia)s\n are defined in the default configuration files to make it easier to specify the standard osg reporting.\n\n\nThis section is contained in \n/etc/osg/config.d/30-gratia.ini\n which is provided by the \nosg-configure-gratia\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n , \nFalse\n, \nIgnore\n\n\nThis should be set to True if gratia should be configured and enabled on the installation being configured.\n\n\n\n\n\n\nresource\n\n\nString\n\n\nThis should be set to the resource name as given in the OIM registration\n\n\n\n\n\n\nprobes\n\n\nString\n\n\nThis should be set to the gratia probes that should be enabled. A probe is specified by using as \n[probe_type]:server:port\n. See note\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nprobes\n:\n\nLegal values for \nprobe_type\n are:\n\n\n\n\nmetric\n (for RSV)\n\n\njobmanager\n (for the appropriate jobmanager probe)\n\n\ngridftp\n (for the GridFTP transfer probe)\n\n\n\n\n\n\nInfo Services\n\n\nReporting to the central CE Collectors is configured in this section.  In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations.\n\n\nThis section is contained in \n/etc/osg/config.d/30-infoservices.ini\n, which is provided by the \nosg-configure-infoservices\n RPM. (This is for historical reasons.)\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nTrue if reporting should be configured and enabled\n\n\n\n\n\n\nce_collectors\n\n\nString\n\n\nThe server(s) HTCondor-CE information should be sent to. See note\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nce_collectors\n:\n\n\n\n\nSet this to \nDEFAULT\n to report to the OSG Production or ITB servers (depending on your \nSite Information\n configuration).\n\n\nSet this to \nPRODUCTION\n to report to the OSG Production servers\n\n\nSet this to \nITB\n to report to the OSG ITB servers\n\n\nOtherwise, set this to the \nhostname:port\n of a host running a \ncondor-ce-collector\n daemon\n\n\n\n\n\n\nRSV\n\n\nThis section handles the configuration and setup of the RSV services.\n\n\nThis section is contained in \n/etc/osg/config.d/30-rsv.ini\n which is provided by the \nosg-configure-rsv\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the rsv  service is being used or not.\n\n\n\n\n\n\nrsv_user\n\n\nString\n\n\nThis gives username that rsv will run under.  If this is blank or set to \nUNAVAILABLE\n, it will default to rsv.\n\n\n\n\n\n\ngratia_probes\n\n\nString\n\n\nThis settings indicates which rsv gratia probes should be used.  It is a list of probes separated by a comma.  Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer\n\n\n\n\n\n\nce_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the CEs that generic RSV CE probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:2812\n).\n\n\n\n\n\n\nhtcondor_ce_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:2812\n).\n\n\n\n\n\n\ngums_hosts\n\n\nString\n\n\nThis option lists the serviceURI or FQDN of the CEs or SEs, using GUMS for authentication, that the RSV GUMS probes should check.  This should be a list of \nCE\n or \nSE\n FQDNs (and \nnot a GUMS server FQDN\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3\n).\n\n\n\n\n\n\ngridftp_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the gridftp servers that the RSV gridftp probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host.iu.edu:2812,my.host2,my.host3\n).\n\n\n\n\n\n\ngridftp_dir\n\n\nString\n\n\nThis should be the directory that the gridftp probes should use during testing.  This defaults to \n/tmp\n if left blank or set to \nUNAVAILABLE\n.\n\n\n\n\n\n\nsrm_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the srm servers that the RSV srm probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:8444\n).\n\n\n\n\n\n\nsrm_dir\n\n\nString\n\n\nThis should be the directory that the srm probes should use during testing.\n\n\n\n\n\n\nsrm_webservice_path\n\n\nString\n\n\nThis option gives the webservice path that SRM probes need to use along with the host:port. See note.\n\n\n\n\n\n\nservice_cert\n\n\nString\n\n\nThis option should point to the public key file (pem) for your service  certificate. If this is left blank or set to \nUNAVAILABLE\n and the \nuser_proxy\n setting is set, it will default to \n/etc/grid-security/rsvcert.pem\n\n\n\n\n\n\nservice_key\n\n\nString\n\n\nThis option should point to the private key file (pem) for your service  certificate. If this is left blank or set to \nUNAVAILABLE\n and the \nservice_cert\n setting is enabled, it will default to \n/etc/grid-security/rsvkey.pem\n .\n\n\n\n\n\n\nservice_proxy\n\n\nString\n\n\nThis should point to the location of the rsv proxy file. If this is left blank or set to \nUNAVAILABLE\n and the use_service_cert  setting is enabled, it will default to \n/tmp/rsvproxy\n.\n\n\n\n\n\n\nuser_proxy\n\n\nString\n\n\nIf you don't use a service certificate for rsv, you will need to specify a  proxy file that RSV should use in the proxy_file setting.  If this is set, then  service_cert, service_key, and service_proxy should be left blank, or set to \nUNAVAILABE\n or \nDEFAULT\n.\n\n\n\n\n\n\nenable_gratia\n\n\nTrue\n, \nFalse\n\n\nThis option will enable RSV record uploading to central RSV collector at the GOC.   This should be set to True on all OSG resources (and to False on non-OSG resources).\n\n\n\n\n\n\nsetup_rsv_nagios\n\n\nTrue\n, \nFalse\n\n\nThis option indicates whether rsv should upload results to a local  nagios server instance. This should be set to True or False.\n This plugin is provided as an experimental component, and admins are recommend \nnot to enable\n it on production resources.\n\n\n\n\n\n\nrsv_nagios_conf_file\n\n\nString\n\n\nThis option indicates the location of the rsv nagios  file to use for configuration details. This file \nneeds to be configured locally for RSV-Nagios forwarding to work\n -- see inline comments in file for more information.\n\n\n\n\n\n\ncondor_location\n\n\nString\n\n\nIf you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it)  you must specify the path to the install dir here.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nsrm_webservice_path\n:\n\nFor dcache installations, this should work if left blank. However\nBestman-xrootd SEs normally use \nsrm/v2/server\n as web service path, and so\nBestman-xrootd admins will have to pass this option with the appropriate\nvalue (for example: \nsrm/v2/server\n) for the SRM probes to pass on their\nSE.\n\n\n\n\nSubcluster / Resource Entry\n\n\nSubcluster and Resource Entry configuration is for reporting about the worker resources on your site. A \nsubcluster\n is a homogeneous set of worker node hardware; a \nresource\n is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system.\n\n\nAt least one Subcluster or Resource Entry section\n is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources.\n\n\nThis section is contained in \n/etc/osg/config.d/30-gip.ini\n which is provided by the \nosg-configure-gip\n RPM. (This is for historical reasons.)\n\n\nThis configuration uses multiple sections of the OSG configuration files:\n\n\n\n\nSubcluster*\n: options about homogeneous subclusters\n\n\nResource Entry*\n: options for specifying ATLAS queues for AGIS\n\n\n\n\nNotes for multi-CE sites.\n\n\nIf you would like to properly advertise multiple CEs per cluster, make sure that you:\n\n\n\n\nSet the value of site_name in the \"Site Information\" section to be the same for each CE.\n\n\nHave the \nexact\n same configuration values for the Subcluster* and Resource Entry* sections in each CE.\n\n\n\n\nSubcluster Configuration\n\n\nEach homogeneous set of worker node hardware is called a \nsubcluster\n. For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format: \n[Subcluster CHANGEME]\n, where CHANGEME is the globally unique subcluster name (yes, it must be a \nglobally\n unique name for the whole grid, not just unique to your site. Get creative.)\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nThe same name that is in the Section label; it should be \nglobally unique\n\n\n\n\n\n\nram_mb\n\n\nPositive Integer\n\n\nMegabytes of RAM per node\n\n\n\n\n\n\ncores_per_node\n\n\nPositive Integer\n\n\nNumber of cores per node\n\n\n\n\n\n\nallowed_vos\n\n\nComma-separated List or \n*\n\n\nThe VOs that are allowed to run jobs on this subcluster (autodetected if \n*\n). Optional on OSG 3.3\n\n\n\n\n\n\n\n\nThe following attributes are optional:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nmax_wall_time\n\n\nPositive Integer\n\n\nMaximum wall-clock time, in minutes, that a job is allowed to run on this subcluster (the default is one day or 1440 mins)\n\n\n\n\n\n\nqueue\n\n\nString\n\n\nThe queue to which jobs should be submitted in order to run on this subcluster\n\n\n\n\n\n\nextra_transforms\n\n\nClassad\n\n\nTransformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nallowed_vos\n is mandatory\n\n\n\n\nResource Entry Configuration (ATLAS only)\n\n\nIf you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new \nResource Entry\n section with a unique name in the following format: \n[Resource Entry RESOURCE]\n where RESOURCE is a globally unique resource name (it must be a \nglobally\n unique name for the whole grid, not just unique to your site). The following options are required for the \nResource Entry\n section and are used to generate the data required by AGIS:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nThe same name that is in the \nResource Entry\n label; it must be \nglobally unique\n\n\n\n\n\n\nmax_wall_time\n\n\nPositive Integer\n\n\nMaximum wall-clock time, in minutes, that a job is allowed to run on this resource\n\n\n\n\n\n\nqueue\n\n\nString\n\n\nThe queue to which jobs should be submitted to run on this resource\n\n\n\n\n\n\ncpucount\n (alias \ncores_per_node\n)\n\n\nPositive Integer\n\n\nNumber of cores that a job using this resource can get\n\n\n\n\n\n\nmaxmemory\n (alias \nram_mb\n)\n\n\nPositive Integer\n\n\nMaximum amount of memory (in MB) that a job using this resource can get\n\n\n\n\n\n\nallowed_vos\n\n\nComma-separated List or \n*\n\n\nThe VOs that are allowed to run jobs on this resource (autodetected if \n*\n). Optional on OSG 3.3\n\n\n\n\n\n\n\n\nThe following attributes are optional:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nsubclusters\n\n\nComma-separated List\n\n\nThe physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file\n\n\n\n\n\n\nvo_tag\n\n\nString\n\n\nAn arbitrary label that is added to jobs routed through this resource\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nallowed_vos\n is mandatory\n\n\n\n\nGateway\n\n\nThis section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses \ncondor-ce\n as the gateway.\n\n\nThis section is contained in \n/etc/osg/config.d/10-gateway.ini\n which is provided by the \nosg-configure-gateway\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nhtcondor_gateway_enabled\n\n\nTrue\n, \nFalse\n\n\n(default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes.\n\n\n\n\n\n\njob_envvar_path\n\n\nString\n\n\nThe value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination.\n\n\n\n\n\n\n\n\nLocal Settings\n\n\nThis section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the \nosg-local-job-environment.conf\n verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The \nosg-local-job-environment.conf\n file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system.\n\n\nAdding a line such as \nMy_Setting = my_Value\n would result in the an environment variable called \nMy_Setting\n set to \nmy_Value\n in the job's environment. \nmy_Value\n can also be defined in terms of an environment variable (i.e \nMy_Setting = $my_Value\n) that will be evaluated on the worker node. For example, to add a variable \nMY_PATH\n set to \n/usr/local/myapp\n, you'd have the following:\n\n\n[Local Settings]\n\n\n\nMY_PATH\n \n=\n \n/usr/local/myapp\n\n\n\n\n\n\nThis section is contained in \n/etc/osg/config.d/40-localsettings.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\nMisc Services\n\n\nThis section handles the configuration of services that do not have a dedicated section for their configuration.\n\n\nThis section is contained in \n/etc/osg/config.d/10-misc.ini\n which is provided by the \nosg-configure-misc\n RPM.\n\n\nThis section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the \nauthentication section of the HTCondor-CE install documents\n.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nglexec_location\n\n\nString\n\n\nThis gives the location of the glExec installation on the worker nodes, if it is present. Can be defined in terms of an environment variable (e.g. \n$FOO\n) that will be evaluated on the worker node. If it is not installed, set this to \nUNAVAILABLE\n. glExec does not work with the \nvomsmap\n authorization method on OSG 3.3 and \nis entirely unsupported starting in OSG 3.4\n\n\n\n\n\n\ngums_host\n\n\nString\n\n\nThis setting is used to indicate the hostname of the GUMS host that should be used for authentication, if the authorization method below is set to \nxacml\n. If GUMS is not used, this should be set to \nUNAVAILABLE\n. \nGUMS is deprecated in OSG 3.4\n\n\n\n\n\n\nauthorization_method\n\n\ngridmap\n, \nxacml\n, \nlocal-gridmap\n, \nvomsmap\n\n\nThis indicates which authorization method your site uses. \nxacml\n \nis deprecated in OSG 3.4\n\n\n\n\n\n\nedit_lcmaps_db\n\n\nTrue\n, \nFalse\n\n\n(Optional, default True) If true, osg-configure will overwrite \n/etc/lcmaps.db\n to set your authorization method. The previous version will be backed up to \n/etc/lcmaps.db.pre-configure\n\n\n\n\n\n\ncopy_host_cert_for_service_certs\n\n\nTrue\n, \nFalse\n\n\n(Optional, default False) If true, osg-configure will create a copy or copies of your host cert and key as service certs for RSV and (on OSG 3.3) GUMS\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nglexec_location\n must be \nUNAVAILABLE\n or unset\n\n\nauthorization_method\n defaults to \nvomsmap\n\n\nauthorization_method\n will raise a warning if set to \nxacml\n\n\n\n\nSite Information\n\n\nThe settings found in the \nSite Information\n section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc.\n\n\nThis section is contained in \n/etc/osg/config.d/40-siteinfo.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngroup\n\n\nOSG\n , \nOSG-ITB\n\n\nThis should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG\n\n\n\n\n\n\nhost_name\n\n\nString\n\n\nThis should be set to be hostname of the CE that is being configured\n\n\n\n\n\n\nresource\n\n\nString\n\n\nThe resource name of this CE endpoint as registered in OIM.\n\n\n\n\n\n\nresource_group\n\n\nString\n\n\nThe resource_group of this CE as registered in OIM.\n\n\n\n\n\n\nsponsor\n\n\nString\n\n\nThis should be set to the sponsor of the resource. See note.\n\n\n\n\n\n\nsite_policy\n\n\nUrl\n\n\nThis should be a url pointing to the resource's usage policy\n\n\n\n\n\n\ncontact\n\n\nString\n\n\nThis should be the name of the resource's admin contact\n\n\n\n\n\n\nemail\n\n\nEmail address\n\n\nThis should be the email address of the admin contact for the resource\n\n\n\n\n\n\ncity\n\n\nString\n\n\nThis should be the city that the resource is located in\n\n\n\n\n\n\ncountry\n\n\nString\n\n\nThis should be two letter country code for the country that the resource is located in.\n\n\n\n\n\n\nlongitude\n\n\nNumber\n\n\nThis should be the longitude of the resource. It should be a number between -180 and 180.\n\n\n\n\n\n\nlatitude\n\n\nNumber\n\n\nThis should be the latitude of the resource. It should be a number between -90 and 90.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nsponsor\n:\n\nIf your resource has multiple sponsors, you can separate them using commas\nor specify the percentage using the following format 'osg, atlas, cms' or\n'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple\nsponsors are used. If you have a sponsor that is not an OSG VO, you can\nindicate this by using 'local' as the VO.\n\n\n\n\nSquid\n\n\nThis section handles the configuration and setup of the squid web caching and proxy service.\n\n\nThis section is contained in \n/etc/osg/config.d/01-squid.ini\n which is provided by the \nosg-configure-squid\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the squid service is being used or not.\n\n\n\n\n\n\nlocation\n\n\nString\n\n\nThis should be set to the \nhostname:port\n of the squid server.\n\n\n\n\n\n\n\n\nStorage\n\n\nThis section gives information about the options in the Storage section of the configuration file.\nSeveral of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models.\nPlease review the Storage Related Parameters section of the\n\nEnvironment Variables\n\ndescription and \nSite Planning\n\ndiscussions for explanations of the various storage models and the requirements for them.\n\n\nThis section is contained in \n/etc/osg/config.d/10-storage.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nse_available\n\n\nTrue\n, \nFalse\n\n\nThis indicates whether there is an associated SE available.\n\n\n\n\n\n\ndefault_se\n\n\nString\n\n\nIf an SE is available at your cluster, set default_se to the hostname of this SE, otherwise set default_se to UNAVAILABLE.\n\n\n\n\n\n\ngrid_dir\n\n\nString\n\n\nThis setting should point to the directory which holds the files from the OSG worker node package. See note\n\n\n\n\n\n\napp_dir\n\n\nString\n\n\nThis setting should point to the directory which contains the VO specific applications. See note\n\n\n\n\n\n\ndata_dir\n\n\nString\n\n\nThis setting should point to a directory that can be used to store and stage data in and out of the cluster. See note\n\n\n\n\n\n\nworker_node_temp\n\n\nString\n\n\nThis directory should point to a directory that can be used as scratch space on compute nodes. If not set, the default is UNAVAILABLE. See note\n\n\n\n\n\n\nsite_read\n\n\nString\n\n\nThis setting should be the location or url to a directory that can be read to stage in data via the variable \n$OSG_SITE_READ\n. This is an url if you are using a SE. If not set, the default is UNAVAILABLE\n\n\n\n\n\n\nsite_write\n\n\nString\n\n\nThis setting should be the location or url to a directory that can be write to stage out data via the variable \n$OSG_SITE_WRITE\n. This is an url if you are using a SE. If not set, the default is UNAVAILABLE\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nAll of these can be defined in terms of an environment variable\n(e.g. \n$FOO\n) that will be evaluated on the worker node.\n\n\n\n\ngrid_dir\n:\n\nIf you have installed the worker node client via RPM (the normal case) it\nshould be \n/etc/osg/wn-client\n.  If you have installed the worker node in a\nspecial location (perhaps via the worker node client tarball or via OASIS),\nit should be the location of that directory.\n\n\nThis directory will be accessed via the \n$OSG_GRID\n environment variable.\nIt should be visible on all of the compute nodes. Read access is required,\nthough worker nodes don't need write access.\n\n\napp_dir\n:\n\nThis directory will be accesed via the \n$OSG_APP\n environment variable. It\nshould be visible on both the CE and worker nodes. Only the CE needs to\nhave write access to this directory. This directory must also contain a\nsub-directory \netc/\n with 1777 permissions.\n\n\ndata_dir\n:\n\nThis directory can be accessed via the \n$OSG_DATA\n environment variable. It\nshould be readable and writable on both the CE and worker nodes.\n\n\nworker_node_temp\n:\n\nThis directory will be accessed via the \n$OSG_WN_TMP\n environment variable.\nIt should allow read and write access on a worker node and can be visible\nto just that worker node.", 
            "title": "Configuration with OSG-Configure"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#configuration-with-osg-configure", 
            "text": "About this document  Invocation and script usage  Syntax and layout  Configuration sections  Job managers (batch systems):  Bosco  Condor  LSF  PBS  SGE  Slurm    Monitoring/reporting:  Gratia  Info Services  RSV  Subcluster / Resource Entry    Gateway  Local Settings  Misc Services  Site Information  Squid  Storage", 
            "title": "Configuration with OSG-Configure"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#about-this-document", 
            "text": "OSG-Configure and the INI files in  /etc/osg/config.d  allow a high level configuration of OSG services.\nThis document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software.  This page gives an overview of the options for each of the sections of the configuration files that  osg-configure  uses.", 
            "title": "About this document"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#invocation-and-script-usage", 
            "text": "The  osg-configure  script is used to process the INI files and apply changes to the system. osg-configure  must be run as root.  The typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes.  To verify the config files, run:  [root@server] osg-configure -v   OSG-Configure will list any errors in your configuration, usually including the section and option where the problem is.\nPotential problems are:   Required option not filled in  Invalid value  Syntax error  Inconsistencies between options   To apply changes, run:  [root@server] osg-configure -c   If your INI files do not change, then re-running  osg-configure -c  will result in the same configuration as when you ran it the last time.\nThis allows you to experiment with your settings without having to worry about messing up your system.  OSG-Configure is split up into modules. Normally, all modules are run when calling  osg-configure .\nHowever, it is possible to run specific modules separately.\nTo see a list of modules, including whether they can be run separately, run:  [root@server] osg-configure -l   If the module can be run separately, specify it with the  -m  MODULE  option:  [root@server] osg-configure -c -m  MODULE   Options may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses.\nYou may query the final value of an option via one of these methods:  [root@server] osg-configure -o  OPTION  [root@server] osg-configure -o  SECTION . OPTION   Logs are written to  /var/log/osg/osg-configure.log .\nIf something goes wrong, specify the  -d  flag to add more verbose output to  osg-configure.log .  The rest of this document will detail what to specify in the INI files.", 
            "title": "Invocation and script usage"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#conventions", 
            "text": "In the tables below:   Mandatory options for a section are given in  bold  type. Sometime the default value may be OK and no edit required, but the variable has to be in the file.  Options that are not found in the default ini file are in  italics .", 
            "title": "Conventions"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#syntax-and-layout", 
            "text": "The configuration files used by  osg-configure  are the one supported by Python's  SafeConfigParser , similar in format to the  INI configuration file  used by MS Windows:   Config files are separated into sections, specified by a section name in square brackets (e.g.  [Section 1] )  Options should be set using  name = value  pairs  Lines that begin with  ;  or  #  are comments  Long lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in  email RFC 822 )  Variable substitutions are supported --  see below   osg-configure  reads and uses all of the files in  /etc/osg/config.d  that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called  [PBS]  can be given in  20-pbs.ini  as well as  99-local-site-settings.ini .  Each of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g.  99-local-site-settings.ini ) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated.", 
            "title": "Syntax and layout"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#variable-substitution", 
            "text": "The osg-configure parser allows variables to be defined and used in the configuration file:\nany option set in a given section can be used as a variable in that section.  Assuming that you have set an option with the name  myoption  in the section, you can substitute the value of that option elsewhere in the section by referring to it as  %(myoption)s .   Note  The trailing  s  is required. Also, option names cannot have a variable subsitution in them.", 
            "title": "Variable substitution"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#special-settings", 
            "text": "If a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible.", 
            "title": "Special Settings"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#ignore-setting", 
            "text": "The  enabled  option, specifying whether a service is enabled or not, is a boolean but also accepts  Ignore  as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using  False  (or the  %(disabled)s  variable), because using  False  results in the service associated with the section being disabled.  osg-configure  will not change the configuration of the service if the  enabled  is set to  Ignore .  This is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the  Ignore  setting so that  osg-configure  does not alter the service's configuration and status.", 
            "title": "Ignore setting"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#configuration-sections", 
            "text": "The OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g.  [Section 1] ). The configuration is split in multiple files and options form one section can be in more than one files.  The following sections give an overview of the options for each of the sections of the configuration files that  osg-configure  uses.", 
            "title": "Configuration sections"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#bosco", 
            "text": "This section is contained in  /etc/osg/config.d/20-bosco.ini  which is provided by the  osg-configure-bosco  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Bosco jobmanager is being used or not.    users  String  A comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here.    endpoint  String  The remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of  , exactly as you would use to ssh into the remote cluster.    batch  String  The type of scheduler installed on the remote cluster.    ssh_key  String  The location of the ssh key, as created above.", 
            "title": "Bosco"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#condor", 
            "text": "This section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-condor.ini  which is provided by the  osg-configure-condor  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Condor jobmanager is being used or not.    condor_location  String  This should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the  osg-configure  script will try to get this from the CONDOR_LOCATION environment variable if available otherwise it will use  /usr  which works for the RPM installation.    condor_config  String  This should be set to be path where the condor_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the  osg-configure  script will try to get this from the CONDOR_CONFIG environment variable if available otherwise it will use  /etc/condor/condor_config , the default for the RPM installation.", 
            "title": "Condor"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#lsf", 
            "text": "This section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-lsf.ini  which is provided by the  osg-configure-lsf  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the LSF jobmanager is being used or not.    lsf_location  String  This should be set to be directory where lsf is installed", 
            "title": "LSF"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#pbs", 
            "text": "This section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-pbs.ini  which is provided by the  osg-configure-pbs  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the PBS jobmanager is being used or not.    pbs_location  String  This should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs_location/bin.    accounting_log_directory  String  This setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting.    pbs_server  String  This setting is optional and should point to your PBS server node if it is different from your OSG CE", 
            "title": "PBS"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#sge", 
            "text": "This section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-sge.ini  which is provided by the  osg-configure-sge  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the SGE jobmanager is being used or not.    sge_root  String  This should be set to be directory where sge is installed (e.g. same as  $SGE_ROOT  variable).    sge_cell  String  The sge_cell setting should be set to the value of $SGE_CELL for your SGE install.    default_queue  String  This setting determines queue that jobs should be placed in if the job description does not specify a queue.    available_queues  String  This setting indicates which queues are available on the cluster and should be used for validation when  validate_queues  is set.    validate_queues  String  This setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note.      Note  validate_queues : \nIf  available_queues  is set, that list of queues will be used for\nvalidation, otherwise SGE will be queried for available queues.", 
            "title": "SGE"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#slurm", 
            "text": "This section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-slurm.ini  which is provided by the  osg-configure-slurm  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Slurm jobmanager is being used or not.    slurm_location  String  This should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm_location/bin.    db_host  String  Hostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe.    db_port  String  Port of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe.    db_user  String  Username used to access the SLURM database. This information is needed to configure the SLURM gratia probe.    db_pass  String  The location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe.    db_name  String  Name of the SLURM database. This information is needed to configure the SLURM gratia probe.    slurm_cluster  String  The name of the Slurm cluster", 
            "title": "Slurm"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#gratia", 
            "text": "This section configures Gratia. If  probes  is set to  UNAVAILABLE , then  osg-configure  will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes,  %(osg-jobmanager-gratia)s ,  %(osg-gridftp-gratia)s ,  %(osg-metric-gratia)s ,  %(itb-jobmanager-gratia)s ,  %(itb-gridftp-gratia)s ,  %(itb-metric-gratia)s  are defined in the default configuration files to make it easier to specify the standard osg reporting.  This section is contained in  /etc/osg/config.d/30-gratia.ini  which is provided by the  osg-configure-gratia  RPM.     Option  Values Accepted  Explanation      enabled  True  ,  False ,  Ignore  This should be set to True if gratia should be configured and enabled on the installation being configured.    resource  String  This should be set to the resource name as given in the OIM registration    probes  String  This should be set to the gratia probes that should be enabled. A probe is specified by using as  [probe_type]:server:port . See note      Note  probes : \nLegal values for  probe_type  are:   metric  (for RSV)  jobmanager  (for the appropriate jobmanager probe)  gridftp  (for the GridFTP transfer probe)", 
            "title": "Gratia"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#info-services", 
            "text": "Reporting to the central CE Collectors is configured in this section.  In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations.  This section is contained in  /etc/osg/config.d/30-infoservices.ini , which is provided by the  osg-configure-infoservices  RPM. (This is for historical reasons.)     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  True if reporting should be configured and enabled    ce_collectors  String  The server(s) HTCondor-CE information should be sent to. See note      Note  ce_collectors :   Set this to  DEFAULT  to report to the OSG Production or ITB servers (depending on your  Site Information  configuration).  Set this to  PRODUCTION  to report to the OSG Production servers  Set this to  ITB  to report to the OSG ITB servers  Otherwise, set this to the  hostname:port  of a host running a  condor-ce-collector  daemon", 
            "title": "Info Services"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#rsv", 
            "text": "This section handles the configuration and setup of the RSV services.  This section is contained in  /etc/osg/config.d/30-rsv.ini  which is provided by the  osg-configure-rsv  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the rsv  service is being used or not.    rsv_user  String  This gives username that rsv will run under.  If this is blank or set to  UNAVAILABLE , it will default to rsv.    gratia_probes  String  This settings indicates which rsv gratia probes should be used.  It is a list of probes separated by a comma.  Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer    ce_hosts  String  This option lists the serviceURI of the CEs that generic RSV CE probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:2812 ).    htcondor_ce_hosts  String  This option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:2812 ).    gums_hosts  String  This option lists the serviceURI or FQDN of the CEs or SEs, using GUMS for authentication, that the RSV GUMS probes should check.  This should be a list of  CE  or  SE  FQDNs (and  not a GUMS server FQDN ) separated by a comma (e.g.  my.host,my.host2,my.host3 ).    gridftp_hosts  String  This option lists the serviceURI of the gridftp servers that the RSV gridftp probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host.iu.edu:2812,my.host2,my.host3 ).    gridftp_dir  String  This should be the directory that the gridftp probes should use during testing.  This defaults to  /tmp  if left blank or set to  UNAVAILABLE .    srm_hosts  String  This option lists the serviceURI of the srm servers that the RSV srm probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:8444 ).    srm_dir  String  This should be the directory that the srm probes should use during testing.    srm_webservice_path  String  This option gives the webservice path that SRM probes need to use along with the host:port. See note.    service_cert  String  This option should point to the public key file (pem) for your service  certificate. If this is left blank or set to  UNAVAILABLE  and the  user_proxy  setting is set, it will default to  /etc/grid-security/rsvcert.pem    service_key  String  This option should point to the private key file (pem) for your service  certificate. If this is left blank or set to  UNAVAILABLE  and the  service_cert  setting is enabled, it will default to  /etc/grid-security/rsvkey.pem  .    service_proxy  String  This should point to the location of the rsv proxy file. If this is left blank or set to  UNAVAILABLE  and the use_service_cert  setting is enabled, it will default to  /tmp/rsvproxy .    user_proxy  String  If you don't use a service certificate for rsv, you will need to specify a  proxy file that RSV should use in the proxy_file setting.  If this is set, then  service_cert, service_key, and service_proxy should be left blank, or set to  UNAVAILABE  or  DEFAULT .    enable_gratia  True ,  False  This option will enable RSV record uploading to central RSV collector at the GOC.   This should be set to True on all OSG resources (and to False on non-OSG resources).    setup_rsv_nagios  True ,  False  This option indicates whether rsv should upload results to a local  nagios server instance. This should be set to True or False.  This plugin is provided as an experimental component, and admins are recommend  not to enable  it on production resources.    rsv_nagios_conf_file  String  This option indicates the location of the rsv nagios  file to use for configuration details. This file  needs to be configured locally for RSV-Nagios forwarding to work  -- see inline comments in file for more information.    condor_location  String  If you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it)  you must specify the path to the install dir here.      Note  srm_webservice_path : \nFor dcache installations, this should work if left blank. However\nBestman-xrootd SEs normally use  srm/v2/server  as web service path, and so\nBestman-xrootd admins will have to pass this option with the appropriate\nvalue (for example:  srm/v2/server ) for the SRM probes to pass on their\nSE.", 
            "title": "RSV"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#subcluster-resource-entry", 
            "text": "Subcluster and Resource Entry configuration is for reporting about the worker resources on your site. A  subcluster  is a homogeneous set of worker node hardware; a  resource  is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system.  At least one Subcluster or Resource Entry section  is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources.  This section is contained in  /etc/osg/config.d/30-gip.ini  which is provided by the  osg-configure-gip  RPM. (This is for historical reasons.)  This configuration uses multiple sections of the OSG configuration files:   Subcluster* : options about homogeneous subclusters  Resource Entry* : options for specifying ATLAS queues for AGIS", 
            "title": "Subcluster / Resource Entry"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#notes-for-multi-ce-sites", 
            "text": "If you would like to properly advertise multiple CEs per cluster, make sure that you:   Set the value of site_name in the \"Site Information\" section to be the same for each CE.  Have the  exact  same configuration values for the Subcluster* and Resource Entry* sections in each CE.", 
            "title": "Notes for multi-CE sites."
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#subcluster-configuration", 
            "text": "Each homogeneous set of worker node hardware is called a  subcluster . For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format:  [Subcluster CHANGEME] , where CHANGEME is the globally unique subcluster name (yes, it must be a  globally  unique name for the whole grid, not just unique to your site. Get creative.)     Option  Values Accepted  Explanation      name  String  The same name that is in the Section label; it should be  globally unique    ram_mb  Positive Integer  Megabytes of RAM per node    cores_per_node  Positive Integer  Number of cores per node    allowed_vos  Comma-separated List or  *  The VOs that are allowed to run jobs on this subcluster (autodetected if  * ). Optional on OSG 3.3     The following attributes are optional:     Option  Values Accepted  Explanation      max_wall_time  Positive Integer  Maximum wall-clock time, in minutes, that a job is allowed to run on this subcluster (the default is one day or 1440 mins)    queue  String  The queue to which jobs should be submitted in order to run on this subcluster    extra_transforms  Classad  Transformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster     OSG 3.4 changes:   allowed_vos  is mandatory", 
            "title": "Subcluster Configuration"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#resource-entry-configuration-atlas-only", 
            "text": "If you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new  Resource Entry  section with a unique name in the following format:  [Resource Entry RESOURCE]  where RESOURCE is a globally unique resource name (it must be a  globally  unique name for the whole grid, not just unique to your site). The following options are required for the  Resource Entry  section and are used to generate the data required by AGIS:     Option  Values Accepted  Explanation      name  String  The same name that is in the  Resource Entry  label; it must be  globally unique    max_wall_time  Positive Integer  Maximum wall-clock time, in minutes, that a job is allowed to run on this resource    queue  String  The queue to which jobs should be submitted to run on this resource    cpucount  (alias  cores_per_node )  Positive Integer  Number of cores that a job using this resource can get    maxmemory  (alias  ram_mb )  Positive Integer  Maximum amount of memory (in MB) that a job using this resource can get    allowed_vos  Comma-separated List or  *  The VOs that are allowed to run jobs on this resource (autodetected if  * ). Optional on OSG 3.3     The following attributes are optional:     Option  Values Accepted  Explanation      subclusters  Comma-separated List  The physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file    vo_tag  String  An arbitrary label that is added to jobs routed through this resource     OSG 3.4 changes:   allowed_vos  is mandatory", 
            "title": "Resource Entry Configuration (ATLAS only)"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#gateway", 
            "text": "This section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses  condor-ce  as the gateway.  This section is contained in  /etc/osg/config.d/10-gateway.ini  which is provided by the  osg-configure-gateway  RPM.     Option  Values Accepted  Explanation      htcondor_gateway_enabled  True ,  False  (default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes.    job_envvar_path  String  The value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination.", 
            "title": "Gateway"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#local-settings", 
            "text": "This section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the  osg-local-job-environment.conf  verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The  osg-local-job-environment.conf  file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system.  Adding a line such as  My_Setting = my_Value  would result in the an environment variable called  My_Setting  set to  my_Value  in the job's environment.  my_Value  can also be defined in terms of an environment variable (i.e  My_Setting = $my_Value ) that will be evaluated on the worker node. For example, to add a variable  MY_PATH  set to  /usr/local/myapp , you'd have the following:  [Local Settings]  MY_PATH   =   /usr/local/myapp   This section is contained in  /etc/osg/config.d/40-localsettings.ini  which is provided by the  osg-configure-ce  RPM.", 
            "title": "Local Settings"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#misc-services", 
            "text": "This section handles the configuration of services that do not have a dedicated section for their configuration.  This section is contained in  /etc/osg/config.d/10-misc.ini  which is provided by the  osg-configure-misc  RPM.  This section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the  authentication section of the HTCondor-CE install documents .     Option  Values Accepted  Explanation      glexec_location  String  This gives the location of the glExec installation on the worker nodes, if it is present. Can be defined in terms of an environment variable (e.g.  $FOO ) that will be evaluated on the worker node. If it is not installed, set this to  UNAVAILABLE . glExec does not work with the  vomsmap  authorization method on OSG 3.3 and  is entirely unsupported starting in OSG 3.4    gums_host  String  This setting is used to indicate the hostname of the GUMS host that should be used for authentication, if the authorization method below is set to  xacml . If GUMS is not used, this should be set to  UNAVAILABLE .  GUMS is deprecated in OSG 3.4    authorization_method  gridmap ,  xacml ,  local-gridmap ,  vomsmap  This indicates which authorization method your site uses.  xacml   is deprecated in OSG 3.4    edit_lcmaps_db  True ,  False  (Optional, default True) If true, osg-configure will overwrite  /etc/lcmaps.db  to set your authorization method. The previous version will be backed up to  /etc/lcmaps.db.pre-configure    copy_host_cert_for_service_certs  True ,  False  (Optional, default False) If true, osg-configure will create a copy or copies of your host cert and key as service certs for RSV and (on OSG 3.3) GUMS     OSG 3.4 changes:   glexec_location  must be  UNAVAILABLE  or unset  authorization_method  defaults to  vomsmap  authorization_method  will raise a warning if set to  xacml", 
            "title": "Misc Services"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#site-information", 
            "text": "The settings found in the  Site Information  section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc.  This section is contained in  /etc/osg/config.d/40-siteinfo.ini  which is provided by the  osg-configure-ce  RPM.     Option  Values Accepted  Description      group  OSG  ,  OSG-ITB  This should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG    host_name  String  This should be set to be hostname of the CE that is being configured    resource  String  The resource name of this CE endpoint as registered in OIM.    resource_group  String  The resource_group of this CE as registered in OIM.    sponsor  String  This should be set to the sponsor of the resource. See note.    site_policy  Url  This should be a url pointing to the resource's usage policy    contact  String  This should be the name of the resource's admin contact    email  Email address  This should be the email address of the admin contact for the resource    city  String  This should be the city that the resource is located in    country  String  This should be two letter country code for the country that the resource is located in.    longitude  Number  This should be the longitude of the resource. It should be a number between -180 and 180.    latitude  Number  This should be the latitude of the resource. It should be a number between -90 and 90.      Note  sponsor : \nIf your resource has multiple sponsors, you can separate them using commas\nor specify the percentage using the following format 'osg, atlas, cms' or\n'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple\nsponsors are used. If you have a sponsor that is not an OSG VO, you can\nindicate this by using 'local' as the VO.", 
            "title": "Site Information"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#squid", 
            "text": "This section handles the configuration and setup of the squid web caching and proxy service.  This section is contained in  /etc/osg/config.d/01-squid.ini  which is provided by the  osg-configure-squid  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the squid service is being used or not.    location  String  This should be set to the  hostname:port  of the squid server.", 
            "title": "Squid"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#storage", 
            "text": "This section gives information about the options in the Storage section of the configuration file.\nSeveral of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models.\nPlease review the Storage Related Parameters section of the Environment Variables \ndescription and  Site Planning \ndiscussions for explanations of the various storage models and the requirements for them.  This section is contained in  /etc/osg/config.d/10-storage.ini  which is provided by the  osg-configure-ce  RPM.     Option  Values Accepted  Explanation      se_available  True ,  False  This indicates whether there is an associated SE available.    default_se  String  If an SE is available at your cluster, set default_se to the hostname of this SE, otherwise set default_se to UNAVAILABLE.    grid_dir  String  This setting should point to the directory which holds the files from the OSG worker node package. See note    app_dir  String  This setting should point to the directory which contains the VO specific applications. See note    data_dir  String  This setting should point to a directory that can be used to store and stage data in and out of the cluster. See note    worker_node_temp  String  This directory should point to a directory that can be used as scratch space on compute nodes. If not set, the default is UNAVAILABLE. See note    site_read  String  This setting should be the location or url to a directory that can be read to stage in data via the variable  $OSG_SITE_READ . This is an url if you are using a SE. If not set, the default is UNAVAILABLE    site_write  String  This setting should be the location or url to a directory that can be write to stage out data via the variable  $OSG_SITE_WRITE . This is an url if you are using a SE. If not set, the default is UNAVAILABLE      Note  All of these can be defined in terms of an environment variable\n(e.g.  $FOO ) that will be evaluated on the worker node.   grid_dir : \nIf you have installed the worker node client via RPM (the normal case) it\nshould be  /etc/osg/wn-client .  If you have installed the worker node in a\nspecial location (perhaps via the worker node client tarball or via OASIS),\nit should be the location of that directory.  This directory will be accessed via the  $OSG_GRID  environment variable.\nIt should be visible on all of the compute nodes. Read access is required,\nthough worker nodes don't need write access.  app_dir : \nThis directory will be accesed via the  $OSG_APP  environment variable. It\nshould be visible on both the CE and worker nodes. Only the CE needs to\nhave write access to this directory. This directory must also contain a\nsub-directory  etc/  with 1777 permissions.  data_dir : \nThis directory can be accessed via the  $OSG_DATA  environment variable. It\nshould be readable and writable on both the CE and worker nodes.  worker_node_temp : \nThis directory will be accessed via the  $OSG_WN_TMP  environment variable.\nIt should allow read and write access on a worker node and can be visible\nto just that worker node.", 
            "title": "Storage"
        }, 
        {
            "location": "/worker-node/using-wn/", 
            "text": "Introduction\n\n\nThe Worker Node Client is a collection of useful software components that is expected to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that can be used to locate resources..\n\n\nThis page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node.\n\n\nThe OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself (preferred; most portable) or utilize CVMFS. Sites are not required to provide any specific tools (\ngcc\n, \nlapack\n, \nblas\n, etc.) beyond the ones in the OSG worker node client.\n\n\nCommon software available on worker nodes.\n\n\nThe OSG worker node client (called the \nosg-wn-client\n package) contains the following software:\n\n\n\n\nThe supported set of CA certificates (located in $X509_CERT_DIR after the environment is set up)\n\n\nProxy management tools:\n\n\nCreate proxies: \nvoms-proxy-init\n and \ngrid-proxy-init\n\n\nShow proxy info: \nvoms-proxy-info\n and \ngrid-proxy-info\n\n\nDestroy the current proxy: \nvoms-proxy-destroy\n and \ngrid-proxy-destroy\n\n\n\n\n\n\nData transfer tools:\n\n\nHTTP/plain FTP protocol tools (via system dependencies):\n\n\nwget\n and \ncurl\n: standard tools for downloading files with HTTP and FTP\n\n\n\n\n\n\nTransfer clients\n\n\nGFAL\n-based client (\ngfal-copy\n and others).  GFAL supports SRM, GridFTP, and HTTP protocols.\n\n\nGlobus GridFTP client (\nglobus-url-copy\n)\n\n\n\n\n\n\n\n\n\n\nMyProxy client tools\n\n\n\n\nThe Worker Node Environment\n\n\nThe following table outlines the various important directories and information in the worker node environment. A job running on an OSG worker node can refer to each directory using the corresponding environment variable.\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nPurpose\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n$X509_CERT_DIR\n\n\nLocation of the CA certificates\n\n\n\n\n\n\n\n\n$OSG_WN_TMP\n\n\nTemporary storage area in which your job(s) run\n\n\nLocal to each batch slot. Create a directory under this as your work area.\n\n\n\n\n\n\n$_CONDOR_SCRATCH_DIR\n\n\nSuggested temporary storage are for glideinWMS-based VOs.\n\n\nUsers should prefer this environment variable if running inside glideinWMS.\n\n\n\n\n\n\n$OSG_SQUID_LOCATION\n, \nhttp_proxy\n\n\nLocation of a HTTP caching proxy server\n\n\nUtilize this service for downloading files via HTTP for cache-friendly workflows.\n\n\n\n\n\n\n$OSG_GRID\n\n\nLocation of additional environment variables.\n\n\nPilots should source \n$OSG_GRID/setup.sh\n in order to guarantee the environment contains the worker node binaries in \n$PATH\n.\n\n\n\n\n\n\n$OSG_SITE_NAME\n\n\nName of the site where the worker node is located.\n\n\n\n\n\n\n\n\n\n\nBe careful with using \n$OSG_WN_TMP\n; at some sites, this directory might be shared with other VOs. We recommend creating a new sub-directory as a precautio:\n\n\nmkdir -p \n$OSG_WN_TMP\n/MYVO\n\nexport\n \nmydir\n=\n`\nmktemp -d -t MYVO\n`\n\n\ncd\n \n$mydir\n\n\n# Run the rest of your application\n\nrm -rf \n$mydir\n\n\n\n\n\n\nA significant number of sites use the batch system to make an independent directory for each user job, and change \n$OSG_WN_TMP\n on the fly to point to this directory.\n\n\nThere is no way to know in advance how much scratch disk space any given worker node has available; recall, what disk space is available may be shared among a number of job slots.", 
            "title": "Worker Node Environment"
        }, 
        {
            "location": "/worker-node/using-wn/#introduction", 
            "text": "The Worker Node Client is a collection of useful software components that is expected to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that can be used to locate resources..  This page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node.  The OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself (preferred; most portable) or utilize CVMFS. Sites are not required to provide any specific tools ( gcc ,  lapack ,  blas , etc.) beyond the ones in the OSG worker node client.", 
            "title": "Introduction"
        }, 
        {
            "location": "/worker-node/using-wn/#common-software-available-on-worker-nodes", 
            "text": "The OSG worker node client (called the  osg-wn-client  package) contains the following software:   The supported set of CA certificates (located in $X509_CERT_DIR after the environment is set up)  Proxy management tools:  Create proxies:  voms-proxy-init  and  grid-proxy-init  Show proxy info:  voms-proxy-info  and  grid-proxy-info  Destroy the current proxy:  voms-proxy-destroy  and  grid-proxy-destroy    Data transfer tools:  HTTP/plain FTP protocol tools (via system dependencies):  wget  and  curl : standard tools for downloading files with HTTP and FTP    Transfer clients  GFAL -based client ( gfal-copy  and others).  GFAL supports SRM, GridFTP, and HTTP protocols.  Globus GridFTP client ( globus-url-copy )      MyProxy client tools", 
            "title": "Common software available on worker nodes."
        }, 
        {
            "location": "/worker-node/using-wn/#the-worker-node-environment", 
            "text": "The following table outlines the various important directories and information in the worker node environment. A job running on an OSG worker node can refer to each directory using the corresponding environment variable.     Environment Variable  Purpose  Notes      $X509_CERT_DIR  Location of the CA certificates     $OSG_WN_TMP  Temporary storage area in which your job(s) run  Local to each batch slot. Create a directory under this as your work area.    $_CONDOR_SCRATCH_DIR  Suggested temporary storage are for glideinWMS-based VOs.  Users should prefer this environment variable if running inside glideinWMS.    $OSG_SQUID_LOCATION ,  http_proxy  Location of a HTTP caching proxy server  Utilize this service for downloading files via HTTP for cache-friendly workflows.    $OSG_GRID  Location of additional environment variables.  Pilots should source  $OSG_GRID/setup.sh  in order to guarantee the environment contains the worker node binaries in  $PATH .    $OSG_SITE_NAME  Name of the site where the worker node is located.      Be careful with using  $OSG_WN_TMP ; at some sites, this directory might be shared with other VOs. We recommend creating a new sub-directory as a precautio:  mkdir -p  $OSG_WN_TMP /MYVO export   mydir = ` mktemp -d -t MYVO `  cd   $mydir  # Run the rest of your application \nrm -rf  $mydir   A significant number of sites use the batch system to make an independent directory for each user job, and change  $OSG_WN_TMP  on the fly to point to this directory.  There is no way to know in advance how much scratch disk space any given worker node has available; recall, what disk space is available may be shared among a number of job slots.", 
            "title": "The Worker Node Environment"
        }, 
        {
            "location": "/data/bestman-overview/", 
            "text": "Berkeley Storage Manager\n\n\nBeStMan, or Berkeley Storage Manager, is a full implementation of SRM v2.2, developed by Lawrence Berkeley National Laboratory, for a disk based storage and mass storage systems. End users may have their own personal BeStMan that manages and gives an SRM interface to their own local disks. It works on top of existing disk-based unix file systems, and has been reported so far to work on file systems such as NFS, GPFS, PVFS, GFS, Ibrix, HFS+, Hadoop, XrootdFS and Lustre. It also works with any existing file transfer service, such as gsiftp, http, https, bbftp and ftp. It requires minimal administrative efforts on the deployment and updates. BeStMan2 is a Jetty based implementation of SRM v2.2.\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of June 2018.\n\n\nPlanning\n\n\n\n\nInformation on various BeStMan architectures\n\n\nBestman home page\n: Bestman project home\n\n\n\n\nInstallation\n\n\n\n\nInstall Bestman SE\n: BeStMan installation interfacing with local disk\n\n\nInstall Bestman Gateway Xrootd\n: BeStMan gateway mode on top of XRootD\n\n\nInstall Bestman Gateway Hadoop\n: BeStMan gateway mode on top of Hadoop\n\n\nLBNL Configuration Reference\n\n\n\n\nOperation\n\n\n\n\nLBNL SRM Client\n: SRM client provided with BeStMan.  This is deprecated; use of \ngfal-utils\n is strongly encouraged instead.\n\n\nBeStMan SRMTester\n: Tester for BeStMan SRM instances\n\n\nUsing Adler checksums with BeStMan", 
            "title": "BeStMan Overview"
        }, 
        {
            "location": "/data/bestman-overview/#berkeley-storage-manager", 
            "text": "BeStMan, or Berkeley Storage Manager, is a full implementation of SRM v2.2, developed by Lawrence Berkeley National Laboratory, for a disk based storage and mass storage systems. End users may have their own personal BeStMan that manages and gives an SRM interface to their own local disks. It works on top of existing disk-based unix file systems, and has been reported so far to work on file systems such as NFS, GPFS, PVFS, GFS, Ibrix, HFS+, Hadoop, XrootdFS and Lustre. It also works with any existing file transfer service, such as gsiftp, http, https, bbftp and ftp. It requires minimal administrative efforts on the deployment and updates. BeStMan2 is a Jetty based implementation of SRM v2.2.  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of June 2018.", 
            "title": "Berkeley Storage Manager"
        }, 
        {
            "location": "/data/bestman-overview/#planning", 
            "text": "Information on various BeStMan architectures  Bestman home page : Bestman project home", 
            "title": "Planning"
        }, 
        {
            "location": "/data/bestman-overview/#installation", 
            "text": "Install Bestman SE : BeStMan installation interfacing with local disk  Install Bestman Gateway Xrootd : BeStMan gateway mode on top of XRootD  Install Bestman Gateway Hadoop : BeStMan gateway mode on top of Hadoop  LBNL Configuration Reference", 
            "title": "Installation"
        }, 
        {
            "location": "/data/bestman-overview/#operation", 
            "text": "LBNL SRM Client : SRM client provided with BeStMan.  This is deprecated; use of  gfal-utils  is strongly encouraged instead.  BeStMan SRMTester : Tester for BeStMan SRM instances  Using Adler checksums with BeStMan", 
            "title": "Operation"
        }, 
        {
            "location": "/data/bestman-install/", 
            "text": "Installing BeStMan\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of June 2018.\n\n\n\n\nAbout this Document\n\n\nThis document explains how to install a BeStMan SRMv2 service. This procedure will guide one through the installation and configuration of a basic \nbestman2\n host with an underlying GridFTP server. This will allow the service to service requests via the SRM (Storage Resource Manager) protocol or the GridFTP protocol.\n\n\nInstalling BeStMan Storage Element\n\n\nThis procedure explains how to install the stand-alone BeStMan Storage Element server; \nsee below\n for notes on upgrading.  The service has the following components:\n\n\n\n\nBeStMan - provides load-balancing across GridFTP servers.\n\n\nGridFTP server - provides file transfer services using the GridFTP protocol.\n\n\nGratia gridftp transfer probe\n (optional) - provides transfer accounting information to the OSG.\n\n\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nYou need at least one node in order to install this service.\n\n\nThe OS must be in the \nsupported platforms\n list.\n\n\nThe \nOSG software repositories\n must be configured correctly.\n\n\nAll procedures in this document require \nroot\n privileges.\n\n\n\n\nUsers\n\n\nThis installation will create following users unless they are already created:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nbestman\n\n\nUsed by Bestman SRM server\n\n\n\n\n\n\n\n\nFor full functionality, the \nbestman\n account will need limited \nsudo\n access to a few commands, described below.\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server. For GUMS sites, this means that each user that can be authenticated by GUMS should be created on the server.\n\n\nNote that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in GUMS, then new users should also be added here.\n\n\nCertificates\n\n\nTwo certificates are needed for operation of this service.\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n and \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\nBestman service certificate\n\n\nbestman\n\n\n/etc/grid-security/bestman/bestmancert.pem\n and \n/etc/grid-security/bestman/bestmankey.pem\n\n\n\n\n\n\n\n\nFollowing the \ninstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates. Note that the \nosg-se-bestman\n package will automatically install a certificate package but will not necessarily pick the cert package you expect; see \nthe CA certificates\n documentation for more information.\n\n\nNetworking\n\n\nFor more details on overall firewall configuration, please see our \nfirewall documentation\n.\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_PORT_RANGE\n\n\nX\n\n\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_SOURCE_RANGE\n\n\n\n\nX\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP control channel\n\n\ntcp\n\n\n2811\n\n\nX\n\n\n\n\n\n\n\n\n\n\nSRM\n\n\ntcp\n\n\n8443\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nEngineering Considerations\n\n\nPlease answer following questions before you proceed with installation and configuration of BeStMan storage element:\n\n\nQ. \nWhat authorization mechanism should I use?\n\n\nDecide between a \ngrid-mapfile\n or a \nGUMS\n server for authorization.  Both mechanisms are deprecated with a planned removal by June 2018.  The replacement mechanism, however, does not work with \nbestman2\n.\n\n\nQ. \nHow many GridFTP servers will I need?\n\n\nChoose to run multiple GridFTP servers for load balancing and better performance. We recommend to install additional GridFTP servers if your Storage Element:\n\n\n\n\nIs serving data to more than 1000 cores for VOs that use storage heavily (e.g. CMS, ATLAS, CDF, and D0),\n\n\nIs managing more than 500 TB of disk space, OR\n\n\nHas more than 10Gbps bandwidth\n\n\n\n\nWe recommend approximately one GridFTP server for each 8Gbps of desired utilized bandwidth.\n\n\nQ. \nDo I need to change default configuration of Gridftp server?\n\n\nYes, you may want to do this if the node on which GridFTP server will be installed has multiple network interfaces. Read \nthis section\n for more details.\n\n\nQ. \nDo you need to enable Gratia gridftp-transfer probes?\n \n\nThe Gratia gridftp-transfer probes provide OSG storage statistics for accounting purposes. The reports include the source and destination of transfers, certificate subject of transfer initiator, as well as the size and status of the transferred file. The probe needs to be installed on every GridFTP server.\n\n\nInstall Instructions\n\n\nInstalling BeStMan2\n\n\n\n\nInstall Java using \nthese instructions\n.\n\n\nInstall the BeStMan Storage element meta-package:\n\n\n\n\n[root@client ~] #\n yum install osg-se-bestman\n\n\n\n\n\nAuthorization\n\n\nThere are two authorization options:\n\n\n\n\nGridmap file\n\n\nGUMS authentication server\n\n\n\n\nPlease choose one of these and follow the instructions in one of the two following sections.\n\n\nConfiguring Gridmap Support\n\n\nBy default, GridFTP uses a gridmap file, found in \n/etc/grid-security/grid-mapfile\n. This file is not generated by default. can generate this file. You can generate this file manually, by including DN/username combinations (this is most useful for debugging). Otherwise, you can use \nedg-mkgridmap\n, which will periodically contact a list of VOMS servers that you specify.\n\n\nOnce \nedg-mkgridmap\n is configured, you will have to modify \n/etc/bestman2/conf/bestman2.rc\n and change \nGridMapFileName\n from \n/etc/bestman2/conf/grid-mapfile.empty\n to:\n\n\nGridMapFileName=/etc/grid-security/grid-mapfile\n\n\n\n\n\nIn \n/etc/sysconfig/bestman2\n, change\n\n\nBESTMAN_GUMS_ENABLED\n=\nno\n\n\n\n\n\nConfiguring GUMS support\n\n\nBy default, GridFTP uses a gridmap file, found in \n/etc/grid-security/gridmap-file\n. If you want to use GUMS security (recommended), you will need to enable it using the following steps.\n\n\nFirst, edit \n/etc/grid-security/gsi-authz.conf\n and uncomment the authorization callout:\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\n\nNext edit \n/etc/lcmaps.db\n to enter the correct GUMS hostname:\n\n\ngumsclient = \nlcmaps_gums_client.mod\n\n             \n-resourcetype ce\n\n             \n-actiontype execute-now\n\n             \n-capath /etc/grid-security/certificates\n\n             \n-cert   /etc/grid-security/hostcert.pem\n\n             \n-key    /etc/grid-security/hostkey.pem\n\n             \n--cert-owner root\n\n# Change this URL to your GUMS server\n             \n--endpoint https://\nGUMS_HOSTNAME\n:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\n\nYou will need to modify the following settings in \n/etc/sysconfig/bestman2\n\n\nBESTMAN_GUMSCERTPATH\n=\n/etc/grid-security/bestman/bestmancert.pem\n\nBESTMAN_GUMSKEYPATH\n=\n/etc/grid-security/bestman/bestmankey.pem\n\n\n\n\n\nYou will need to modify the following settings in \n/etc/bestman2/conf/bestman2.rc\n\n\nGUMSserviceURL=https://\nGUMS_HOST\n:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\nEdit Bestman Settings\n\n\nBestman settings are split into three files:\n\n\n\n\nEnvironment variables (except those that represent server and client libraries) are stored in \n/etc/sysconfig/bestman2\n.\n\n\nThe server and client library environment variables are stored in \n/etc/sysconfig/bestman2lib\n.\n\n\nConfiguration is stored in \n/etc/bestman2/conf/bestman2.rc\n.\n\n\n\n\nYou should review these settings to make sure all of them comply with your environment. You are not expected to edit \n/etc/sysconfig/bestman2lib\n .\n\n\n\n\nNote\n\n\nIf you are upgrading from a version prior to 2.3.0-9, you will need to remove \nall\n entries for \nBESTMAN2_SERVER_LIB\n and \nBESTMAN2_CLIENT_LIB\n in file \n/etc/sysconfig/bestman2.\n These settings are now present in file \n/etc/sysconfig/bestman2lib\n\n\n\n\nYou will likely need to modify the following settings in \n/etc/bestman2/conf/bestman2.rc\n:\n\n\nlocalPathListAllowed=/tmp\nCertFileName=/etc/grid-security/bestman/bestmancert.pem\nKeyFileName=/etc/grid-security/bestman/bestmankey.pem\nsupportedProtocolList=gsiftp://\nGRIDFTP_HOSTNAME\n;gsiftp://\nGRIDFTP_HOSTNAME2\n\n\n\n\n\n\n\n\nNote\n\n\nMake sure the value for \nlocalPathListAllowed\n is correctly entered - i.e. each path separated by a \n;\n. If it is not, this parameter may not be effective.\nMake sure the permissions for the \nlocalPathListAllowed\n directory(ies) are set to 1777, which is the default for \n/tmp\n. Further, note that on many systems, \n/tmp\n gets cleared out automatically, so you may want to use a different location to ensure that the files persist.\n\n\n\n\nBeStMan requires up to two sets of certificate pairs. One is for host services; when clients connect to BeStMan, they will receive this certificate (\nCertFileName\n, \nKeyFileName\n) as proof of the server identity. The second certificate pair (\nBESTMAN_GUMSCERTPATH\n, \nBESTMAN_GUMSKEYPATH\n) is used to communicate with GUMS when verifying identity information (this only applicable for GUMS-enabled sites). These two can (and usually will be) the same files, but can be split if your GUMS setup requires a specific identity.\n\n\nlocalPathListAllowed\n determines which paths users will be able to access via SRM.\n\n\nsupportedProtocolList\n is a semi-colon list of GridFTP servers that the BeStMan will use as transfer agents. If you are using anything but the standard GridFTP port 2811, you will also have to add the port (ie \ngsiftp://\nHOSTNAME\n:port\n).\n\n\nFinally, modify \nGUMSserviceURL\n to use your local GUMS installation if you are using GUMS.\n\n\nModify \n/etc/sudoers\n\n\nBeStman requires the \nsudo\n command in order to write information as the proper user. You will need to give the \nbestman\n user the proper permissions to run these commands.\n\n\nModify \n/etc/sudoers\n and comment the following line.\n\n\n#Defaults    requiretty\n\n\n\n\n\nThen add the following lines at the end of the \n/etc/sudoers\n file.\n\n\nCmnd_Alias SRM_CMD = /bin/rm, /bin/mkdir, /bin/rmdir, /bin/mv, /bin/cp, /bin/ls\nRunas_Alias SRM_USR = ALL, !root\nbestman   ALL=(SRM_USR) NOPASSWD: SRM_CMD\n\n\n\n\n\nCopying certificates to an alternate location\n\n\nBeStMan requires a certificate pair to function; this must be readable by the \nbestman\n user.\n\n\n[root@client ~] #\n cp /etc/grid-security/hostkey.pem /etc/grid-security/bestman/bestmankey.pem\n\n[root@client ~] #\n cp /etc/grid-security/hostcert.pem /etc/grid-security/bestman/bestmancert.pem\n\n[root@client ~] #\n chown -R bestman:bestman /etc/grid-security/bestman/\n\n\n\n\n\nVerify \nCertFileName\n and \nKeyFileName\n in \n/etc/bestman2/conf/bestman2.rc\n are set appropriately.\n\n\n(Optional) Using a different bestman user\n\n\nIf you would like to use a different user than the default \nbestman\n user (\nnot recommended\n), you will need to change the following:\n\n\n\n\nOwnership of bestman certs in \n/etc/grid-security/bestman\n.\n\n\nSRM_OWNER\n in \n/etc/sysconfig/bestman2\n to the new user.\n\n\nUser in \n/etc/sudoers\n. The last line (\nbestman ALL(SRM_USR) NOPASSWD: SRM_CMD\n) should be changed from \nbestman\n to the new user.\n\n\nOwnership of \n/var/log/bestman2\n\n\n\n\n\n\nWarning\n\n\nCurrently the RPM packaging will change the ownership of the \n/var/log/bestman2\n directory back to \nbestman\n on upgrades.\n\n\n\n\n(Optional) Modifying default logging for \nevent.srm.log\n\n\nThe logging directory (\n/var/log/bestman2\n) has two types of logs - \nbestman2.log\n and \nevent.srm.log\n.\n\n\nLog-rotation of \nbestman2.log\n file is controlled by \n/etc/logrotate.d/bestman2\n file.\n\n\nBy default, the size of \nevent.srm.log\n log file is set to 50MB within the Bestman code itself.\n\n\nLeft unchanged, \nevent.srm.log\n file counts will keep increasing indefinitely.  Depending on the usage, the number of these files can become high enough to fill up the partition that holds these logs.\n\n\nThere are 3 ways to avoid this -\n\n\n\n\n\n\nModify following parameters (commented by default) in the \n/etc/sysconfig/bestman2\n file\n\n\n# Number of files to keep\n\n\nBESTMAN_EVENT_LOG_COUNT\n=\n10\n\n\n# Size of each file in bytes\n\n\nBESTMAN_EVENT_LOG_SIZE\n=\n20971520\n\n\n\n\n\n\nThe optimal value for these depends on usage of the service.\n\n\n\n\n\n\nCreate a directory under a much bigger partition and have a symlink from \n/var/log/bestman2\n to that directory.\n\n\n\n\nLeave the default settings, but have your own custom script that cleans these files according to your needs.\n\n\n\n\nStarting Services\n\n\n\n\n\n\nfetch-crl\n\n\nFor RHEL 6:\n\n\n[root@client ~] #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs \n\n\n[root@client ~] #\n /sbin/service fetch-crl-boot start\n\n[root@client ~] #\n /sbin/service fetch-crl-cron start\n\n\n\n\n\nFor RHEL 7:\n\n\n[root@client ~] #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs \n\n\n[root@client ~] #\n systemctl start fetch-crl-boot\n\n[root@client ~] #\n systemctl start fetch-crl-cron\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n[root@client ~] #\n service globus-gridftp-server start\n\n\n\n\n\n\n\n\n\nBestman\n\n\n[root@client ~] #\n service bestman2 start\n\n\n\n\n\nTo start Bestman automatically at boot time\n\n\n[root@client ~] #\n chkconfig bestman2 on\n\n\n\n\n\n\n\n\n\nGratia transfer probe:\n\n\n[root@client ~] #\n service gratia-gridftp-transfer start\n\n\n\n\n\n\n\n\n\nStopping Services\n\n\n\n\n\n\nfetch-crl\n\n\nFor RHEL 6:\n\n\n[root@client ~] #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs\n\n\n[root@client ~] #\n /sbin/service fetch-crl-boot stop\n\n[root@client ~] #\n /sbin/service fetch-crl-cron stop\n\n\n\n\n\nFor RHEL 7:\n\n\n[root@client ~] #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs\n\n\n[root@client ~] #\n systemctl stop fetch-crl-boot\n\n[root@client ~] #\n systemctl stop fetch-crl-cron\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n[root@client ~] #\n service globus-gridftp-server stop\n\n\n\n\n\n\n\n\n\nBestman\n\n\n[root@client ~] #\n service bestman2 stop\n\n\n\n\n\n\n\n\n\nGratia transfer probe\n\n\n[root@client ~] #\n service gratia-gridftp-transfer stop\n\n\n\n\n\n\n\n\n\nValidation of Service Operation\n\n\nOnce you have your SE setup and configured, there are several ways to monitor your installation. Refer to the following pages for more information:\n\n\n\n\nBeStMan SRM Tester\n.\n\n\nRSV\n which includes SRM probes as well.\n\n\n\n\nYou can also self-test to verify your installation with an SRM client such as \ngfal-copy\n.\n\n\nTroubleshooting\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\n/var/log/bestman2/bestman2.log\n\n\nBeStMan2 server log and errors\n\n\n\n\n\n\n\n\n/var/log/bestman2/event.srm.log\n\n\nRecords all SRM transactions\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nTransfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nAuthentication log\n\n\n\n\n\n\n\n\n/var/log/messages\n\n\nMain system log (look here for LCMAPS errors)\n\n\n\n\n\n\n\n\nDebugging Procedure\n\n\nIf system validation failed, you would probably need to check each component in order to verify your installation. In order to do so, you should check all of them in the following order\n\n\n\n\nGUMS (if in use)\n\n\nGridFTP\n\n\nBeStMan\n\n\n\n\nVerifying GUMS\n\n\nMake sure that the service certificate you specified for BeStMan configuration with \nGUMSHOSTCERT\n, \nGUMSHOSTKEY\n options and GridFTP service certificate are accepted by GUMS.\n\n\nTest GUMS by running:\n\n\n[root@client ~] #\n srm-ping srm://\nBESTMAN_HOST\n:8443/srm/v2/server\n\n\n\n\n\nIn the output, check that your \ngumsIDMapped\n is not \nnull\n. It returns the \nuid\n that GUMS will map you to. This can be obtained from your GUMS administrator. Verify that this \nuid\n exists on BeStMan and GridFTP node.\n\n\nVerifying GridFTP\n\n\nLogin on the node where your certificate and \nOSG Worker Node Client\n is installed You will need to generate your proxy credentials using \ngrid-proxy-init\n or \nvoms-proxy-init\n.\n\n\nThen test GridFTP using \nglobus-url-copy\n:\n\n\n[user@client ~] $\n \necho\n \nThis is a test\n \n/tmp/test \n\n[user@client ~] $\n globus-url-copy -dbg file:///tmp/test gsiftp://\nGRIDFTP_HOST\n/tmp/test \n\n\n\n\n\nCheck the GridFTP logs to see if you have encountered any errors.\n\n\nVerifying BeStMan\n\n\nMake sure that the BeStMan process is running\n\n\n[root@client ~] #\n ps -ef \n|\n grep bestman\n\nbestman   5121     1 99 19:59 ?        00:00:01 /usr/java/latest/bin/java -server -Xmx1024m -XX:MaxDirectMemorySize=1024m -DX509_CERT_DIR=/etc/grid-security/certificates -DCADIR=/etc/grid-security/certificates -Daxis.socketSecureFactory=org.glite.security.trustmanager.axis.AXISSocketFactory -DsslCAFiles=/etc/grid-security/certificates/*.0 -DsslCertfile=/etc/grid-security/bestman/bestmancert.pem -DsslKey=/etc/grid-security/bestman/bestmankey.pem -DJettyConfiguration=/etc/bestman2/conf/WEB-INF/jetty.xml -DJettyDescriptor=/etc/bestman2/conf/WEB-INF/web.xml -DJettyResource=/etc/bestman2/conf/ -Dorg.eclipse.jetty.util.log.IGNORE=true gov.lbl.srm.server.Server /etc/bestman2/conf/bestman2.rc\n\n\n\n\n\n\nIf \nbestman2\n is not running, check information in the log file \n/var/log/bestman2/bestman2.log\n.\n\n\nUseful Configuration and Log Files\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\n/etc/bestman2/conf/bestman2.rc\n\n\nMain BeStMan2 configuration file\n\n\n\n\n\n\n\n\n/etc/sysconfig/bestman2\n\n\nEnvironment variables used by BeStMan2\n\n\n\n\n\n\n\n\n/etc/sysconfig/bestman2lib\n\n\nEnvironment variables that store values of various client and server libraries used by BeStMan2\n\n\n\n\n\n\n\n\n/etc/bestman2/conf/*\n\n\nOther runtime configuration files\n\n\n\n\n\n\n\n\n/etc/init.d/bestman2\n\n\ninit.d startup script\n\n\n\n\n\n\n\n\n/etc/gridftp.conf\n\n\nStartup parameters\n\n\n\n\n\n\nGridFTP\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nEnvironment variables for GridFTP\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\nGridFTP Gratia Probe configuration\n\n\n\n\n\n\n\n\n/etc/cron.d/gratia-probe-gridftp-transfer.cron\n\n\nCron tab file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\n/var/log/bestman2/bestman2.log\n\n\nBeStMan2 container log\n\n\n\n\n\n\n\n\n/var/log/bestman2/event.srm.log\n\n\nRecords all SRM transactions\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nTransfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nGridFTP authorization log\n\n\n\n\n\n\n\n\n/var/log/messages\n\n\nMain system log (look here for LCMAPS errors)\n\n\n\n\n\n\nGratia probe\n\n\n/var/log/gratia\n\n\n\n\n\n\n\n\n\n\nUpgrading BeStMan\n\n\nUpgrading BeStMan can be done by\n\n\n[root@client ~] #\n yum upgrade bestman2-server\n\n\n\n\n\nThere are a few notes to be aware of when upgrading BeStMan.\n\n\n\n\nFrom many of the versions of the BeStMan, configuration changes have taken place. Do not ignore any warnings about rpmsave or rpmnew files. You will need to especially be careful about and \n/etc/bestman2/conf/bestman2.rc\n.\n\n\nBeginning with BeStMan 2.3.0-9, many dependency locations changed. Be sure that \n/etc/sysconfig/bestman2lib\n contains the \nbuild-classpath\n directives in the \nBESTMAN2_SERVER_LIB\n and \nBESTMAN2_CLIENT_LIB\n. Otherwise, you may get java class loading errors on startup or on run-time. Be sure to remove these entries from the \n/etc/sysconfig/bestman2\n file.\n\n\nFor BeStMan 2.1.3, certain versions had a combined sysconfig and configuration file. You may need to split these files apart if this is the case.\n\n\n\n\nFor more help, please contact the GOC to create a support ticket.\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug support and issues, submit a ticket to the \nGrid Operations Center\n.\n\n\nFor community support and best-effort software team support contact \n.\n\n\n\n\nFor a full set of help options, see the \nHelp Procedure\n.\n\n\nReferences\n\n\n\n\nStorage infrastructure software\n\n\nInformation on planning, installing and validating storage software\n\n\nTips and FAQ\n\n\nOSG Gratia Transfer Probe page\n\n\nSRM v2.2 LBNL client command line examples\n\n\nSRM-Tester\n\n\nBeStMan official website\n\n\nBeStMan User guides\n\n\nBeStMan FAQ\n\n\n\n\n\n\nSLAC Gateway mode Instruction\n - SLAC guide on gateway mode\n\n\nUS ATLAS instruction page\n\n\nSRM specifications and collaboration\n - from SRM collaboration working group\n\n\nS2\n - A SRM v2.2 test suite from CERN. It provides basic functionality tests based on use cases, and cross-copy tests, as part of the certification process and supports file access/transfer protocols: rfio, dcap, gsidcap, gsiftp\n\n\nSHA-2 compliance page\n\n\nBestman scalability tuning\n\n\n\n\nKnown Issues\n\n\nRequesting host certificates in RHEL6\n\n\nBestman may not start if the certificates were requested on slc6. This is be caused by a bug in JGlobus (see \nJGlobusIssue118\n), a \nbestman2\n dependency. A known workaround is to run this command\n\n\n[user@client ~] $\n openssl rsa -in mykey.pem -out mykey.pem.old\n\n\n\n\n\nThis command on converts \nmykey.pem\n to \nmykey.pem.old\n; the latter format is supported.", 
            "title": "BeStMan Install"
        }, 
        {
            "location": "/data/bestman-install/#installing-bestman", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of June 2018.", 
            "title": "Installing BeStMan"
        }, 
        {
            "location": "/data/bestman-install/#about-this-document", 
            "text": "This document explains how to install a BeStMan SRMv2 service. This procedure will guide one through the installation and configuration of a basic  bestman2  host with an underlying GridFTP server. This will allow the service to service requests via the SRM (Storage Resource Manager) protocol or the GridFTP protocol.", 
            "title": "About this Document"
        }, 
        {
            "location": "/data/bestman-install/#installing-bestman-storage-element", 
            "text": "This procedure explains how to install the stand-alone BeStMan Storage Element server;  see below  for notes on upgrading.  The service has the following components:   BeStMan - provides load-balancing across GridFTP servers.  GridFTP server - provides file transfer services using the GridFTP protocol.  Gratia gridftp transfer probe  (optional) - provides transfer accounting information to the OSG.", 
            "title": "Installing BeStMan Storage Element"
        }, 
        {
            "location": "/data/bestman-install/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/bestman-install/#host-and-os", 
            "text": "You need at least one node in order to install this service.  The OS must be in the  supported platforms  list.  The  OSG software repositories  must be configured correctly.  All procedures in this document require  root  privileges.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/data/bestman-install/#users", 
            "text": "This installation will create following users unless they are already created:     User  Comment      bestman  Used by Bestman SRM server     For full functionality, the  bestman  account will need limited  sudo  access to a few commands, described below.  For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server. For GUMS sites, this means that each user that can be authenticated by GUMS should be created on the server.  Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in GUMS, then new users should also be added here.", 
            "title": "Users"
        }, 
        {
            "location": "/data/bestman-install/#certificates", 
            "text": "Two certificates are needed for operation of this service.     Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem  and  /etc/grid-security/hostkey.pem    Bestman service certificate  bestman  /etc/grid-security/bestman/bestmancert.pem  and  /etc/grid-security/bestman/bestmankey.pem     Following the  instructions  to request a service certificate.  You will also need a copy of CA certificates. Note that the  osg-se-bestman  package will automatically install a certificate package but will not necessarily pick the cert package you expect; see  the CA certificates  documentation for more information.", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/bestman-install/#networking", 
            "text": "For more details on overall firewall configuration, please see our  firewall documentation .     Service Name  Protocol  Port Number  Inbound  Outbound  Comment      GridFTP data channels  tcp  GLOBUS_TCP_PORT_RANGE  X   contiguous range of ports is necessary.    GridFTP data channels  tcp  GLOBUS_TCP_SOURCE_RANGE   X  contiguous range of ports is necessary.    GridFTP control channel  tcp  2811  X      SRM  tcp  8443  X", 
            "title": "Networking"
        }, 
        {
            "location": "/data/bestman-install/#engineering-considerations", 
            "text": "Please answer following questions before you proceed with installation and configuration of BeStMan storage element:  Q.  What authorization mechanism should I use?  Decide between a  grid-mapfile  or a  GUMS  server for authorization.  Both mechanisms are deprecated with a planned removal by June 2018.  The replacement mechanism, however, does not work with  bestman2 .  Q.  How many GridFTP servers will I need?  Choose to run multiple GridFTP servers for load balancing and better performance. We recommend to install additional GridFTP servers if your Storage Element:   Is serving data to more than 1000 cores for VOs that use storage heavily (e.g. CMS, ATLAS, CDF, and D0),  Is managing more than 500 TB of disk space, OR  Has more than 10Gbps bandwidth   We recommend approximately one GridFTP server for each 8Gbps of desired utilized bandwidth.  Q.  Do I need to change default configuration of Gridftp server?  Yes, you may want to do this if the node on which GridFTP server will be installed has multiple network interfaces. Read  this section  for more details.  Q.  Do you need to enable Gratia gridftp-transfer probes?   \nThe Gratia gridftp-transfer probes provide OSG storage statistics for accounting purposes. The reports include the source and destination of transfers, certificate subject of transfer initiator, as well as the size and status of the transferred file. The probe needs to be installed on every GridFTP server.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/data/bestman-install/#install-instructions", 
            "text": "", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/data/bestman-install/#installing-bestman2", 
            "text": "Install Java using  these instructions .  Install the BeStMan Storage element meta-package:   [root@client ~] #  yum install osg-se-bestman", 
            "title": "Installing BeStMan2"
        }, 
        {
            "location": "/data/bestman-install/#authorization", 
            "text": "There are two authorization options:   Gridmap file  GUMS authentication server   Please choose one of these and follow the instructions in one of the two following sections.", 
            "title": "Authorization"
        }, 
        {
            "location": "/data/bestman-install/#configuring-gridmap-support", 
            "text": "By default, GridFTP uses a gridmap file, found in  /etc/grid-security/grid-mapfile . This file is not generated by default. can generate this file. You can generate this file manually, by including DN/username combinations (this is most useful for debugging). Otherwise, you can use  edg-mkgridmap , which will periodically contact a list of VOMS servers that you specify.  Once  edg-mkgridmap  is configured, you will have to modify  /etc/bestman2/conf/bestman2.rc  and change  GridMapFileName  from  /etc/bestman2/conf/grid-mapfile.empty  to:  GridMapFileName=/etc/grid-security/grid-mapfile  In  /etc/sysconfig/bestman2 , change  BESTMAN_GUMS_ENABLED = no", 
            "title": "Configuring Gridmap Support"
        }, 
        {
            "location": "/data/bestman-install/#configuring-gums-support", 
            "text": "By default, GridFTP uses a gridmap file, found in  /etc/grid-security/gridmap-file . If you want to use GUMS security (recommended), you will need to enable it using the following steps.  First, edit  /etc/grid-security/gsi-authz.conf  and uncomment the authorization callout:  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  Next edit  /etc/lcmaps.db  to enter the correct GUMS hostname:  gumsclient =  lcmaps_gums_client.mod \n              -resourcetype ce \n              -actiontype execute-now \n              -capath /etc/grid-security/certificates \n              -cert   /etc/grid-security/hostcert.pem \n              -key    /etc/grid-security/hostkey.pem \n              --cert-owner root \n# Change this URL to your GUMS server\n              --endpoint https:// GUMS_HOSTNAME :8443/gums/services/GUMSXACMLAuthorizationServicePort   You will need to modify the following settings in  /etc/sysconfig/bestman2  BESTMAN_GUMSCERTPATH = /etc/grid-security/bestman/bestmancert.pem BESTMAN_GUMSKEYPATH = /etc/grid-security/bestman/bestmankey.pem  You will need to modify the following settings in  /etc/bestman2/conf/bestman2.rc  GUMSserviceURL=https:// GUMS_HOST :8443/gums/services/GUMSXACMLAuthorizationServicePort", 
            "title": "Configuring GUMS support"
        }, 
        {
            "location": "/data/bestman-install/#edit-bestman-settings", 
            "text": "Bestman settings are split into three files:   Environment variables (except those that represent server and client libraries) are stored in  /etc/sysconfig/bestman2 .  The server and client library environment variables are stored in  /etc/sysconfig/bestman2lib .  Configuration is stored in  /etc/bestman2/conf/bestman2.rc .   You should review these settings to make sure all of them comply with your environment. You are not expected to edit  /etc/sysconfig/bestman2lib  .   Note  If you are upgrading from a version prior to 2.3.0-9, you will need to remove  all  entries for  BESTMAN2_SERVER_LIB  and  BESTMAN2_CLIENT_LIB  in file  /etc/sysconfig/bestman2.  These settings are now present in file  /etc/sysconfig/bestman2lib   You will likely need to modify the following settings in  /etc/bestman2/conf/bestman2.rc :  localPathListAllowed=/tmp\nCertFileName=/etc/grid-security/bestman/bestmancert.pem\nKeyFileName=/etc/grid-security/bestman/bestmankey.pem\nsupportedProtocolList=gsiftp:// GRIDFTP_HOSTNAME ;gsiftp:// GRIDFTP_HOSTNAME2    Note  Make sure the value for  localPathListAllowed  is correctly entered - i.e. each path separated by a  ; . If it is not, this parameter may not be effective.\nMake sure the permissions for the  localPathListAllowed  directory(ies) are set to 1777, which is the default for  /tmp . Further, note that on many systems,  /tmp  gets cleared out automatically, so you may want to use a different location to ensure that the files persist.   BeStMan requires up to two sets of certificate pairs. One is for host services; when clients connect to BeStMan, they will receive this certificate ( CertFileName ,  KeyFileName ) as proof of the server identity. The second certificate pair ( BESTMAN_GUMSCERTPATH ,  BESTMAN_GUMSKEYPATH ) is used to communicate with GUMS when verifying identity information (this only applicable for GUMS-enabled sites). These two can (and usually will be) the same files, but can be split if your GUMS setup requires a specific identity.  localPathListAllowed  determines which paths users will be able to access via SRM.  supportedProtocolList  is a semi-colon list of GridFTP servers that the BeStMan will use as transfer agents. If you are using anything but the standard GridFTP port 2811, you will also have to add the port (ie  gsiftp:// HOSTNAME :port ).  Finally, modify  GUMSserviceURL  to use your local GUMS installation if you are using GUMS.", 
            "title": "Edit Bestman Settings"
        }, 
        {
            "location": "/data/bestman-install/#modify-etcsudoers", 
            "text": "BeStman requires the  sudo  command in order to write information as the proper user. You will need to give the  bestman  user the proper permissions to run these commands.  Modify  /etc/sudoers  and comment the following line.  #Defaults    requiretty  Then add the following lines at the end of the  /etc/sudoers  file.  Cmnd_Alias SRM_CMD = /bin/rm, /bin/mkdir, /bin/rmdir, /bin/mv, /bin/cp, /bin/ls\nRunas_Alias SRM_USR = ALL, !root\nbestman   ALL=(SRM_USR) NOPASSWD: SRM_CMD", 
            "title": "Modify /etc/sudoers"
        }, 
        {
            "location": "/data/bestman-install/#copying-certificates-to-an-alternate-location", 
            "text": "BeStMan requires a certificate pair to function; this must be readable by the  bestman  user.  [root@client ~] #  cp /etc/grid-security/hostkey.pem /etc/grid-security/bestman/bestmankey.pem [root@client ~] #  cp /etc/grid-security/hostcert.pem /etc/grid-security/bestman/bestmancert.pem [root@client ~] #  chown -R bestman:bestman /etc/grid-security/bestman/  Verify  CertFileName  and  KeyFileName  in  /etc/bestman2/conf/bestman2.rc  are set appropriately.", 
            "title": "Copying certificates to an alternate location"
        }, 
        {
            "location": "/data/bestman-install/#optional-using-a-different-bestman-user", 
            "text": "If you would like to use a different user than the default  bestman  user ( not recommended ), you will need to change the following:   Ownership of bestman certs in  /etc/grid-security/bestman .  SRM_OWNER  in  /etc/sysconfig/bestman2  to the new user.  User in  /etc/sudoers . The last line ( bestman ALL(SRM_USR) NOPASSWD: SRM_CMD ) should be changed from  bestman  to the new user.  Ownership of  /var/log/bestman2    Warning  Currently the RPM packaging will change the ownership of the  /var/log/bestman2  directory back to  bestman  on upgrades.", 
            "title": "(Optional) Using a different bestman user"
        }, 
        {
            "location": "/data/bestman-install/#optional-modifying-default-logging-for-eventsrmlog", 
            "text": "The logging directory ( /var/log/bestman2 ) has two types of logs -  bestman2.log  and  event.srm.log .  Log-rotation of  bestman2.log  file is controlled by  /etc/logrotate.d/bestman2  file.  By default, the size of  event.srm.log  log file is set to 50MB within the Bestman code itself.  Left unchanged,  event.srm.log  file counts will keep increasing indefinitely.  Depending on the usage, the number of these files can become high enough to fill up the partition that holds these logs.  There are 3 ways to avoid this -    Modify following parameters (commented by default) in the  /etc/sysconfig/bestman2  file  # Number of files to keep  BESTMAN_EVENT_LOG_COUNT = 10  # Size of each file in bytes  BESTMAN_EVENT_LOG_SIZE = 20971520   The optimal value for these depends on usage of the service.    Create a directory under a much bigger partition and have a symlink from  /var/log/bestman2  to that directory.   Leave the default settings, but have your own custom script that cleans these files according to your needs.", 
            "title": "(Optional) Modifying default logging for event.srm.log"
        }, 
        {
            "location": "/data/bestman-install/#starting-services", 
            "text": "fetch-crl  For RHEL 6:  [root@client ~] #  /usr/sbin/fetch-crl    # This fetches the CRLs   [root@client ~] #  /sbin/service fetch-crl-boot start [root@client ~] #  /sbin/service fetch-crl-cron start  For RHEL 7:  [root@client ~] #  /usr/sbin/fetch-crl    # This fetches the CRLs   [root@client ~] #  systemctl start fetch-crl-boot [root@client ~] #  systemctl start fetch-crl-cron    GridFTP  [root@client ~] #  service globus-gridftp-server start    Bestman  [root@client ~] #  service bestman2 start  To start Bestman automatically at boot time  [root@client ~] #  chkconfig bestman2 on    Gratia transfer probe:  [root@client ~] #  service gratia-gridftp-transfer start", 
            "title": "Starting Services"
        }, 
        {
            "location": "/data/bestman-install/#stopping-services", 
            "text": "fetch-crl  For RHEL 6:  [root@client ~] #  /usr/sbin/fetch-crl    # This fetches the CRLs  [root@client ~] #  /sbin/service fetch-crl-boot stop [root@client ~] #  /sbin/service fetch-crl-cron stop  For RHEL 7:  [root@client ~] #  /usr/sbin/fetch-crl    # This fetches the CRLs  [root@client ~] #  systemctl stop fetch-crl-boot [root@client ~] #  systemctl stop fetch-crl-cron    GridFTP  [root@client ~] #  service globus-gridftp-server stop    Bestman  [root@client ~] #  service bestman2 stop    Gratia transfer probe  [root@client ~] #  service gratia-gridftp-transfer stop", 
            "title": "Stopping Services"
        }, 
        {
            "location": "/data/bestman-install/#validation-of-service-operation", 
            "text": "Once you have your SE setup and configured, there are several ways to monitor your installation. Refer to the following pages for more information:   BeStMan SRM Tester .  RSV  which includes SRM probes as well.   You can also self-test to verify your installation with an SRM client such as  gfal-copy .", 
            "title": "Validation of Service Operation"
        }, 
        {
            "location": "/data/bestman-install/#troubleshooting", 
            "text": "Service/Process  Log File  Description      BeStMan2  /var/log/bestman2/bestman2.log  BeStMan2 server log and errors     /var/log/bestman2/event.srm.log  Records all SRM transactions    GridFTP  /var/log/gridftp.log  Transfer log     /var/log/gridftp-auth.log  Authentication log     /var/log/messages  Main system log (look here for LCMAPS errors)", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/data/bestman-install/#debugging-procedure", 
            "text": "If system validation failed, you would probably need to check each component in order to verify your installation. In order to do so, you should check all of them in the following order   GUMS (if in use)  GridFTP  BeStMan", 
            "title": "Debugging Procedure"
        }, 
        {
            "location": "/data/bestman-install/#verifying-gums", 
            "text": "Make sure that the service certificate you specified for BeStMan configuration with  GUMSHOSTCERT ,  GUMSHOSTKEY  options and GridFTP service certificate are accepted by GUMS.  Test GUMS by running:  [root@client ~] #  srm-ping srm:// BESTMAN_HOST :8443/srm/v2/server  In the output, check that your  gumsIDMapped  is not  null . It returns the  uid  that GUMS will map you to. This can be obtained from your GUMS administrator. Verify that this  uid  exists on BeStMan and GridFTP node.", 
            "title": "Verifying GUMS"
        }, 
        {
            "location": "/data/bestman-install/#verifying-gridftp", 
            "text": "Login on the node where your certificate and  OSG Worker Node Client  is installed You will need to generate your proxy credentials using  grid-proxy-init  or  voms-proxy-init .  Then test GridFTP using  globus-url-copy :  [user@client ~] $   echo   This is a test   /tmp/test  [user@client ~] $  globus-url-copy -dbg file:///tmp/test gsiftp:// GRIDFTP_HOST /tmp/test   Check the GridFTP logs to see if you have encountered any errors.", 
            "title": "Verifying GridFTP"
        }, 
        {
            "location": "/data/bestman-install/#verifying-bestman", 
            "text": "Make sure that the BeStMan process is running  [root@client ~] #  ps -ef  |  grep bestman bestman   5121     1 99 19:59 ?        00:00:01 /usr/java/latest/bin/java -server -Xmx1024m -XX:MaxDirectMemorySize=1024m -DX509_CERT_DIR=/etc/grid-security/certificates -DCADIR=/etc/grid-security/certificates -Daxis.socketSecureFactory=org.glite.security.trustmanager.axis.AXISSocketFactory -DsslCAFiles=/etc/grid-security/certificates/*.0 -DsslCertfile=/etc/grid-security/bestman/bestmancert.pem -DsslKey=/etc/grid-security/bestman/bestmankey.pem -DJettyConfiguration=/etc/bestman2/conf/WEB-INF/jetty.xml -DJettyDescriptor=/etc/bestman2/conf/WEB-INF/web.xml -DJettyResource=/etc/bestman2/conf/ -Dorg.eclipse.jetty.util.log.IGNORE=true gov.lbl.srm.server.Server /etc/bestman2/conf/bestman2.rc   If  bestman2  is not running, check information in the log file  /var/log/bestman2/bestman2.log .", 
            "title": "Verifying BeStMan"
        }, 
        {
            "location": "/data/bestman-install/#useful-configuration-and-log-files", 
            "text": "Service/Process  Configuration File  Description      BeStMan2  /etc/bestman2/conf/bestman2.rc  Main BeStMan2 configuration file     /etc/sysconfig/bestman2  Environment variables used by BeStMan2     /etc/sysconfig/bestman2lib  Environment variables that store values of various client and server libraries used by BeStMan2     /etc/bestman2/conf/*  Other runtime configuration files     /etc/init.d/bestman2  init.d startup script     /etc/gridftp.conf  Startup parameters    GridFTP  /etc/sysconfig/globus-gridftp-server  Environment variables for GridFTP    Gratia Probe  /etc/gratia/gridftp-transfer/ProbeConfig  GridFTP Gratia Probe configuration     /etc/cron.d/gratia-probe-gridftp-transfer.cron  Cron tab file        Service/Process  Log File  Description      BeStMan2  /var/log/bestman2/bestman2.log  BeStMan2 container log     /var/log/bestman2/event.srm.log  Records all SRM transactions    GridFTP  /var/log/gridftp.log  Transfer log     /var/log/gridftp-auth.log  GridFTP authorization log     /var/log/messages  Main system log (look here for LCMAPS errors)    Gratia probe  /var/log/gratia", 
            "title": "Useful Configuration and Log Files"
        }, 
        {
            "location": "/data/bestman-install/#upgrading-bestman", 
            "text": "Upgrading BeStMan can be done by  [root@client ~] #  yum upgrade bestman2-server  There are a few notes to be aware of when upgrading BeStMan.   From many of the versions of the BeStMan, configuration changes have taken place. Do not ignore any warnings about rpmsave or rpmnew files. You will need to especially be careful about and  /etc/bestman2/conf/bestman2.rc .  Beginning with BeStMan 2.3.0-9, many dependency locations changed. Be sure that  /etc/sysconfig/bestman2lib  contains the  build-classpath  directives in the  BESTMAN2_SERVER_LIB  and  BESTMAN2_CLIENT_LIB . Otherwise, you may get java class loading errors on startup or on run-time. Be sure to remove these entries from the  /etc/sysconfig/bestman2  file.  For BeStMan 2.1.3, certain versions had a combined sysconfig and configuration file. You may need to split these files apart if this is the case.   For more help, please contact the GOC to create a support ticket.", 
            "title": "Upgrading BeStMan"
        }, 
        {
            "location": "/data/bestman-install/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug support and issues, submit a ticket to the  Grid Operations Center .  For community support and best-effort software team support contact  .   For a full set of help options, see the  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/data/bestman-install/#references", 
            "text": "Storage infrastructure software  Information on planning, installing and validating storage software  Tips and FAQ  OSG Gratia Transfer Probe page  SRM v2.2 LBNL client command line examples  SRM-Tester  BeStMan official website  BeStMan User guides  BeStMan FAQ    SLAC Gateway mode Instruction  - SLAC guide on gateway mode  US ATLAS instruction page  SRM specifications and collaboration  - from SRM collaboration working group  S2  - A SRM v2.2 test suite from CERN. It provides basic functionality tests based on use cases, and cross-copy tests, as part of the certification process and supports file access/transfer protocols: rfio, dcap, gsidcap, gsiftp  SHA-2 compliance page  Bestman scalability tuning", 
            "title": "References"
        }, 
        {
            "location": "/data/bestman-install/#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/data/bestman-install/#requesting-host-certificates-in-rhel6", 
            "text": "Bestman may not start if the certificates were requested on slc6. This is be caused by a bug in JGlobus (see  JGlobusIssue118 ), a  bestman2  dependency. A known workaround is to run this command  [user@client ~] $  openssl rsa -in mykey.pem -out mykey.pem.old  This command on converts  mykey.pem  to  mykey.pem.old ; the latter format is supported.", 
            "title": "Requesting host certificates in RHEL6"
        }, 
        {
            "location": "/data/install-bestman-xrootd/", 
            "text": "Install Bestman Xrootd SE\n\n\nAbout this Document\n\n\nThis page explains how to install the BeStMan Storage Element with underlying XRootD storage.\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nOS is Red Hat Enterprise Linux 6, 7, and variants (see \ndetails...\n)\n\n\nEPEL\n repos enabled.\n\n\nA working XRootD Server. See \nInstallXrootd\n for details.\n\n\nRoot access\n\n\n\n\nUsers\n\n\nThis installation will create several users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nbestman\n\n\nUsed by Bestman SRM server (needs sudo access).\n\n\n\n\n\n\ndaemon\n\n\nUsed by globus-gridftp-server.\n\n\n\n\n\n\nxrootd\n\n\nUsed by the xrootd client to contact xrootd redirector.\n\n\n\n\n\n\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n\n\nFor grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n\n\nFor gums users, this means that each user that can be authenticated by gums should be created on the server.\n\n\nNote that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\nBestman service certificate\n\n\nbestman\n\n\n/etc/grid-security/bestman/bestmancert.pem\n \n/etc/grid-security/bestman/bestmankey.pem\n\n\n\n\n\n\n\n\nInstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates (see below).\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nCommen\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\ntcp\n\n\n2811 and \nGLOBUS_TCP_SOURCE_RANGE\n\n\nYES\n\n\n\n\ncontiguous range of ports\n\n\n\n\n\n\nStorage Resource Manager\n\n\ntcp\n\n\n8443\n\n\nYES\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Instructions\n\n\nNote that this package is primarily intended for Bestman-Gateway acting as an endpoint for XRootD server. If you have not installed an XRootD server yet, follow the instructions in \nInstallXrootd\n.\n\n\nCertificates\n\n\nGridFTP, which is a part of this meta-package, requires a certificate package to run. If you require a specific certificate package, follow the \nInstallCertAuth\n instructions to install it. If you do not install a grid certificate package first, the install procedure will install one for you as part of its dependencies. (usually osg-ca-certs).\n\n\nPackage installation instructions\n\n\n\n\nInstall Java using \nthese instructions\n\n\nInstall the BeStMan Gateway XRootD Storage element meta-package:\nroot@host #\n yum install osg-se-bestman-xrootd\n\n\n\n\n\n\n\n\n\nConfiguring GridFTP authentication\n\n\nFor information on how to configure authentication for your GridFTP installation, please refer to the \nconfiguring authentication section of the GridFTP guide\n.\n\n\nConfiguring GridFTP XRootD support\n\n\nIn order to configure GridFTP to work with XRootD, you will need to configure the Data Storage Interface (DSI) module with Xrootd pre-load libraries. This module is used to access Xrootd and POSIX file systems.\n\n\nEdit \n/etc/sysconfig/xrootd-dsi\n (create it if it is missing) and set XROOTD_VMP (XRootD Virtual Mount Point) to use your Xrootd redirector.\n\n\nexport XROOTD_VMP=\nredirector:1094:/local_path=/remote_path\n\n\n\n\n\n\n\n\nNote\n\n\nThe syntax of the above environment variable is a little confusing, so make sure that you adhere to the following directions for XROOTD_VMP (Virtual Mount Point):\n\n\n- Redirector: This is the hostname and domain of the local XRootD redirector server.\n- local_path: This is the path used to access the GridFTP server (ie this server).\n- remote_path: This is the path used to access the XRootD redirector.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe xrootd-dsi module overloads the \ngridftp.conf\n file and uses the alternate file \n/etc/xrootd-dsi/gridftp-xrootd.conf\n. If you have made local changes to your \ngridftp.conf\n file, then you will need to carry them over to \n/etc/xrootd-dsi/gridftp-xrootd.conf\n.\n\n\n\n\nConfiguring xrootdfs\n\n\nThough the DSI module will work for GridFTP, you will need a FUSE mount in order for BeStMan to work correctly with XRootD. Configure it using the following steps.\n\n\nModify \n/etc/fstab\n by adding the following entries:\n\n\n....\nxrootdfs                \n/mnt/xrootd\n              fuse    rdr=xroot://\nredirector1.domain.com\n:1094/\n/path/\n,uid=xrootd 0 0\n\n\n\n\n\nReplace \n/mnt/xrootd\n with the path that you would like to access with BeStMan. This should also match the GridFTP settings for the \nXROOTD_VMP\n local path. Create \n/mnt/xrootd\n directory. Once you are finished, you can mount it:\n\n\nmount /mnt/xrootd\n\n\n\n\n\nYou should now be able to run UNIX commands such as \nls /mnt/xrootd\n to see the contents of the XRootD server.\n\n\n(Optional) Configuring secured xrootdfs\n\n\nIf you want to enable security for access to xrootd via xrootdfs you will need to modify xrootd configuration and perform several steps to make xrootdfs secured.\n\n\n\n\n\n\nOn the xrootd redirector node, execute the following command:\n\n\nroot@host #\n xrdsssadmin -k \nmy_key_name\n -u anybody -g usrgroup add \nkeyfile\n\n\n\neg:\n\n\n\nroot@host #\n xrdsssadmin -k top_secret -u anybody -g usrgroup add /etc/xrootd/xrootd.key\n\n\n\n\n\n\n\n\n\nSet ownership\n\n\nroot@host #\n chown xrootd.xrootd /etc/xrootd/xrootd.key\n\n\n\n\n\n\n\n\n\nOn the node where xrootdfs is installed modify \n/etc/fstab\n add security information:\n\n\nroot@host #\n xrootdfs \n/mnt/xrootd %ENDCOLOR\n fuse rdr=xroot://\nredirector1.domain.com\n:1094/\n/path/redirector1\n,uid=xrootd,sss=\nkeyfile\n 0 0\n\n\n\n\n\n\n\n\n\n\nOn all xrootd data servers and redirector node, modify xrootd configuration (\n/etc/xrootd/xrootd-clustered.cfg\n) by adding the following segment: \n\n\n# ENABLE_SECURITY_BEGIN \n   xrootd.seclib /usr/lib64/libXrdSec.so \n   #the line below should be before \nsec.protocol ... unix\n\n   \nsec.protocol /usr/lib64 sss -s keyfile \n\n   sec.protocol /usr/lib64 unix \n   # this specify that we use the \nunix\n authentication module, additional one can be specified. \n   # this is the authorization file\n   acc.authdb /etc/xrootd/auth_file\n   ofs.authorize \n   # ENABLE_SECURITY_END\n\n\n\n\n\n\n\n\n\nOn all xrootd data server nodes, edit /etc/xrootd/auth_file to add authorized users of the form \nu \nusername\n \n/directoryname\n lr\n where \"lr\" is the permission set.\n\n\n\n\n\n\nCopy \nkeyfile\n from redirector node to every data server node and the xrootdfs node. Make sure that this file is owned by the \nxrootd\n user.\n\n\n\n\n\n\nRestart xrootd cluster by following \nthese instructions\n\n\n\n\n\n\nOn xroodfs node execute mount:\n\n\nroot@host #\n mount \n/mnt/xrootd\n\n\n\n\n\n\n\n\n\n\nVerify that you can access the mount point (df,ls) and can not write into unauthorized path, e.g:\n\n\nroot@host #\n cp /bin/sh /mnt/xrootd/tlevshin/test1 cp: \n\ncannot create regular file \\`/mnt/xrootd/tlevshin/test1\n: Permission denied\n\n\n\n\n\n\nLogin as yourself and try:\n\n\nroot@host #\n su - tlevshin \n\nuser@host $\n cp /bin/sh /mnt/xrootd/tlevshin/test1\n\n\n\n\n\n\n\n\n\nEdit Bestman Settings\n\n\nSee \nEdit Bestman Settings\n\n\nModify \n/etc/sudoers\n\n\nSee \nBestman Instructions\n\n\n(Optional) Copying certificates to a bestman location\n\n\nSee \nBestman Instructions\n\n\nValidation\n\n\nValidation can be done similar to a stand-alone BeStMan or GridFTP server. For more information, see \nBeStMan Validation\n and \nGridFTP Validation\n.\n\n\nServices\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nfetch-crl\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee  \nCA documentation\n\n\n\n\n\n\nGridftp\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\nBestMan\n\n\nbestman2\n\n\nSee \nBestman Services\n\n\n\n\n\n\nGratia probes\n\n\ngratia-xrootd-transfer\n and \ngratia-xrootd-storage\n\n\n\n\n\n\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\n\n\n\n\nStop a service\n\n\nservice \nSERVICE-NAME\n stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\n\n\n\n\n\n\nNotes on Upgrading Bestman\n\n\nSee \nUpgrading Bestman\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug support and issues, submit a ticket to the \nGrid Operations Center\n.\n\n\n\n\nFor a full set of help options, see \nHelp Procedure\n.", 
            "title": "BeStMan Xrootd SE"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#install-bestman-xrootd-se", 
            "text": "", 
            "title": "Install Bestman Xrootd SE"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#about-this-document", 
            "text": "This page explains how to install the BeStMan Storage Element with underlying XRootD storage.", 
            "title": "About this Document"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#host-and-os", 
            "text": "OS is Red Hat Enterprise Linux 6, 7, and variants (see  details... )  EPEL  repos enabled.  A working XRootD Server. See  InstallXrootd  for details.  Root access", 
            "title": "Host and OS"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#users", 
            "text": "This installation will create several users unless they are already created.     User  Comment      bestman  Used by Bestman SRM server (needs sudo access).    daemon  Used by globus-gridftp-server.    xrootd  Used by the xrootd client to contact xrootd redirector.     For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.  For gums users, this means that each user that can be authenticated by gums should be created on the server.  Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.", 
            "title": "Users"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem   /etc/grid-security/hostkey.pem    Bestman service certificate  bestman  /etc/grid-security/bestman/bestmancert.pem   /etc/grid-security/bestman/bestmankey.pem     Instructions  to request a service certificate.  You will also need a copy of CA certificates (see below).", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Commen      GridFTP  tcp  2811 and  GLOBUS_TCP_SOURCE_RANGE  YES   contiguous range of ports    Storage Resource Manager  tcp  8443  YES", 
            "title": "Networking"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#install-instructions", 
            "text": "Note that this package is primarily intended for Bestman-Gateway acting as an endpoint for XRootD server. If you have not installed an XRootD server yet, follow the instructions in  InstallXrootd .", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#certificates_1", 
            "text": "GridFTP, which is a part of this meta-package, requires a certificate package to run. If you require a specific certificate package, follow the  InstallCertAuth  instructions to install it. If you do not install a grid certificate package first, the install procedure will install one for you as part of its dependencies. (usually osg-ca-certs).", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#package-installation-instructions", 
            "text": "Install Java using  these instructions  Install the BeStMan Gateway XRootD Storage element meta-package: root@host #  yum install osg-se-bestman-xrootd", 
            "title": "Package installation instructions"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#configuring-gridftp-authentication", 
            "text": "For information on how to configure authentication for your GridFTP installation, please refer to the  configuring authentication section of the GridFTP guide .", 
            "title": "Configuring GridFTP authentication"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#configuring-gridftp-xrootd-support", 
            "text": "In order to configure GridFTP to work with XRootD, you will need to configure the Data Storage Interface (DSI) module with Xrootd pre-load libraries. This module is used to access Xrootd and POSIX file systems.  Edit  /etc/sysconfig/xrootd-dsi  (create it if it is missing) and set XROOTD_VMP (XRootD Virtual Mount Point) to use your Xrootd redirector.  export XROOTD_VMP= redirector:1094:/local_path=/remote_path    Note  The syntax of the above environment variable is a little confusing, so make sure that you adhere to the following directions for XROOTD_VMP (Virtual Mount Point):  - Redirector: This is the hostname and domain of the local XRootD redirector server.\n- local_path: This is the path used to access the GridFTP server (ie this server).\n- remote_path: This is the path used to access the XRootD redirector.    Note  The xrootd-dsi module overloads the  gridftp.conf  file and uses the alternate file  /etc/xrootd-dsi/gridftp-xrootd.conf . If you have made local changes to your  gridftp.conf  file, then you will need to carry them over to  /etc/xrootd-dsi/gridftp-xrootd.conf .", 
            "title": "Configuring GridFTP XRootD support"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#configuring-xrootdfs", 
            "text": "Though the DSI module will work for GridFTP, you will need a FUSE mount in order for BeStMan to work correctly with XRootD. Configure it using the following steps.  Modify  /etc/fstab  by adding the following entries:  ....\nxrootdfs                 /mnt/xrootd               fuse    rdr=xroot:// redirector1.domain.com :1094/ /path/ ,uid=xrootd 0 0  Replace  /mnt/xrootd  with the path that you would like to access with BeStMan. This should also match the GridFTP settings for the  XROOTD_VMP  local path. Create  /mnt/xrootd  directory. Once you are finished, you can mount it:  mount /mnt/xrootd  You should now be able to run UNIX commands such as  ls /mnt/xrootd  to see the contents of the XRootD server.", 
            "title": "Configuring xrootdfs"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#optional-configuring-secured-xrootdfs", 
            "text": "If you want to enable security for access to xrootd via xrootdfs you will need to modify xrootd configuration and perform several steps to make xrootdfs secured.    On the xrootd redirector node, execute the following command:  root@host #  xrdsssadmin -k  my_key_name  -u anybody -g usrgroup add  keyfile  eg:  root@host #  xrdsssadmin -k top_secret -u anybody -g usrgroup add /etc/xrootd/xrootd.key    Set ownership  root@host #  chown xrootd.xrootd /etc/xrootd/xrootd.key    On the node where xrootdfs is installed modify  /etc/fstab  add security information:  root@host #  xrootdfs  /mnt/xrootd %ENDCOLOR  fuse rdr=xroot:// redirector1.domain.com :1094/ /path/redirector1 ,uid=xrootd,sss= keyfile  0 0     On all xrootd data servers and redirector node, modify xrootd configuration ( /etc/xrootd/xrootd-clustered.cfg ) by adding the following segment:   # ENABLE_SECURITY_BEGIN \n   xrootd.seclib /usr/lib64/libXrdSec.so \n   #the line below should be before  sec.protocol ... unix \n    sec.protocol /usr/lib64 sss -s keyfile  \n   sec.protocol /usr/lib64 unix \n   # this specify that we use the  unix  authentication module, additional one can be specified. \n   # this is the authorization file\n   acc.authdb /etc/xrootd/auth_file\n   ofs.authorize \n   # ENABLE_SECURITY_END    On all xrootd data server nodes, edit /etc/xrootd/auth_file to add authorized users of the form  u  username   /directoryname  lr  where \"lr\" is the permission set.    Copy  keyfile  from redirector node to every data server node and the xrootdfs node. Make sure that this file is owned by the  xrootd  user.    Restart xrootd cluster by following  these instructions    On xroodfs node execute mount:  root@host #  mount  /mnt/xrootd     Verify that you can access the mount point (df,ls) and can not write into unauthorized path, e.g:  root@host #  cp /bin/sh /mnt/xrootd/tlevshin/test1 cp:  cannot create regular file \\`/mnt/xrootd/tlevshin/test1 : Permission denied   Login as yourself and try:  root@host #  su - tlevshin  user@host $  cp /bin/sh /mnt/xrootd/tlevshin/test1", 
            "title": "(Optional) Configuring secured xrootdfs"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#edit-bestman-settings", 
            "text": "See  Edit Bestman Settings", 
            "title": "Edit Bestman Settings"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#modify-etcsudoers", 
            "text": "See  Bestman Instructions", 
            "title": "Modify /etc/sudoers"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#optional-copying-certificates-to-a-bestman-location", 
            "text": "See  Bestman Instructions", 
            "title": "(Optional) Copying certificates to a bestman location"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#validation", 
            "text": "Validation can be done similar to a stand-alone BeStMan or GridFTP server. For more information, see  BeStMan Validation  and  GridFTP Validation .", 
            "title": "Validation"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#services", 
            "text": "Software  Service name  Notes      fetch-crl  fetch-crl-boot  and  fetch-crl-cron  See   CA documentation    Gridftp  globus-gridftp-server     BestMan  bestman2  See  Bestman Services    Gratia probes  gratia-xrootd-transfer  and  gratia-xrootd-storage      As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service  SERVICE-NAME  start    Stop a service  service  SERVICE-NAME  stop    Enable a service to start during boot  chkconfig  SERVICE-NAME  on    Disable a service from starting during boot  chkconfig  SERVICE-NAME  off", 
            "title": "Services"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#notes-on-upgrading-bestman", 
            "text": "See  Upgrading Bestman", 
            "title": "Notes on Upgrading Bestman"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug support and issues, submit a ticket to the  Grid Operations Center .   For a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/security/edg-mkgridmap/", 
            "text": "Warning\n\n\nEDG-Mkgridmap is no longer be supported in OSG 3.4. The \nLCMAPS VOMS Plugin\n is the preferred method for site authentication in the OSG and is available in both OSG 3.3 and OSG 3.4.\n\n\n\n\nAbout this Document\n\n\nThis document describes how you can use \nedg-mkgridmap\n to authorize users accessing the resources that you provide.\n\n\nRequirements\n\n\n\n\nThe purpose of using \nedg-mkgridmap\n is to create a grid-mapfile that will contain users from all VOs that are supported on your site. For this to happen, make sure that \nbefore\n you run \nedg-mkgridmap\n, you have created user accounts for all such VOs.\n\n\nThe RPM named \nedg-mkgridmap\n should be installed on your node\n\n\n\n\nCertificates\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nOwnership and permissions\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n, \n0644\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n, \n0600\n\n\n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nConfiguration\n\n\nIf you claim to support a VO, make sure the following is true:\n\n\n\n\nThe VO is enabled in \n/etc/edg-mkgridmap.conf\n\n\nThe local user account to which VO members are mapped to exists on the CE\n\n\nAfter you run \nedg-mkgridmap\n, the VO should exist in \n/var/lib/osg/supported-vo-list\n\n\n\n\nLet's begin with configuration:\n\n\nStep 1\n Make sure for all Virtual Organizations (VOs) that you want to support, you have created a local user account. After that, you need to tell \nedg-mkgridmap\n, which Virtual Organizations (VOs) you want to support. For this, edit \n/etc/edg-mkgridmap.conf\n such that all VOs that you want to support are uncommented and all VOs that you do not want to support are commented out. Here is an example \nedg-mkgridmap.conf\n in which \ncdf\n VO is disabled and \nfermilab\n VO is enabled:\n\n\n/etc/edg-mkgridmap.conf\n:\n\n\n#### GROUP: group URI [lcluser]\n#\n#-------------------\n# USER-VO-MAP cdf CDF -- 1 -- Dennis Box (dbox@fnal.gov)\n#group vomss://voms.fnal.gov:8443/voms/cdf cdf\n#group vomss://voms.cnaf.infn.it:8443/voms/cdf cdf\n#-------------------\n# USER-VO-MAP fermilab FERMILAB -- 2 -- Fermilab Service Desk (servicedesk@fnal.gov)\ngroup vomss://voms.fnal.gov:8443/voms/fermilab fermilab\n[root@server]#\n\n\n\n\n\nStep 2\n For any additional DNs that you want to support (i.e. you want them to be present in the final \ngrid-mapfile\n that gets created), you need to add their \nDN\n \nusername\n mapping to a local grid-mapfile, for example \n/etc/osg/local-user-vo-map\n. Here is an example of how each line of this file should look like.\n\n\n/etc/grid-security/local-grid-mapfile\n:\n\n\n/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Neha Sharma/CN=UID:neha\n neha\n\n\n\n\n\n\n\nNote\n\n\nThe local gridmap file is used also to add certificates of service accounts like the one used for RSV testing.\n\n\n\n\nStep 3\n You need to tell \nedg-mkgridmap\n about this local grid-mapfile. For this, you need to use the \ngmf_local\n directive. And if the entry in the local gridmap introduces a new user you should add it to a VO by adding a line \nusername\n \nvoname\n in the local vo-map file \n. And if you use a new VO you must add a VO naming line in \nedg-mkgridmap.conf\n as well (one starting with \n# USER-VO-MAP ...\n ). Here is an example of how your \nedg-mkgridmap.conf\n would now look like:\n\n\n/etc/edg-mkgridmap.conf\n:\n\n\n#### GROUP: group URI [lcluser]\n\n\n#\n\n\n#-------------------\n\n\n# USER-VO-MAP cdf CDF -- 1 -- Dennis Box (dbox@fnal.gov)     \n\n\n#group vomss://voms.fnal.gov:8443/voms/cdf cdf\n\n\n#group vomss://voms.cnaf.infn.it:8443/voms/cdf cdf\n\n\n#-------------------\n\n\n# USER-VO-MAP fermilab FERMILAB -- 2 -- Fermilab Service Desk (servicedesk@fnal.gov)  \n\n\ngroup\n \nvomss:\n//\nvoms\n.\nfnal\n.\ngov:8443\n/\nvoms\n/\nfermilab\n \nfermilab\n\n\n# USER-VO-MAP rsv RSV -- 101 -- Your Name (your@email)\n\n\ngmf_local\n /\netc\n/\ngrid-security\n/\nlocal-grid-mapfile\n\n\n\n\n\n\nStep 4\n If you have additional \nuseraccount vo\n mappings that you want to be included in final \n/var/lib/osg/user-vo-map\n file, add those mappings to \n/etc/osg/local-user-vo-map\n file.\n\n\n\n\nNote\n\n\nMore information on \nedg-mkgridmap.conf\n file can be found \nhere\n\n\n\n\nEnable/Disable\n\n\nEnable\n\n\nYou need to configure \nedg-mkgridmap\n to run automatically via a cron job. To do this, you need to run the following command.\n\n\n[root@server]#\n service edg-mkgridmap start\n\nEnabling periodic edg-mkgridmap:                           [  OK  ]\n\n\n\n\n\n\nThe entry in cron file (\n/etc/cron.d/edg-mkgridmap-cron\n) looks like\n\n\n/etc/cron.d/edg-mkgridmap-cron\n:\n\n\n# Cron job running by default every 6 hours.\n# The lock file can be enabled or disabled via a\n# service edg-mkgridmap start\n# chkconfig edg-mkgridmap on\n\n# Note the lock file not existing is success hence the the slightly odd logic\n# below.\n# run every 6 hours but sleep for random time every 5 hours\n\n0 */6 * * * root perl -e \nsleep rand 18000\n; [ ! -f /var/lock/subsys/edg-mkgridmap ] || /usr/sbin/edg-mkgridmap\n\n\n\n\n\n\n\nNote\n\n\nAs you can see above, this cron will run every 6 hours. If you do not want to wait that long, you can execute the command \n/usr/sbin/edg-mkgridmap\n directly on command line\n\n\n\n\nDisable\n\n\n[root@server]#\n service edg-mkgridmap stop\n\nDisabling periodic edg-mkgridmap:                          [  OK  ]\n\n\n\n\n\n\n5.0 Chkconfig on/off\n\n\n5.1 On\n\n\n[root@server]#\n chkconfig edg-mkgridmap on\n\n[root@server]#\n chkconfig --list edg-mkgridmap\n\nedg-mkgridmap   0:off   1:off   2:on    3:on    4:on    5:on    6:off\n\n\n\n\n\n\n5.2 Off\n\n\n[root@server]#\n chkconfig edg-mkgridmap off\n\n[root@server]#\n chkconfig --list edg-mkgridmap\n\nedg-mkgridmap   0:off   1:off   2:off   3:off   4:off   5:off   6:off\n\n\n\n\n\n\nWhat happens behind the scenes?\n\n\nUpon execution, \nedg-mkgridmap\n reads the configuration file at location \n/etc/edg-mkgridmap.conf\n and for all VOs for which a local user account exists, it contacts their VOMS server to retrieve list of all users that belong to that VO. Its output is the following files\n\n\n\n\n/var/lib/osg/supported-vo-list\n. This file contains names of all VOs you want to support minus all VOs for whom you do not have a user account. In other words, this is list of VOs that will \nactually\n be supported.\n\n\n/var/lib/osg/user-vo-map\n . This file contains mapping of form 'useraccount VO' for all VOs in \n/var/lib/osg/supported-vo-list\n and also mappings from \n/etc/osg/local-user-vo-map\n file.\n\n\n/var/lib/osg/undefined-accounts\n. This file contains names of all VOs for which it \ncould not\n find user account.\n\n\n/etc/grid-security/grid-mapfile\n. This file contains 'DN username' mappings that are present in local grid-mapfile (if you have one) and mappings for \nall\n users from \nall\n VOs that are present in \n/var/lib/osg/supported-vo-list\n.\n\n\n\n\nWhat happens when you perform an upgrade?\n\n\nThere are two scenerios:\n\n\nYou do not have a custom edg-mkgridmap configuration\n\n\nThis means you are using the same \nedg-mkgridmap.conf\n file that came with prior installation of vo-client-edgmkgridmap package. In this case, during upgrade, the newer \nedg-mkgridmap.conf\n file will replace the old one. It will probably have new VOs in it and if you want to support them, you need to satisfy conditions as mentioned in Configuration section above.\n\n\nYou have a custom edg-mkgridmap configuration\n\n\nThis means you have modified the \nedg-mkgridmap.conf\n file. In this case, during upgrade, the existing file \n/etc/edg-mkgridmap.conf\n will remain untouched and new configuration file will be saved as \n/etc/edg-mkgridmap.conf.rpmnew\n. You can compare the two files and make changes as required.\n\n\nFiles\n\n\n\n\n\n\n\n\nFile Location\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n/etc/edg-mkgridmap.conf\n\n\nMain Edg-mkgridmap configuration file\n\n\n\n\n\n\n/etc/cron.d/edg-mkgridmap-cron\n\n\nEdg-mkgridmap cron file\n\n\n\n\n\n\n/etc/rc.d/init.d/edg-mkgridmap\n\n\nEdg-mkgridmap init file\n\n\n\n\n\n\n/var/log/edg-mkgridmap.log\n\n\nEdg-mkgridmap log file\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nCommon Problems\n\n\n\n\ngrid-mapfile not created\n\n\nError code 64\n\n\n\n\nTroubleshooting Procedure\n\n\nIn case of problems first of all check the service log file \n/var/log/edg-mkgridmap.log\n. This file lists:\n\n\n\n\nany errors accessing individual voms servers or any general errors\n\n\nany mappings eliminated from the \n/var/lib/osg/user-vo-map\n and \n/var/lib/osg/supported-vo-list\n files where the Unix account a user would be mapped to does not exist. There is an \n/var/lib/osg/undefined-accounts\n file created that lists these accounts.\n\n\nchanges from the last time the script was run\n\n\n\n\nThere is also a \nlast_checked\n file in \n/etc/grid-security\n that tells you the last time the cron process was run successfully (the \ngrid-mapfile\n only gets updated if there is a difference from the previous run).\n\n\n[root@server]#\n ls -al /etc/grid-security\n\n-rw-r--r--  1 root     root     1162888 Jul 12 02:14 grid-mapfile\n\n\n-rw-r--r--  1 root     root           0 Jul 12 08:14 grid-mapfile.last_checked\n\n\n\n\n\n\nGrid-Mapfile was not created\n\n\nFirst make sure that the \nedg-mkgridmap\n service is enabled:\n\n\n[root@server]#\n /sbin/service edg-mkgridmap status\n\nPeriodic edg-mkgridmap is enabled.\n\n\n\n\n\n\nNext, make sure an entry for \nedg-mkgridmap\n is present:\n\n\n[root@server]#\n cat /etc/cron.d/edg-mkgridmap-cron\n\n#\n Cron job running by default every \n6\n hours.\n\n#\n The lock file can be enabled or disabled via a\n\n#\n service edg-mkgridmap start\n\n#\n chkconfig edg-mkgridmap on\n\n\n#\n Note the lock file not existing is success hence the the slightly odd logic\n\n#\n below.\n\n#\n run every \n6\n hours but sleep \nfor\n random \ntime\n every \n5\n hours\n\n\n0 */6 * * * root perl -e \nsleep rand 18000\n; [ ! -f /var/lock/subsys/edg-mkgridmap ] || /usr/sbin/edg-mkgridmap\n\n\n\n\n\n\n\n\nNote\n\n\nIf no entry for \nedg-mkgridmap\n is present, activate the service.\n\n\n\n\nLast, run the \nedg-mkgridmap\n script on the command line and check that \n/etc/grid-security/grid-mapfile\n gets created:\n\n\n[root@server]#\n edg-mkgridmap\n\n\n\n\n\n\n\nNote\n\n\nThe script will not output to the command line. Check the log file at \n/var/log/edg-mkgridmap.log\n instead.\n\n\n\n\nError Code 64\n\n\nThis type of error occurs when the script established contact with the VOMS server but the requested group or subgroup does not exist there. It is really a \n'not found'\n type error. It is usually caused by a bad entry in the \n/etc/edg-mkgridmap.conf\n file.\n\n\nIn the example below the configuration file contains a request for the VOMS server to send a list of all members of the uscms VO belonging to a non-existent \n/uscms/production\n group.\n\n\nConfiguration file \n/etc/edg-mkgridmap.conf\n entry:\n\n\n group vomss://voms.fnal.gov:8443/voms/uscms/production uscms01\n\n\n\n\n\nCommand run on CE node:\n\n\n[root@server]#\n edg-mkgridmap\n\n  voms search(https://voms.fnal.gov:8443/voms/uscms/production/services/VOMSAdmin?method=listMembers): /voms/uscms/production/services/VOMSAdmin\n\n\n\n   Exit with error(s) (code=64)\n\n\n\n\n\n\nOn the VOMS host you are attempting to access, the following error can be seen in the \n/var/log/tomcat6/voms-admin-VO_NAME.log\n or \n/var/log/tomcat/voms-admin-VO_NAME.log\n\n\n2005-05-26 15:51:04,003 INFO  [Ajp13Processor[8892][1]]  Connection from \n131.225.82.73\n by\n        /DC=org/DC=doegrids/OU=Services/CN=ce_host (serial 4511) -  service.InitSecurityContext\n2005-05-26 15:51:04,004 INFO  [Ajp13Processor[8892][1]]  listMembers (\n/uscms/production\n) -  admin.VOMSAdminSoapBindingImpl\n2005-05-26 15:51:04,013 ERROR [Ajp13Processor[8892][1]]  org.edg.security.voms.service.NotInDatabase:\n       Not in database: group \n/uscms/production\n -  connection.Database\n\n\n\n\n\nUseful Options for edg-mkgridmap troubleshooting\n\n\n\n\n--help\n\n\n--version\n\n\n--conf=\nconfig_file\n\n    (Default configuration file is at \n/etc/edg-mkgridmap.conf\n)\n\n\n--output=\noutput_file\n\n\n--verbose\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem or have general questions, there are several ways to receive help:\n\n\n\n\nFor bug support and issues, submit a ticket to the \nGrid Operations Center\n.\n\n\nFor community support and best-effort software team support contact \n.\n\n\n\n\nFor a full set of help options, see \nHelp Procedure\n.", 
            "title": "EDG-Mkgridmap Install"
        }, 
        {
            "location": "/security/edg-mkgridmap/#about-this-document", 
            "text": "This document describes how you can use  edg-mkgridmap  to authorize users accessing the resources that you provide.", 
            "title": "About this Document"
        }, 
        {
            "location": "/security/edg-mkgridmap/#requirements", 
            "text": "The purpose of using  edg-mkgridmap  is to create a grid-mapfile that will contain users from all VOs that are supported on your site. For this to happen, make sure that  before  you run  edg-mkgridmap , you have created user accounts for all such VOs.  The RPM named  edg-mkgridmap  should be installed on your node", 
            "title": "Requirements"
        }, 
        {
            "location": "/security/edg-mkgridmap/#certificates", 
            "text": "Certificates     Certificate  Ownership and permissions  Path to certificate      Host certificate  root ,  0644  /etc/grid-security/hostcert.pem    Host key  root ,  0600  /etc/grid-security/hostkey.pem", 
            "title": "Certificates"
        }, 
        {
            "location": "/security/edg-mkgridmap/#configuration", 
            "text": "If you claim to support a VO, make sure the following is true:   The VO is enabled in  /etc/edg-mkgridmap.conf  The local user account to which VO members are mapped to exists on the CE  After you run  edg-mkgridmap , the VO should exist in  /var/lib/osg/supported-vo-list   Let's begin with configuration:  Step 1  Make sure for all Virtual Organizations (VOs) that you want to support, you have created a local user account. After that, you need to tell  edg-mkgridmap , which Virtual Organizations (VOs) you want to support. For this, edit  /etc/edg-mkgridmap.conf  such that all VOs that you want to support are uncommented and all VOs that you do not want to support are commented out. Here is an example  edg-mkgridmap.conf  in which  cdf  VO is disabled and  fermilab  VO is enabled:  /etc/edg-mkgridmap.conf :  #### GROUP: group URI [lcluser]\n#\n#-------------------\n# USER-VO-MAP cdf CDF -- 1 -- Dennis Box (dbox@fnal.gov)\n#group vomss://voms.fnal.gov:8443/voms/cdf cdf\n#group vomss://voms.cnaf.infn.it:8443/voms/cdf cdf\n#-------------------\n# USER-VO-MAP fermilab FERMILAB -- 2 -- Fermilab Service Desk (servicedesk@fnal.gov)\ngroup vomss://voms.fnal.gov:8443/voms/fermilab fermilab\n[root@server]#  Step 2  For any additional DNs that you want to support (i.e. you want them to be present in the final  grid-mapfile  that gets created), you need to add their  DN   username  mapping to a local grid-mapfile, for example  /etc/osg/local-user-vo-map . Here is an example of how each line of this file should look like.  /etc/grid-security/local-grid-mapfile :  /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Neha Sharma/CN=UID:neha  neha   Note  The local gridmap file is used also to add certificates of service accounts like the one used for RSV testing.   Step 3  You need to tell  edg-mkgridmap  about this local grid-mapfile. For this, you need to use the  gmf_local  directive. And if the entry in the local gridmap introduces a new user you should add it to a VO by adding a line  username   voname  in the local vo-map file  . And if you use a new VO you must add a VO naming line in  edg-mkgridmap.conf  as well (one starting with  # USER-VO-MAP ...  ). Here is an example of how your  edg-mkgridmap.conf  would now look like:  /etc/edg-mkgridmap.conf :  #### GROUP: group URI [lcluser]  #  #-------------------  # USER-VO-MAP cdf CDF -- 1 -- Dennis Box (dbox@fnal.gov)       #group vomss://voms.fnal.gov:8443/voms/cdf cdf  #group vomss://voms.cnaf.infn.it:8443/voms/cdf cdf  #-------------------  # USER-VO-MAP fermilab FERMILAB -- 2 -- Fermilab Service Desk (servicedesk@fnal.gov)    group   vomss: // voms . fnal . gov:8443 / voms / fermilab   fermilab  # USER-VO-MAP rsv RSV -- 101 -- Your Name (your@email)  gmf_local  / etc / grid-security / local-grid-mapfile   Step 4  If you have additional  useraccount vo  mappings that you want to be included in final  /var/lib/osg/user-vo-map  file, add those mappings to  /etc/osg/local-user-vo-map  file.   Note  More information on  edg-mkgridmap.conf  file can be found  here", 
            "title": "Configuration"
        }, 
        {
            "location": "/security/edg-mkgridmap/#enabledisable", 
            "text": "", 
            "title": "Enable/Disable"
        }, 
        {
            "location": "/security/edg-mkgridmap/#enable", 
            "text": "You need to configure  edg-mkgridmap  to run automatically via a cron job. To do this, you need to run the following command.  [root@server]#  service edg-mkgridmap start Enabling periodic edg-mkgridmap:                           [  OK  ]   The entry in cron file ( /etc/cron.d/edg-mkgridmap-cron ) looks like  /etc/cron.d/edg-mkgridmap-cron :  # Cron job running by default every 6 hours.\n# The lock file can be enabled or disabled via a\n# service edg-mkgridmap start\n# chkconfig edg-mkgridmap on\n\n# Note the lock file not existing is success hence the the slightly odd logic\n# below.\n# run every 6 hours but sleep for random time every 5 hours\n\n0 */6 * * * root perl -e  sleep rand 18000 ; [ ! -f /var/lock/subsys/edg-mkgridmap ] || /usr/sbin/edg-mkgridmap   Note  As you can see above, this cron will run every 6 hours. If you do not want to wait that long, you can execute the command  /usr/sbin/edg-mkgridmap  directly on command line", 
            "title": "Enable"
        }, 
        {
            "location": "/security/edg-mkgridmap/#disable", 
            "text": "[root@server]#  service edg-mkgridmap stop Disabling periodic edg-mkgridmap:                          [  OK  ]", 
            "title": "Disable"
        }, 
        {
            "location": "/security/edg-mkgridmap/#50-chkconfig-onoff", 
            "text": "", 
            "title": "5.0 Chkconfig on/off"
        }, 
        {
            "location": "/security/edg-mkgridmap/#51-on", 
            "text": "[root@server]#  chkconfig edg-mkgridmap on [root@server]#  chkconfig --list edg-mkgridmap edg-mkgridmap   0:off   1:off   2:on    3:on    4:on    5:on    6:off", 
            "title": "5.1 On"
        }, 
        {
            "location": "/security/edg-mkgridmap/#52-off", 
            "text": "[root@server]#  chkconfig edg-mkgridmap off [root@server]#  chkconfig --list edg-mkgridmap edg-mkgridmap   0:off   1:off   2:off   3:off   4:off   5:off   6:off", 
            "title": "5.2 Off"
        }, 
        {
            "location": "/security/edg-mkgridmap/#what-happens-behind-the-scenes", 
            "text": "Upon execution,  edg-mkgridmap  reads the configuration file at location  /etc/edg-mkgridmap.conf  and for all VOs for which a local user account exists, it contacts their VOMS server to retrieve list of all users that belong to that VO. Its output is the following files   /var/lib/osg/supported-vo-list . This file contains names of all VOs you want to support minus all VOs for whom you do not have a user account. In other words, this is list of VOs that will  actually  be supported.  /var/lib/osg/user-vo-map  . This file contains mapping of form 'useraccount VO' for all VOs in  /var/lib/osg/supported-vo-list  and also mappings from  /etc/osg/local-user-vo-map  file.  /var/lib/osg/undefined-accounts . This file contains names of all VOs for which it  could not  find user account.  /etc/grid-security/grid-mapfile . This file contains 'DN username' mappings that are present in local grid-mapfile (if you have one) and mappings for  all  users from  all  VOs that are present in  /var/lib/osg/supported-vo-list .", 
            "title": "What happens behind the scenes?"
        }, 
        {
            "location": "/security/edg-mkgridmap/#what-happens-when-you-perform-an-upgrade", 
            "text": "There are two scenerios:", 
            "title": "What happens when you perform an upgrade?"
        }, 
        {
            "location": "/security/edg-mkgridmap/#you-do-not-have-a-custom-edg-mkgridmap-configuration", 
            "text": "This means you are using the same  edg-mkgridmap.conf  file that came with prior installation of vo-client-edgmkgridmap package. In this case, during upgrade, the newer  edg-mkgridmap.conf  file will replace the old one. It will probably have new VOs in it and if you want to support them, you need to satisfy conditions as mentioned in Configuration section above.", 
            "title": "You do not have a custom edg-mkgridmap configuration"
        }, 
        {
            "location": "/security/edg-mkgridmap/#you-have-a-custom-edg-mkgridmap-configuration", 
            "text": "This means you have modified the  edg-mkgridmap.conf  file. In this case, during upgrade, the existing file  /etc/edg-mkgridmap.conf  will remain untouched and new configuration file will be saved as  /etc/edg-mkgridmap.conf.rpmnew . You can compare the two files and make changes as required.", 
            "title": "You have a custom edg-mkgridmap configuration"
        }, 
        {
            "location": "/security/edg-mkgridmap/#files", 
            "text": "File Location  Description      /etc/edg-mkgridmap.conf  Main Edg-mkgridmap configuration file    /etc/cron.d/edg-mkgridmap-cron  Edg-mkgridmap cron file    /etc/rc.d/init.d/edg-mkgridmap  Edg-mkgridmap init file    /var/log/edg-mkgridmap.log  Edg-mkgridmap log file", 
            "title": "Files"
        }, 
        {
            "location": "/security/edg-mkgridmap/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/security/edg-mkgridmap/#common-problems", 
            "text": "grid-mapfile not created  Error code 64", 
            "title": "Common Problems"
        }, 
        {
            "location": "/security/edg-mkgridmap/#troubleshooting-procedure", 
            "text": "In case of problems first of all check the service log file  /var/log/edg-mkgridmap.log . This file lists:   any errors accessing individual voms servers or any general errors  any mappings eliminated from the  /var/lib/osg/user-vo-map  and  /var/lib/osg/supported-vo-list  files where the Unix account a user would be mapped to does not exist. There is an  /var/lib/osg/undefined-accounts  file created that lists these accounts.  changes from the last time the script was run   There is also a  last_checked  file in  /etc/grid-security  that tells you the last time the cron process was run successfully (the  grid-mapfile  only gets updated if there is a difference from the previous run).  [root@server]#  ls -al /etc/grid-security -rw-r--r--  1 root     root     1162888 Jul 12 02:14 grid-mapfile  -rw-r--r--  1 root     root           0 Jul 12 08:14 grid-mapfile.last_checked", 
            "title": "Troubleshooting Procedure"
        }, 
        {
            "location": "/security/edg-mkgridmap/#grid-mapfile-was-not-created", 
            "text": "First make sure that the  edg-mkgridmap  service is enabled:  [root@server]#  /sbin/service edg-mkgridmap status Periodic edg-mkgridmap is enabled.   Next, make sure an entry for  edg-mkgridmap  is present:  [root@server]#  cat /etc/cron.d/edg-mkgridmap-cron #  Cron job running by default every  6  hours. #  The lock file can be enabled or disabled via a #  service edg-mkgridmap start #  chkconfig edg-mkgridmap on #  Note the lock file not existing is success hence the the slightly odd logic #  below. #  run every  6  hours but sleep  for  random  time  every  5  hours 0 */6 * * * root perl -e  sleep rand 18000 ; [ ! -f /var/lock/subsys/edg-mkgridmap ] || /usr/sbin/edg-mkgridmap    Note  If no entry for  edg-mkgridmap  is present, activate the service.   Last, run the  edg-mkgridmap  script on the command line and check that  /etc/grid-security/grid-mapfile  gets created:  [root@server]#  edg-mkgridmap   Note  The script will not output to the command line. Check the log file at  /var/log/edg-mkgridmap.log  instead.", 
            "title": "Grid-Mapfile was not created"
        }, 
        {
            "location": "/security/edg-mkgridmap/#error-code-64", 
            "text": "This type of error occurs when the script established contact with the VOMS server but the requested group or subgroup does not exist there. It is really a  'not found'  type error. It is usually caused by a bad entry in the  /etc/edg-mkgridmap.conf  file.  In the example below the configuration file contains a request for the VOMS server to send a list of all members of the uscms VO belonging to a non-existent  /uscms/production  group.  Configuration file  /etc/edg-mkgridmap.conf  entry:   group vomss://voms.fnal.gov:8443/voms/uscms/production uscms01  Command run on CE node:  [root@server]#  edg-mkgridmap   voms search(https://voms.fnal.gov:8443/voms/uscms/production/services/VOMSAdmin?method=listMembers): /voms/uscms/production/services/VOMSAdmin     Exit with error(s) (code=64)   On the VOMS host you are attempting to access, the following error can be seen in the  /var/log/tomcat6/voms-admin-VO_NAME.log  or  /var/log/tomcat/voms-admin-VO_NAME.log  2005-05-26 15:51:04,003 INFO  [Ajp13Processor[8892][1]]  Connection from  131.225.82.73  by\n        /DC=org/DC=doegrids/OU=Services/CN=ce_host (serial 4511) -  service.InitSecurityContext\n2005-05-26 15:51:04,004 INFO  [Ajp13Processor[8892][1]]  listMembers ( /uscms/production ) -  admin.VOMSAdminSoapBindingImpl\n2005-05-26 15:51:04,013 ERROR [Ajp13Processor[8892][1]]  org.edg.security.voms.service.NotInDatabase:\n       Not in database: group  /uscms/production  -  connection.Database", 
            "title": "Error Code 64"
        }, 
        {
            "location": "/security/edg-mkgridmap/#useful-options-for-edg-mkgridmap-troubleshooting", 
            "text": "--help  --version  --conf= config_file \n    (Default configuration file is at  /etc/edg-mkgridmap.conf )  --output= output_file  --verbose", 
            "title": "Useful Options for edg-mkgridmap troubleshooting"
        }, 
        {
            "location": "/security/edg-mkgridmap/#how-to-get-help", 
            "text": "If you cannot resolve the problem or have general questions, there are several ways to receive help:   For bug support and issues, submit a ticket to the  Grid Operations Center .  For community support and best-effort software team support contact  .   For a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/security/install-gums/", 
            "text": "Warning\n\n\nGUMS is no longer be supported in OSG 3.4. The \nLCMAPS VOMS Plugin\n is the preferred method for site authentication in the OSG and is available in both OSG 3.3 and OSG 3.4.\n\n\n\n\nGUMS Install Guide\n\n\nThis document is intended for site administrators who want to install and configure the GUMS service.\n\n\nGUMS is a service that authorizes and maps users from their global (X.509) identity to a local (Linux user) identity. It is not a required service (some sites use the simple \nedg-mkgridmap\n to construct a \ngrid-mapfile\n instead), but it is commonly used. GUMS is useful when more than one resource (Compute Element, Storage Element, etc.) needs to authorize or map users, because it helps them share data. It is particularly helpful when using gLExec at a site, because gLExec runs on every worker node and needs authorization and mapping information. GUMS is a web application that runs in Tomcat.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux users \nmysql\n (UID 27) and \ntomcat\n (UID 91)\n\n\nService certificate:\n The GUMS service uses a host certificate at \n/etc/grid-security/http/httpcert.pem\n and an accompanying key at \n/etc/grid-security/http/httpkey.pem\n\n\nNetwork ports:\n Hosts using your GUMS server for authentication (e.g., HTCondor-CE, GridFTP) must be able to contact it on port 8443 (TCP)\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling GUMS\n\n\n\n\nWhether installing or upgrading GUMS, please make sure to follow \nthese instructions\n for updating/installing Java to work correctly with GUMS\n\n\nIf you have an existing GUMS installation on the same host, shut down Tomcat.\n    The service is named \ntomcat6\n on EL6 and \ntomcat\n on EL7\n\n\n\n\nInstall the GUMS Service:\n\n\n[root@server]#\n yum install osg-gums\n\n\n\n\n\n\n\n\n\nConfigure Tomcat to use GSI:\n\n\n[root@server]#\n /var/lib/trustmanager-tomcat/configure.sh\n\n\n\n\n\n\n\nNote\n\n\nThis step will overwrite your \nserver.xml\n file in your tomcat configuration directory (\n/etc/tomcat6\n on EL6, \n/etc/tomcat\n on EL7).\nIf you are using Tomcat on this host for non-grid purposes, you may want to save the \nserver.xml\n file first, run the script, then merge your own configuration back into the file\n\n\n\n\n\n\n\n\nConfiguring GUMS\n\n\nConfigure GUMS database\n\n\nIn this section, you will configure the GUMS MySQL database, either by creating a new database or by copying an existing GUMS database. Pick the appropriate subsection below for your environment.\n\n\nNew Installation\n\n\n\n\n\n\nStart the database server\n\n\n\n\n\n\nOn EL6, this is MySQL:\n\n\n[root@server]#\n service mysqld start\n\n\n\n\n\n\n\n\n\nOn EL7, this is MariaDB:\n\n\n[root@server]#\n systemctl start mariadb\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a new GUMS database:\n\n\n[root@server]#\n /usr/bin/gums-setup-mysql-database \n\\\n\n    --user gums --host localhost:3306 --password \npassword\n \n\\\n\n    --template /etc/gums/gums.config.template\n\n\n\n\n\n\n\nNote\n\n\nThe password is saved in plaintext in the \n/etc/gums/gums.config\n file, so choose one that is not used elsewhere. The \ngums.config\n file should be readable only by the \ntomcat\n user, but this situation provides light security at best\n\n\n\n\n\n\nNote\n\n\nWhen specifying the database host with the \n--host\n option above, it is also possible to use \n$HOSTNAME\n instead of \nlocalhost\n, though \nlocalhost\n is recommended. This value corresponds to \n@SERVER@\n in a \ngums.config.template\n file, discussed below\n\n\n\n\n\n\nNote\n\n\nAlthough it is possible to specify a different location for the \n--template\n or to omit the option and use the default GUMS template (at \n/usr/lib/gums/config/gums.config.template\n), neither option is typically useful for an OSG installation\n\n\n\n\n\n\n\n\nAdd yourself as a GUMS administrator:\n\n\n[root@server]#\n gums-add-mysql-admin \nYOUR DN\n\n\n\n\n\n\n\n\n\n\n[Optional but recommended:] Apply reasonable MySQL security settings:\n\n\n[root@server]#\n /usr/bin/mysql_secure_installation\n\n\n\n\n\n\n\n\n\nUpgrade from existing GUMS server on another host\n\n\nNote for GUMS 1.4+:\n Since the database schema has not changed between GUMS 1.3 and 1.4, the database name continues to be \nGUMS_1_3\n. \nDo not\n rename \nGUMS_1_3\n database references to \nGUMS_1_4\n. There was a schema change within the GUMS 1.4 series, but this happens automatically when GUMS is started - make sure the GUMS user has permission to perform schema changes. Nevertheless the database name remains \nGUMS_1_3\n in GUMS 1.4 and GUMS 1.5.\n\n\n\n\n\n\nOn the older host, dump the GUMS_1_3 database to a text file:\n\n\n[root@server]#\n mysqldump GUMS_1_3 \n gums_1_3.sql\n\n\n\n\n\n\n\n\n\nCopy the \ngums_1_3.sql\n file from the old host to the new one\n\n\n\n\n\n\nStart MySQL:\n\n\nFor EL 6:\n\n\n[root@server]#\n service mysqld start\n\n\n\n\n\nFor EL 7:\n\n\n[root@server]#\n systemctl start mariadb\n\n\n\n\n\n\n\n\n\nLoad the old GUMS data into the new MySQL database:\n\n\n[root@server]#\n \necho\n \nCREATE DATABASE IF NOT EXISTS GUMS_1_3;\n \n|\n mysql\n\n[root@server]#\n mysql GUMS_1_3 \n gums_1_3.sql\n\n\n\n\n\n\n\n\n\n[Optional but recommended for new MySQL instances:] Apply reasonable MySQL security settings:\n\n\n[root@server]#\n /usr/bin/mysql_secure_installation\n\n\n\n\n\n\n\n\n\nSet Initial GUMS Configuration\n\n\nIn this section, you will set up an initial GUMS configuration file, either by copying in an OSG template or by copying an existing configuration from an old installation. Pick the appropriate subsection below for your environment.\n\n\nNew Installation\n\n\nIf you ran the \ngums-setup-mysql-database\n command above with the \n--template\n option, the OSG GUMS template will be used. This should have created a suitable \n/etc/gums/gums.config\n with the configuration values in this section already filled in. \nIn that case, you can skip this section.\n\n\nIf you ran the \ngums-setup-mysql-database\n command above \nwithout\n a \n--template\n option, it created a default, pre-configured \n/etc/gums/gums.config\n file. It is almost certainly not what you want. Instead, it is recommended that you start with an OSG template for your configuration.\n\n\n\n\n\n\nCopy the OSG template over the default configuration file:\n\n\n[root@server]#\n cp /etc/gums/gums.config.template /etc/gums/gums.config\n\n\n\n\n\n\n\n\n\nEdit the new \n/etc/gums/gums.config\n file and change the following settings (note: each placeholder occurs exactly once in the file):\n\n\n\n\n\n\n\n\nSearch for\n\n\nReplace with\n\n\n\n\n\n\n\n\n\n\n@USER@\n\n\nThe name of the MySQL GUMS user. If you followed the instructions above, this will be \ngums\n\n\n\n\n\n\n@PASSWORD@\n\n\nThe password for the MYSQL GUMS user (see above)\n\n\n\n\n\n\n@SERVER@\n\n\nThe name of your computer and port (e.g. \nlocalhost\n or \nmy.computer:3306\n). See note\n\n\n\n\n\n\n@DOMAINNAME@\n\n\nYour local domain (e.g. \nwisc.edu\n)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nNormally MySQL is running on the same machine as GUMS (as in the instructions above).\n\nWe \nhighly recommend\n using \nlocalhost\n instead of the actual hostname; this will cause MySQL to use a local Unix socket instead of listening on the network, which is more secure.\nIf you use \nlocalhost\n, there is no need to specify a port.\nIn either case, the value for \n@SERVER@\n \nmust\n match the value for \n--host\n used when setting up the GUMS database with the \ngums-setup-mysql-database\n command\n\n\n\n\n\n\n\n\nConfigure Log Rotation\n\n\nBy default, certain output is written to files named \n/var/log/tomcat*/catalina.YYYY-MM-DD.log\n without automatic cleanup of old logs. To configure log rotation and cleanup of \ncatalina.log\n, follow the steps below:\n\n\n\n\n\n\nChoose the Tomcat directory name based on your operating system:\n\n\n\n\n\n\n\n\nIf your operating system is...\n\n\nThen your TOMCAT DIR NAME is...\n\n\n\n\n\n\n\n\n\n\nEL6\n\n\ntomcat6\n\n\n\n\n\n\nEL7\n\n\ntomcat\n\n\n\n\n\n\n\n\n\n\n\n\nEdit \n/etc/tomcat*/logging.properties\n so that Tomcat only produces a single, undated log file:\n\n\n--- /etc/\nTOMCAT DIR NAME\n/logging.properties.orig\n\n\n+++ /etc/\nTOMCAT DIR NAME\n/logging.properties\n\n\n@@ -24,7 +24,8 @@\n\n catalina.org.apache.juli.FileHandler.level = FINE\n catalina.org.apache.juli.FileHandler.directory = ${catalina.base}/logs\n\n-catalina.org.apache.juli.FileHandler.prefix = catalina.\n\n\n+catalina.org.apache.juli.FileHandler.prefix = catalina\n\n\n+catalina.org.apache.juli.FileHandler.rotatable = false\n\n\n localhost.org.apache.juli.FileHandler.level = FINE\n localhost.org.apache.juli.FileHandler.directory = ${catalina.base}/logs\n\n\n\n\n\n\n\n\n\nWrite \n/etc/logrotate.d/tomcat_catalina_logs\n to configure \nlogrotate\n:\n\n\n/var/log/\nTOMCAT DIR NAME\n/catalina.log\n{ copytruncate weekly rotate 52 compress missingok create 0644 tomcat tomcat }\n\n\n\n\n\n\n\n\n\nServices\n\n\nThe GUMS service is actually a web application running within the Tomcat web application server. It also uses the MySQL database server for storage and the Fetch CRL service to maintain each CRL. Choose the list of services based on your host's operating system:\n\n\n\n\n\n\nFor EL6 hosts\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nMySQL\n\n\nmysqld\n\n\n\n\n\n\n\n\nTomcat\n\n\ntomcat6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor EL7 hosts\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nMariaDB\n\n\nmariadb\n\n\n\n\n\n\n\n\nTomcat\n\n\ntomcat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nValidating GUMS\n\n\nThis section is optional, but if you would like to verify that your GUMS installation and configuration are good, consider using some or all of the sections below.\n\n\nConnect to the GUMS web page\n\n\nConnect to \nhttps://\nHOSTNAME\n:8443/gums/\n to use your GUMS instance. You must have the certificate that you used for \ngums-add-mysql-admin\n above loaded in your browser. You should see the GUMS web page load.\n\n\n\n\nNote\n\n\nJavascript must be enabled in order to make any configuration changes on the web interface.\n\n\n\n\nIf you do not see it load, check a few things:\n\n\nFor EL 6:\n\n\n\n\nLook for errors in \n/var/log/tomcat6/catalina.out\n and \n/var/log/tomcat6/catalina.err\n.\n\n\nLook for errors in \n/var/log/tomcat6/trustmanager.log\n. There are likely to be CRL errors in this file, this can be ignored unless all your CA's get CRL errors in which case you should check to make sure that your CRL updates are running correctly.\n\n\n\n\nFor EL 7:\n\n\n\n\nLook for errors in \n/var/log/tomcat/catalina.*.log\n\n\nLook for errors in \n/var/log/tomcat/trustmanager.log\n. There are likely to be CRL errors in this file, this can be ignored unless all your CA's get CRL errors in which case you should check to make sure that your CRL updates are running correctly.\n\n\n\n\nFor all systems:\n\n\n\n\nEnsure that you have an http certificate in \n/etc/grid-security/http/httpcert.pem\n and \n/etc/grid-security/http/httpkey.pem\n. Make sure it is readable by the \ntomcat\n user. Permissions should be as follows:\n\n\n\n\n[root@server]#\n ls -l /etc/grid-security/http/\n\ntotal 8\n\n\n-r--r--r-- 1 tomcat tomcat 1671 Jul 2 15:54 httpcert.pem\n\n\n-r-------- 1 tomcat tomcat 1675 Jul 2 15:54 httpkey.pem\n\n\n\n\n\n\nIf you change the permissions/ownership, make sure to restart tomcat so that your changes take effect.\n\n\nCheck accounts\n\n\nAfter you connect to the GUMS web page, go to the Summary tab to check the configuration. You should see several dozen OSG VOs listed.\n\n\nIn the Account column on the summary page, you will see the local Unix user accounts that these VO users will be mapped to. It is critical that these accounts exist on the gatekeeper and worker nodes at your site. If they do not, there will be errors when users attempt to access your site.\n\n\nUpdate VO members list\n\n\nGUMS contacts each VOMS server to update its knowledge of VO membership every 6 hours. After installing or updating GUMS, you should trigger the update manually by going to the Update VO Members tab, and clicking update.\n\n\nYou can track the progress of the update process by watching a log file.\n\n\nFor EL 6:\n\n\n[root@server]#\n tail -f /var/log/tomcat6/gums-service-admin.log\n\n\n\n\n\nFor EL 7:\n\n\n[root@server]#\n tail -f /var/log/tomcat/gums-service-admin.log\n\n\n\n\n\nWith so many VOMS servers in the OSG config, several member updates may fail for various reasons (e.g., host down \"connect timed out\", bad or expired host certificates, etc.). Unfortunately, this situation is normal. Typically, you will see about 5 or 6 failed updates, with the rest succeeding. The update will take a while and then should display any errors that occurred during the updates. To get more details or track the update process in real time, look at \n/var/log/gums-service-admin.log\n.\n\n\nMap a known good user DN\n\n\n\n\nGo to Map Grid Identity to Account tab: \nhttps://\nHOSTNAME\n:8443/gums/map_grid_identity_form.jsp\n\n\nFill in the required info. Service DN means the DN of the host certificate of your CE (see above). Use the DN of a user (probably yourself) who you know belongs to a particular VO. Fill in the VO name in the VOMS FQAN field.\n\n\nClick \"map user\". A failed mapping will display \"null\". A successful mapping will display a UNIX account name.\n\n\n\n\nMiscellaneous Procedures\n\n\nForcing GUMS to update the set of users\n\n\nGUMS automatically contacts each VOMS server every 6 hours to update its knowledge of VO membership. To trigger a manual update:\n\n\n\n\nAccess the \u201cUpdate VO Members\u201d tab\n\n\nClick \"Update\"\n\n\n\n\n[Optional:] Monitor update progress via a log file:\n\n\nFor EL 6:\n\n\n[root@server]#\n tail -f /var/log/tomcat6/gums-service-admin.log\n\n\n\n\n\nFor EL 7:\n\n\n[root@server]#\n tail -f /var/log/tomcat/gums-service-admin.log\n\n\n\n\n\n\n\n\n\nWith so many VOMS servers in the OSG config, several member updates may fail for various reasons (e.g., host down \"connect timed out\", bad or expired host certificates, etc.). Unfortunately, this situation is normal.\n\n\nUpdating the GUMS configuration\n\n\nPeriodically, the OSG Grid Operations Center will release an updated template for the GUMS configuration that updates information about an existing VO or adds a new VO. You may get the update as part of a regular update process, or you can force an update by using yum:\n\n\n[root@server]#\n yum update osg-gums-config\n\n\n\n\n\nThis step does \nnot\n update your GUMS configuration (\n/etc/gums/gums.config\n) but will update the template for your configuration (\n/etc/gums/gums.config.template\n), because RPM cannot merge configuration changes. Instead, use GUMS to merge in the new VO configuration information:\n\n\n\n\nGo to the Merge Configuration tab: \nhttps://\nHOSTNAME\n:8443/gums/mergeConfiguration.jsp\n\n\n\n\nCut and paste the URL of the OSG template into the Configuration URI field\n\n\nFor the template provided in the RPM, use: \nfile:///etc/gums/gums.config.template\n\n\nTo fetch it directly from the GOC, use \nhttp://repo.grid.iu.edu/pacman/tarballs/vo-version/gums.template\n\n\n\n\n\n\nClick Merge\n\n\nYou should get a green success message if it has worked, along with a suggestion that you update the VO members\n\n\n\n\n\n\nCheck the Summary tab to verify the set of VOs you have, as well as their accounts\n\n\n\n\n\n\nTroubleshooting\n\n\nUseful Configuration and Log Files\n\n\nConfiguration Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMySQL\n\n\n/etc/my.cnf\n\n\nMySQL configuration, e.g. server port\n\n\n\n\n\n\ntomcat (EL6)\n\n\n/etc/tomcat6/\n\n\nTomcat configuration files\n\n\n\n\n\n\ntomcat (EL7)\n\n\n/etc/tomcat/\n\n\nTomcat configuration files\n\n\n\n\n\n\n\n\nLog files\n\n\n\n\n\n\n\n\nService or Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntomcat (EL6)\n\n\n/var/log/tomcat6/catalina.out\n\n\nThis is the Tomcat log file. Problems (and a lot of noise) are reported here.\n\n\n\n\n\n\n\n\n/var/log/tomcat6/trustmanager.log\n\n\nThe trustmanager handles things related to authentication. Useful errors are sometimes here.\n\n\n\n\n\n\ntomcat (EL6/EL7)\n\n\n/var/log/tomcat*/catalina.*.log\n\n\nThese are the Tomcat log files. Problems (and a lot of noise) are reported here.\n\n\n\n\n\n\ntomcat (EL6/EL7)\n\n\n/var/log/tomcat*/catalina.log\n\n\nAlternate non-rotated location for tomcat log file. Not the same as \ncatalina.out\n. Problems (and a lot of noise) are reported here.\n\n\n\n\n\n\n\n\n/var/log/tomcat/trustmanager.log\n\n\nThe trustmanager handles things related to authentication. Useful errors are sometimes here.\n\n\n\n\n\n\nGUMS\n (EL6)\n\n\n/var/log/tomcat6/gums-service-admin.log\n\n\nGUMS outputs error messages related to its operations here.\n\n\n\n\n\n\n\n\n/var/log/tomcat6/gums-service-cybersecurity.log\n\n\nGUMS outputs security related messages to this file.\n\n\n\n\n\n\nGUMS\n (EL7)\n\n\n/var/log/tomcat/gums-service-admin.log\n\n\nGUMS outputs error messages related to its operations here.\n\n\n\n\n\n\n\n\n/var/log/tomcat/gums-service-cybersecurity.log\n\n\nGUMS outputs security related messages to this file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUMS\n (EL6/EL7)\n\n\n/var/log/gums/gums-developer.root.log\n\n\n\n\n\n\n\n\n\n\n/var/log/gums/gums-egee-security.root.log\n\n\nGUMS may also output some security related messages to this file as well.\n\n\n\n\n\n\n\n\n/var/log/gums/gums-privilege.root.log\n\n\nGUMS outputs mapping related errors to this file.\n\n\n\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use the \nHelp Procedure\n.\n\n\nReferences\n\n\n\n\nOfficial Tomcat 6 documentation\n\n\nOfficial Tomcat 7 documentation\n\n\nOfficial Hibernate documentation\n (Hibernate is the GUMS database interface)\n\n\n\n\nHost\n\n\nFor security reasons, it is recommended to install GUMS on a separate host from the CE,\n    but it is not necessary\n\n\nUsers\n\n\nThe GUMS installation will create two users unless they exist already:\n\n\n\n\n\n\n\n\nUser\n\n\nDefault UID\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nmysql\n\n\n27\n\n\nRuns the MySQL database server, which GUMS uses\n\n\n\n\n\n\ntomcat\n\n\n91\n\n\nRuns the Tomcat web application server, which runs GUMS\n\n\n\n\n\n\n\n\nNote that if UIDs 27 and 91 are taken already but not used for the appropriate users, you will experience errors.\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nOwner, Permissions\n\n\nPath\n\n\n\n\n\n\n\n\n\n\nHTTP service certificate\n\n\ntomcat:tomcat\n, 0644\n\n\n/etc/grid-security/http/httpcert.pem\n\n\n\n\n\n\nHTTP service key\n\n\ntomcat:tomcat\n, 0600\n\n\n/etc/grid-security/http/httpkey.pem\n\n\n\n\n\n\n\n\nNetworking\n\n\nGUMS communicates on TCP port 8443; this port must be accessible to the Compute Element and any other hosts that need to authenticate via GUMS.", 
            "title": "GUMS"
        }, 
        {
            "location": "/security/install-gums/#gums-install-guide", 
            "text": "This document is intended for site administrators who want to install and configure the GUMS service.  GUMS is a service that authorizes and maps users from their global (X.509) identity to a local (Linux user) identity. It is not a required service (some sites use the simple  edg-mkgridmap  to construct a  grid-mapfile  instead), but it is commonly used. GUMS is useful when more than one resource (Compute Element, Storage Element, etc.) needs to authorize or map users, because it helps them share data. It is particularly helpful when using gLExec at a site, because gLExec runs on every worker node and needs authorization and mapping information. GUMS is a web application that runs in Tomcat.", 
            "title": "GUMS Install Guide"
        }, 
        {
            "location": "/security/install-gums/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux users  mysql  (UID 27) and  tomcat  (UID 91)  Service certificate:  The GUMS service uses a host certificate at  /etc/grid-security/http/httpcert.pem  and an accompanying key at  /etc/grid-security/http/httpkey.pem  Network ports:  Hosts using your GUMS server for authentication (e.g., HTCondor-CE, GridFTP) must be able to contact it on port 8443 (TCP)   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/security/install-gums/#installing-gums", 
            "text": "Whether installing or upgrading GUMS, please make sure to follow  these instructions  for updating/installing Java to work correctly with GUMS  If you have an existing GUMS installation on the same host, shut down Tomcat.\n    The service is named  tomcat6  on EL6 and  tomcat  on EL7   Install the GUMS Service:  [root@server]#  yum install osg-gums    Configure Tomcat to use GSI:  [root@server]#  /var/lib/trustmanager-tomcat/configure.sh   Note  This step will overwrite your  server.xml  file in your tomcat configuration directory ( /etc/tomcat6  on EL6,  /etc/tomcat  on EL7).\nIf you are using Tomcat on this host for non-grid purposes, you may want to save the  server.xml  file first, run the script, then merge your own configuration back into the file", 
            "title": "Installing GUMS"
        }, 
        {
            "location": "/security/install-gums/#configuring-gums", 
            "text": "", 
            "title": "Configuring GUMS"
        }, 
        {
            "location": "/security/install-gums/#configure-gums-database", 
            "text": "In this section, you will configure the GUMS MySQL database, either by creating a new database or by copying an existing GUMS database. Pick the appropriate subsection below for your environment.", 
            "title": "Configure GUMS database"
        }, 
        {
            "location": "/security/install-gums/#new-installation", 
            "text": "Start the database server    On EL6, this is MySQL:  [root@server]#  service mysqld start    On EL7, this is MariaDB:  [root@server]#  systemctl start mariadb      Create a new GUMS database:  [root@server]#  /usr/bin/gums-setup-mysql-database  \\ \n    --user gums --host localhost:3306 --password  password   \\ \n    --template /etc/gums/gums.config.template   Note  The password is saved in plaintext in the  /etc/gums/gums.config  file, so choose one that is not used elsewhere. The  gums.config  file should be readable only by the  tomcat  user, but this situation provides light security at best    Note  When specifying the database host with the  --host  option above, it is also possible to use  $HOSTNAME  instead of  localhost , though  localhost  is recommended. This value corresponds to  @SERVER@  in a  gums.config.template  file, discussed below    Note  Although it is possible to specify a different location for the  --template  or to omit the option and use the default GUMS template (at  /usr/lib/gums/config/gums.config.template ), neither option is typically useful for an OSG installation     Add yourself as a GUMS administrator:  [root@server]#  gums-add-mysql-admin  YOUR DN     [Optional but recommended:] Apply reasonable MySQL security settings:  [root@server]#  /usr/bin/mysql_secure_installation", 
            "title": "New Installation"
        }, 
        {
            "location": "/security/install-gums/#upgrade-from-existing-gums-server-on-another-host", 
            "text": "Note for GUMS 1.4+:  Since the database schema has not changed between GUMS 1.3 and 1.4, the database name continues to be  GUMS_1_3 .  Do not  rename  GUMS_1_3  database references to  GUMS_1_4 . There was a schema change within the GUMS 1.4 series, but this happens automatically when GUMS is started - make sure the GUMS user has permission to perform schema changes. Nevertheless the database name remains  GUMS_1_3  in GUMS 1.4 and GUMS 1.5.    On the older host, dump the GUMS_1_3 database to a text file:  [root@server]#  mysqldump GUMS_1_3   gums_1_3.sql    Copy the  gums_1_3.sql  file from the old host to the new one    Start MySQL:  For EL 6:  [root@server]#  service mysqld start  For EL 7:  [root@server]#  systemctl start mariadb    Load the old GUMS data into the new MySQL database:  [root@server]#   echo   CREATE DATABASE IF NOT EXISTS GUMS_1_3;   |  mysql [root@server]#  mysql GUMS_1_3   gums_1_3.sql    [Optional but recommended for new MySQL instances:] Apply reasonable MySQL security settings:  [root@server]#  /usr/bin/mysql_secure_installation", 
            "title": "Upgrade from existing GUMS server on another host"
        }, 
        {
            "location": "/security/install-gums/#set-initial-gums-configuration", 
            "text": "In this section, you will set up an initial GUMS configuration file, either by copying in an OSG template or by copying an existing configuration from an old installation. Pick the appropriate subsection below for your environment.", 
            "title": "Set Initial GUMS Configuration"
        }, 
        {
            "location": "/security/install-gums/#new-installation_1", 
            "text": "If you ran the  gums-setup-mysql-database  command above with the  --template  option, the OSG GUMS template will be used. This should have created a suitable  /etc/gums/gums.config  with the configuration values in this section already filled in.  In that case, you can skip this section.  If you ran the  gums-setup-mysql-database  command above  without  a  --template  option, it created a default, pre-configured  /etc/gums/gums.config  file. It is almost certainly not what you want. Instead, it is recommended that you start with an OSG template for your configuration.    Copy the OSG template over the default configuration file:  [root@server]#  cp /etc/gums/gums.config.template /etc/gums/gums.config    Edit the new  /etc/gums/gums.config  file and change the following settings (note: each placeholder occurs exactly once in the file):     Search for  Replace with      @USER@  The name of the MySQL GUMS user. If you followed the instructions above, this will be  gums    @PASSWORD@  The password for the MYSQL GUMS user (see above)    @SERVER@  The name of your computer and port (e.g.  localhost  or  my.computer:3306 ). See note    @DOMAINNAME@  Your local domain (e.g.  wisc.edu )      Note  Normally MySQL is running on the same machine as GUMS (as in the instructions above). \nWe  highly recommend  using  localhost  instead of the actual hostname; this will cause MySQL to use a local Unix socket instead of listening on the network, which is more secure.\nIf you use  localhost , there is no need to specify a port.\nIn either case, the value for  @SERVER@   must  match the value for  --host  used when setting up the GUMS database with the  gums-setup-mysql-database  command", 
            "title": "New Installation"
        }, 
        {
            "location": "/security/install-gums/#configure-log-rotation", 
            "text": "By default, certain output is written to files named  /var/log/tomcat*/catalina.YYYY-MM-DD.log  without automatic cleanup of old logs. To configure log rotation and cleanup of  catalina.log , follow the steps below:    Choose the Tomcat directory name based on your operating system:     If your operating system is...  Then your TOMCAT DIR NAME is...      EL6  tomcat6    EL7  tomcat       Edit  /etc/tomcat*/logging.properties  so that Tomcat only produces a single, undated log file:  --- /etc/ TOMCAT DIR NAME /logging.properties.orig  +++ /etc/ TOMCAT DIR NAME /logging.properties  @@ -24,7 +24,8 @@ \n catalina.org.apache.juli.FileHandler.level = FINE\n catalina.org.apache.juli.FileHandler.directory = ${catalina.base}/logs -catalina.org.apache.juli.FileHandler.prefix = catalina.  +catalina.org.apache.juli.FileHandler.prefix = catalina  +catalina.org.apache.juli.FileHandler.rotatable = false \n\n localhost.org.apache.juli.FileHandler.level = FINE\n localhost.org.apache.juli.FileHandler.directory = ${catalina.base}/logs    Write  /etc/logrotate.d/tomcat_catalina_logs  to configure  logrotate :  /var/log/ TOMCAT DIR NAME /catalina.log\n{ copytruncate weekly rotate 52 compress missingok create 0644 tomcat tomcat }", 
            "title": "Configure Log Rotation"
        }, 
        {
            "location": "/security/install-gums/#services", 
            "text": "The GUMS service is actually a web application running within the Tomcat web application server. It also uses the MySQL database server for storage and the Fetch CRL service to maintain each CRL. Choose the list of services based on your host's operating system:    For EL6 hosts     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    MySQL  mysqld     Tomcat  tomcat6        For EL7 hosts     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    MariaDB  mariadb     Tomcat  tomcat        Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME", 
            "title": "Services"
        }, 
        {
            "location": "/security/install-gums/#validating-gums", 
            "text": "This section is optional, but if you would like to verify that your GUMS installation and configuration are good, consider using some or all of the sections below.", 
            "title": "Validating GUMS"
        }, 
        {
            "location": "/security/install-gums/#connect-to-the-gums-web-page", 
            "text": "Connect to  https:// HOSTNAME :8443/gums/  to use your GUMS instance. You must have the certificate that you used for  gums-add-mysql-admin  above loaded in your browser. You should see the GUMS web page load.   Note  Javascript must be enabled in order to make any configuration changes on the web interface.   If you do not see it load, check a few things:  For EL 6:   Look for errors in  /var/log/tomcat6/catalina.out  and  /var/log/tomcat6/catalina.err .  Look for errors in  /var/log/tomcat6/trustmanager.log . There are likely to be CRL errors in this file, this can be ignored unless all your CA's get CRL errors in which case you should check to make sure that your CRL updates are running correctly.   For EL 7:   Look for errors in  /var/log/tomcat/catalina.*.log  Look for errors in  /var/log/tomcat/trustmanager.log . There are likely to be CRL errors in this file, this can be ignored unless all your CA's get CRL errors in which case you should check to make sure that your CRL updates are running correctly.   For all systems:   Ensure that you have an http certificate in  /etc/grid-security/http/httpcert.pem  and  /etc/grid-security/http/httpkey.pem . Make sure it is readable by the  tomcat  user. Permissions should be as follows:   [root@server]#  ls -l /etc/grid-security/http/ total 8  -r--r--r-- 1 tomcat tomcat 1671 Jul 2 15:54 httpcert.pem  -r-------- 1 tomcat tomcat 1675 Jul 2 15:54 httpkey.pem   If you change the permissions/ownership, make sure to restart tomcat so that your changes take effect.", 
            "title": "Connect to the GUMS web page"
        }, 
        {
            "location": "/security/install-gums/#check-accounts", 
            "text": "After you connect to the GUMS web page, go to the Summary tab to check the configuration. You should see several dozen OSG VOs listed.  In the Account column on the summary page, you will see the local Unix user accounts that these VO users will be mapped to. It is critical that these accounts exist on the gatekeeper and worker nodes at your site. If they do not, there will be errors when users attempt to access your site.", 
            "title": "Check accounts"
        }, 
        {
            "location": "/security/install-gums/#update-vo-members-list", 
            "text": "GUMS contacts each VOMS server to update its knowledge of VO membership every 6 hours. After installing or updating GUMS, you should trigger the update manually by going to the Update VO Members tab, and clicking update.  You can track the progress of the update process by watching a log file.  For EL 6:  [root@server]#  tail -f /var/log/tomcat6/gums-service-admin.log  For EL 7:  [root@server]#  tail -f /var/log/tomcat/gums-service-admin.log  With so many VOMS servers in the OSG config, several member updates may fail for various reasons (e.g., host down \"connect timed out\", bad or expired host certificates, etc.). Unfortunately, this situation is normal. Typically, you will see about 5 or 6 failed updates, with the rest succeeding. The update will take a while and then should display any errors that occurred during the updates. To get more details or track the update process in real time, look at  /var/log/gums-service-admin.log .", 
            "title": "Update VO members list"
        }, 
        {
            "location": "/security/install-gums/#map-a-known-good-user-dn", 
            "text": "Go to Map Grid Identity to Account tab:  https:// HOSTNAME :8443/gums/map_grid_identity_form.jsp  Fill in the required info. Service DN means the DN of the host certificate of your CE (see above). Use the DN of a user (probably yourself) who you know belongs to a particular VO. Fill in the VO name in the VOMS FQAN field.  Click \"map user\". A failed mapping will display \"null\". A successful mapping will display a UNIX account name.", 
            "title": "Map a known good user DN"
        }, 
        {
            "location": "/security/install-gums/#miscellaneous-procedures", 
            "text": "", 
            "title": "Miscellaneous Procedures"
        }, 
        {
            "location": "/security/install-gums/#forcing-gums-to-update-the-set-of-users", 
            "text": "GUMS automatically contacts each VOMS server every 6 hours to update its knowledge of VO membership. To trigger a manual update:   Access the \u201cUpdate VO Members\u201d tab  Click \"Update\"   [Optional:] Monitor update progress via a log file:  For EL 6:  [root@server]#  tail -f /var/log/tomcat6/gums-service-admin.log  For EL 7:  [root@server]#  tail -f /var/log/tomcat/gums-service-admin.log    With so many VOMS servers in the OSG config, several member updates may fail for various reasons (e.g., host down \"connect timed out\", bad or expired host certificates, etc.). Unfortunately, this situation is normal.", 
            "title": "Forcing GUMS to update the set of users"
        }, 
        {
            "location": "/security/install-gums/#updating-the-gums-configuration", 
            "text": "Periodically, the OSG Grid Operations Center will release an updated template for the GUMS configuration that updates information about an existing VO or adds a new VO. You may get the update as part of a regular update process, or you can force an update by using yum:  [root@server]#  yum update osg-gums-config  This step does  not  update your GUMS configuration ( /etc/gums/gums.config ) but will update the template for your configuration ( /etc/gums/gums.config.template ), because RPM cannot merge configuration changes. Instead, use GUMS to merge in the new VO configuration information:   Go to the Merge Configuration tab:  https:// HOSTNAME :8443/gums/mergeConfiguration.jsp   Cut and paste the URL of the OSG template into the Configuration URI field  For the template provided in the RPM, use:  file:///etc/gums/gums.config.template  To fetch it directly from the GOC, use  http://repo.grid.iu.edu/pacman/tarballs/vo-version/gums.template    Click Merge  You should get a green success message if it has worked, along with a suggestion that you update the VO members    Check the Summary tab to verify the set of VOs you have, as well as their accounts", 
            "title": "Updating the GUMS configuration"
        }, 
        {
            "location": "/security/install-gums/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/security/install-gums/#useful-configuration-and-log-files", 
            "text": "Configuration Files     Service or Process  Configuration File  Description      MySQL  /etc/my.cnf  MySQL configuration, e.g. server port    tomcat (EL6)  /etc/tomcat6/  Tomcat configuration files    tomcat (EL7)  /etc/tomcat/  Tomcat configuration files     Log files     Service or Process  Log File  Description      tomcat (EL6)  /var/log/tomcat6/catalina.out  This is the Tomcat log file. Problems (and a lot of noise) are reported here.     /var/log/tomcat6/trustmanager.log  The trustmanager handles things related to authentication. Useful errors are sometimes here.    tomcat (EL6/EL7)  /var/log/tomcat*/catalina.*.log  These are the Tomcat log files. Problems (and a lot of noise) are reported here.    tomcat (EL6/EL7)  /var/log/tomcat*/catalina.log  Alternate non-rotated location for tomcat log file. Not the same as  catalina.out . Problems (and a lot of noise) are reported here.     /var/log/tomcat/trustmanager.log  The trustmanager handles things related to authentication. Useful errors are sometimes here.    GUMS  (EL6)  /var/log/tomcat6/gums-service-admin.log  GUMS outputs error messages related to its operations here.     /var/log/tomcat6/gums-service-cybersecurity.log  GUMS outputs security related messages to this file.    GUMS  (EL7)  /var/log/tomcat/gums-service-admin.log  GUMS outputs error messages related to its operations here.     /var/log/tomcat/gums-service-cybersecurity.log  GUMS outputs security related messages to this file.               GUMS  (EL6/EL7)  /var/log/gums/gums-developer.root.log      /var/log/gums/gums-egee-security.root.log  GUMS may also output some security related messages to this file as well.     /var/log/gums/gums-privilege.root.log  GUMS outputs mapping related errors to this file.", 
            "title": "Useful Configuration and Log Files"
        }, 
        {
            "location": "/security/install-gums/#how-to-get-help", 
            "text": "To get assistance please use the  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/security/install-gums/#references", 
            "text": "Official Tomcat 6 documentation  Official Tomcat 7 documentation  Official Hibernate documentation  (Hibernate is the GUMS database interface)", 
            "title": "References"
        }, 
        {
            "location": "/security/install-gums/#host", 
            "text": "For security reasons, it is recommended to install GUMS on a separate host from the CE,\n    but it is not necessary", 
            "title": "Host"
        }, 
        {
            "location": "/security/install-gums/#users", 
            "text": "The GUMS installation will create two users unless they exist already:     User  Default UID  Comment      mysql  27  Runs the MySQL database server, which GUMS uses    tomcat  91  Runs the Tomcat web application server, which runs GUMS     Note that if UIDs 27 and 91 are taken already but not used for the appropriate users, you will experience errors.", 
            "title": "Users"
        }, 
        {
            "location": "/security/install-gums/#certificates", 
            "text": "Certificate  Owner, Permissions  Path      HTTP service certificate  tomcat:tomcat , 0644  /etc/grid-security/http/httpcert.pem    HTTP service key  tomcat:tomcat , 0600  /etc/grid-security/http/httpkey.pem", 
            "title": "Certificates"
        }, 
        {
            "location": "/security/install-gums/#networking", 
            "text": "GUMS communicates on TCP port 8443; this port must be accessible to the Compute Element and any other hosts that need to authenticate via GUMS.", 
            "title": "Networking"
        }, 
        {
            "location": "/worker-node/install-glexec/", 
            "text": "Glexec Installation Guide\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of June 2018.\n\n\n\n\nThis document is intended for System Administrators that are installing the OSG version of glexec.\n\n\nGlexec is commonly used for what are referred to as \"pilot\" or \"glidein\" jobs.\n\n\nTraditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper). The user job was authenticated/authorized to run at that site based on the user\u2019s proxy credentials and run under the local unix account assigned.\n\n\nIn a pilot-based infrastructure, users submit their jobs to a centralized site (or queue). The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources. It will then submit what is called a pilot/glidein job to a remote site. This pilot job gets authenticated/authorized to run on a worker node in that site\u2019s cluster. It will then \"pull\" down user jobs from the centralized queue and execute them. Both the pilot and the user job are run under the pilot job\u2019s proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user\u2019s proxy credentials and, thus, the user\u2019s jobs do not run using it\u2019s own local unix account.\n\n\nGlexec is a security tool that can be used to resolve this problem. It is meant to be used by VOs that run these pilot-based jobs. It has a number of authentication plugins and can be used both by European grid and by OSG.\n\n\nThe pilot job will \"pull\" user jobs down from the central queue and invoke glexec which will then\n\n\n\n\nauthenticate the user job\u2019s proxy,\n\n\nperform an authorization callout (to GUMS in the case of OSG, or possibly a gridmapfile) similar to that done by the gatekeeper,\n\n\nand then run the user job under the local account assigned by the authorization service for that user.\n\n\n\n\nIn effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node. The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.\n\n\nMany worker node clusters use shared file systems like NFS for much of their software and user home accounts. Since glexec is an suid program, it must be installed on every single worker node individually. Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.\n\n\nFor more information regarding pilot-based systems and glexec:\n\n\n\n\nglideinWMS - The glidein based WMS\n\n\nAddressing the pilot security problem with gLExec (pdf)\n\n\n\n\nEngineering Considerations\n\n\nIf glexec is to be used at a site, it should be installed and configured on every worker node.\n\n\nA large number of batch slots using glexec can occasionally put an enormous strain on GUMS servers and cause overloading and client timeouts. In order to survive peak loads, the sysctl parameter \nnet.core.somaxconn\n on a GUMS server machine should be set at least as high as the maximum number of job slots that might attempt to contact the server at about the same time.  For example, Fermilab set the value to 4096 on each of two servers and tested with a continuous load from 5000 job slots.\n\n\nRequirements\n\n\nThese are the requirements that must be met to install glexec.\n\n\n\n\nNote\n\n\nNormally you will install the \nOSG worker node\n first. Installing the \nosg-wn-client-glexec\n package will also install the worker node, but we do not duplicate instructions specific to the worker node here.\n\n\n\n\n\n\nVerify you are using one of the \nsupported platforms\n.\n\n\nPrior to installing \nglexec\n, verify the \nyum repositories\n are correctly configured.\n\n\nroot\n access to the host is required.\n\n\nThe worker nodes will need the \nCA certificates\n used to sign the proxies.\n\n\nfetch-crl\n should be enabled and working.\n\n\n\n\nThe glexec installation will create two users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nglexec\n\n\nReduced privilege separate id used to improve security. If creating the account by hand, set the default gid of the \nglexec\n user to be a group that is also called \nglexec\n.\n\n\n\n\n\n\ngratia\n\n\nNeeded for the glexec gratia probe which is also automatically installed.\n\n\n\n\n\n\n\n\nIn addition, OSG glexec requires a range of \ngroup ids\n for tracking purposes. You don\u2018t actually have to create the group entries but it is recommended to do so in order to reserve the gids and so they can be associated with names in the \n/usr/bin/id\n command. The recommended names are \nglexecNN\n where NN is a number starting from 00.\n\n\n\n\nDefine at least 4 group ids per batch slot per worker node. A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.\n\n\nThey must be consecutive and in any range (default range is 65000-65049, configured in the \nconfiguring glexec\n section below).\n\n\nThe same group IDs can be used on every worker node.\n\n\n\n\nInstall Instructions\n\n\n\n\nNote\n\n\nThe glexec tracking function requires a part of HTCondor. There are multiple ways to install HTCondor, for details see \nthese instructions\n. If you want a minimal install, you can run just this command to install the needed piece from the OSG distribution:\n\n\n[root@client ~] #\n yum install condor-procd\n\n\n\n\n\n\n\nAfter meeting all the requirements in the previous section, install glexec with this command:\n\n\n[root@client ~] #\n yum install osg-wn-client-glexec\n\n\n\n\n\nConfiguring glexec\n\n\nThe following steps need to be done after the glexec installation is complete.\n\n\n\n\nFirst, review the contents of \n/etc/glexec.conf\n. All of the defaults should be fine, but if you want to change the behavior, the parameters are described in \nman glexec.conf\n.\n\n\n\n\nNext, review all of the contents of \n/etc/lcmaps.db\n and in particular update the following pieces.\n\n\n\n\n\n\nIf you have GUMS, change \nGUMS_HOST\n in the following line to the fully qualified domain name of your GUMS server:\n\n\n\u2013endpoint https://\nGUMS_HOST\n:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\n\n\n\n\n\nIf you want to use a range of tracking group ids other than the default as described in the \nRequirements\n section above, uncomment and change the \n-min-gid\n and \n-max-gid\n lines to your chosen values:\n\n\n-min-gid 65000\n \n-max-gid 65049\n\n\n\n\n\n\n\n\n\n\nUncomment the following two lines:\n\n\nglexectracking = \nlcmaps_glexec_tracking.mod\n\n                 \n-exec /usr/sbin/glexec_monitor\n\n\n\n\n\n\n\n\n\n\nIf you have GUMS, uncomment the following policy toward the end of the file:\n\n\nverifyproxy -\n gumsclient\ngumsclient -\n glexectracking\n\n\n\n\n\nor if you have do not have GUMS and want to use a gridmapfile, uncomment the following policy:\n\n\nverifyproxy -\n gridmapfile\ngridmapfile -\n glexectracking\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting the Installation of glexec\n\n\nNow, \nas a non-privileged user (not root)\n, do the following (where \n is your VO, and \n is your uid as reported by \n/usr/bin/id\n):\n\n\n[user@client ~] $\n voms-proxy-init -voms \nYOURVO\n:/\nYOURVO\n\n\n[user@client ~] $\n \nexport\n \nGLEXEC_CLIENT_CERT\n=\n/tmp/x509up_u\nUID\n\n\n[user@client ~] $\n \nexport\n \nX509_USER_PROXY\n=\n/tmp/x509up_u\nUID\n\n\n[user@client ~] $\n /usr/sbin/glexec /usr/bin/id\n\n[user@client ~] $\n \nuid\n=\n13160\n(\nfnalgrid\n)\n \ngid\n=\n9767\n(\nfnalgrid\n)\n \ngroups\n=\n65000\n(\nglexec00\n)\n\n\n\n\n\n\nIf \nglexec\n is successful, it will print out the uid and gid that your proxy would normally be mapped to by your GUMS server, plus a supplementary tracking group. (The actual names and numbers will be different from what you see above.)\n\n\nIf you have problems, please read about \ntroubleshooting glexec\n.\n\n\nGlexec log files\n\n\nGlexec sends all its log information by default to syslog. By default they go to \n/var/log/messages\n, but this may differ if you have customized your syslog setup. Here are some sample messages:\n\n\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-PluginInit(): plugin glexectracking not found (arguments: )\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-lcmaps_startPluginManager(): error initializing plugin: glexectracking\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps_init() error: could not start plugin manager\nApr 25 16:36:16 fermicloud053 glexec[2867]: Initialisation of LCMAPS failed.\n\n\n\n\n\nThese particular messages are pretty common, caused by forgetting to uncomment the beginning of the \nglexectracking\n rule in \n/etc/lcmaps.db\n.\n\n\nIt is possible to redirect glexec log messages to a different file with standard syslog. To do that, choose one of the \nLOG_LOCAL[0-7]\n log facilities that are unused, for example \nLOG_LOCAL1\n. Then set the following in \n/etc/glexec.conf\n:\n\n\nsyslog_facility = LOG_LOCAL1\n\n\n\n\n\nand add a corresponding parameter to the \nlcmaps_glexec_tracking.mod\n entry in \n/etc/lcmaps.db\n:\n\n\n  \n-log-facility LOG_LOCAL1\n\n\n\n\n\n\nThen in \n/etc/rsyslog.conf\n, add a line like this:\n\n\nlocal1.* /var/log/glexec.log\n\n\n\n\n\nand also exclude those messages from \n/var/log/messages\n by adding \nlocal1.none\n after other wildcards on the existing \n/var/log/messages\n line, for example:\n\n\n*.info;local1.none;mail.none;authpriv.none;cron.none /var/log/messages\n\n\n\n\n\nBe sure to notify the system logger to re-read the configuration file with \nservice rsyslog restart\n.\n\n\nrsyslog\n, by default, limits the rate at which messages may be logged, and if maximum debugging is enabled in glexec this limit is reached. To avoid that, you can add the following to \n/etc/rsyslog.conf\n after the line \"$ModLoad imuxsock.so\":\n\n\n$SystemLogRateLimitInterval 0\n$SystemLogRateLimitBurst 0\n\n\n\n\n\nand do \nservice rsyslog restart\n.\n\n\nAlternatively, \nsyslog-ng\n (available in the EPEL repository) can do the same job by matching all the messages that have the string \"glexec\" in the name.\nThese rules in \n/etc/syslog-ng/syslog-ng.conf\n will separate the glexec messages into \n/var/log/glexec.log\n:\n\n\ndestination d_glexec { file(\n/var/log/glexec.log\n); };\nfilter f_glexec { program(\n^glexec\n); };\nfilter f_notglexec { not program(\n^glexec\n); };\nlog { source(s_sys); filter(f_glexec); destination(d_glexec); };\n\n\n\n\n\nThen later, in the log rule writing sending to \nd_mesg\n, add a \nfilter(f_notglexec);\n before the destination rule to keep glexec messages out of \n/var/log/messages\n:\n\n\nlog { source(s_sys); filter(f_filter1); filter(f_notglexec); destination(d_mesg); };\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use the \nHelp Procedure\n.", 
            "title": "Worker node glexec Install"
        }, 
        {
            "location": "/worker-node/install-glexec/#glexec-installation-guide", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of June 2018.   This document is intended for System Administrators that are installing the OSG version of glexec.  Glexec is commonly used for what are referred to as \"pilot\" or \"glidein\" jobs.  Traditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper). The user job was authenticated/authorized to run at that site based on the user\u2019s proxy credentials and run under the local unix account assigned.  In a pilot-based infrastructure, users submit their jobs to a centralized site (or queue). The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources. It will then submit what is called a pilot/glidein job to a remote site. This pilot job gets authenticated/authorized to run on a worker node in that site\u2019s cluster. It will then \"pull\" down user jobs from the centralized queue and execute them. Both the pilot and the user job are run under the pilot job\u2019s proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user\u2019s proxy credentials and, thus, the user\u2019s jobs do not run using it\u2019s own local unix account.  Glexec is a security tool that can be used to resolve this problem. It is meant to be used by VOs that run these pilot-based jobs. It has a number of authentication plugins and can be used both by European grid and by OSG.  The pilot job will \"pull\" user jobs down from the central queue and invoke glexec which will then   authenticate the user job\u2019s proxy,  perform an authorization callout (to GUMS in the case of OSG, or possibly a gridmapfile) similar to that done by the gatekeeper,  and then run the user job under the local account assigned by the authorization service for that user.   In effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node. The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.  Many worker node clusters use shared file systems like NFS for much of their software and user home accounts. Since glexec is an suid program, it must be installed on every single worker node individually. Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.  For more information regarding pilot-based systems and glexec:   glideinWMS - The glidein based WMS  Addressing the pilot security problem with gLExec (pdf)", 
            "title": "Glexec Installation Guide"
        }, 
        {
            "location": "/worker-node/install-glexec/#engineering-considerations", 
            "text": "If glexec is to be used at a site, it should be installed and configured on every worker node.  A large number of batch slots using glexec can occasionally put an enormous strain on GUMS servers and cause overloading and client timeouts. In order to survive peak loads, the sysctl parameter  net.core.somaxconn  on a GUMS server machine should be set at least as high as the maximum number of job slots that might attempt to contact the server at about the same time.  For example, Fermilab set the value to 4096 on each of two servers and tested with a continuous load from 5000 job slots.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/worker-node/install-glexec/#requirements", 
            "text": "These are the requirements that must be met to install glexec.   Note  Normally you will install the  OSG worker node  first. Installing the  osg-wn-client-glexec  package will also install the worker node, but we do not duplicate instructions specific to the worker node here.    Verify you are using one of the  supported platforms .  Prior to installing  glexec , verify the  yum repositories  are correctly configured.  root  access to the host is required.  The worker nodes will need the  CA certificates  used to sign the proxies.  fetch-crl  should be enabled and working.   The glexec installation will create two users unless they are already created.     User  Comment      glexec  Reduced privilege separate id used to improve security. If creating the account by hand, set the default gid of the  glexec  user to be a group that is also called  glexec .    gratia  Needed for the glexec gratia probe which is also automatically installed.     In addition, OSG glexec requires a range of  group ids  for tracking purposes. You don\u2018t actually have to create the group entries but it is recommended to do so in order to reserve the gids and so they can be associated with names in the  /usr/bin/id  command. The recommended names are  glexecNN  where NN is a number starting from 00.   Define at least 4 group ids per batch slot per worker node. A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.  They must be consecutive and in any range (default range is 65000-65049, configured in the  configuring glexec  section below).  The same group IDs can be used on every worker node.", 
            "title": "Requirements"
        }, 
        {
            "location": "/worker-node/install-glexec/#install-instructions", 
            "text": "Note  The glexec tracking function requires a part of HTCondor. There are multiple ways to install HTCondor, for details see  these instructions . If you want a minimal install, you can run just this command to install the needed piece from the OSG distribution:  [root@client ~] #  yum install condor-procd   After meeting all the requirements in the previous section, install glexec with this command:  [root@client ~] #  yum install osg-wn-client-glexec", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/worker-node/install-glexec/#configuring-glexec", 
            "text": "The following steps need to be done after the glexec installation is complete.   First, review the contents of  /etc/glexec.conf . All of the defaults should be fine, but if you want to change the behavior, the parameters are described in  man glexec.conf .   Next, review all of the contents of  /etc/lcmaps.db  and in particular update the following pieces.    If you have GUMS, change  GUMS_HOST  in the following line to the fully qualified domain name of your GUMS server:  \u2013endpoint https:// GUMS_HOST :8443/gums/services/GUMSXACMLAuthorizationServicePort     If you want to use a range of tracking group ids other than the default as described in the  Requirements  section above, uncomment and change the  -min-gid  and  -max-gid  lines to your chosen values:  -min-gid 65000   -max-gid 65049     Uncomment the following two lines:  glexectracking =  lcmaps_glexec_tracking.mod \n                  -exec /usr/sbin/glexec_monitor     If you have GUMS, uncomment the following policy toward the end of the file:  verifyproxy -  gumsclient\ngumsclient -  glexectracking  or if you have do not have GUMS and want to use a gridmapfile, uncomment the following policy:  verifyproxy -  gridmapfile\ngridmapfile -  glexectracking", 
            "title": "Configuring glexec"
        }, 
        {
            "location": "/worker-node/install-glexec/#testing-the-installation-of-glexec", 
            "text": "Now,  as a non-privileged user (not root) , do the following (where   is your VO, and   is your uid as reported by  /usr/bin/id ):  [user@client ~] $  voms-proxy-init -voms  YOURVO :/ YOURVO  [user@client ~] $   export   GLEXEC_CLIENT_CERT = /tmp/x509up_u UID  [user@client ~] $   export   X509_USER_PROXY = /tmp/x509up_u UID  [user@client ~] $  /usr/sbin/glexec /usr/bin/id [user@client ~] $   uid = 13160 ( fnalgrid )   gid = 9767 ( fnalgrid )   groups = 65000 ( glexec00 )   If  glexec  is successful, it will print out the uid and gid that your proxy would normally be mapped to by your GUMS server, plus a supplementary tracking group. (The actual names and numbers will be different from what you see above.)  If you have problems, please read about  troubleshooting glexec .", 
            "title": "Testing the Installation of glexec"
        }, 
        {
            "location": "/worker-node/install-glexec/#glexec-log-files", 
            "text": "Glexec sends all its log information by default to syslog. By default they go to  /var/log/messages , but this may differ if you have customized your syslog setup. Here are some sample messages:  Apr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-PluginInit(): plugin glexectracking not found (arguments: )\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-lcmaps_startPluginManager(): error initializing plugin: glexectracking\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps_init() error: could not start plugin manager\nApr 25 16:36:16 fermicloud053 glexec[2867]: Initialisation of LCMAPS failed.  These particular messages are pretty common, caused by forgetting to uncomment the beginning of the  glexectracking  rule in  /etc/lcmaps.db .  It is possible to redirect glexec log messages to a different file with standard syslog. To do that, choose one of the  LOG_LOCAL[0-7]  log facilities that are unused, for example  LOG_LOCAL1 . Then set the following in  /etc/glexec.conf :  syslog_facility = LOG_LOCAL1  and add a corresponding parameter to the  lcmaps_glexec_tracking.mod  entry in  /etc/lcmaps.db :     -log-facility LOG_LOCAL1   Then in  /etc/rsyslog.conf , add a line like this:  local1.* /var/log/glexec.log  and also exclude those messages from  /var/log/messages  by adding  local1.none  after other wildcards on the existing  /var/log/messages  line, for example:  *.info;local1.none;mail.none;authpriv.none;cron.none /var/log/messages  Be sure to notify the system logger to re-read the configuration file with  service rsyslog restart .  rsyslog , by default, limits the rate at which messages may be logged, and if maximum debugging is enabled in glexec this limit is reached. To avoid that, you can add the following to  /etc/rsyslog.conf  after the line \"$ModLoad imuxsock.so\":  $SystemLogRateLimitInterval 0\n$SystemLogRateLimitBurst 0  and do  service rsyslog restart .  Alternatively,  syslog-ng  (available in the EPEL repository) can do the same job by matching all the messages that have the string \"glexec\" in the name.\nThese rules in  /etc/syslog-ng/syslog-ng.conf  will separate the glexec messages into  /var/log/glexec.log :  destination d_glexec { file( /var/log/glexec.log ); };\nfilter f_glexec { program( ^glexec ); };\nfilter f_notglexec { not program( ^glexec ); };\nlog { source(s_sys); filter(f_glexec); destination(d_glexec); };  Then later, in the log rule writing sending to  d_mesg , add a  filter(f_notglexec);  before the destination rule to keep glexec messages out of  /var/log/messages :  log { source(s_sys); filter(f_filter1); filter(f_notglexec); destination(d_mesg); };", 
            "title": "Glexec log files"
        }, 
        {
            "location": "/worker-node/install-glexec/#how-to-get-help", 
            "text": "To get assistance please use the  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/common/help/", 
            "text": "How to Get Help\n\n\nThis page is aimed at OSG site administrators looking for support. Help for OSG users is found at \nour support desk\n.\n\n\nGrid Operations Center\n\n\nThe Grid Operations Center (GOC) is available to coordinate users, site admins, and developers around an issue.  Additionally, the GOC can provide basic monitoring and troubleshooting.  There are several ways to receive support:\n\n\n\n\nYou can \nsubmit a trouble ticket\n or send an email to \ngoc@opensciencegrid.org\n (which also accept general inquiries not intended for tickets.)\n\n\nThe \ntrouble ticket system\n is searchable.  Historical tickets may contain the solution for similar problems others have encountered.\n\n\nThe \noperations blog\n contains information about recent software releases, and important outage and maintenance notifications of central OSG services.\n\n\nFor emergencies, the OSG Grid Operation Center provides extended support. Operators are on hand 24x7 at the GOC and can be reached via phone at +1 317-278-9699.   Non-emergency issues can be opened 24x7 but will be handled during normal business hours.\n\n\n\n\nSecurity incident\n\n\nSecurity incidents can be reported by following the instructions on the \nIncident Discovery and Reporting\n page.  Additional steps to aid in the incident handling process are also linked from that page.\n\n\nInformation Required to Help You\n\n\nIf you came to this page from an installation or other document in this website, then follow instructions in the \nTroubleshooting\n and \nDebugging\n sections of that document and include results in your support inquiry, no matter which channel you choose (email, trouble ticket, web chat, ...)\n\n\nFor problems with installation of some software run \nosg-system-profiler\n:\n\n\n[root@client ~] #\n osg-system-profiler\n\n\n\n\n\nAttach the generated \nosg-profile.txt\n to your support inquiry.\n\n\nCommunity-specific Resources\n\n\nSome OSG VOs have dedicated forums or mechanisms for community-specific support.  If your VO provides user support, that should be a user's first line of support because the VO is most familiar with your applications and requirements.\n\n\n\n\nThe list of support contacts for OSG VOs can be found in the \nSupport Center Tab on MyOSG\n.\n\n\nResources for \nCMS\n sites:\n\n\nhttp://www.uscms.org/uscms_at_work/physics/computing/grid/index.shtml\n\n\nCMS Hyper News: \nhttps://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3.html\n\n\nCMS Twiki: \nhttps://twiki.cern.ch/twiki/bin/viewauth/CMS/USTier3Computing", 
            "title": "Get Help"
        }, 
        {
            "location": "/common/help/#how-to-get-help", 
            "text": "This page is aimed at OSG site administrators looking for support. Help for OSG users is found at  our support desk .", 
            "title": "How to Get Help"
        }, 
        {
            "location": "/common/help/#grid-operations-center", 
            "text": "The Grid Operations Center (GOC) is available to coordinate users, site admins, and developers around an issue.  Additionally, the GOC can provide basic monitoring and troubleshooting.  There are several ways to receive support:   You can  submit a trouble ticket  or send an email to  goc@opensciencegrid.org  (which also accept general inquiries not intended for tickets.)  The  trouble ticket system  is searchable.  Historical tickets may contain the solution for similar problems others have encountered.  The  operations blog  contains information about recent software releases, and important outage and maintenance notifications of central OSG services.  For emergencies, the OSG Grid Operation Center provides extended support. Operators are on hand 24x7 at the GOC and can be reached via phone at +1 317-278-9699.   Non-emergency issues can be opened 24x7 but will be handled during normal business hours.", 
            "title": "Grid Operations Center"
        }, 
        {
            "location": "/common/help/#security-incident", 
            "text": "Security incidents can be reported by following the instructions on the  Incident Discovery and Reporting  page.  Additional steps to aid in the incident handling process are also linked from that page.", 
            "title": "Security incident"
        }, 
        {
            "location": "/common/help/#information-required-to-help-you", 
            "text": "If you came to this page from an installation or other document in this website, then follow instructions in the  Troubleshooting  and  Debugging  sections of that document and include results in your support inquiry, no matter which channel you choose (email, trouble ticket, web chat, ...)  For problems with installation of some software run  osg-system-profiler :  [root@client ~] #  osg-system-profiler  Attach the generated  osg-profile.txt  to your support inquiry.", 
            "title": "Information Required to Help You"
        }, 
        {
            "location": "/common/help/#community-specific-resources", 
            "text": "Some OSG VOs have dedicated forums or mechanisms for community-specific support.  If your VO provides user support, that should be a user's first line of support because the VO is most familiar with your applications and requirements.   The list of support contacts for OSG VOs can be found in the  Support Center Tab on MyOSG .  Resources for  CMS  sites:  http://www.uscms.org/uscms_at_work/physics/computing/grid/index.shtml  CMS Hyper News:  https://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3.html  CMS Twiki:  https://twiki.cern.ch/twiki/bin/viewauth/CMS/USTier3Computing", 
            "title": "Community-specific Resources"
        }
    ]
}