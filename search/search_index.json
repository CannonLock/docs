{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OSG Site Documentation \u00b6 User documentation If you are a researcher interested in accessing OSG resources, please consult our user documentation instead. The Open Science Grid (OSG) provides common service and support for resource providers and scientific institutions (i.e., \"sites\") using a distributed fabric of high throughput computational services. The OSG does not own resources but provides software and services to users and resource providers alike to enable the opportunistic usage and sharing of resources. This documentation aims to provide HTC/HPC system administrators with the necessary information to contribute resources to the OSG. Contributing to the OSG \u00b6 We offer two models for sites to contribute resources to OSG users: one where the OSG hosts and maintains resource provisioning services for OSG users; and the traditional model where the site hosts and maintains these same services. In both of these cases, the following will be needed: An existing compute cluster running on a supported operating system with a supported batch system: Grid Engine , HTCondor , LSF , PBS Pro / Torque , or Slurm . Outbound network connectivity from your cluster's worker nodes Temporary scratch space on each worker node Don't meet the requirements? If your site does not meet the above conditions, please contact us to discuss your options for contributing to the OSG. OSG-hosted services \u00b6 To contribute computational resources with OSG-hosted services, your site will also need the following: Allow SSH access to your local cluster's login host from a known IP address Shared home directories on each cluster node Next steps If you are interested in OSG-hosted services, please contact us for a consultation, even if your site does not meet the conditions as outlined above! Self-hosted services \u00b6 If you are interested in contributing resources by hosting your own OSG services, please continue with the site planning page.","title":"Home"},{"location":"#osg-site-documentation","text":"User documentation If you are a researcher interested in accessing OSG resources, please consult our user documentation instead. The Open Science Grid (OSG) provides common service and support for resource providers and scientific institutions (i.e., \"sites\") using a distributed fabric of high throughput computational services. The OSG does not own resources but provides software and services to users and resource providers alike to enable the opportunistic usage and sharing of resources. This documentation aims to provide HTC/HPC system administrators with the necessary information to contribute resources to the OSG.","title":"OSG Site Documentation"},{"location":"#contributing-to-the-osg","text":"We offer two models for sites to contribute resources to OSG users: one where the OSG hosts and maintains resource provisioning services for OSG users; and the traditional model where the site hosts and maintains these same services. In both of these cases, the following will be needed: An existing compute cluster running on a supported operating system with a supported batch system: Grid Engine , HTCondor , LSF , PBS Pro / Torque , or Slurm . Outbound network connectivity from your cluster's worker nodes Temporary scratch space on each worker node Don't meet the requirements? If your site does not meet the above conditions, please contact us to discuss your options for contributing to the OSG.","title":"Contributing to the OSG"},{"location":"#osg-hosted-services","text":"To contribute computational resources with OSG-hosted services, your site will also need the following: Allow SSH access to your local cluster's login host from a known IP address Shared home directories on each cluster node Next steps If you are interested in OSG-hosted services, please contact us for a consultation, even if your site does not meet the conditions as outlined above!","title":"OSG-hosted services"},{"location":"#self-hosted-services","text":"If you are interested in contributing resources by hosting your own OSG services, please continue with the site planning page.","title":"Self-hosted services"},{"location":"detailed-overview/","text":"Detailed Overview \u00b6 This document outlines the overall installation process for an OSG site and provides many links into detailed installation, configuration, troubleshooting, and similar pages. If you do not see software-related technical documentation listed here, try the search bar at the top or contacting us at help@opensciencegrid.org . Plan the Site \u00b6 If you have not done so already, plan the overall architecture of your OSG site . It is recommended that your plan be sufficiently detailed to include the OSG hosts that are needed and the main software components for each host. Be sure to consider the operating systems that OSG supports . For example, a basic site might include: Purpose Host Major Software Compute Entrypoint (CE) osg-ce.example.edu OSG CE, HTCondor Central Manager, etc. ( osg-ce-condor ) Worker Nodes wNNN.cluster.example.edu OSG worker node client ( osg-wn-client ) Prepare the Batch System \u00b6 The assumption is that you have an existing batch system at your site. Currently, we support HTCondor , LSF , PBS and Torque , SGE , and Slurm batch systems. For smaller sites (less than 50 worker nodes), the most common way to add a site to OSG is to install the OSG Compute Element (CE) on the central host of your batch system. At such a site - especially if you have minimal time to maintain a CE - you may want to contact help@opensciencegrid.org to ask about using an OSG-hosted CE instead of running your own. Before proceeding with an install, be sure that you can submit and successfully run a job from your OSG CE host into your batch system. Add OSG Software \u00b6 If necessary, provision all OSG hosts that are in your site plan that do not exist yet. The general steps to installing an OSG site are: Install OSG Yum Repos and the Compute Entrypoint software on your CE host Install the Worker Node client on your worker nodes. Install optional software to increase the capabilities of your site. Note For sites with more than a handful of worker nodes, it is recommended to use some sort of configuration management tool to install, configure, and maintain your site. While beyond the scope of OSG\u2019s documentation to explain how to select and use such a system, some popular configuration management tools are Puppet , Chef , Ansible , and CFEngine . General Installation Instructions \u00b6 Security information for OSG signed RPMs Using Yum and RPM Install the OSG Yum repositories OSG Software release series - look here to upgrade to OSG 3.5 Installing and Managing Certificates for Site Security \u00b6 Installing the grid certificate authorities (CAs) How do I get X.509 host certificates? Automatically updating the grid certificate authorities (CAs) OSG PKI command line client reference Installing and Configuring the Compute Entrypoint \u00b6 Install the compute entrypoint (HTCondor-CE and other software): Overview and architecture Request a Hosted CE Install HTCondor-CE Configure the HTCondor-CE job router , including common recipes Troubleshooting HTCondor-CE installations Submitting jobs to HTCondor-CE osg-configure Reference Adding OSG Software to Worker Nodes \u00b6 Worker Node (WN) Client Overview Install the WN client software on every worker node \u2013 pick a method: Using RPMs \u2013 useful when managing your worker nodes with a tool (e.g., Puppet, Chef) Using a tarball \u2013 useful for installation onto a shared filesystem (does not require root access) Using OASIS \u2013 useful when CVMFS is already mounted on your worker nodes (optional) Install the CernVM-FS client to make it easy for user jobs to use needed software from OSG's OASIS repositories (optional) Install singularity on the OSG worker node , to allow pilot jobs to isolate user jobs. Installing and Configuring Other Services \u00b6 All of these node types and their services are optional, although OSG requires an HTTP caching service if you have installed CVMFS on your worker nodes. Install Frontier Squid , an HTTP caching proxy service. Storage element: Existing POSIX-based systems (such as NFS, Lustre, or GPFS): Install standalone OSG GridFTP : GridFTP server (optional) Install load-balanced OSG GridFTP : when a single GridFTP server isn't enough Hadoop Distributed File System (HDFS): Hadoop Overview : HDFS information, planning, and guides XRootD: XRootd Overview : XRootD information, planning, and guides Install XRootD Server : XRootD redirector installation RSV monitoring to monitor and report to OSG on the health of your site Install RSV Install the GlideinWMS VO Frontend if your want your users' jobs to run on the OSG Install the RSV GlideinWMS Tester if you want to test your front-end's ability to submit jobs to sites in the OSG Get Help \u00b6 If you need help with your site, or need to report a security incident, follow the contact instructions .","title":"Detailed overview"},{"location":"detailed-overview/#detailed-overview","text":"This document outlines the overall installation process for an OSG site and provides many links into detailed installation, configuration, troubleshooting, and similar pages. If you do not see software-related technical documentation listed here, try the search bar at the top or contacting us at help@opensciencegrid.org .","title":"Detailed Overview"},{"location":"detailed-overview/#plan-the-site","text":"If you have not done so already, plan the overall architecture of your OSG site . It is recommended that your plan be sufficiently detailed to include the OSG hosts that are needed and the main software components for each host. Be sure to consider the operating systems that OSG supports . For example, a basic site might include: Purpose Host Major Software Compute Entrypoint (CE) osg-ce.example.edu OSG CE, HTCondor Central Manager, etc. ( osg-ce-condor ) Worker Nodes wNNN.cluster.example.edu OSG worker node client ( osg-wn-client )","title":"Plan the Site"},{"location":"detailed-overview/#prepare-the-batch-system","text":"The assumption is that you have an existing batch system at your site. Currently, we support HTCondor , LSF , PBS and Torque , SGE , and Slurm batch systems. For smaller sites (less than 50 worker nodes), the most common way to add a site to OSG is to install the OSG Compute Element (CE) on the central host of your batch system. At such a site - especially if you have minimal time to maintain a CE - you may want to contact help@opensciencegrid.org to ask about using an OSG-hosted CE instead of running your own. Before proceeding with an install, be sure that you can submit and successfully run a job from your OSG CE host into your batch system.","title":"Prepare the Batch System"},{"location":"detailed-overview/#add-osg-software","text":"If necessary, provision all OSG hosts that are in your site plan that do not exist yet. The general steps to installing an OSG site are: Install OSG Yum Repos and the Compute Entrypoint software on your CE host Install the Worker Node client on your worker nodes. Install optional software to increase the capabilities of your site. Note For sites with more than a handful of worker nodes, it is recommended to use some sort of configuration management tool to install, configure, and maintain your site. While beyond the scope of OSG\u2019s documentation to explain how to select and use such a system, some popular configuration management tools are Puppet , Chef , Ansible , and CFEngine .","title":"Add OSG Software"},{"location":"detailed-overview/#general-installation-instructions","text":"Security information for OSG signed RPMs Using Yum and RPM Install the OSG Yum repositories OSG Software release series - look here to upgrade to OSG 3.5","title":"General Installation Instructions"},{"location":"detailed-overview/#installing-and-managing-certificates-for-site-security","text":"Installing the grid certificate authorities (CAs) How do I get X.509 host certificates? Automatically updating the grid certificate authorities (CAs) OSG PKI command line client reference","title":"Installing and Managing Certificates for Site Security"},{"location":"detailed-overview/#installing-and-configuring-the-compute-entrypoint","text":"Install the compute entrypoint (HTCondor-CE and other software): Overview and architecture Request a Hosted CE Install HTCondor-CE Configure the HTCondor-CE job router , including common recipes Troubleshooting HTCondor-CE installations Submitting jobs to HTCondor-CE osg-configure Reference","title":"Installing and Configuring the Compute Entrypoint"},{"location":"detailed-overview/#adding-osg-software-to-worker-nodes","text":"Worker Node (WN) Client Overview Install the WN client software on every worker node \u2013 pick a method: Using RPMs \u2013 useful when managing your worker nodes with a tool (e.g., Puppet, Chef) Using a tarball \u2013 useful for installation onto a shared filesystem (does not require root access) Using OASIS \u2013 useful when CVMFS is already mounted on your worker nodes (optional) Install the CernVM-FS client to make it easy for user jobs to use needed software from OSG's OASIS repositories (optional) Install singularity on the OSG worker node , to allow pilot jobs to isolate user jobs.","title":"Adding OSG Software to Worker Nodes"},{"location":"detailed-overview/#installing-and-configuring-other-services","text":"All of these node types and their services are optional, although OSG requires an HTTP caching service if you have installed CVMFS on your worker nodes. Install Frontier Squid , an HTTP caching proxy service. Storage element: Existing POSIX-based systems (such as NFS, Lustre, or GPFS): Install standalone OSG GridFTP : GridFTP server (optional) Install load-balanced OSG GridFTP : when a single GridFTP server isn't enough Hadoop Distributed File System (HDFS): Hadoop Overview : HDFS information, planning, and guides XRootD: XRootd Overview : XRootD information, planning, and guides Install XRootD Server : XRootD redirector installation RSV monitoring to monitor and report to OSG on the health of your site Install RSV Install the GlideinWMS VO Frontend if your want your users' jobs to run on the OSG Install the RSV GlideinWMS Tester if you want to test your front-end's ability to submit jobs to sites in the OSG","title":"Installing and Configuring Other Services"},{"location":"detailed-overview/#get-help","text":"If you need help with your site, or need to report a security incident, follow the contact instructions .","title":"Get Help"},{"location":"site-maintenance/","text":"Site Maintenance \u00b6 This document outlines how to maintain your OSG site, including steps to take if you suspect that OSG jobs are causing issues. Handle Misbehaving Jobs \u00b6 In rare instances, you may experience issues at your site caused by misbehaving jobs (e.g., over-utilization of memory) from an OSG community or Virtual Organization (VO). If this occurs, you should immediately stop accepting job submissions from the OSG and remove the offending jobs: Configure your batch system to stop accepting jobs from the VO: For HTCondor batch systems, set the following in /etc/condor/config.d/ on your HTCondor-CE or Access Point accepting jobs from an OSG Hosted CE: SUBMIT_REQUIREMENT_Ban_OSG = (Owner != \"<OFFENDING VO USER>\") SUBMIT_REQUIREMENT_Ban_OSG_REASON = \"OSG pilot job submission temporarily disabled\" SUBMIT_REQUIREMENT_NAMES = $(SUBMIT_REQUIREMENT_NAMES) Ban_OSG Replacing <OFFENDING VO USER> with the name of the local Unix account corresponding to the problematic VO. For Slurm batch systems, disable the relevant Slurm partition : [root@host] # scontrol update PartitionName = <OSG PARTITION> State = DOWN Replacing <OSG PARTITION> with the name of the partition where you are sending OSG jobs. Remove the VO's jobs: For HTCondor batch systems, run the following command on your HTCondor-CE or Access Point accepting jobs from an OSG Hosted CE: [root@access-point] # condor_rm <OFFENDING VO USER> Replacing <OFFENDING VO USER> with the name of the local Unix account corresponding to the problematic VO. For Slurm batch systems, run the following command: [root@host] # scancel -u <OFFENDING VO USER> Replacing <OFFENDING VO USER> with the name of the local Unix account corresponding to the problematic VO. Let us know so that we can track down the offending software or user: the same issue that you're experiencing may also be affecting other sites! Keep OSG Software Updated \u00b6 It is important to keep your software and data (e.g., CAs and VO client) up-to-date with the latest OSG release. See the release notes for your installed release series: OSG 3.5 release notes OSG 3.6 release notes To stay abreast of software releases, we recommend subscribing to the osg-sites@opensciencegrid.org mailing list. Notify OSG of Major Changes \u00b6 To avoid potential issues with OSG job submissions, please notify us of major changes to your site, including: Major OS version changes on the worker nodes (e.g., upgraded from EL 7 to EL 8) Adding or removing container support through Singularity Policy changes regarding OSG resource requests (e.g., number of cores or GPUs, memory usage, or maximum walltime) Scheduled or unscheduled downtimes Site topology changes such as additions, modifications, or retirements of OSG services Changes to site contacts, such as administrative or security staff Help \u00b6 If you need help with your site, or need to report a security incident, follow the contact instructions .","title":"Site Maintenance"},{"location":"site-maintenance/#site-maintenance","text":"This document outlines how to maintain your OSG site, including steps to take if you suspect that OSG jobs are causing issues.","title":"Site Maintenance"},{"location":"site-maintenance/#handle-misbehaving-jobs","text":"In rare instances, you may experience issues at your site caused by misbehaving jobs (e.g., over-utilization of memory) from an OSG community or Virtual Organization (VO). If this occurs, you should immediately stop accepting job submissions from the OSG and remove the offending jobs: Configure your batch system to stop accepting jobs from the VO: For HTCondor batch systems, set the following in /etc/condor/config.d/ on your HTCondor-CE or Access Point accepting jobs from an OSG Hosted CE: SUBMIT_REQUIREMENT_Ban_OSG = (Owner != \"<OFFENDING VO USER>\") SUBMIT_REQUIREMENT_Ban_OSG_REASON = \"OSG pilot job submission temporarily disabled\" SUBMIT_REQUIREMENT_NAMES = $(SUBMIT_REQUIREMENT_NAMES) Ban_OSG Replacing <OFFENDING VO USER> with the name of the local Unix account corresponding to the problematic VO. For Slurm batch systems, disable the relevant Slurm partition : [root@host] # scontrol update PartitionName = <OSG PARTITION> State = DOWN Replacing <OSG PARTITION> with the name of the partition where you are sending OSG jobs. Remove the VO's jobs: For HTCondor batch systems, run the following command on your HTCondor-CE or Access Point accepting jobs from an OSG Hosted CE: [root@access-point] # condor_rm <OFFENDING VO USER> Replacing <OFFENDING VO USER> with the name of the local Unix account corresponding to the problematic VO. For Slurm batch systems, run the following command: [root@host] # scancel -u <OFFENDING VO USER> Replacing <OFFENDING VO USER> with the name of the local Unix account corresponding to the problematic VO. Let us know so that we can track down the offending software or user: the same issue that you're experiencing may also be affecting other sites!","title":"Handle Misbehaving Jobs"},{"location":"site-maintenance/#keep-osg-software-updated","text":"It is important to keep your software and data (e.g., CAs and VO client) up-to-date with the latest OSG release. See the release notes for your installed release series: OSG 3.5 release notes OSG 3.6 release notes To stay abreast of software releases, we recommend subscribing to the osg-sites@opensciencegrid.org mailing list.","title":"Keep OSG Software Updated"},{"location":"site-maintenance/#notify-osg-of-major-changes","text":"To avoid potential issues with OSG job submissions, please notify us of major changes to your site, including: Major OS version changes on the worker nodes (e.g., upgraded from EL 7 to EL 8) Adding or removing container support through Singularity Policy changes regarding OSG resource requests (e.g., number of cores or GPUs, memory usage, or maximum walltime) Scheduled or unscheduled downtimes Site topology changes such as additions, modifications, or retirements of OSG services Changes to site contacts, such as administrative or security staff","title":"Notify OSG of Major Changes"},{"location":"site-maintenance/#help","text":"If you need help with your site, or need to report a security incident, follow the contact instructions .","title":"Help"},{"location":"site-planning/","text":"Site Planning \u00b6 The OSG vision is to integrate computing across different resource types and business models to allow campus IT to offer a maximally flexible high throughput computing (HTC) environment for their researchers. This document is for System Administrators and aims to provide an overview of the different options to consider when planning to share resources via the OSG. After reading, you should be able to understand what software or services you want to provide to support your researchers Note This document covers the most common options. OSG is a diverse infrastructure: depending on what groups you want to support, you may need to install additional services. Coordinate with your local researchers. OSG Site Services \u00b6 The OSG Software stack tries to provide a uniform computing and storage fabric across many independently-managed computing and storage resources. These individual services will be accessed by virtual organizations (VOs), which will delegate the resources to scientists, researchers, and students. Sharing is a fundamental principle for the OSG: your site is encouraged to support as many OSG-registered VOs as local conditions allow. Autonomy is another principle: you are not required to support any VOs you do not want. As the administrator, your task is to make your existing computing and storage resources available to and reliable for your supported VOs. We break this down into three tasks: Getting \"pilot jobs\" submitted to your site batch system. Establishing an OSG runtime environment for running jobs. Delivering data to payload applications to be processed. There are multiple approaches for each item, depending on the VOs you support, and time you have to invest in the OSG. Note An essential concept in the OSG is the \"pilot job\". The pilot, which arrives at your batch system, is sent by the VO to get a resource allocation. However, it does not contain any research payload. Once started, it will connect back to a resource pool and pull down individuals' research \"payload jobs\". Hence, we do not think about submitting \"jobs\" to sites but rather \"resource requests\". Pilot Jobs \u00b6 Traditionally, an OSG Compute Entrypoint (CE) provides remote access for VOs to submit pilot jobs to your local batch system . There are two options for accepting pilot jobs at your site: Hosted CE : OSG will run and operate the CE services at no cost; the site only needs to provide a SSH pubkey-based authentication access to the central OSG host. OSG will interface with the VO and submit pilots directly to your batch system via SSH. By far, this is the simplest option : however, it is less-scalable and the site delegates many of the scheduling decisions to the OSG. Contact help@opensciencegrid.org for more information on the hosted CE. OSG CE : The traditional option where the site installs and operates a HTCondor-based CE on a dedicated host. This provides the best scalability and flexibility, but may require an ongoing time investment from the site. The OSG CE install and operation is covered in this documentation page . There are additional ways that pilots can be started at a site (either by the site administrator or an end-user); see resource sharing for more details. Runtime environment \u00b6 The OSG requires a very minimal runtime environment that can be deployed via tarball , RPM , or through a global filesystem on your cluster's worker nodes. We believe that all research applications should be portable and self-contained, with no OS dependencies. This provides access to the most resources and minimizes the presence at sites. However, this ideal is often difficult to achieve in practice. For sites that want to support a uniform runtime environment, we provide a global filesystem called CVMFS that VOs can use to distribute their own software dependencies. Finally, many researchers use applications that require a specific OS environment - not just individual dependencies - that is distributed as a container. OSG supports the use of the Singularity container runtime with Docker-based image distribution. Data Services \u00b6 Whether accessed through CVMFS or command-line software like curl , the majority of software is moved via HTTP in cache-friendly patterns. All sites are highly encouraged to use an HTTP proxy to reduce the load on the WAN from the cluster. Depending on the VOs you want to support, additional data services may be necessary: Some VOs elect to stream their larger input data from offsite using OSG's \"StashCache\" service. This requires no services to be run by the site The largest sites will additionally run large-scale data services such as a \"storage element\". This is often required for sites that want to support more complex organizations such as ATLAS or CMS. Site Policies \u00b6 Sites are encouraged to clearly specify and communicate their local policies regarding resource access. One common mechanism to do this is post them on a web page and make this page part of your site registration . Written policies help external entities understand what your site wants to accomplish with the OSG -- and are often internally clarifying. In line of our principle of sharing , we encourage you to allow virtual organizations registered with the OSG \"opportunistic use\" of your resources. You may need to preempt those jobs when higher priority jobs come around. The end-users using the OSG generally prefer having access to your site subject to preemption over having no access at all. Getting Help \u00b6 If you need help with planning your site, follow the contact instructions .","title":"Site Planning"},{"location":"site-planning/#site-planning","text":"The OSG vision is to integrate computing across different resource types and business models to allow campus IT to offer a maximally flexible high throughput computing (HTC) environment for their researchers. This document is for System Administrators and aims to provide an overview of the different options to consider when planning to share resources via the OSG. After reading, you should be able to understand what software or services you want to provide to support your researchers Note This document covers the most common options. OSG is a diverse infrastructure: depending on what groups you want to support, you may need to install additional services. Coordinate with your local researchers.","title":"Site Planning"},{"location":"site-planning/#osg-site-services","text":"The OSG Software stack tries to provide a uniform computing and storage fabric across many independently-managed computing and storage resources. These individual services will be accessed by virtual organizations (VOs), which will delegate the resources to scientists, researchers, and students. Sharing is a fundamental principle for the OSG: your site is encouraged to support as many OSG-registered VOs as local conditions allow. Autonomy is another principle: you are not required to support any VOs you do not want. As the administrator, your task is to make your existing computing and storage resources available to and reliable for your supported VOs. We break this down into three tasks: Getting \"pilot jobs\" submitted to your site batch system. Establishing an OSG runtime environment for running jobs. Delivering data to payload applications to be processed. There are multiple approaches for each item, depending on the VOs you support, and time you have to invest in the OSG. Note An essential concept in the OSG is the \"pilot job\". The pilot, which arrives at your batch system, is sent by the VO to get a resource allocation. However, it does not contain any research payload. Once started, it will connect back to a resource pool and pull down individuals' research \"payload jobs\". Hence, we do not think about submitting \"jobs\" to sites but rather \"resource requests\".","title":"OSG Site Services"},{"location":"site-planning/#pilot-jobs","text":"Traditionally, an OSG Compute Entrypoint (CE) provides remote access for VOs to submit pilot jobs to your local batch system . There are two options for accepting pilot jobs at your site: Hosted CE : OSG will run and operate the CE services at no cost; the site only needs to provide a SSH pubkey-based authentication access to the central OSG host. OSG will interface with the VO and submit pilots directly to your batch system via SSH. By far, this is the simplest option : however, it is less-scalable and the site delegates many of the scheduling decisions to the OSG. Contact help@opensciencegrid.org for more information on the hosted CE. OSG CE : The traditional option where the site installs and operates a HTCondor-based CE on a dedicated host. This provides the best scalability and flexibility, but may require an ongoing time investment from the site. The OSG CE install and operation is covered in this documentation page . There are additional ways that pilots can be started at a site (either by the site administrator or an end-user); see resource sharing for more details.","title":"Pilot Jobs"},{"location":"site-planning/#runtime-environment","text":"The OSG requires a very minimal runtime environment that can be deployed via tarball , RPM , or through a global filesystem on your cluster's worker nodes. We believe that all research applications should be portable and self-contained, with no OS dependencies. This provides access to the most resources and minimizes the presence at sites. However, this ideal is often difficult to achieve in practice. For sites that want to support a uniform runtime environment, we provide a global filesystem called CVMFS that VOs can use to distribute their own software dependencies. Finally, many researchers use applications that require a specific OS environment - not just individual dependencies - that is distributed as a container. OSG supports the use of the Singularity container runtime with Docker-based image distribution.","title":"Runtime environment"},{"location":"site-planning/#data-services","text":"Whether accessed through CVMFS or command-line software like curl , the majority of software is moved via HTTP in cache-friendly patterns. All sites are highly encouraged to use an HTTP proxy to reduce the load on the WAN from the cluster. Depending on the VOs you want to support, additional data services may be necessary: Some VOs elect to stream their larger input data from offsite using OSG's \"StashCache\" service. This requires no services to be run by the site The largest sites will additionally run large-scale data services such as a \"storage element\". This is often required for sites that want to support more complex organizations such as ATLAS or CMS.","title":"Data Services"},{"location":"site-planning/#site-policies","text":"Sites are encouraged to clearly specify and communicate their local policies regarding resource access. One common mechanism to do this is post them on a web page and make this page part of your site registration . Written policies help external entities understand what your site wants to accomplish with the OSG -- and are often internally clarifying. In line of our principle of sharing , we encourage you to allow virtual organizations registered with the OSG \"opportunistic use\" of your resources. You may need to preempt those jobs when higher priority jobs come around. The end-users using the OSG generally prefer having access to your site subject to preemption over having no access at all.","title":"Site Policies"},{"location":"site-planning/#getting-help","text":"If you need help with planning your site, follow the contact instructions .","title":"Getting Help"},{"location":"site-verification/","text":"Site Verification \u00b6 After installing and registering services from the site planning document , you will need to perform some verification steps before your site can scale up to full production . Verify OSG Software \u00b6 To verify your site's installation of OSG Software, you will need to: Submit local test jobs Contact the OSG for end-to-end tests of pilot job submission Check that OSG usage is reported to the GRACC Local verification \u00b6 It is useful to submit jobs from within your site to verify CE's ability to submit jobs to your local batch system. Consult the document for submitting jobs into an HTCondor-CE for detailed instructions on how to test job submission. Verify end-to-end pilot job submission \u00b6 Once you have validated job submission from within your site, request test pilot jobs from OSG Factory Operations and provide the following information: The fully qualified domain name of the CE Registered OSG resource name Supported OS version of your worker nodes (e.g., EL7, EL8, or a combination) Support for multicore jobs Support for GPUs Maximum job walltime Maximum job memory usage Once the Factory Operations team has enough information, they will start submitting pilots to your CE. Initially, this will be a handful of pilots at a time but once the factory verifies that pilot jobs are running successfully, that number will be ramped up. Verify reporting and monitoring \u00b6 To verify that your site is correctly reporting to the OSG, visit OSG's Accounting Portal and select your registered OSG site name from the Site dropdown. If you don't see your site in the dropdown, please contact us for assistance . Scale Up to Full Production \u00b6 After verifying end-to-end pilot job submission and usage reporting, your site is ready for production! In the same OSG Factory Operations ticket that you opened above , let OSG staff know when you are ready to accept production pilots. After requesting production pilots, review the documentation for how to maintain an OSG site . Getting Help \u00b6 If you need help with your site, or need to report a security incident, follow the contact instructions .","title":"Site Verification"},{"location":"site-verification/#site-verification","text":"After installing and registering services from the site planning document , you will need to perform some verification steps before your site can scale up to full production .","title":"Site Verification"},{"location":"site-verification/#verify-osg-software","text":"To verify your site's installation of OSG Software, you will need to: Submit local test jobs Contact the OSG for end-to-end tests of pilot job submission Check that OSG usage is reported to the GRACC","title":"Verify OSG Software"},{"location":"site-verification/#local-verification","text":"It is useful to submit jobs from within your site to verify CE's ability to submit jobs to your local batch system. Consult the document for submitting jobs into an HTCondor-CE for detailed instructions on how to test job submission.","title":"Local verification"},{"location":"site-verification/#verify-end-to-end-pilot-job-submission","text":"Once you have validated job submission from within your site, request test pilot jobs from OSG Factory Operations and provide the following information: The fully qualified domain name of the CE Registered OSG resource name Supported OS version of your worker nodes (e.g., EL7, EL8, or a combination) Support for multicore jobs Support for GPUs Maximum job walltime Maximum job memory usage Once the Factory Operations team has enough information, they will start submitting pilots to your CE. Initially, this will be a handful of pilots at a time but once the factory verifies that pilot jobs are running successfully, that number will be ramped up.","title":"Verify end-to-end pilot job submission"},{"location":"site-verification/#verify-reporting-and-monitoring","text":"To verify that your site is correctly reporting to the OSG, visit OSG's Accounting Portal and select your registered OSG site name from the Site dropdown. If you don't see your site in the dropdown, please contact us for assistance .","title":"Verify reporting and monitoring"},{"location":"site-verification/#scale-up-to-full-production","text":"After verifying end-to-end pilot job submission and usage reporting, your site is ready for production! In the same OSG Factory Operations ticket that you opened above , let OSG staff know when you are ready to accept production pilots. After requesting production pilots, review the documentation for how to maintain an OSG site .","title":"Scale Up to Full Production"},{"location":"site-verification/#getting-help","text":"If you need help with your site, or need to report a security incident, follow the contact instructions .","title":"Getting Help"},{"location":"common/ca/","text":"Installing Certificate Authorities (CAs) \u00b6 The certificate authorities (CAs) provide the trust roots for the public key infrastructure OSG uses to maintain integrity of its sites and services. This document provides details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation lists (CRLs) on your OSG hosts. We provide three options for installing CA certificates that offer varying levels of control: Install an RPM for a specific set of CA certificates ( default ) Install osg-ca-scripts , a set of scripts that provide fine-grained CA management Install an RPM that doesn't install any CAs. This is useful if you'd like to manage CAs yourself while satisfying RPM dependencies. Prior to following the instructions on this page, you must enable our yum repositories Installing CA Certificates \u00b6 Please choose one of the three options to install CA certificates. Option 1: Install an RPM for a specific set of CA certificates \u00b6 Note This option is the default if you install OSG software without pre-installing CAs. For example, yum install osg-ce will bring in osg-ca-certs by default. In the OSG repositories, you will find two different sets of predefined CA certificates: ( default ) The OSG CA certificates. This is similar to the IGTF set but may have a small number of additions or deletions The IGTF CA certificates See this page for details of the contents of the OSG CA package. If you chose... Then run the following command... OSG CA certificates yum install osg-ca-certs IGTF CA certificates yum install igtf-ca-certs To automatically keep your RPM installation of CAs up to date, we recommend the OSG CA certificates updater service. Option 2: Install osg-ca-scripts \u00b6 The osg-ca-scripts package provides scripts to install and update predefined sets of CAs with the ability to add or remove specific CAs. The OSG CA certificates. This is similar to the IGTF set but may have a small number of additions or deletions The IGTF CA certificates See this page for details of the contents of the OSG CA package. Install the osg-ca-scripts package: root@host # yum install osg-ca-scripts Choose and install the CA certificate set: If you choose... Then run the following command... OSG CA certificates osg-ca-manage setupCA --location root --url osg IGTF CA certificates osg-ca-manage setupCA --location root --url igtf Enable the osg-update-certs-cron service to enable periodic CA updates. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> (Optional) To add a new CA: osg-ca-manage add [--dir <local_dir>] --hash <CA-HASH> (Optional) To remove a CA osg-ca-manage remove --hash <CA-HASH> A complete set of options available though osg-ca-manage command, can be found in the osg-ca-manage documentation Option 3: Site-managed CAs \u00b6 If you want to handle the list of CAs completely internally to your site, you can utilize the empty-ca-certs RPM to satisfy RPM dependencies while not actually installing any CAs. To install this RPM, run the following command: root@host # yum install empty-ca-certs \u2013-enablerepo = osg-empty Warning If you choose this option, you are responsible for installing and maintaining the CA certificates. They must be installed in /etc/grid-security/certificates , or a symlink must be made from that location to the directory that contains the CA certificates. Installing other CAs \u00b6 In addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is, osg-ca-certs and the like, but not osg-ca-scripts .) They are in addition to the above RPMs, so do not only install these extra CAs. Set of CAs RPM name Installation command (as root) cilogon-openid cilogon-openid-ca-cert yum install cilogon-openid-ca-cert Verifying CA Certificates \u00b6 After installing or updating the CA certificates, they can be verified with the following command: root@host # curl --cacert <CA FILE> \\ --capath <CA DIRECTORY> \\ -o /dev/null \\ https://gracc.opensciencegrid.org \\ && echo \"CA certificate installation verified\" Where <CA FILE> is the path to a valid X.509 CA certificate and <CA DIRECTORY> is the path to the directory containing the installed CA certificates. For example, the following command can be used to verify a default OSG CA certificate installation: root@host # curl --cacert /etc/grid-security/certificates/cilogon-osg.pem \\ --capath /etc/grid-security/certificates/ \\ -o /dev/null \\ https://gracc.opensciencegrid.org \\ && echo \"CA certificate installation verified\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 22005 0 22005 0 0 86633 0 --:--:-- --:--:-- --:--:-- 499k CA certificate installation verified If you do not see CA certificate installation verified this means that your CA certificate installation is broken. First, ensure that your CA installation is up-to-date and if you continue to see issues please contact us . Keeping CA Certificates Up-to-date \u00b6 It is important to keep CA certificates up-to-date for grid services and their clients to maintain integrity of the production grid. To verify that your CA certificates are on the latest version on a given host, determine the most recently released versions and the method by which your CA certificates have been installed: Retrieve the versions of the most recently released IGTF CA certificates and OSG CA certificates Determine which of the three CA certificate installation methods you are using: # rpm -q igtf-ca-certs osg-ca-certs osg-ca-scripts empty-ca-certs Based on which package is installed from the output in the previous step, choose one of the following options: If igtf-ca-certs or osg-ca-certs is installed , compare the installed version from step 2 to the corresponding version from step 1. If the version is older than the corresponding version from step 1, continue onto option 1 to upgrade your current installation and keep your installation up-to-date. If the versions match, your CA certificates are up-to-date! If osg-ca-scripts is installed , run the following command to update your CA certificates: # osg-ca-manage refreshCA And continue to the instructions in option 2 to enable automatic updates of your CA certificates. If empty-ca-scripts is installed , then you are responsible for maintaining your own CA certificates as outlined in option 3 . If none of the packages are installed , your host likely does not need CA certificates and you are done. Managing Certificate Revocation Lists \u00b6 In addition to CA certificates, you must have updated Certificate Revocation Lists (CRLs). CRLs contain certificate blacklists that OSG software uses to ensure that your hosts are only talking to valid clients or servers. To maintain up to date CAs, you will need to run the fetch-crl services. Note Normally fetch-crl is installed when you install the rest of the software and you do not need to explicitly install it. If you do wish to install it manually, run the following command: root@host # yum install fetch-crl If you do not wish to change the frequency of fetch-crl updates (default: every 6 hours) or use syslog for fetch-crl output, skip to the service management section Optional: configuring fetch-crl \u00b6 The following sub-sections contain optional configuration instructions. Note Note that the nosymlinks option in the configuration files refers to ignoring links within the certificates directory (e.g. two different names for the same file). It is perfectly fine if the path of the CA certificates directory itself ( infodir ) is a link to a directory. Changing the frequency of fetch-crl-cron \u00b6 To modify the times that fetch-crl-cron runs, edit /etc/cron.d/fetch-crl . Logging with syslog \u00b6 fetch-crl can produce quite a bit of output when run in verbose mode. To send fetch-crl output to syslog, use the following instructions: Change the configuration file to enable syslog: logmode = syslog syslogfacility = daemon Make sure the file /var/log/daemon exists, e.g. touching the file Change /etc/logrotate.d files to rotate it Managing fetch-crl services \u00b6 fetch-crl is installed as two different system services. The fetch-crl-boot service runs fetch-crl and is intended to only be enabled or disabled. The fetch-crl-cron service runs fetch-crl every 6 hours (with a random sleep time included). Both services are disabled by default. At the very minimum, the fetch-crl-cron service needs to be enabled and started, otherwise services will begin to fail as existing CRLs expire. Software Service name Notes Fetch CRL fetch-crl-cron Runs fetch-crl every 6 hours fetch-crl-boot Runs fetch-crl immediately Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> Getting Help \u00b6 To get assistance, please use the this page . References \u00b6 Some guides on X.509 certificates: Useful commands: http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html Install GSI authentication on a server: http://security.ncsa.illinois.edu/research/wssec/gsihttps/ Certificates how-to: http://www.nordugrid.org/documents/certificate_howto.html See this page for examples of verifying certificates. Related software: osg-ca-manage osg-ca-certs-updater Configuration files \u00b6 Package File Description Location Comment All CA Packages CA File Location /etc/grid-security/certificates All CA Packages Index files /etc/grid-security/certificates/INDEX.html or /etc/grid-security/certificates/INDEX.txt Latest version also available at http://repo.opensciencegrid.org/cadist/ All CA Packages Change Log /etc/grid-security/certificates/CHANGES Latest version also available at http://repo.opensciencegrid.org/cadist/CHANGES osg-ca-certs or igtf-ca-certs contain only CA files osg-ca-scripts Configuration File for osg-update-certs /etc/osg/osg-update-certs.conf This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters. fetch-crl-3.x Configuration file /etc/fetch-crl.conf The index and change log files contain a summary of all the CA distributed and their version. Logs files \u00b6 Package File Description Location osg-ca-scripts Log file of osg-update-certs /var/log/osg-update-certs.log osg-ca-scripts Stdout of osg-update-certs /var/log/osg-ca-certs-status.system.out osg-ca-scripts Stdout of osg-ca-manage /var/log/osg-ca-manage.system.out osg-ca-scripts Stdout of initial CA setup /var/log/osg-setup-ca-certificates.system.out","title":"Overview"},{"location":"common/ca/#installing-certificate-authorities-cas","text":"The certificate authorities (CAs) provide the trust roots for the public key infrastructure OSG uses to maintain integrity of its sites and services. This document provides details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation lists (CRLs) on your OSG hosts. We provide three options for installing CA certificates that offer varying levels of control: Install an RPM for a specific set of CA certificates ( default ) Install osg-ca-scripts , a set of scripts that provide fine-grained CA management Install an RPM that doesn't install any CAs. This is useful if you'd like to manage CAs yourself while satisfying RPM dependencies. Prior to following the instructions on this page, you must enable our yum repositories","title":"Installing Certificate Authorities (CAs)"},{"location":"common/ca/#installing-ca-certificates","text":"Please choose one of the three options to install CA certificates.","title":"Installing CA Certificates"},{"location":"common/ca/#option-1-install-an-rpm-for-a-specific-set-of-ca-certificates","text":"Note This option is the default if you install OSG software without pre-installing CAs. For example, yum install osg-ce will bring in osg-ca-certs by default. In the OSG repositories, you will find two different sets of predefined CA certificates: ( default ) The OSG CA certificates. This is similar to the IGTF set but may have a small number of additions or deletions The IGTF CA certificates See this page for details of the contents of the OSG CA package. If you chose... Then run the following command... OSG CA certificates yum install osg-ca-certs IGTF CA certificates yum install igtf-ca-certs To automatically keep your RPM installation of CAs up to date, we recommend the OSG CA certificates updater service.","title":"Option 1: Install an RPM for a specific set of CA certificates"},{"location":"common/ca/#option-2-install-osg-ca-scripts","text":"The osg-ca-scripts package provides scripts to install and update predefined sets of CAs with the ability to add or remove specific CAs. The OSG CA certificates. This is similar to the IGTF set but may have a small number of additions or deletions The IGTF CA certificates See this page for details of the contents of the OSG CA package. Install the osg-ca-scripts package: root@host # yum install osg-ca-scripts Choose and install the CA certificate set: If you choose... Then run the following command... OSG CA certificates osg-ca-manage setupCA --location root --url osg IGTF CA certificates osg-ca-manage setupCA --location root --url igtf Enable the osg-update-certs-cron service to enable periodic CA updates. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> (Optional) To add a new CA: osg-ca-manage add [--dir <local_dir>] --hash <CA-HASH> (Optional) To remove a CA osg-ca-manage remove --hash <CA-HASH> A complete set of options available though osg-ca-manage command, can be found in the osg-ca-manage documentation","title":"Option 2: Install osg-ca-scripts"},{"location":"common/ca/#option-3-site-managed-cas","text":"If you want to handle the list of CAs completely internally to your site, you can utilize the empty-ca-certs RPM to satisfy RPM dependencies while not actually installing any CAs. To install this RPM, run the following command: root@host # yum install empty-ca-certs \u2013-enablerepo = osg-empty Warning If you choose this option, you are responsible for installing and maintaining the CA certificates. They must be installed in /etc/grid-security/certificates , or a symlink must be made from that location to the directory that contains the CA certificates.","title":"Option 3: Site-managed CAs"},{"location":"common/ca/#installing-other-cas","text":"In addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is, osg-ca-certs and the like, but not osg-ca-scripts .) They are in addition to the above RPMs, so do not only install these extra CAs. Set of CAs RPM name Installation command (as root) cilogon-openid cilogon-openid-ca-cert yum install cilogon-openid-ca-cert","title":"Installing other CAs"},{"location":"common/ca/#verifying-ca-certificates","text":"After installing or updating the CA certificates, they can be verified with the following command: root@host # curl --cacert <CA FILE> \\ --capath <CA DIRECTORY> \\ -o /dev/null \\ https://gracc.opensciencegrid.org \\ && echo \"CA certificate installation verified\" Where <CA FILE> is the path to a valid X.509 CA certificate and <CA DIRECTORY> is the path to the directory containing the installed CA certificates. For example, the following command can be used to verify a default OSG CA certificate installation: root@host # curl --cacert /etc/grid-security/certificates/cilogon-osg.pem \\ --capath /etc/grid-security/certificates/ \\ -o /dev/null \\ https://gracc.opensciencegrid.org \\ && echo \"CA certificate installation verified\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 22005 0 22005 0 0 86633 0 --:--:-- --:--:-- --:--:-- 499k CA certificate installation verified If you do not see CA certificate installation verified this means that your CA certificate installation is broken. First, ensure that your CA installation is up-to-date and if you continue to see issues please contact us .","title":"Verifying CA Certificates"},{"location":"common/ca/#keeping-ca-certificates-up-to-date","text":"It is important to keep CA certificates up-to-date for grid services and their clients to maintain integrity of the production grid. To verify that your CA certificates are on the latest version on a given host, determine the most recently released versions and the method by which your CA certificates have been installed: Retrieve the versions of the most recently released IGTF CA certificates and OSG CA certificates Determine which of the three CA certificate installation methods you are using: # rpm -q igtf-ca-certs osg-ca-certs osg-ca-scripts empty-ca-certs Based on which package is installed from the output in the previous step, choose one of the following options: If igtf-ca-certs or osg-ca-certs is installed , compare the installed version from step 2 to the corresponding version from step 1. If the version is older than the corresponding version from step 1, continue onto option 1 to upgrade your current installation and keep your installation up-to-date. If the versions match, your CA certificates are up-to-date! If osg-ca-scripts is installed , run the following command to update your CA certificates: # osg-ca-manage refreshCA And continue to the instructions in option 2 to enable automatic updates of your CA certificates. If empty-ca-scripts is installed , then you are responsible for maintaining your own CA certificates as outlined in option 3 . If none of the packages are installed , your host likely does not need CA certificates and you are done.","title":"Keeping CA Certificates Up-to-date"},{"location":"common/ca/#managing-certificate-revocation-lists","text":"In addition to CA certificates, you must have updated Certificate Revocation Lists (CRLs). CRLs contain certificate blacklists that OSG software uses to ensure that your hosts are only talking to valid clients or servers. To maintain up to date CAs, you will need to run the fetch-crl services. Note Normally fetch-crl is installed when you install the rest of the software and you do not need to explicitly install it. If you do wish to install it manually, run the following command: root@host # yum install fetch-crl If you do not wish to change the frequency of fetch-crl updates (default: every 6 hours) or use syslog for fetch-crl output, skip to the service management section","title":"Managing Certificate Revocation Lists"},{"location":"common/ca/#optional-configuring-fetch-crl","text":"The following sub-sections contain optional configuration instructions. Note Note that the nosymlinks option in the configuration files refers to ignoring links within the certificates directory (e.g. two different names for the same file). It is perfectly fine if the path of the CA certificates directory itself ( infodir ) is a link to a directory.","title":"Optional: configuring fetch-crl"},{"location":"common/ca/#changing-the-frequency-of-fetch-crl-cron","text":"To modify the times that fetch-crl-cron runs, edit /etc/cron.d/fetch-crl .","title":"Changing the frequency of fetch-crl-cron"},{"location":"common/ca/#logging-with-syslog","text":"fetch-crl can produce quite a bit of output when run in verbose mode. To send fetch-crl output to syslog, use the following instructions: Change the configuration file to enable syslog: logmode = syslog syslogfacility = daemon Make sure the file /var/log/daemon exists, e.g. touching the file Change /etc/logrotate.d files to rotate it","title":"Logging with syslog"},{"location":"common/ca/#managing-fetch-crl-services","text":"fetch-crl is installed as two different system services. The fetch-crl-boot service runs fetch-crl and is intended to only be enabled or disabled. The fetch-crl-cron service runs fetch-crl every 6 hours (with a random sleep time included). Both services are disabled by default. At the very minimum, the fetch-crl-cron service needs to be enabled and started, otherwise services will begin to fail as existing CRLs expire. Software Service name Notes Fetch CRL fetch-crl-cron Runs fetch-crl every 6 hours fetch-crl-boot Runs fetch-crl immediately Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME>","title":"Managing fetch-crl services"},{"location":"common/ca/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"common/ca/#references","text":"Some guides on X.509 certificates: Useful commands: http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html Install GSI authentication on a server: http://security.ncsa.illinois.edu/research/wssec/gsihttps/ Certificates how-to: http://www.nordugrid.org/documents/certificate_howto.html See this page for examples of verifying certificates. Related software: osg-ca-manage osg-ca-certs-updater","title":"References"},{"location":"common/ca/#configuration-files","text":"Package File Description Location Comment All CA Packages CA File Location /etc/grid-security/certificates All CA Packages Index files /etc/grid-security/certificates/INDEX.html or /etc/grid-security/certificates/INDEX.txt Latest version also available at http://repo.opensciencegrid.org/cadist/ All CA Packages Change Log /etc/grid-security/certificates/CHANGES Latest version also available at http://repo.opensciencegrid.org/cadist/CHANGES osg-ca-certs or igtf-ca-certs contain only CA files osg-ca-scripts Configuration File for osg-update-certs /etc/osg/osg-update-certs.conf This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters. fetch-crl-3.x Configuration file /etc/fetch-crl.conf The index and change log files contain a summary of all the CA distributed and their version.","title":"Configuration files"},{"location":"common/ca/#logs-files","text":"Package File Description Location osg-ca-scripts Log file of osg-update-certs /var/log/osg-update-certs.log osg-ca-scripts Stdout of osg-update-certs /var/log/osg-ca-certs-status.system.out osg-ca-scripts Stdout of osg-ca-manage /var/log/osg-ca-manage.system.out osg-ca-scripts Stdout of initial CA setup /var/log/osg-setup-ca-certificates.system.out","title":"Logs files"},{"location":"common/help/","text":"How to Get Help \u00b6 This page is aimed at OSG site administrators looking for support. Help for OSG users can be found at our support desk . Security Incidents \u00b6 Security incidents can be reported by following the instructions on the Incident Discovery and Reporting page. Software or Service Support \u00b6 If you are experiencing issues with OSG software or services, please consult the following resources before opening a support inquiry: Troubleshooting sections or pages for the problematic software Recent OSG Software release notes Outage information for OSG services Submitting support inquiries \u00b6 If your problem still hasn't been resolved by consulting the resources above, please submit a support inquiry with the information noted below: If you came to this page from an installation guide, please provide the following information: Commands and output from any Troubleshooting sections or pages The OSG system profile ( osg-profile.txt ), generated by running the following command: root@host # osg-system-profiler Submit a support inquiry to the system based on the VOs that you are associated with: If you are primarily associated with... Submit new tickets to... LHC VOs GGUS Anyone else help@opensciencegrid.org Community-specific support \u00b6 Some OSG VOs have dedicated forums or mechanisms for community-specific support. If your VO provides user support, that should be a user's first line of support because the VO is most familiar with your applications and requirements. The list of support centers for OSG VOs can be found in the here . Resources for CMS sites: http://www.uscms.org/uscms_at_work/physics/computing/grid/index.shtml CMS Hyper News: https://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3.html CMS Twiki: https://twiki.cern.ch/twiki/bin/viewauth/CMS/USTier3Computing","title":"Help"},{"location":"common/help/#how-to-get-help","text":"This page is aimed at OSG site administrators looking for support. Help for OSG users can be found at our support desk .","title":"How to Get Help"},{"location":"common/help/#security-incidents","text":"Security incidents can be reported by following the instructions on the Incident Discovery and Reporting page.","title":"Security Incidents"},{"location":"common/help/#software-or-service-support","text":"If you are experiencing issues with OSG software or services, please consult the following resources before opening a support inquiry: Troubleshooting sections or pages for the problematic software Recent OSG Software release notes Outage information for OSG services","title":"Software or Service Support"},{"location":"common/help/#submitting-support-inquiries","text":"If your problem still hasn't been resolved by consulting the resources above, please submit a support inquiry with the information noted below: If you came to this page from an installation guide, please provide the following information: Commands and output from any Troubleshooting sections or pages The OSG system profile ( osg-profile.txt ), generated by running the following command: root@host # osg-system-profiler Submit a support inquiry to the system based on the VOs that you are associated with: If you are primarily associated with... Submit new tickets to... LHC VOs GGUS Anyone else help@opensciencegrid.org","title":"Submitting support inquiries"},{"location":"common/help/#community-specific-support","text":"Some OSG VOs have dedicated forums or mechanisms for community-specific support. If your VO provides user support, that should be a user's first line of support because the VO is most familiar with your applications and requirements. The list of support centers for OSG VOs can be found in the here . Resources for CMS sites: http://www.uscms.org/uscms_at_work/physics/computing/grid/index.shtml CMS Hyper News: https://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3.html CMS Twiki: https://twiki.cern.ch/twiki/bin/viewauth/CMS/USTier3Computing","title":"Community-specific support"},{"location":"common/registration/","text":"Registering in the OSG \u00b6 The OSG keeps a registry containing active projects, virtual organizations (VOs), resources, and resource downtimes stored as YAML files in the topology GitHub repository . This registry is used for accounting data , contact information, and resource availability, particularly if your site is part of the World LHC Computing Grid (WLCG). Use this page to learn how to register information in the OSG. Registration Requirements \u00b6 The instructions in this document require the following: A GitHub account A working knowledge of GitHub collaboration OSG contact registration Registering Contacts \u00b6 The OSG keeps a database of contact information for resources, virtual organizations, and projects. Before submitting any registrations in the topology repository, you must have an entry in the contact database with an associated GitHub ID. To register as an OSG contact or add your GitHub ID to your pre-existing contact entry, search for your name in the contact database and choose one of the following: If you find your name and see an associated GitHub account , you can start submitting registrations! If you find your name but do not see an associated GitHub account , send an email to help@opensciencegrid.org with your GitHub ID. If you cannot find your name , send an email to help@opensciencegrid.org with the following information: Full name Primary email address GitHub user name Description of your OSG affiliation, e.g. FermiGrid site administrator, senior scientist for the DUNE experiment, etc. Contact information of site, virtual organization, or project sponsor to prove your affiliation Privacy The OSG treats any email addresses and phone numbers as confidential data but does not make any guarantees of privacy. Registering Resources \u00b6 An OSG resource is a host that provides grid services, e.g. Compute Entrypoints, storage endpoints, or perfSonar hosts. See the full list of services that should be registered in the OSG topology here . OSG resources are stored under a hierarchy of facilities, sites, and resource groups, defined as follows: Facility : The institution or company name where your resource is located. Site : Smaller than a facility; typically represents a computing center or an academic department. Frequently used as the display name for accounting dashboards . Resource Group : A logical grouping of resources at a site, i.e. all resources associated with a specific computing cluster. Multi-resource downtimes are easiest to declare across a resource group. Production and testing resources must be placed into separate resource groups. Resource : A host that provides grid services, e.g. Compute Entrypoints, storage endpoints, or perfSonar hosts. Throughout this document, you will be asked to substitute your own facility, site, resource group, and resource names when registering with the OSG. If you don't already know the relevant names for your resource, using the following naming conventions: Level Naming convention Facility Unabbreviated institution or company name, e.g. University of Wisconsin - Madison Site Computing center or academic department, e.g. CHTC , MWT2 ATLAS UC , San Diego Supercomputer Center Resource Group Abbreviated facility, site, and cluster name. Resource groups used for testing purposes should have an -ITB or - ITB suffix, e.g. TCNJ-ELSA-ITB Resource In all capital letters, <ABBREV FACILTY>-<CLUSTER>-<RESOURCE TYPE> , for example: TCNJ-ELSA-CE or NMSU-AGGIE-GRID-SQUID If you don't know which VO to use, pick OSG . OSG resources are stored in the GitHub repository as YAML files under a directory structure that reflects the above hierarchy, i.e. topology/<FACILITY>/<SITE>/<RESOURCE GROUP>.yaml from the root of the topology repository . New site \u00b6 To register a site, first choose a name for it (see the naming conventions table above ) The site name will appear in OSG accounting in places such as the GRACC site dashboard . Once you have chosen a site name, open the following in your browser: https://github.com/opensciencegrid/topology/new/master?filename=topology/<FACILITY>/<SITE>/SITE.yaml (replacing <FACILITY> and <SITE> with the facility and the site name that you chose ). \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the site template as a guide. You may leave the ID field blank. When adding new entries, make sure that the formatting and indentation of your entry matches that of the template. Submit your changes as a pull request, providing a descriptive commit message. For example: Adding AggieGrid cluster for New Mexico State Searching for resources \u00b6 Whether you are registering a new resource or modifying an existing resource, start by searching for the FQDN of your host to avoid any duplicate registrations: Open the topology repository in your browser. Search the repository for the FQDN of your resource wrapped in double-quotes using the GitHub search bar (e.g., \"glidein2.chtc.wisc.edu\" ): If the search doesn't return any results , skip to these instructions for registering a new resource. If the search returns a single YAML file , open the link to the YAML file and skip to these instructions for modifying existing resources. If the search returns more than one YAML file , please contact us . Note If you are adding a new service to a host which is already registered as a resource, follow the instructions for modifying existing resources. New resources \u00b6 Before registering a new resource, make sure that its FQDN is not already registered . To register a new resource, follow the instructions below: Find the facility, site, and resource group for your resource in the topology repository under this directory structure: topology/<FACILITY>/<SITE>/<RESOURCE GROUP>.yaml . When searching for these, keep in mind that case and spaces matter. If you do not have a facility, contact help@opensciencegrid.org for help. If you have a facility but not a site, first follow the instructions for registering a site above. If you have a facility and a site but not a resource group, pick a resource group name . Once you have your facility, site, and resource group, follow the instructions below, replacing instances of <FACILITY> , <SITE> , and <RESOURCE GROUP> with the corresponding names that you chose above : If your resource group already exists under your facility and site, open the following URL in your browser: https://github.com/opensciencegrid/topology/edit/master/topology/<FACILITY>/<SITE>/<RESOURCE GROUP>.yaml For example, to add a resource to the CHTC resource group for the CHTC site at the University of Wisconsin , open the following URL: https://github.com/opensciencegrid/topology/edit/master/topology/University of Wisconsin/CHTC/CHTC.yaml If your resource group does not exist, open the following URL in your browser: https://github.com/opensciencegrid/topology/new/master?filename=topology/<FACILITY>/<SITE>/<RESOURCE GROUP>.yaml For example, to create a CHTC-Slurm-HPC resource group for the Center for High Throughput Computing ( CHTC ) at the University of Wisconsin , open the following URL: https://github.com/opensciencegrid/topology/new/master?filename=topology/University of Wisconsin/CHTC/CHTC-Slurm-HPC.yaml \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the resource group template as a guide. You may leave any ID or GroupID fields blank. When adding new entries, make sure that the formatting and indentation of your entry matches that of the template. Submit your changes as a pull request, providing a descriptive commit message. For example: Adding a new compute entrypoint to the CHTC Modifying existing resources \u00b6 To modify an existing resource, follow these instructions: Find the resource that you would like to modify by searching GitHub , and open the link to the YAML file. Click the branch selector button next to the file path and select the master branch. Make changes with the GitHub file editor using the resource group template as a guide. You may leave any ID or GroupID fields blank. Make sure that the formatting and indentation of the modified entry does not change. If you are adding a new service to a host that is already registered as a resource, add the new service to the existing resource; do not create a new resource for the same host. !!! note \"\"You're editing a file in a project you don't have write access to.\"\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Submit your changes as a pull request, providing a descriptive commit message. For example: Updating administrative contact information for CHTC-glidein2 Retiring resources \u00b6 To retire an already registered resource, set Active: false . For example: ... Production: true Resources: GLOW: Active: false ... Services: CE: Description: Compute Entrypoint Details: hidden: false If the Active attribute does not already exist within the resource definition, add it. If your resource becomes available again, set Active: true . Registering Resource Downtimes \u00b6 Resource downtime is a finite period of time for which one or more of the grid services of a registered resource are unavailable. Warning If you expect your resource to be indefinitely unavailable, retire the resource instead of registering a downtime. Downtimes are stored in YAML files alongside the resource group YAML files as described here . For example, downtimes for resources in the CHTC-Slurm-HPC resource group of the CHTC site at the University of Wisconsin can be found and registered in the following file, relative to the root of the topology repository : topology/University of Wisconsin/CHTC/CHTC-Slurm-HPC_downtime.yaml Note Do not put downtime updates in the same pull request as other topology updates. Registering new downtime \u00b6 To register a new downtime for a resource or for multiples resources that are part of a resource group, you will use webforms to generate the contents of the downtime entry, copy it into the downtime file corresponding to your resource, and submit it as a GitHub pull request. Follow the instructions below: Open one of the downtime generation webforms in your browser: Use the resource downtime generator if you only need to declare a downtime for a single resource. Use the resource group downtime generator if you need to declare a downtime for multiple resources across a resource group. Select your facility, site, resource group, and/or resource from the corresponding lists. For the single resource downtime form: Select all the services that will be down. To select multiple, use Control-Click on Windows and Linux, or Command-Click on macOS. Fill the other fields with information about the downtime. Click the Generate button. If the information is valid, a block of text will be displayed in the box labeled Generated YAML . Otherwise, check for error messages and fix your input. Follow the instructions shown below the generated block of text. \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Wait for OSG staff to approve and merge your new downtime. Modifying existing downtime \u00b6 In case an already registered downtime is incorrect or need to be updated to reflect new information, you can modify existing downtime entries using the GitHub editor. Failure Changes to the ID or CreatedTime fields will be rejected. To modify an existing downtime entry for a registered resource, manually make the changes in the matching downtime YAML file. Follow the instructions below: Open the topology repository in your browser. If you do not know the facility, site, and resource group of the resource the downtime entry refers to, search the repository for the FQDN of your resource wrapped in double-quotes using the GitHub search bar (e.g., \"glidein2.chtc.wisc.edu\" ): If the search returns a single YAML file , note the name of the facility, site, and resource group and continue to the next step. If the search doesn't return any results or returns more than one YAML file , please contact us . Open the following URL in your browser using the facility, site, and resource group names to replace <FACILITY> , <SITE> , and <RESOURCE GROUP> , respectively: https://github.com/opensciencegrid/topology/edit/master/topology/<FACILITY>/<SITE>/<RESOURCE GROUP>_downtime.yaml \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the downtime template as a reference. Make sure that the formatting and indentation of the modified entry does not change. Submit your changes as a pull request, providing a descriptive commit message. For example: Move forward end date for CHTC-glidein2 regular maintenance Wait for OSG staff to approve and merge your modified downtime. Registering Virtual Organizations \u00b6 Virtual Organizations (VOs) are sets of groups or individuals defined by some common cyber-infrastructure need. This can be a scientific experiment, a university campus or a distributed research effort. A VO represents all its members and their common needs in a grid environment. A VO also includes the group\u2019s computing/storage resources and services. For more information about VOs, see this page . Info Before submitting a registration for a new VO, please contact us describing your organization's computing needs. VO information is stored as YAML files in the virtual-organizations directory of the topology repository . To modify a VO's information or register a new VO, follow the instructions below: Open the topology repository in your browser. If you see your VO in the list, open the file and continue to the next step. If you do not see your VO in the list, click Create new file button: In the new file dialog, enter <VO>.yaml , replacing <VO> with the name of your VO. \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the VO template as a guide. You may leave any ID fields blank. If you are modifying existing entries, make sure you do not change formatting or indentation of the modified entry. Submit your changes as a pull request, providing a descriptive commit message. For example: Updating contact information for the GLOW VO Registering Projects \u00b6 Info Before submitting a registration for a new project, please contact us describing your organization's computing needs. Project information is stored as YAML files in the projects directory of the topology repository . To modify a VO's information or register a new VO, follow the instructions below: Open the topology repository in your browser. If you see your project in the list, open the file and continue to the next step. If you do not see your project in the list, click Create new file button: In the new file dialog, enter <PROJECT>.yaml , replacing <PROJECT> with the name of your project. \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the project template as a guide. You may leave any ID fields blank. If you are modifying existing entries, make sure you do not change formatting or indentation of the modified entry. Submit your changes as a pull request, providing a descriptive commit message. For example: Updating contact information for the Mu2e project Getting Help \u00b6 To get assistance, please use the this page .","title":"Registration"},{"location":"common/registration/#registering-in-the-osg","text":"The OSG keeps a registry containing active projects, virtual organizations (VOs), resources, and resource downtimes stored as YAML files in the topology GitHub repository . This registry is used for accounting data , contact information, and resource availability, particularly if your site is part of the World LHC Computing Grid (WLCG). Use this page to learn how to register information in the OSG.","title":"Registering in the OSG"},{"location":"common/registration/#registration-requirements","text":"The instructions in this document require the following: A GitHub account A working knowledge of GitHub collaboration OSG contact registration","title":"Registration Requirements"},{"location":"common/registration/#registering-contacts","text":"The OSG keeps a database of contact information for resources, virtual organizations, and projects. Before submitting any registrations in the topology repository, you must have an entry in the contact database with an associated GitHub ID. To register as an OSG contact or add your GitHub ID to your pre-existing contact entry, search for your name in the contact database and choose one of the following: If you find your name and see an associated GitHub account , you can start submitting registrations! If you find your name but do not see an associated GitHub account , send an email to help@opensciencegrid.org with your GitHub ID. If you cannot find your name , send an email to help@opensciencegrid.org with the following information: Full name Primary email address GitHub user name Description of your OSG affiliation, e.g. FermiGrid site administrator, senior scientist for the DUNE experiment, etc. Contact information of site, virtual organization, or project sponsor to prove your affiliation Privacy The OSG treats any email addresses and phone numbers as confidential data but does not make any guarantees of privacy.","title":"Registering Contacts"},{"location":"common/registration/#registering-resources","text":"An OSG resource is a host that provides grid services, e.g. Compute Entrypoints, storage endpoints, or perfSonar hosts. See the full list of services that should be registered in the OSG topology here . OSG resources are stored under a hierarchy of facilities, sites, and resource groups, defined as follows: Facility : The institution or company name where your resource is located. Site : Smaller than a facility; typically represents a computing center or an academic department. Frequently used as the display name for accounting dashboards . Resource Group : A logical grouping of resources at a site, i.e. all resources associated with a specific computing cluster. Multi-resource downtimes are easiest to declare across a resource group. Production and testing resources must be placed into separate resource groups. Resource : A host that provides grid services, e.g. Compute Entrypoints, storage endpoints, or perfSonar hosts. Throughout this document, you will be asked to substitute your own facility, site, resource group, and resource names when registering with the OSG. If you don't already know the relevant names for your resource, using the following naming conventions: Level Naming convention Facility Unabbreviated institution or company name, e.g. University of Wisconsin - Madison Site Computing center or academic department, e.g. CHTC , MWT2 ATLAS UC , San Diego Supercomputer Center Resource Group Abbreviated facility, site, and cluster name. Resource groups used for testing purposes should have an -ITB or - ITB suffix, e.g. TCNJ-ELSA-ITB Resource In all capital letters, <ABBREV FACILTY>-<CLUSTER>-<RESOURCE TYPE> , for example: TCNJ-ELSA-CE or NMSU-AGGIE-GRID-SQUID If you don't know which VO to use, pick OSG . OSG resources are stored in the GitHub repository as YAML files under a directory structure that reflects the above hierarchy, i.e. topology/<FACILITY>/<SITE>/<RESOURCE GROUP>.yaml from the root of the topology repository .","title":"Registering Resources"},{"location":"common/registration/#new-site","text":"To register a site, first choose a name for it (see the naming conventions table above ) The site name will appear in OSG accounting in places such as the GRACC site dashboard . Once you have chosen a site name, open the following in your browser: https://github.com/opensciencegrid/topology/new/master?filename=topology/<FACILITY>/<SITE>/SITE.yaml (replacing <FACILITY> and <SITE> with the facility and the site name that you chose ). \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the site template as a guide. You may leave the ID field blank. When adding new entries, make sure that the formatting and indentation of your entry matches that of the template. Submit your changes as a pull request, providing a descriptive commit message. For example: Adding AggieGrid cluster for New Mexico State","title":"New site"},{"location":"common/registration/#searching-for-resources","text":"Whether you are registering a new resource or modifying an existing resource, start by searching for the FQDN of your host to avoid any duplicate registrations: Open the topology repository in your browser. Search the repository for the FQDN of your resource wrapped in double-quotes using the GitHub search bar (e.g., \"glidein2.chtc.wisc.edu\" ): If the search doesn't return any results , skip to these instructions for registering a new resource. If the search returns a single YAML file , open the link to the YAML file and skip to these instructions for modifying existing resources. If the search returns more than one YAML file , please contact us . Note If you are adding a new service to a host which is already registered as a resource, follow the instructions for modifying existing resources.","title":"Searching for resources"},{"location":"common/registration/#new-resources","text":"Before registering a new resource, make sure that its FQDN is not already registered . To register a new resource, follow the instructions below: Find the facility, site, and resource group for your resource in the topology repository under this directory structure: topology/<FACILITY>/<SITE>/<RESOURCE GROUP>.yaml . When searching for these, keep in mind that case and spaces matter. If you do not have a facility, contact help@opensciencegrid.org for help. If you have a facility but not a site, first follow the instructions for registering a site above. If you have a facility and a site but not a resource group, pick a resource group name . Once you have your facility, site, and resource group, follow the instructions below, replacing instances of <FACILITY> , <SITE> , and <RESOURCE GROUP> with the corresponding names that you chose above : If your resource group already exists under your facility and site, open the following URL in your browser: https://github.com/opensciencegrid/topology/edit/master/topology/<FACILITY>/<SITE>/<RESOURCE GROUP>.yaml For example, to add a resource to the CHTC resource group for the CHTC site at the University of Wisconsin , open the following URL: https://github.com/opensciencegrid/topology/edit/master/topology/University of Wisconsin/CHTC/CHTC.yaml If your resource group does not exist, open the following URL in your browser: https://github.com/opensciencegrid/topology/new/master?filename=topology/<FACILITY>/<SITE>/<RESOURCE GROUP>.yaml For example, to create a CHTC-Slurm-HPC resource group for the Center for High Throughput Computing ( CHTC ) at the University of Wisconsin , open the following URL: https://github.com/opensciencegrid/topology/new/master?filename=topology/University of Wisconsin/CHTC/CHTC-Slurm-HPC.yaml \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the resource group template as a guide. You may leave any ID or GroupID fields blank. When adding new entries, make sure that the formatting and indentation of your entry matches that of the template. Submit your changes as a pull request, providing a descriptive commit message. For example: Adding a new compute entrypoint to the CHTC","title":"New resources"},{"location":"common/registration/#modifying-existing-resources","text":"To modify an existing resource, follow these instructions: Find the resource that you would like to modify by searching GitHub , and open the link to the YAML file. Click the branch selector button next to the file path and select the master branch. Make changes with the GitHub file editor using the resource group template as a guide. You may leave any ID or GroupID fields blank. Make sure that the formatting and indentation of the modified entry does not change. If you are adding a new service to a host that is already registered as a resource, add the new service to the existing resource; do not create a new resource for the same host. !!! note \"\"You're editing a file in a project you don't have write access to.\"\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Submit your changes as a pull request, providing a descriptive commit message. For example: Updating administrative contact information for CHTC-glidein2","title":"Modifying existing resources"},{"location":"common/registration/#retiring-resources","text":"To retire an already registered resource, set Active: false . For example: ... Production: true Resources: GLOW: Active: false ... Services: CE: Description: Compute Entrypoint Details: hidden: false If the Active attribute does not already exist within the resource definition, add it. If your resource becomes available again, set Active: true .","title":"Retiring resources"},{"location":"common/registration/#registering-resource-downtimes","text":"Resource downtime is a finite period of time for which one or more of the grid services of a registered resource are unavailable. Warning If you expect your resource to be indefinitely unavailable, retire the resource instead of registering a downtime. Downtimes are stored in YAML files alongside the resource group YAML files as described here . For example, downtimes for resources in the CHTC-Slurm-HPC resource group of the CHTC site at the University of Wisconsin can be found and registered in the following file, relative to the root of the topology repository : topology/University of Wisconsin/CHTC/CHTC-Slurm-HPC_downtime.yaml Note Do not put downtime updates in the same pull request as other topology updates.","title":"Registering Resource Downtimes"},{"location":"common/registration/#registering-new-downtime","text":"To register a new downtime for a resource or for multiples resources that are part of a resource group, you will use webforms to generate the contents of the downtime entry, copy it into the downtime file corresponding to your resource, and submit it as a GitHub pull request. Follow the instructions below: Open one of the downtime generation webforms in your browser: Use the resource downtime generator if you only need to declare a downtime for a single resource. Use the resource group downtime generator if you need to declare a downtime for multiple resources across a resource group. Select your facility, site, resource group, and/or resource from the corresponding lists. For the single resource downtime form: Select all the services that will be down. To select multiple, use Control-Click on Windows and Linux, or Command-Click on macOS. Fill the other fields with information about the downtime. Click the Generate button. If the information is valid, a block of text will be displayed in the box labeled Generated YAML . Otherwise, check for error messages and fix your input. Follow the instructions shown below the generated block of text. \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Wait for OSG staff to approve and merge your new downtime.","title":"Registering new downtime"},{"location":"common/registration/#modifying-existing-downtime","text":"In case an already registered downtime is incorrect or need to be updated to reflect new information, you can modify existing downtime entries using the GitHub editor. Failure Changes to the ID or CreatedTime fields will be rejected. To modify an existing downtime entry for a registered resource, manually make the changes in the matching downtime YAML file. Follow the instructions below: Open the topology repository in your browser. If you do not know the facility, site, and resource group of the resource the downtime entry refers to, search the repository for the FQDN of your resource wrapped in double-quotes using the GitHub search bar (e.g., \"glidein2.chtc.wisc.edu\" ): If the search returns a single YAML file , note the name of the facility, site, and resource group and continue to the next step. If the search doesn't return any results or returns more than one YAML file , please contact us . Open the following URL in your browser using the facility, site, and resource group names to replace <FACILITY> , <SITE> , and <RESOURCE GROUP> , respectively: https://github.com/opensciencegrid/topology/edit/master/topology/<FACILITY>/<SITE>/<RESOURCE GROUP>_downtime.yaml \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the downtime template as a reference. Make sure that the formatting and indentation of the modified entry does not change. Submit your changes as a pull request, providing a descriptive commit message. For example: Move forward end date for CHTC-glidein2 regular maintenance Wait for OSG staff to approve and merge your modified downtime.","title":"Modifying existing downtime"},{"location":"common/registration/#registering-virtual-organizations","text":"Virtual Organizations (VOs) are sets of groups or individuals defined by some common cyber-infrastructure need. This can be a scientific experiment, a university campus or a distributed research effort. A VO represents all its members and their common needs in a grid environment. A VO also includes the group\u2019s computing/storage resources and services. For more information about VOs, see this page . Info Before submitting a registration for a new VO, please contact us describing your organization's computing needs. VO information is stored as YAML files in the virtual-organizations directory of the topology repository . To modify a VO's information or register a new VO, follow the instructions below: Open the topology repository in your browser. If you see your VO in the list, open the file and continue to the next step. If you do not see your VO in the list, click Create new file button: In the new file dialog, enter <VO>.yaml , replacing <VO> with the name of your VO. \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the VO template as a guide. You may leave any ID fields blank. If you are modifying existing entries, make sure you do not change formatting or indentation of the modified entry. Submit your changes as a pull request, providing a descriptive commit message. For example: Updating contact information for the GLOW VO","title":"Registering Virtual Organizations"},{"location":"common/registration/#registering-projects","text":"Info Before submitting a registration for a new project, please contact us describing your organization's computing needs. Project information is stored as YAML files in the projects directory of the topology repository . To modify a VO's information or register a new VO, follow the instructions below: Open the topology repository in your browser. If you see your project in the list, open the file and continue to the next step. If you do not see your project in the list, click Create new file button: In the new file dialog, enter <PROJECT>.yaml , replacing <PROJECT> with the name of your project. \"You're editing a file in a project you don't have write access to.\" If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write access to the OSG copy of the topology data, which is why you are creating a pull request. Make changes with the GitHub file editor using the project template as a guide. You may leave any ID fields blank. If you are modifying existing entries, make sure you do not change formatting or indentation of the modified entry. Submit your changes as a pull request, providing a descriptive commit message. For example: Updating contact information for the Mu2e project","title":"Registering Projects"},{"location":"common/registration/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"common/yum/","text":"OSG Yum Repositories \u00b6 This document introduces Yum repositories and how they are used in the OSG. If you are unfamiliar with Yum, see the documentation on using Yum and RPM . Repositories \u00b6 The OSG hosts multiple repositories at repo.opensciencegrid.org that are intended for public use: The OSG Yum repositories... Contain RPMs that... osg , osg-upcoming are considered production-ready (default). osg-rolling , osg-upcoming-rolling are considered production-ready but are released at faster pace than the osg repository (see note). osg-testing , osg-upcoming-testing have passed developer or integration testing but not acceptance testing osg-development , osg-upcoming-development have not passed developer, integration or acceptance testing. Do not use without instruction from the OSG Software and Release Team. osg-contrib have been contributed from outside of the OSG Software and Release Team. See this section for details. Note The upcoming repositories contain newer software that might require manual action after an update. They are not enabled by default and must be enabled in addition to the main osg repository. See the upcoming software section for details. Note In the 3.5 series (and earlier), packages are added to the osg and osg-upcoming repositories during discrete, numbered releases (e.g. 3.5.31); packages are added to the osg-rolling and osg-upcoming-rolling repositories as soon as they are considered production-ready. In the 3.6 series, there are no osg-rolling or osg-upcoming-rolling repositories, and packages are added to the osg and osg-upcoming repositories as soon as they are considered production ready. OSG's RPM packages also rely on external packages provided by supported OSes and EPEL. You must have the following repositories available and enabled: OS repositories, including the following ones that aren't enabled by default: extras (SL 7, CentOS 7/8) Server-Extras (RHEL 7) PowerTools (CentOS 8.0 through 8.2) powertools (CentOS 8.3 and newer) CodeReady Builder (RHEL 8) EPEL repositories OSG repositories If any of these repositories are missing, you may end up with installation issues or missing dependencies. Danger Other repositories, such as jpackage , dag , or rpmforge , are not supported and you may encounter problems if you use them. Note If you upgrade from CentOS 8.0 through 8.2 to CentOS 8.3 or newer, you may have to re-enable some repos as described below Upcoming Software \u00b6 Certain sites have requested new versions of software that would be considered \"disruptive\" or \"experimental\": upgrading to them would likely require manual intervention after their installation. We do not want sites to unwittingly upgrade to these versions. We have placed such software in separate repositories. Their names start with osg-upcoming and have the same structure as our standard repositories, as well as the same guarantees of quality and production-readiness. There are separate sets of upcoming repositories for each release series. For example, the OSG 3.5 repos have corresponding 3.5-upcoming repos . The upcoming repositories are meant to be layered on top of our standard repositories: installing software from the upcoming repositories requires also enabling the standard repositories from the same release. Contrib Software \u00b6 In addition to our regular software repositories, we also have a contrib (short for \"contributed\") software repository. This is software that is does not go through the same software testing and release processes as the official OSG Software release, but may be useful to you. Particularly, contrib software is not guaranteed to be compatible with the rest of the OSG Software stack nor is it supported by the OSG. The definitive list of software in the contrib repository can be found here: OSG 3.5 EL7 contrib software repository OSG 3.5 EL8 contrib software repository OSG 3.6 EL7 contrib software repository OSG 3.6 EL8 contrib software repository If you would like to distribute your software in the OSG contrib repository, please contact us with a description of your software, what users it serves, and relevant RPM packaging. Installing Yum Repositories \u00b6 Install the Yum priorities plugin (EL7) \u00b6 The Yum priorities plugin is used to tell Yum to prefer OSG packages over EPEL or OS packages. It is important to install and enable the Yum priorities plugin before installing grid software to ensure that you are getting the OSG-supported versions. This plugin is built into Yum on EL8 distributions. Install the Yum priorities package: root@host # yum install yum-plugin-priorities Ensure that /etc/yum.conf has the following line in the [main] section: plugins=1 Enable additional OS repositories \u00b6 Some packages depend on packages that are in OS repositories not enabled by default. The repositories to enable, as well as the instructions to enable them, are OS-dependent. Note A repository is enabled if it has enabled=1 in its definition, or if the enabled line is missing (i.e. it is enabled unless specified otherwise.) SL 7 \u00b6 Install the yum-conf-extras RPM package. Ensure that the sl-extras repo in /etc/yum.repos.d/sl-extras.repo is enabled. CentOS 7 \u00b6 Ensure that the extras repo in /etc/yum.repos.d/CentOS-Base.repo is enabled. CentOS 8.0 to 8.2 \u00b6 Ensure that the extras repo in /etc/yum.repos.d/CentOS-Extras.repo is enabled. Ensure that the PowerTools repo in /etc/yum.repos.d/CentOS-PowerTools.repo is enabled. CentOS 8.3 and newer \u00b6 Ensure that the extras repo in /etc/yum.repos.d/CentOS-Linux-Extras.repo is enabled. Ensure that the powertools repo in /etc/yum.repos.d/CentOS-Linux-PowerTools.repo is enabled. RHEL 7 \u00b6 Ensure that the Server-Extras channel is enabled. RHEL 8 \u00b6 Ensure that the CodeReady Linux Builder channel is enabled. See Red Hat's instructions on how to enable this repo. Install the EPEL repositories \u00b6 OSG software depends on packages distributed via the EPEL repositories. You must install and enable these first. Install the EPEL repository, if not already present. Choose the right version to match your OS version. # # EPEL 7 (For RHEL 7, CentOS 7, and SL 7) root@host # yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # # EPEL 8 (For RHEL 8 and CentOS 8) root@host # yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Verify that /etc/yum.repos.d/epel.repo exists; the [epel] section should contain: The line enabled=1 Either no priority setting, or a priority setting that is 99 or higher Warning If you have your own mirror or configuration of the EPEL repository, you MUST verify that the priority of the EPEL repository is either missing, or 99 or a higher number. The OSG repositories must have a better (numerically lower) priority than the EPEL repositories; otherwise, you might have dependency resolution (\"depsolving\") issues. Install the OSG Repositories \u00b6 This document assumes a fresh install. For instructions on upgrading from one OSG series to another, see the release series document . Install the OSG repository for your OS version and the OSG release series that you wish to use: OSG 3.5 EL7: root@host # yum install https://repo.opensciencegrid.org/osg/3.5/osg-3.5-el7-release-latest.rpm OSG 3.5 EL8: root@host # yum install https://repo.opensciencegrid.org/osg/3.5/osg-3.5-el8-release-latest.rpm OSG 3.6 EL7: root@host # yum install https://repo.opensciencegrid.org/osg/3.6/osg-3.6-el7-release-latest.rpm Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series OSG 3.6 EL8: root@host # yum install https://repo.opensciencegrid.org/osg/3.6/osg-3.6-el8-release-latest.rpm Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series The only OSG repository enabled by default is the release one. If you want to enable another one (e.g. osg-testing ), then edit its file (e.g. /etc/yum.repos.d/osg-testing.repo ) and change the enabled option from 0 to 1: [osg-testing] name=OSG Software for Enterprise Linux 7 - Testing - $basearch #baseurl=https://repo.opensciencegrid.org/osg/3.5/el7/testing/$basearch mirrorlist=https://repo.opensciencegrid.org/mirror/osg/3.5/el7/testing/$basearch failovermethod=priority priority=98 enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG-2 Optional Configuration \u00b6 Enable automatic security updates \u00b6 For production services, we suggest only changing software versions during controlled downtime. Therefore we recommend security-only automatic updates or disabling automatic updates entirely. Note RHEL/CentOS 8 automatic updates are provided in the dnf-automatic RPM, which is not installed by default. To enable only security related automatic updates: On RHEL 7/SL 7, edit /etc/yum/yum-cron.conf and set update_cmd = security On RHEL 8, edit /etc/dnf/automatic.conf and set upgrade_type = security CentOS 7/8 does not support security-only automatic updates; doing any of the above steps will prevent automatic updates from happening at all. To disable automatic updates entirely: On EL7, run: root@host # service yum-cron stop On EL8, run: root@host # systemctl disable --now dnf-automatic.timer Configuring Spacewalk priorities \u00b6 Sites using Spacewalk to manage RPM packages will need to configure OSG Yum repository priorities using their Spacewalk ID. For example, if the OSG 3.4 repository's Spacewalk ID is centos_7_osg34_dev , modify /etc/yum/pluginconf.d/90-osg.conf to include the following: [centos_7_osg_34_dev] priority = 98 Repository Mirrors \u00b6 If you run a large site (>20 nodes), you should consider setting up a local mirror for the OSG repositories. A local Yum mirror allows you to reduce the amount of external bandwidth used when updating or installing packages. Add the following to a file in /etc/cron.d : <RANDOM> * * * * root rsync -aH rsync://repo-rsync.opensciencegrid.org/osg/ /var/www/html/osg/ Or, to mirror only a single repository: <RANDOM> * * * * root rsync -aH rsync://repo-rsync.opensciencegrid.org/osg/<OSG_RELEASE>/el7/development /var/www/html/osg/<OSG_RELEASE>/el7 Replace <OSG_RELEASE> with the OSG release you would like to use (e.g. 3.5 ) and <RANDOM> with a number between 0 and 59. On your worker node, you can replace the baseurl line of /etc/yum.repos.d/osg.repo with the appropriate URL for your mirror. If you are interested in having your mirror be part of the OSG's default set of mirrors, please file a support ticket . Reference \u00b6 Basic use of Yum","title":"OSG Yum Repos"},{"location":"common/yum/#osg-yum-repositories","text":"This document introduces Yum repositories and how they are used in the OSG. If you are unfamiliar with Yum, see the documentation on using Yum and RPM .","title":"OSG Yum Repositories"},{"location":"common/yum/#repositories","text":"The OSG hosts multiple repositories at repo.opensciencegrid.org that are intended for public use: The OSG Yum repositories... Contain RPMs that... osg , osg-upcoming are considered production-ready (default). osg-rolling , osg-upcoming-rolling are considered production-ready but are released at faster pace than the osg repository (see note). osg-testing , osg-upcoming-testing have passed developer or integration testing but not acceptance testing osg-development , osg-upcoming-development have not passed developer, integration or acceptance testing. Do not use without instruction from the OSG Software and Release Team. osg-contrib have been contributed from outside of the OSG Software and Release Team. See this section for details. Note The upcoming repositories contain newer software that might require manual action after an update. They are not enabled by default and must be enabled in addition to the main osg repository. See the upcoming software section for details. Note In the 3.5 series (and earlier), packages are added to the osg and osg-upcoming repositories during discrete, numbered releases (e.g. 3.5.31); packages are added to the osg-rolling and osg-upcoming-rolling repositories as soon as they are considered production-ready. In the 3.6 series, there are no osg-rolling or osg-upcoming-rolling repositories, and packages are added to the osg and osg-upcoming repositories as soon as they are considered production ready. OSG's RPM packages also rely on external packages provided by supported OSes and EPEL. You must have the following repositories available and enabled: OS repositories, including the following ones that aren't enabled by default: extras (SL 7, CentOS 7/8) Server-Extras (RHEL 7) PowerTools (CentOS 8.0 through 8.2) powertools (CentOS 8.3 and newer) CodeReady Builder (RHEL 8) EPEL repositories OSG repositories If any of these repositories are missing, you may end up with installation issues or missing dependencies. Danger Other repositories, such as jpackage , dag , or rpmforge , are not supported and you may encounter problems if you use them. Note If you upgrade from CentOS 8.0 through 8.2 to CentOS 8.3 or newer, you may have to re-enable some repos as described below","title":"Repositories"},{"location":"common/yum/#upcoming-software","text":"Certain sites have requested new versions of software that would be considered \"disruptive\" or \"experimental\": upgrading to them would likely require manual intervention after their installation. We do not want sites to unwittingly upgrade to these versions. We have placed such software in separate repositories. Their names start with osg-upcoming and have the same structure as our standard repositories, as well as the same guarantees of quality and production-readiness. There are separate sets of upcoming repositories for each release series. For example, the OSG 3.5 repos have corresponding 3.5-upcoming repos . The upcoming repositories are meant to be layered on top of our standard repositories: installing software from the upcoming repositories requires also enabling the standard repositories from the same release.","title":"Upcoming Software"},{"location":"common/yum/#contrib-software","text":"In addition to our regular software repositories, we also have a contrib (short for \"contributed\") software repository. This is software that is does not go through the same software testing and release processes as the official OSG Software release, but may be useful to you. Particularly, contrib software is not guaranteed to be compatible with the rest of the OSG Software stack nor is it supported by the OSG. The definitive list of software in the contrib repository can be found here: OSG 3.5 EL7 contrib software repository OSG 3.5 EL8 contrib software repository OSG 3.6 EL7 contrib software repository OSG 3.6 EL8 contrib software repository If you would like to distribute your software in the OSG contrib repository, please contact us with a description of your software, what users it serves, and relevant RPM packaging.","title":"Contrib Software"},{"location":"common/yum/#installing-yum-repositories","text":"","title":"Installing Yum Repositories"},{"location":"common/yum/#install-the-yum-priorities-plugin-el7","text":"The Yum priorities plugin is used to tell Yum to prefer OSG packages over EPEL or OS packages. It is important to install and enable the Yum priorities plugin before installing grid software to ensure that you are getting the OSG-supported versions. This plugin is built into Yum on EL8 distributions. Install the Yum priorities package: root@host # yum install yum-plugin-priorities Ensure that /etc/yum.conf has the following line in the [main] section: plugins=1","title":"Install the Yum priorities plugin (EL7)"},{"location":"common/yum/#enable-additional-os-repositories","text":"Some packages depend on packages that are in OS repositories not enabled by default. The repositories to enable, as well as the instructions to enable them, are OS-dependent. Note A repository is enabled if it has enabled=1 in its definition, or if the enabled line is missing (i.e. it is enabled unless specified otherwise.)","title":"Enable additional OS repositories"},{"location":"common/yum/#sl-7","text":"Install the yum-conf-extras RPM package. Ensure that the sl-extras repo in /etc/yum.repos.d/sl-extras.repo is enabled.","title":"SL 7"},{"location":"common/yum/#centos-7","text":"Ensure that the extras repo in /etc/yum.repos.d/CentOS-Base.repo is enabled.","title":"CentOS 7"},{"location":"common/yum/#centos-80-to-82","text":"Ensure that the extras repo in /etc/yum.repos.d/CentOS-Extras.repo is enabled. Ensure that the PowerTools repo in /etc/yum.repos.d/CentOS-PowerTools.repo is enabled.","title":"CentOS 8.0 to 8.2"},{"location":"common/yum/#centos-83-and-newer","text":"Ensure that the extras repo in /etc/yum.repos.d/CentOS-Linux-Extras.repo is enabled. Ensure that the powertools repo in /etc/yum.repos.d/CentOS-Linux-PowerTools.repo is enabled.","title":"CentOS 8.3 and newer"},{"location":"common/yum/#rhel-7","text":"Ensure that the Server-Extras channel is enabled.","title":"RHEL 7"},{"location":"common/yum/#rhel-8","text":"Ensure that the CodeReady Linux Builder channel is enabled. See Red Hat's instructions on how to enable this repo.","title":"RHEL 8"},{"location":"common/yum/#install-the-epel-repositories","text":"OSG software depends on packages distributed via the EPEL repositories. You must install and enable these first. Install the EPEL repository, if not already present. Choose the right version to match your OS version. # # EPEL 7 (For RHEL 7, CentOS 7, and SL 7) root@host # yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # # EPEL 8 (For RHEL 8 and CentOS 8) root@host # yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm Verify that /etc/yum.repos.d/epel.repo exists; the [epel] section should contain: The line enabled=1 Either no priority setting, or a priority setting that is 99 or higher Warning If you have your own mirror or configuration of the EPEL repository, you MUST verify that the priority of the EPEL repository is either missing, or 99 or a higher number. The OSG repositories must have a better (numerically lower) priority than the EPEL repositories; otherwise, you might have dependency resolution (\"depsolving\") issues.","title":"Install the EPEL repositories"},{"location":"common/yum/#install-the-osg-repositories","text":"This document assumes a fresh install. For instructions on upgrading from one OSG series to another, see the release series document . Install the OSG repository for your OS version and the OSG release series that you wish to use: OSG 3.5 EL7: root@host # yum install https://repo.opensciencegrid.org/osg/3.5/osg-3.5-el7-release-latest.rpm OSG 3.5 EL8: root@host # yum install https://repo.opensciencegrid.org/osg/3.5/osg-3.5-el8-release-latest.rpm OSG 3.6 EL7: root@host # yum install https://repo.opensciencegrid.org/osg/3.6/osg-3.6-el7-release-latest.rpm Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series OSG 3.6 EL8: root@host # yum install https://repo.opensciencegrid.org/osg/3.6/osg-3.6-el8-release-latest.rpm Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series The only OSG repository enabled by default is the release one. If you want to enable another one (e.g. osg-testing ), then edit its file (e.g. /etc/yum.repos.d/osg-testing.repo ) and change the enabled option from 0 to 1: [osg-testing] name=OSG Software for Enterprise Linux 7 - Testing - $basearch #baseurl=https://repo.opensciencegrid.org/osg/3.5/el7/testing/$basearch mirrorlist=https://repo.opensciencegrid.org/mirror/osg/3.5/el7/testing/$basearch failovermethod=priority priority=98 enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG-2","title":"Install the OSG Repositories"},{"location":"common/yum/#optional-configuration","text":"","title":"Optional Configuration"},{"location":"common/yum/#enable-automatic-security-updates","text":"For production services, we suggest only changing software versions during controlled downtime. Therefore we recommend security-only automatic updates or disabling automatic updates entirely. Note RHEL/CentOS 8 automatic updates are provided in the dnf-automatic RPM, which is not installed by default. To enable only security related automatic updates: On RHEL 7/SL 7, edit /etc/yum/yum-cron.conf and set update_cmd = security On RHEL 8, edit /etc/dnf/automatic.conf and set upgrade_type = security CentOS 7/8 does not support security-only automatic updates; doing any of the above steps will prevent automatic updates from happening at all. To disable automatic updates entirely: On EL7, run: root@host # service yum-cron stop On EL8, run: root@host # systemctl disable --now dnf-automatic.timer","title":"Enable automatic security updates"},{"location":"common/yum/#configuring-spacewalk-priorities","text":"Sites using Spacewalk to manage RPM packages will need to configure OSG Yum repository priorities using their Spacewalk ID. For example, if the OSG 3.4 repository's Spacewalk ID is centos_7_osg34_dev , modify /etc/yum/pluginconf.d/90-osg.conf to include the following: [centos_7_osg_34_dev] priority = 98","title":"Configuring Spacewalk priorities"},{"location":"common/yum/#repository-mirrors","text":"If you run a large site (>20 nodes), you should consider setting up a local mirror for the OSG repositories. A local Yum mirror allows you to reduce the amount of external bandwidth used when updating or installing packages. Add the following to a file in /etc/cron.d : <RANDOM> * * * * root rsync -aH rsync://repo-rsync.opensciencegrid.org/osg/ /var/www/html/osg/ Or, to mirror only a single repository: <RANDOM> * * * * root rsync -aH rsync://repo-rsync.opensciencegrid.org/osg/<OSG_RELEASE>/el7/development /var/www/html/osg/<OSG_RELEASE>/el7 Replace <OSG_RELEASE> with the OSG release you would like to use (e.g. 3.5 ) and <RANDOM> with a number between 0 and 59. On your worker node, you can replace the baseurl line of /etc/yum.repos.d/osg.repo with the appropriate URL for your mirror. If you are interested in having your mirror be part of the OSG's default set of mirrors, please file a support ticket .","title":"Repository Mirrors"},{"location":"common/yum/#reference","text":"Basic use of Yum","title":"Reference"},{"location":"compute-element/covid-19/","text":"Supporting COVID-19 Research on the OSG \u00b6 Info The instructions in this document are deprecated, as COVID-19 jobs are no longer prioritized. There a few options available for sites with computing resources who want to support the important and urgent work of COVID-19 researchers using the OSG. As we're currently routing such projects through the OSG VO, your site can be configured to accept pilots that exclusively run OSG VO jobs relating to COVID-19 research (among other pilots you support), allowing you to prioritize these pilots and account for this usage separately from other OSG activity. To support COVID-19 work, the overall process includes the following: Make the site computing resources available through a HTCondor-CE if you have not already done so. You can install a locally-managed instance or ask OSG to host the CE on your behalf. If neither solution is viable, or you'd like to discuss the options, please send email to help@opensciencegrid.org and we'll work with you to arrive at the best solution. If you already provide resources through an OSG Hosted CE, skip to this section . Enable the OSG VO on your HTCondor-CE. Setup a job route specific to COVID-19 pilot jobs (documented below). The job route will allow you to prioritize these jobs using local policy in your site's cluster. (Optional) To attract more user jobs, install CVMFS and Singularity on your site's worker nodes Send email to help@opensciencegrid.org requesting that your CE receive COVID-19 pilots. We will need to know the CE hostname and any special restrictions that might apply to these pilots. Setting up a COVID-19 Job Route \u00b6 By default, COVID-19 pilots will look identical to OSG pilots except they will have the attribute IsCOVID19 = true . They do not require mapping to a distinct Unix account but can be sent to a prioritized queue or accounting group. Job routes are controlled by the JOB_ROUTER_ENTRIES configuration variable in HTCondor-CE. Customizations may be placed in /etc/condor-ce/config.d/ where files are parsed in lexicographical order, e.g. JOB_ROUTER_ENTRIES specified in 50-covid-routes.conf will override JOB_ROUTER_ENTRIES in 02-local-slurm.conf . For Non-HTCondor batch systems \u00b6 To add a new route for COVID-19 pilots for non-HTCondor batch systems: Note the names of your currently enabled routes: condor_ce_job_router_info -config Add the following configuration to a file in /etc/condor-ce/config.d/ (files are parsed in lexicographical order): JOB_ROUTER_ENTRIES @=jre [ name = \"OSG_COVID19_Jobs\"; GridResource = \"batch slurm\"; TargetUniverse = 9; set_default_queue = \"covid19\"; Requirements = (TARGET.IsCOVID19 =?= true); ] $(JOB_ROUTER_ENTRIES) @jre Replacing slurm in the GridResource attribute with the appropriate value for your batch system (e.g., lsf , pbs , sge , or slurm ); and the value of set_default_queue with the name of the partition or queue of your local batch system dedicated to COVID-19 work. Ensure that COVID-19 jobs match to the new route. Choose one of the options below depending on your HTCondor version ( condor_version ): For versions of HTCondor >= 8.8.7 and < 8.9.0; or HTCondor >= 8.9.6: specify the routes considered by the job router and the order in which they're considered by adding the following configuration to a file in /etc/condor-ce/config.d/ : JOB_ROUTER_ROUTE_NAMES = OSG_COVID19_Jobs, $(JOB_ROUTER_ROUTE_NAMES) If your configuration does not already define JOB_ROUTER_ROUTE_NAMES , you need to add the name of all previous routes to it, leaving OSG_COVID19_Jobs at the start of the list. For example: JOB_ROUTER_ROUTE_NAMES = OSG_COVID19_Jobs, Local_Condor, $(JOB_ROUTER_ROUTE_NAMES) For older versions of HTCondor: add (TARGET.IsCOVID19 =!= true) to the Requirements of any existing routes. For example, the following job route: JOB_ROUTER_ENTRIES @=jre [ name = \"Local_Slurm\" GridResource = \"batch slurm\"; TargetUniverse = 9; set_default_queue = \"atlas; Requirements = (TARGET.Owner =!= \"osg\"); ] @jre Should be updated as follows: JOB_ROUTER_ENTRIES @=jre [ name = \"Local_Slurm\" GridResource = \"batch slurm\"; TargetUniverse = 9; set_default_queue = \"atlas; Requirements = (TARGET.Owner =!= \"osg\") && (TARGET.IsCOVID19 =!= true); ] @jre Reconfigure your HTCondor-CE: condor_ce_reconfig Continue onto this section to verify your configuration For HTCondor batch systems \u00b6 Similarly, at an HTCondor site, one can place these jobs into a separate accounting group by providing the set_AcctGroup and eval_set_AccountingGroup attributes in a new job route. To add a new route for COVID-19 pilots for non-HTCondor batch systems: Note the names of your currently enabled routes: condor_ce_job_router_info -config Add the following configuration to a file in /etc/condor-ce/config.d/ (files are parsed in lexicographical order): JOB_ROUTER_ENTRIES @=jre [ name = \"OSG_COVID19_Jobs\"; TargetUniverse = 5; set_AcctGroup = \"covid19\"; eval_set_AccountingGroup = strcat(AcctGroup, \".\", Owner); Requirements = (TARGET.IsCOVID19 =?= true); ] $(JOB_ROUTER_ENTRIES) @jre Replacing covid19 in set_AcctGroup with the name of the accounting group that you would like to use for COVID-19 jobs. Ensure that COVID-19 jobs match to the new route. Choose one of the options below depending on your HTCondor version ( condor_version ): For versions of HTCondor >= 8.8.7 and < 8.9.0; or HTCondor >= 8.9.6: specify the routes considered by the job router and the order in which they're considered by adding the following configuration to a file in /etc/condor-ce/config.d/ : JOB_ROUTER_ROUTE_NAMES = OSG_COVID19_Jobs, $(JOB_ROUTER_ROUTE_NAMES) For older versions of HTCondor: add (TARGET.IsCOVID19 =!= true) to the Requirements of any existing routes. For example, the following job route: JOB_ROUTER_ENTRIES @=jre [ name = \"Local_Condor\" TargetUniverse = 5; Requirements = (TARGET.Owner =!= \"osg\"); ] @jre Should be updated as follows: JOB_ROUTER_ENTRIES @=jre [ name = \"Local_Condor\" TargetUniverse = 5; Requirements = (TARGET.Owner =!= \"atlas\") && (TARGET.IsCOVID19 =!= true); ] @jre Reconfigure your HTCondor-CE: condor_ce_reconfig Continue onto this section to verify your configuration Verifying the COVID-19 Job Route \u00b6 To verify that your HTCondor-CE is configured to support COVID-19 jobs, perform the following steps: Ensure that the OSG_COVID19_Jobs route appears with all of your other previously enabled routes: condor_ce_job_router_info -config Known issue: removing old routes If your HTCondor-CE has jobs associated with a route that is removed from your configuration, this will result in a crashing Job Router. If you accidentally remove an old route, restore the route or remove all jobs associated with said route. Ensure that COVID-19 jobs will match to your new job route: For versions of HTCondor >= 8.8.7 and < 8.9.0; or HTCondor >= 8.9.6: OSG_COVID19_Jobs should be the first route in the routing table: condor_ce_config_val -verbose JOB_ROUTER_ROUTE_NAMES For older versions of HTCondor: the Requirements expresison of your OSG_COVID19_Jobs route must contain (TARGET.IsCOVID19 =?= true) and all other routes must contain (TARGET.IsCOVID19 =!= true) in their Requirements expression. After requesting COVID-19 jobs , verify that jobs are being routed appropriately, by examining pilots with condor_ce_router_q . Requesting COVID-19 Jobs \u00b6 To receive COVID-19 pilot jobs, send an email to help@opensciencegrid.org with the subject Requesting COVID-19 pilots and the following information: Whether you want to receive only COVID-19 jobs, or if you want to accept COVID-19 and other OSG jobs The hostname(s) of your HTCondor-CE(s) Any other restrictions that may apply to these jobs (e.g. number of available cores) Viewing COVID-19 Contributions \u00b6 You can view how many hours that COVID-19 projects have consumed at your site with this GRACC dashboard . Getting Help \u00b6 To get assistance, please use this page .","title":"Covid 19"},{"location":"compute-element/covid-19/#supporting-covid-19-research-on-the-osg","text":"Info The instructions in this document are deprecated, as COVID-19 jobs are no longer prioritized. There a few options available for sites with computing resources who want to support the important and urgent work of COVID-19 researchers using the OSG. As we're currently routing such projects through the OSG VO, your site can be configured to accept pilots that exclusively run OSG VO jobs relating to COVID-19 research (among other pilots you support), allowing you to prioritize these pilots and account for this usage separately from other OSG activity. To support COVID-19 work, the overall process includes the following: Make the site computing resources available through a HTCondor-CE if you have not already done so. You can install a locally-managed instance or ask OSG to host the CE on your behalf. If neither solution is viable, or you'd like to discuss the options, please send email to help@opensciencegrid.org and we'll work with you to arrive at the best solution. If you already provide resources through an OSG Hosted CE, skip to this section . Enable the OSG VO on your HTCondor-CE. Setup a job route specific to COVID-19 pilot jobs (documented below). The job route will allow you to prioritize these jobs using local policy in your site's cluster. (Optional) To attract more user jobs, install CVMFS and Singularity on your site's worker nodes Send email to help@opensciencegrid.org requesting that your CE receive COVID-19 pilots. We will need to know the CE hostname and any special restrictions that might apply to these pilots.","title":"Supporting COVID-19 Research on the OSG"},{"location":"compute-element/covid-19/#setting-up-a-covid-19-job-route","text":"By default, COVID-19 pilots will look identical to OSG pilots except they will have the attribute IsCOVID19 = true . They do not require mapping to a distinct Unix account but can be sent to a prioritized queue or accounting group. Job routes are controlled by the JOB_ROUTER_ENTRIES configuration variable in HTCondor-CE. Customizations may be placed in /etc/condor-ce/config.d/ where files are parsed in lexicographical order, e.g. JOB_ROUTER_ENTRIES specified in 50-covid-routes.conf will override JOB_ROUTER_ENTRIES in 02-local-slurm.conf .","title":"Setting up a COVID-19 Job Route"},{"location":"compute-element/covid-19/#for-non-htcondor-batch-systems","text":"To add a new route for COVID-19 pilots for non-HTCondor batch systems: Note the names of your currently enabled routes: condor_ce_job_router_info -config Add the following configuration to a file in /etc/condor-ce/config.d/ (files are parsed in lexicographical order): JOB_ROUTER_ENTRIES @=jre [ name = \"OSG_COVID19_Jobs\"; GridResource = \"batch slurm\"; TargetUniverse = 9; set_default_queue = \"covid19\"; Requirements = (TARGET.IsCOVID19 =?= true); ] $(JOB_ROUTER_ENTRIES) @jre Replacing slurm in the GridResource attribute with the appropriate value for your batch system (e.g., lsf , pbs , sge , or slurm ); and the value of set_default_queue with the name of the partition or queue of your local batch system dedicated to COVID-19 work. Ensure that COVID-19 jobs match to the new route. Choose one of the options below depending on your HTCondor version ( condor_version ): For versions of HTCondor >= 8.8.7 and < 8.9.0; or HTCondor >= 8.9.6: specify the routes considered by the job router and the order in which they're considered by adding the following configuration to a file in /etc/condor-ce/config.d/ : JOB_ROUTER_ROUTE_NAMES = OSG_COVID19_Jobs, $(JOB_ROUTER_ROUTE_NAMES) If your configuration does not already define JOB_ROUTER_ROUTE_NAMES , you need to add the name of all previous routes to it, leaving OSG_COVID19_Jobs at the start of the list. For example: JOB_ROUTER_ROUTE_NAMES = OSG_COVID19_Jobs, Local_Condor, $(JOB_ROUTER_ROUTE_NAMES) For older versions of HTCondor: add (TARGET.IsCOVID19 =!= true) to the Requirements of any existing routes. For example, the following job route: JOB_ROUTER_ENTRIES @=jre [ name = \"Local_Slurm\" GridResource = \"batch slurm\"; TargetUniverse = 9; set_default_queue = \"atlas; Requirements = (TARGET.Owner =!= \"osg\"); ] @jre Should be updated as follows: JOB_ROUTER_ENTRIES @=jre [ name = \"Local_Slurm\" GridResource = \"batch slurm\"; TargetUniverse = 9; set_default_queue = \"atlas; Requirements = (TARGET.Owner =!= \"osg\") && (TARGET.IsCOVID19 =!= true); ] @jre Reconfigure your HTCondor-CE: condor_ce_reconfig Continue onto this section to verify your configuration","title":"For Non-HTCondor batch systems"},{"location":"compute-element/covid-19/#for-htcondor-batch-systems","text":"Similarly, at an HTCondor site, one can place these jobs into a separate accounting group by providing the set_AcctGroup and eval_set_AccountingGroup attributes in a new job route. To add a new route for COVID-19 pilots for non-HTCondor batch systems: Note the names of your currently enabled routes: condor_ce_job_router_info -config Add the following configuration to a file in /etc/condor-ce/config.d/ (files are parsed in lexicographical order): JOB_ROUTER_ENTRIES @=jre [ name = \"OSG_COVID19_Jobs\"; TargetUniverse = 5; set_AcctGroup = \"covid19\"; eval_set_AccountingGroup = strcat(AcctGroup, \".\", Owner); Requirements = (TARGET.IsCOVID19 =?= true); ] $(JOB_ROUTER_ENTRIES) @jre Replacing covid19 in set_AcctGroup with the name of the accounting group that you would like to use for COVID-19 jobs. Ensure that COVID-19 jobs match to the new route. Choose one of the options below depending on your HTCondor version ( condor_version ): For versions of HTCondor >= 8.8.7 and < 8.9.0; or HTCondor >= 8.9.6: specify the routes considered by the job router and the order in which they're considered by adding the following configuration to a file in /etc/condor-ce/config.d/ : JOB_ROUTER_ROUTE_NAMES = OSG_COVID19_Jobs, $(JOB_ROUTER_ROUTE_NAMES) For older versions of HTCondor: add (TARGET.IsCOVID19 =!= true) to the Requirements of any existing routes. For example, the following job route: JOB_ROUTER_ENTRIES @=jre [ name = \"Local_Condor\" TargetUniverse = 5; Requirements = (TARGET.Owner =!= \"osg\"); ] @jre Should be updated as follows: JOB_ROUTER_ENTRIES @=jre [ name = \"Local_Condor\" TargetUniverse = 5; Requirements = (TARGET.Owner =!= \"atlas\") && (TARGET.IsCOVID19 =!= true); ] @jre Reconfigure your HTCondor-CE: condor_ce_reconfig Continue onto this section to verify your configuration","title":"For HTCondor batch systems"},{"location":"compute-element/covid-19/#verifying-the-covid-19-job-route","text":"To verify that your HTCondor-CE is configured to support COVID-19 jobs, perform the following steps: Ensure that the OSG_COVID19_Jobs route appears with all of your other previously enabled routes: condor_ce_job_router_info -config Known issue: removing old routes If your HTCondor-CE has jobs associated with a route that is removed from your configuration, this will result in a crashing Job Router. If you accidentally remove an old route, restore the route or remove all jobs associated with said route. Ensure that COVID-19 jobs will match to your new job route: For versions of HTCondor >= 8.8.7 and < 8.9.0; or HTCondor >= 8.9.6: OSG_COVID19_Jobs should be the first route in the routing table: condor_ce_config_val -verbose JOB_ROUTER_ROUTE_NAMES For older versions of HTCondor: the Requirements expresison of your OSG_COVID19_Jobs route must contain (TARGET.IsCOVID19 =?= true) and all other routes must contain (TARGET.IsCOVID19 =!= true) in their Requirements expression. After requesting COVID-19 jobs , verify that jobs are being routed appropriately, by examining pilots with condor_ce_router_q .","title":"Verifying the COVID-19 Job Route"},{"location":"compute-element/covid-19/#requesting-covid-19-jobs","text":"To receive COVID-19 pilot jobs, send an email to help@opensciencegrid.org with the subject Requesting COVID-19 pilots and the following information: Whether you want to receive only COVID-19 jobs, or if you want to accept COVID-19 and other OSG jobs The hostname(s) of your HTCondor-CE(s) Any other restrictions that may apply to these jobs (e.g. number of available cores)","title":"Requesting COVID-19 Jobs"},{"location":"compute-element/covid-19/#viewing-covid-19-contributions","text":"You can view how many hours that COVID-19 projects have consumed at your site with this GRACC dashboard .","title":"Viewing COVID-19 Contributions"},{"location":"compute-element/covid-19/#getting-help","text":"To get assistance, please use this page .","title":"Getting Help"},{"location":"compute-element/hosted-ce/","text":"Requesting an OSG Hosted CE \u00b6 An OSG Hosted Compute Entrypoint (CE) is the entry point for resource requests coming from the OSG; it handles authorization and delegation of resource requests to your existing campus HPC/HTC cluster. Many sites set up their compute entrypoint locally. As an alternative, OSG offers a no-cost Hosted CE option wherein the OSG team will host and operate the HTCondor Compute Entrypoint, and configure it for the communities that you choose to support. This document explains the requirements and the procedure for requesting an OSG Hosted CE. Running more than 10,000 resource requests The Hosted CE can support thousands of concurrent resource request submissions. If you wish to run your own local compute entrypoint or expect to support more than 10,000 concurrently running OSG resource requests, see this page for installing the HTCondor-CE. Before Starting \u00b6 Before preparing your cluster for OSG resource requests, consider the following requirements: An existing compute cluster with a supported batch system running on a supported operating system Outbound network connectivity from the worker nodes (they can be behind NAT) One or more Unix accounts on your cluster's submit server with the following capabilities: Accessible via SSH key Use of SSH remote port forwarding ( AllowTcpForwarding yes ) and SSH multiplexing ( MaxSessions 10 or greater) Permission to submit jobs to your local cluster. Shared user home directories between the submit server and the worker nodes. Not required for HTCondor clusters: see this section for more details. Temporary scratch space on each worker node; site administrators should ensure that files in this directory are regularly cleaned out. OSG resource contributors must inform the OSG of any relevant changes to their site. Site downtimes For an improved turnaround time regarding an outage or downtime at your site, contact us and include downtime in the subject or body of the email. For additional technical details, please consult the reference section below. Don't meet the requirements? If your site does not meet these conditions, please contact us to discuss your options for contributing to the OSG. Scheduling a Planning Consultation \u00b6 Before participating in the OSG, either as a computational resource contributor or consumer, we ask that you contact us to set up a consultation. During this consultation, OSG staff will introduce you and your team to the OSG and develop a plan to meet your resource contribution and/or research goals. Preparing Your Local Cluster \u00b6 After the consultation, ensure that your local cluster meets the requirements as outlined above . In particular, you should now know which accounts to create for the communities that you wish to serve at your cluster. Also consider the size and number of jobs that the OSG should send to your site (e.g., number of cores, memory, GPUs, walltime) as well as their scheduling policy (e.g. preemptible backfill partitions). Additionally, OSG staff may have directed you to follow installation instructions from one or more of the following sections: (Recommended) Providing access to CVMFS \u00b6 Maximize resource utilization; required for GPU support Installing CVMFS on your cluster makes your resources more attractive to OSG user jobs! Additionally, if you plan to contribute GPUs to the OSG, installation of CVMFS is required . Many users in the OSG make of use software modules and/or containers provided by their collaborations or by the OSG Research Facilitation team. In order to support these users without having to install specific software modules on your cluster, you may provide a distributed software repository system called CernVM File System (CVMFS). In order to provide CVMFS at your site, you will need the following: A cluster-wide Frontier Squid proxy service with at least 50GB of cache space; installation instructions for Frontier Squid are provided here . A local CVMFS cache per worker node (10 GB minimum, 20 GB recommended) After setting up the Frontier Squid proxy and worker node local caches, install CVMFS on each worker node. (HTCondor clusters only) Installing the OSG Worker Node Client \u00b6 Skip this section if you have CVMFS or shared home directories! If you have CVMFS installed or shared home directories on your worker nodes, you can skip manual installation of the OSG Worker Node Client. All OSG sites need to provide the OSG Worker Node Client on each worker node in their local cluster. This is normally handled by OSG staff for a Hosted CE but that requires shared home directories across the cluster. However, for sites with an HTCondor batch system, often there is no shared filesystem set up. If you run an HTCondor site and it is easier to install and maintain the Worker Node Client on each worker node than to install CVMFS or maintain shared file system, you have the following options: Install the Worker Node Client from RPM Install the Worker Node Client from tarball Requesting an OSG Hosted CE \u00b6 After preparing your local cluster, apply for a Hosted CE by filling out the cluster integration questionnaire . Your answers will help our operators submit resource requests to your local cluster of the appropriate size and scale. Can I change my answers at a later date? Yes! If you want the OSG to change the size (i.e. CPU, RAM), type (e.g., GPU requests), or number of resource requests, contact us with the FQDN of your login host and the details of your changes. Finalizing Installation \u00b6 After applying for an OSG Hosted CE, our staff will contact you with the following information: IP ranges of OSG hosted services Public SSH key to be installed in the OSG accounts Once this is done, OSG staff will work with you and your team to begin submitting resource requests to your site, first with some tests, then with a steady ramp-up to full production. Validating contributions \u00b6 In addition to any internal validation processes that you may have, the OSG provides monitoring to view which communities and projects within said communities are accessing your site, their fields of science, and home institution. Below is an example of the monitoring views that will be available for your cluster. To view your contributions, select your site from the Facility dropdown of the Payload job summary dashboard. Note that accounting data may take up to 24 hours to display. Reference \u00b6 User accounts \u00b6 Each community in the OSG utilizing the Hosted CEs is mapped to your site as a fixed, specific account; we request the account names are of the form osg01 through osg20 . The mappings from Unix username to community is as follows: User VO(s) Description osg01 OSG Projects (primarily single PI, such as OSG-Connect) supported directly by the OSG organization osg02 GLOW Projects coming from the Center for High Throughput Computing at the University of Wisconsin-Madison osg03 HCC Projects coming from the Holland Computing Center at the University of Nebraska - Lincoln osg04 CMS High-energy physics experiment from the Large Hadron Collider at CERN osg05 Fermilab Experiments from the Fermi National Accelerator Laboratory osg06 JLab Experiments from the Thomas Jefferson National Accelerator Facility osg07 IGWN Gravitational wave detection experiments osg08 IGWN Gravitational wave detection experiments osg09 ATLAS High-energy physics experiment from the Large Hadron Collider at CERN osg10 GlueX Study of quark and gluon degrees of freedom in hadrons using high-energy photons osg11 DUNE Experiment for neutrino science and proton decay studies osg12 IceCube Research based on data from the IceCube neutrino detector osg13 XENON Dark matter search experiment osg14 - osg20 - Unassigned For example, the activities in your batch system corresponding to the user osg02 will always be associated with the GLOW community. Security \u00b6 OSG takes multiple precautions to maintain security and prevent unauthorized usage of resources: Access to the OSG system with SSH keys are restricted to the OSG staff maintaining them Users are carefully vetted before they are allowed to submit jobs to OSG Jobs running through OSG can be traced back to the user that submitted them Job submission can quickly be disabled if needed Our security team is readily contactable in case of an emergency: https://opensciencegrid.org/security/#reporting-a-security-incident How to Get Help \u00b6 If you need help with setup or troubleshooting, contact us .","title":"Request a Hosted CE"},{"location":"compute-element/hosted-ce/#requesting-an-osg-hosted-ce","text":"An OSG Hosted Compute Entrypoint (CE) is the entry point for resource requests coming from the OSG; it handles authorization and delegation of resource requests to your existing campus HPC/HTC cluster. Many sites set up their compute entrypoint locally. As an alternative, OSG offers a no-cost Hosted CE option wherein the OSG team will host and operate the HTCondor Compute Entrypoint, and configure it for the communities that you choose to support. This document explains the requirements and the procedure for requesting an OSG Hosted CE. Running more than 10,000 resource requests The Hosted CE can support thousands of concurrent resource request submissions. If you wish to run your own local compute entrypoint or expect to support more than 10,000 concurrently running OSG resource requests, see this page for installing the HTCondor-CE.","title":"Requesting an OSG Hosted CE"},{"location":"compute-element/hosted-ce/#before-starting","text":"Before preparing your cluster for OSG resource requests, consider the following requirements: An existing compute cluster with a supported batch system running on a supported operating system Outbound network connectivity from the worker nodes (they can be behind NAT) One or more Unix accounts on your cluster's submit server with the following capabilities: Accessible via SSH key Use of SSH remote port forwarding ( AllowTcpForwarding yes ) and SSH multiplexing ( MaxSessions 10 or greater) Permission to submit jobs to your local cluster. Shared user home directories between the submit server and the worker nodes. Not required for HTCondor clusters: see this section for more details. Temporary scratch space on each worker node; site administrators should ensure that files in this directory are regularly cleaned out. OSG resource contributors must inform the OSG of any relevant changes to their site. Site downtimes For an improved turnaround time regarding an outage or downtime at your site, contact us and include downtime in the subject or body of the email. For additional technical details, please consult the reference section below. Don't meet the requirements? If your site does not meet these conditions, please contact us to discuss your options for contributing to the OSG.","title":"Before Starting"},{"location":"compute-element/hosted-ce/#scheduling-a-planning-consultation","text":"Before participating in the OSG, either as a computational resource contributor or consumer, we ask that you contact us to set up a consultation. During this consultation, OSG staff will introduce you and your team to the OSG and develop a plan to meet your resource contribution and/or research goals.","title":"Scheduling a Planning Consultation"},{"location":"compute-element/hosted-ce/#preparing-your-local-cluster","text":"After the consultation, ensure that your local cluster meets the requirements as outlined above . In particular, you should now know which accounts to create for the communities that you wish to serve at your cluster. Also consider the size and number of jobs that the OSG should send to your site (e.g., number of cores, memory, GPUs, walltime) as well as their scheduling policy (e.g. preemptible backfill partitions). Additionally, OSG staff may have directed you to follow installation instructions from one or more of the following sections:","title":"Preparing Your Local Cluster"},{"location":"compute-element/hosted-ce/#recommended-providing-access-to-cvmfs","text":"Maximize resource utilization; required for GPU support Installing CVMFS on your cluster makes your resources more attractive to OSG user jobs! Additionally, if you plan to contribute GPUs to the OSG, installation of CVMFS is required . Many users in the OSG make of use software modules and/or containers provided by their collaborations or by the OSG Research Facilitation team. In order to support these users without having to install specific software modules on your cluster, you may provide a distributed software repository system called CernVM File System (CVMFS). In order to provide CVMFS at your site, you will need the following: A cluster-wide Frontier Squid proxy service with at least 50GB of cache space; installation instructions for Frontier Squid are provided here . A local CVMFS cache per worker node (10 GB minimum, 20 GB recommended) After setting up the Frontier Squid proxy and worker node local caches, install CVMFS on each worker node.","title":"(Recommended) Providing access to CVMFS"},{"location":"compute-element/hosted-ce/#htcondor-clusters-only-installing-the-osg-worker-node-client","text":"Skip this section if you have CVMFS or shared home directories! If you have CVMFS installed or shared home directories on your worker nodes, you can skip manual installation of the OSG Worker Node Client. All OSG sites need to provide the OSG Worker Node Client on each worker node in their local cluster. This is normally handled by OSG staff for a Hosted CE but that requires shared home directories across the cluster. However, for sites with an HTCondor batch system, often there is no shared filesystem set up. If you run an HTCondor site and it is easier to install and maintain the Worker Node Client on each worker node than to install CVMFS or maintain shared file system, you have the following options: Install the Worker Node Client from RPM Install the Worker Node Client from tarball","title":"(HTCondor clusters only) Installing the OSG Worker Node Client"},{"location":"compute-element/hosted-ce/#requesting-an-osg-hosted-ce_1","text":"After preparing your local cluster, apply for a Hosted CE by filling out the cluster integration questionnaire . Your answers will help our operators submit resource requests to your local cluster of the appropriate size and scale. Can I change my answers at a later date? Yes! If you want the OSG to change the size (i.e. CPU, RAM), type (e.g., GPU requests), or number of resource requests, contact us with the FQDN of your login host and the details of your changes.","title":"Requesting an OSG Hosted CE"},{"location":"compute-element/hosted-ce/#finalizing-installation","text":"After applying for an OSG Hosted CE, our staff will contact you with the following information: IP ranges of OSG hosted services Public SSH key to be installed in the OSG accounts Once this is done, OSG staff will work with you and your team to begin submitting resource requests to your site, first with some tests, then with a steady ramp-up to full production.","title":"Finalizing Installation"},{"location":"compute-element/hosted-ce/#validating-contributions","text":"In addition to any internal validation processes that you may have, the OSG provides monitoring to view which communities and projects within said communities are accessing your site, their fields of science, and home institution. Below is an example of the monitoring views that will be available for your cluster. To view your contributions, select your site from the Facility dropdown of the Payload job summary dashboard. Note that accounting data may take up to 24 hours to display.","title":"Validating contributions"},{"location":"compute-element/hosted-ce/#reference","text":"","title":"Reference"},{"location":"compute-element/hosted-ce/#user-accounts","text":"Each community in the OSG utilizing the Hosted CEs is mapped to your site as a fixed, specific account; we request the account names are of the form osg01 through osg20 . The mappings from Unix username to community is as follows: User VO(s) Description osg01 OSG Projects (primarily single PI, such as OSG-Connect) supported directly by the OSG organization osg02 GLOW Projects coming from the Center for High Throughput Computing at the University of Wisconsin-Madison osg03 HCC Projects coming from the Holland Computing Center at the University of Nebraska - Lincoln osg04 CMS High-energy physics experiment from the Large Hadron Collider at CERN osg05 Fermilab Experiments from the Fermi National Accelerator Laboratory osg06 JLab Experiments from the Thomas Jefferson National Accelerator Facility osg07 IGWN Gravitational wave detection experiments osg08 IGWN Gravitational wave detection experiments osg09 ATLAS High-energy physics experiment from the Large Hadron Collider at CERN osg10 GlueX Study of quark and gluon degrees of freedom in hadrons using high-energy photons osg11 DUNE Experiment for neutrino science and proton decay studies osg12 IceCube Research based on data from the IceCube neutrino detector osg13 XENON Dark matter search experiment osg14 - osg20 - Unassigned For example, the activities in your batch system corresponding to the user osg02 will always be associated with the GLOW community.","title":"User accounts"},{"location":"compute-element/hosted-ce/#security","text":"OSG takes multiple precautions to maintain security and prevent unauthorized usage of resources: Access to the OSG system with SSH keys are restricted to the OSG staff maintaining them Users are carefully vetted before they are allowed to submit jobs to OSG Jobs running through OSG can be traced back to the user that submitted them Job submission can quickly be disabled if needed Our security team is readily contactable in case of an emergency: https://opensciencegrid.org/security/#reporting-a-security-incident","title":"Security"},{"location":"compute-element/hosted-ce/#how-to-get-help","text":"If you need help with setup or troubleshooting, contact us .","title":"How to Get Help"},{"location":"compute-element/htcondor-ce-overview/","text":"HTCondor-CE Overview \u00b6 This document serves as an introduction to HTCondor-CE and how it works. Before continuing with the overview, make sure that you are familiar with the following concepts: An OSG site plan What is a batch system and which one will you use ( HTCondor , PBS, LSF, SGE, or SLURM )? Security in the OSG via GSI (i.e., Certificate authorities , user and host certificates , proxies) Pilot jobs, frontends, and factories (i.e., GlideinWMS , AutoPyFactory) What is a Compute Entrypoint? \u00b6 An OSG Compute Entrypoint (CE) is the door for remote organizations to submit requests to temporarily allocate local compute resources. At the heart of the CE is the job gateway software, which is responsible for handling incoming jobs, authenticating and authorizing them, and delegating them to your batch system for execution. Today in the OSG, most jobs that arrive at a CE (called grid jobs ) are not end-user jobs, but rather pilot jobs submitted from factories. Successful pilot jobs create and make available an environment for actual end-user jobs to match and ultimately run within the pilot job container. Eventually pilot jobs remove themselves, typically after a period of inactivity. Note The Compute Entrypoint was previously known as the \"Compute Element\". What is HTCondor-CE? \u00b6 HTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for the OSG. It is configured to use the JobRouter daemon to delegate jobs by transforming and submitting them to the site\u2019s batch system. Benefits of running the HTCondor-CE: Scalability: HTCondor-CE is capable of supporting job workloads of large sites Debugging tools: HTCondor-CE offers many tools to help troubleshoot issues with jobs Routing as configuration: HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs How Jobs Run \u00b6 Once an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the JobRouter creates a transformed copy (called the routed job ) and submits the copy to the batch system (called the batch system job ). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter. Hosted CE over SSH \u00b6 The Hosted CE is intended for small sites or as an introduction to the OSG. The OSG configures and maintains an HTCondor-CE on behalf of the site. The Hosted CE is a special configuration of HTCondor-CE that can submit jobs to a remote cluster over SSH. It provides a simple starting point for opportunistic resource owners that want to start contributing to the OSG with minimal effort: an organization will be able to accept OSG jobs by allowing SSH access to a login node in their cluster. If your site intends to run over 10,000 concurrent OSG jobs, you will need to host your own HTCondor-CE because the Hosted CE has not yet been optimized for such loads. If you are interested in a Hosted CE solution, please follow the instructions on this page . On HTCondor batch systems \u00b6 For a site with an HTCondor batch system , the JobRouter can use HTCondor protocols to place a transformed copy of the grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the same. Thus, there are three representations of your job, each with its own ID (see diagram below): Access point: the HTCondor job ID in the original queue HTCondor-CE: the incoming grid job\u2019s ID HTCondor batch system: the routed job\u2019s ID In an HTCondor-CE/HTCondor setup, files are transferred from HTCondor-CE\u2019s spool directory to the batch system\u2019s spool directory using internal HTCondor protocols. Note The JobRouter copies the job directly into the batch system and does not make use of condor_submit . This means that if the HTCondor batch system is configured to add attributes to incoming jobs when they are submitted (i.e., SUBMIT_EXPRS ), these attributes will not be added to the routed jobs. On other batch systems \u00b6 For non-HTCondor batch systems, the JobRouter transforms the grid job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below): Login node: the HTCondor job ID in the original queue HTCondor-CE: the incoming grid job\u2019s ID and the routed job\u2019s ID HTCondor batch system: the batch system\u2019s job ID Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems: With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its spool directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes. How the CE is Customized \u00b6 Aside from the basic configuration required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all): Deciding which VOs are allowed to run at your site: The recommended method of authorizing VOs at your site is based on the LCMAPS framework How to filter and transform the grid jobs to be run on your batch system: Filtering and transforming grid jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the JobRouter recipes page. Note If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in /etc/condor/ and the other in /etc/condor-ce ) and will need to make sure to differentiate the two when editing any configuration. How Security Works \u00b6 In the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certificates. When these clients and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust. In its default configuration, HTCondor-CE uses GSI-based authentication and authorization to verify the certificate chain, which will work with LCMAPS VOMS authentication . Additionally, it can be reconfigured to provide alternate authentication mechanisms such as Kerberos, SSL, shared secret, or even IP-based authentication. More information about authorization methods can be found here . Next steps \u00b6 Once the basic installation is done, additional activities include: Setting up job routes to customize incoming jobs Submitting jobs to a HTCondor-CE Troubleshooting the HTCondor-CE Register the CE Register with the OSG GlideinWMS factories and/or the ATLAS AutoPyFactory","title":"HTCondor-CE Overview"},{"location":"compute-element/htcondor-ce-overview/#htcondor-ce-overview","text":"This document serves as an introduction to HTCondor-CE and how it works. Before continuing with the overview, make sure that you are familiar with the following concepts: An OSG site plan What is a batch system and which one will you use ( HTCondor , PBS, LSF, SGE, or SLURM )? Security in the OSG via GSI (i.e., Certificate authorities , user and host certificates , proxies) Pilot jobs, frontends, and factories (i.e., GlideinWMS , AutoPyFactory)","title":"HTCondor-CE Overview"},{"location":"compute-element/htcondor-ce-overview/#what-is-a-compute-entrypoint","text":"An OSG Compute Entrypoint (CE) is the door for remote organizations to submit requests to temporarily allocate local compute resources. At the heart of the CE is the job gateway software, which is responsible for handling incoming jobs, authenticating and authorizing them, and delegating them to your batch system for execution. Today in the OSG, most jobs that arrive at a CE (called grid jobs ) are not end-user jobs, but rather pilot jobs submitted from factories. Successful pilot jobs create and make available an environment for actual end-user jobs to match and ultimately run within the pilot job container. Eventually pilot jobs remove themselves, typically after a period of inactivity. Note The Compute Entrypoint was previously known as the \"Compute Element\".","title":"What is a Compute Entrypoint?"},{"location":"compute-element/htcondor-ce-overview/#what-is-htcondor-ce","text":"HTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for the OSG. It is configured to use the JobRouter daemon to delegate jobs by transforming and submitting them to the site\u2019s batch system. Benefits of running the HTCondor-CE: Scalability: HTCondor-CE is capable of supporting job workloads of large sites Debugging tools: HTCondor-CE offers many tools to help troubleshoot issues with jobs Routing as configuration: HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs","title":"What is HTCondor-CE?"},{"location":"compute-element/htcondor-ce-overview/#how-jobs-run","text":"Once an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the JobRouter creates a transformed copy (called the routed job ) and submits the copy to the batch system (called the batch system job ). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter.","title":"How Jobs Run"},{"location":"compute-element/htcondor-ce-overview/#hosted-ce-over-ssh","text":"The Hosted CE is intended for small sites or as an introduction to the OSG. The OSG configures and maintains an HTCondor-CE on behalf of the site. The Hosted CE is a special configuration of HTCondor-CE that can submit jobs to a remote cluster over SSH. It provides a simple starting point for opportunistic resource owners that want to start contributing to the OSG with minimal effort: an organization will be able to accept OSG jobs by allowing SSH access to a login node in their cluster. If your site intends to run over 10,000 concurrent OSG jobs, you will need to host your own HTCondor-CE because the Hosted CE has not yet been optimized for such loads. If you are interested in a Hosted CE solution, please follow the instructions on this page .","title":"Hosted CE over SSH"},{"location":"compute-element/htcondor-ce-overview/#on-htcondor-batch-systems","text":"For a site with an HTCondor batch system , the JobRouter can use HTCondor protocols to place a transformed copy of the grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the same. Thus, there are three representations of your job, each with its own ID (see diagram below): Access point: the HTCondor job ID in the original queue HTCondor-CE: the incoming grid job\u2019s ID HTCondor batch system: the routed job\u2019s ID In an HTCondor-CE/HTCondor setup, files are transferred from HTCondor-CE\u2019s spool directory to the batch system\u2019s spool directory using internal HTCondor protocols. Note The JobRouter copies the job directly into the batch system and does not make use of condor_submit . This means that if the HTCondor batch system is configured to add attributes to incoming jobs when they are submitted (i.e., SUBMIT_EXPRS ), these attributes will not be added to the routed jobs.","title":"On HTCondor batch systems"},{"location":"compute-element/htcondor-ce-overview/#on-other-batch-systems","text":"For non-HTCondor batch systems, the JobRouter transforms the grid job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below): Login node: the HTCondor job ID in the original queue HTCondor-CE: the incoming grid job\u2019s ID and the routed job\u2019s ID HTCondor batch system: the batch system\u2019s job ID Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems: With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its spool directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.","title":"On other batch systems"},{"location":"compute-element/htcondor-ce-overview/#how-the-ce-is-customized","text":"Aside from the basic configuration required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all): Deciding which VOs are allowed to run at your site: The recommended method of authorizing VOs at your site is based on the LCMAPS framework How to filter and transform the grid jobs to be run on your batch system: Filtering and transforming grid jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the JobRouter recipes page. Note If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in /etc/condor/ and the other in /etc/condor-ce ) and will need to make sure to differentiate the two when editing any configuration.","title":"How the CE is Customized"},{"location":"compute-element/htcondor-ce-overview/#how-security-works","text":"In the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certificates. When these clients and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust. In its default configuration, HTCondor-CE uses GSI-based authentication and authorization to verify the certificate chain, which will work with LCMAPS VOMS authentication . Additionally, it can be reconfigured to provide alternate authentication mechanisms such as Kerberos, SSL, shared secret, or even IP-based authentication. More information about authorization methods can be found here .","title":"How Security Works"},{"location":"compute-element/htcondor-ce-overview/#next-steps","text":"Once the basic installation is done, additional activities include: Setting up job routes to customize incoming jobs Submitting jobs to a HTCondor-CE Troubleshooting the HTCondor-CE Register the CE Register with the OSG GlideinWMS factories and/or the ATLAS AutoPyFactory","title":"Next steps"},{"location":"compute-element/install-htcondor-ce/","text":"Installing and Maintaining HTCondor-CE \u00b6 The HTCondor-CE software is a job gateway for an OSG Compute Entrypoint (CE). As such, the OSG will submit resource allocation requests (RARs) jobs to your HTCondor-CE and it will handle authorization and delegation of RARs to your local batch system. In OSG today, RARs are sent to CEs as pilot jobs from a factory, which in turn are able to accept and run end-user jobs. See the upstream documentation for a more detailed introduction. Use this page to learn how to install, configure, run, test, and troubleshoot an OSG HTCondor-CE. OSG Hosted CE Unless you plan on running more than 10k concurrently running RARs or plan on making frequent configuration changes, we suggest requesting an OSG Hosted CE . Note If you are installing an HTCondor-CE for use outside of the OSG, consult the upstream documentation instead. Before Starting \u00b6 Before starting the installation process, consider the following points, consulting the upstream references as needed ( HTCondor-CE 5 , HTCondor-CE 4 ): User IDs: If they do not exist already, the installation will create the Linux users condor (UID 4716) and gratia (UID 42401) SSL certificate: The HTCondor-CE service uses a host certificate at /etc/grid-security/hostcert.pem and an accompanying key at /etc/grid-security/hostkey.pem DNS entries: Forward and reverse DNS must resolve for the HTCondor-CE host Network ports: The pilot factories must be able to contact your HTCondor-CE service on port 9619 (TCP) Access point/login node: HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster File Systems : Non-HTCondor batch systems require a shared file system between the HTCondor-CE host and the batch system worker nodes. As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Install CA certificates Choosing the OSG Yum Repository \u00b6 Before considering OSG 3.6\u2026 Due to potentially disruptive changes in protocols, contact your virtual organization(s) (VO) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series The OSG distributes different versions of HTCondor-CE and HTCondor in separate YUM repositories . Most notably, the repository that you choose will determine the types of credentials that your CE is able to accept. Use the following table to decide OSG YUM repository to install HTCondor-CE: YUM Repository Bearer Tokens GSI and VOMS OSG 3.5 upcoming (recommended) : HTCondor-CE 5, HTCondor 9.0 \u2705 \u2705 OSG 3.5 release: HTCondor-CE 4, HTCondor 8.8 \u2705 OSG 3.6 release: HTCondor-CE 5, HTCondor 9.0 \u2705 Installing HTCondor-CE \u00b6 An HTCondor-CE installation consists of the job gateway (i.e., the HTCondor-CE job router) and other support software (e.g., osg-configure , a Gratia probe for OSG accounting). To simplify installation, OSG provides convenience RPMs that install all required software. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages (Optional) If your batch system is already installed via non-RPM means and is in the following list, install the appropriate 'empty' RPM. Otherwise, skip to the next step. If your batch system is\u2026 Then run the following command\u2026 HTCondor yum install empty-condor --enablerepo=osg-empty SLURM yum install empty-slurm --enablerepo=osg-empty (Optional) If your HTCondor batch system is already installed via non-OSG RPM means, add the line below to /etc/yum.repos.d/osg.repo . Otherwise, skip to the next step. exclude=condor Select the appropriate convenience RPM: If your batch system is... Then use the following package... HTCondor osg-ce-condor LSF osg-ce-lsf PBS osg-ce-pbs SGE osg-ce-sge SLURM osg-ce-slurm Install the CE software where <PACKAGE> is the package you selected in the above step.: If you have decided to install from 3.5 upcoming , run the following command root@host # yum install --enablerepo = osg-upcoming <PACKAGE> Otherwise, run the following command: root@host # yum install <PACKAGE> Configuring HTCondor-CE \u00b6 There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on optional configurations . Configuring the local batch system \u00b6 To configure HTCondor-CE to integrate with your local batch system, please refer to the upstream documentation based on your installed version of HTCondor-CE: HTCondor-CE 5 HTCondor-CE 4 Configuring authentication \u00b6 Depending on the OSG repository from which you have installed HTCondor-CE, you can allow pilot job submission to your CE based on X.509 proxies (i.e., GSI and VOMS), bearer tokens, or both. GSI and VOMS (OSG 3.5 only) \u00b6 To configure which VOs and users are authorized to submit pilot jobs to your HTCondor-CE, follow the instructions in the LCMAPS VOMS plugin document . Bearer Tokens (OSG 3.5 upcoming, OSG 3.6) \u00b6 To configure which VOs are authorized to submit pilot jobs to your HTCondor-CE, consult the \"SciTokens\" section of the upstream documentation . Automatic configuration \u00b6 The OSG CE metapackage brings along a configuration tool, osg-configure , that is designed to automatically configure the different pieces of software required for an OSG HTCondor-CE: Enable your batch system in the HTCondor-CE configuration by editing the enabled field in the /etc/osg/config.d/20-<YOUR BATCH SYSTEM>.ini : enabled = True Read through the other .ini files in the /etc/osg/config.d directory and Validate the configuration settings root@host # osg-configure -v Fix any errors (at least) that osg-configure reports. Once the validation command succeeds without errors, apply the configuration settings: root@host # osg-configure -c Optional configuration \u00b6 In addition to the configurations above, you may need to further configure how pilot jobs are filtered and transformed before they are submitted to your local batch system or otherwise change the behavior of your CE. For detailed instructions, please refer to the upstream documentation based on your installed version of HTCondor-CE: HTCondor-CE 5 Configuring the Job Router Optional configuration HTCondor-CE 4 Configuring the Job Router Optional configuration Accounting with multiple CEs or local user jobs \u00b6 Note For non-HTCondor batch systems only If your site has multiple CEs or you have non-grid users submitting to the same local batch system, the OSG accounting software needs to be configured so that it doesn't over report the number of jobs. Determine which file you need to modify For OSG 3.5 installations, use the following table: If your batch system is\u2026 Then edit the following file on each of your CE(s)\u2026 LSF /etc/gratia/pbs-lsf/ProbeConfig PBS /etc/gratia/pbs-lsf/ProbeConfig SGE /etc/gratia/sge/ProbeConfig SLURM /etc/gratia/slurm/ProbeConfig For OSG 3.6 installations, you'll need to modify /etc/gratia/htcondor-ce/ProbeConfig Edit the value of SuppressNoDNRecords on each of your CE's so that it reads: SuppressNoDNRecords=\"1\" Starting and Validating HTCondor-CE \u00b6 For information on how to start and validate the core HTCondor-CE services, please refer to the upstream documentation based on your installed version of HTCondor-CE: HTCondor-CE 5 HTCondor-CE 4 Enabling OSG accounting (OSG 3.5 only) \u00b6 In addition to the core HTCondor-CE services, an OSG 3.5 HTCondor-CE must also start and enable the accounting service, gratia-probes-cron : root@host # systemctl start gratia-probes-cron root@host # systemctl enable gratia-probes-cron In OSG 3.6, OSG accounting is managed directly by HTCondor-CE (see the update instructions for more details). Troubleshooting HTCondor-CE \u00b6 For information on how to troubleshoot your HTCondor-CE, please refer to the upstream documentation based on your installed version of HTCondor-CE: HTCondor-CE 5: Common issues Debugging tools Helpful logs HTCondor-CE 4 Common issues Debugging tools Helpful logs Registering the CE \u00b6 To contribute to the the OSG Production Grid, your CE must be registered with the OSG . To register your resource: Identify the facility, site, and resource group where your HTCondor-CE is hosted. For example, the Center for High Throughput Computing at the University of Wisconsin-Madison uses the following information: Facility: University of Wisconsin Site: CHTC Resource Group: CHTC Using the above information, create or update the appropriate YAML file, using this template as a guide. Getting Help \u00b6 To get assistance, please use the this page .","title":"Install HTCondor-CE"},{"location":"compute-element/install-htcondor-ce/#installing-and-maintaining-htcondor-ce","text":"The HTCondor-CE software is a job gateway for an OSG Compute Entrypoint (CE). As such, the OSG will submit resource allocation requests (RARs) jobs to your HTCondor-CE and it will handle authorization and delegation of RARs to your local batch system. In OSG today, RARs are sent to CEs as pilot jobs from a factory, which in turn are able to accept and run end-user jobs. See the upstream documentation for a more detailed introduction. Use this page to learn how to install, configure, run, test, and troubleshoot an OSG HTCondor-CE. OSG Hosted CE Unless you plan on running more than 10k concurrently running RARs or plan on making frequent configuration changes, we suggest requesting an OSG Hosted CE . Note If you are installing an HTCondor-CE for use outside of the OSG, consult the upstream documentation instead.","title":"Installing and Maintaining HTCondor-CE"},{"location":"compute-element/install-htcondor-ce/#before-starting","text":"Before starting the installation process, consider the following points, consulting the upstream references as needed ( HTCondor-CE 5 , HTCondor-CE 4 ): User IDs: If they do not exist already, the installation will create the Linux users condor (UID 4716) and gratia (UID 42401) SSL certificate: The HTCondor-CE service uses a host certificate at /etc/grid-security/hostcert.pem and an accompanying key at /etc/grid-security/hostkey.pem DNS entries: Forward and reverse DNS must resolve for the HTCondor-CE host Network ports: The pilot factories must be able to contact your HTCondor-CE service on port 9619 (TCP) Access point/login node: HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster File Systems : Non-HTCondor batch systems require a shared file system between the HTCondor-CE host and the batch system worker nodes. As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Install CA certificates","title":"Before Starting"},{"location":"compute-element/install-htcondor-ce/#choosing-the-osg-yum-repository","text":"Before considering OSG 3.6\u2026 Due to potentially disruptive changes in protocols, contact your virtual organization(s) (VO) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series The OSG distributes different versions of HTCondor-CE and HTCondor in separate YUM repositories . Most notably, the repository that you choose will determine the types of credentials that your CE is able to accept. Use the following table to decide OSG YUM repository to install HTCondor-CE: YUM Repository Bearer Tokens GSI and VOMS OSG 3.5 upcoming (recommended) : HTCondor-CE 5, HTCondor 9.0 \u2705 \u2705 OSG 3.5 release: HTCondor-CE 4, HTCondor 8.8 \u2705 OSG 3.6 release: HTCondor-CE 5, HTCondor 9.0 \u2705","title":"Choosing the OSG Yum Repository"},{"location":"compute-element/install-htcondor-ce/#installing-htcondor-ce","text":"An HTCondor-CE installation consists of the job gateway (i.e., the HTCondor-CE job router) and other support software (e.g., osg-configure , a Gratia probe for OSG accounting). To simplify installation, OSG provides convenience RPMs that install all required software. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages (Optional) If your batch system is already installed via non-RPM means and is in the following list, install the appropriate 'empty' RPM. Otherwise, skip to the next step. If your batch system is\u2026 Then run the following command\u2026 HTCondor yum install empty-condor --enablerepo=osg-empty SLURM yum install empty-slurm --enablerepo=osg-empty (Optional) If your HTCondor batch system is already installed via non-OSG RPM means, add the line below to /etc/yum.repos.d/osg.repo . Otherwise, skip to the next step. exclude=condor Select the appropriate convenience RPM: If your batch system is... Then use the following package... HTCondor osg-ce-condor LSF osg-ce-lsf PBS osg-ce-pbs SGE osg-ce-sge SLURM osg-ce-slurm Install the CE software where <PACKAGE> is the package you selected in the above step.: If you have decided to install from 3.5 upcoming , run the following command root@host # yum install --enablerepo = osg-upcoming <PACKAGE> Otherwise, run the following command: root@host # yum install <PACKAGE>","title":"Installing HTCondor-CE"},{"location":"compute-element/install-htcondor-ce/#configuring-htcondor-ce","text":"There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on optional configurations .","title":"Configuring HTCondor-CE"},{"location":"compute-element/install-htcondor-ce/#configuring-the-local-batch-system","text":"To configure HTCondor-CE to integrate with your local batch system, please refer to the upstream documentation based on your installed version of HTCondor-CE: HTCondor-CE 5 HTCondor-CE 4","title":"Configuring the local batch system"},{"location":"compute-element/install-htcondor-ce/#configuring-authentication","text":"Depending on the OSG repository from which you have installed HTCondor-CE, you can allow pilot job submission to your CE based on X.509 proxies (i.e., GSI and VOMS), bearer tokens, or both.","title":"Configuring authentication"},{"location":"compute-element/install-htcondor-ce/#gsi-and-voms-osg-35-only","text":"To configure which VOs and users are authorized to submit pilot jobs to your HTCondor-CE, follow the instructions in the LCMAPS VOMS plugin document .","title":"GSI and VOMS (OSG 3.5 only)"},{"location":"compute-element/install-htcondor-ce/#bearer-tokens-osg-35-upcoming-osg-36","text":"To configure which VOs are authorized to submit pilot jobs to your HTCondor-CE, consult the \"SciTokens\" section of the upstream documentation .","title":"Bearer Tokens (OSG 3.5 upcoming, OSG 3.6)"},{"location":"compute-element/install-htcondor-ce/#automatic-configuration","text":"The OSG CE metapackage brings along a configuration tool, osg-configure , that is designed to automatically configure the different pieces of software required for an OSG HTCondor-CE: Enable your batch system in the HTCondor-CE configuration by editing the enabled field in the /etc/osg/config.d/20-<YOUR BATCH SYSTEM>.ini : enabled = True Read through the other .ini files in the /etc/osg/config.d directory and Validate the configuration settings root@host # osg-configure -v Fix any errors (at least) that osg-configure reports. Once the validation command succeeds without errors, apply the configuration settings: root@host # osg-configure -c","title":"Automatic configuration"},{"location":"compute-element/install-htcondor-ce/#optional-configuration","text":"In addition to the configurations above, you may need to further configure how pilot jobs are filtered and transformed before they are submitted to your local batch system or otherwise change the behavior of your CE. For detailed instructions, please refer to the upstream documentation based on your installed version of HTCondor-CE: HTCondor-CE 5 Configuring the Job Router Optional configuration HTCondor-CE 4 Configuring the Job Router Optional configuration","title":"Optional configuration"},{"location":"compute-element/install-htcondor-ce/#accounting-with-multiple-ces-or-local-user-jobs","text":"Note For non-HTCondor batch systems only If your site has multiple CEs or you have non-grid users submitting to the same local batch system, the OSG accounting software needs to be configured so that it doesn't over report the number of jobs. Determine which file you need to modify For OSG 3.5 installations, use the following table: If your batch system is\u2026 Then edit the following file on each of your CE(s)\u2026 LSF /etc/gratia/pbs-lsf/ProbeConfig PBS /etc/gratia/pbs-lsf/ProbeConfig SGE /etc/gratia/sge/ProbeConfig SLURM /etc/gratia/slurm/ProbeConfig For OSG 3.6 installations, you'll need to modify /etc/gratia/htcondor-ce/ProbeConfig Edit the value of SuppressNoDNRecords on each of your CE's so that it reads: SuppressNoDNRecords=\"1\"","title":"Accounting with multiple CEs or local user jobs"},{"location":"compute-element/install-htcondor-ce/#starting-and-validating-htcondor-ce","text":"For information on how to start and validate the core HTCondor-CE services, please refer to the upstream documentation based on your installed version of HTCondor-CE: HTCondor-CE 5 HTCondor-CE 4","title":"Starting and Validating HTCondor-CE"},{"location":"compute-element/install-htcondor-ce/#enabling-osg-accounting-osg-35-only","text":"In addition to the core HTCondor-CE services, an OSG 3.5 HTCondor-CE must also start and enable the accounting service, gratia-probes-cron : root@host # systemctl start gratia-probes-cron root@host # systemctl enable gratia-probes-cron In OSG 3.6, OSG accounting is managed directly by HTCondor-CE (see the update instructions for more details).","title":"Enabling OSG accounting (OSG 3.5 only)"},{"location":"compute-element/install-htcondor-ce/#troubleshooting-htcondor-ce","text":"For information on how to troubleshoot your HTCondor-CE, please refer to the upstream documentation based on your installed version of HTCondor-CE: HTCondor-CE 5: Common issues Debugging tools Helpful logs HTCondor-CE 4 Common issues Debugging tools Helpful logs","title":"Troubleshooting HTCondor-CE"},{"location":"compute-element/install-htcondor-ce/#registering-the-ce","text":"To contribute to the the OSG Production Grid, your CE must be registered with the OSG . To register your resource: Identify the facility, site, and resource group where your HTCondor-CE is hosted. For example, the Center for High Throughput Computing at the University of Wisconsin-Madison uses the following information: Facility: University of Wisconsin Site: CHTC Resource Group: CHTC Using the above information, create or update the appropriate YAML file, using this template as a guide.","title":"Registering the CE"},{"location":"compute-element/install-htcondor-ce/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"compute-element/job-router-recipes/","text":"Up-to-date documentation can be found at https://opensciencegrid.org/docs/compute-element/install-htcondor-ce/","title":"Job router recipes"},{"location":"compute-element/submit-htcondor-ce/","text":"Up-to-date documentation can be found at https://opensciencegrid.org/docs/compute-element/install-htcondor-ce/","title":"Submit htcondor ce"},{"location":"compute-element/troubleshoot-htcondor-ce/","text":"Up-to-date documentation can be found at https://opensciencegrid.org/docs/compute-element/install-htcondor-ce/","title":"Troubleshoot htcondor ce"},{"location":"data/external-oasis-repos/","text":"Install an OASIS Repository \u00b6 OASIS (the OSG A pplication S oftware I nstallation S ervice) is an infrastructure, based on CVMFS , for distributing software throughout the OSG. Once software is installed into an OASIS repository, the goal is to make it available across about 90% of the OSG within an hour. OASIS consists of keysigning infrastructure, a content distribution network (CDN), and a shared CVMFS repository that is hosted by the OSG. Many use cases will be covered by utilizing the shared repository ; this document covers how to install, configure, and host your own CVMFS repository server . This server will distribute software via OASIS, but will be hosted and operated externally from the OSG project. OASIS-based distribution and key signing is available to OSG VOs or repositories affiliated with an OSG VO. See the policy page for more information on what repositories OSG is willing to distribute. Before Starting \u00b6 CVMFS repositories work at the kernel filesystem layer, which adds more stringent host requirements than a typical OSG install. The host OS must meet ONE of the following: RHEL 7.3 (or equivalent) or later. This option is recommended . Additionally, User IDs: If it does not exist already, the installation will create the cvmfs Linux user Group IDs: If they do not exist already, the installation will create the Linux groups cvmfs and fuse Network ports: This page will configure the repository to distribute using Apache HTTPD on port 8000. At the minimum, the repository needs in-bound access from the OASIS CDN. Disk space: This host will need enough free disk space to host two copies of the software: one compressed and one uncompressed. /srv/cvmfs will hold all the published data (compressed and de-deuplicated). The /var/spool/cvmfs directory will contain all the data in all current transactions (uncompressed). Root access will be needed to install. Software install will be done as an unprivileged user. Yum will need to be configured to use the OSG repositories . Overlay-FS limitations CVMFS on RHEL7 only supports Overlay-FS if the underlying filesystem is ext3 or ext4 ; make sure /var/spool/cvmfs is one of these filesystem types. If this is not possible, add CVMFS_DONT_CHECK_OVERLAYFS_VERSION=yes to your CVMFS configuration. Using xfs will work if it was created with ftype=1 Installation \u00b6 Installation is a straightforward install via yum : root@host # yum install cvmfs-server osg-oasis Apache and Repository Mounts \u00b6 For all installs, we recommend mounting all the local repositories on startup: root@host # echo \"cvmfs_server mount -a\" >>/etc/rc.local root@host # chmod +x /etc/rc.local The Apache HTTPD service should be configured to listen on port 8000, have the KeepAlive option enabled, and be started: root@host # echo Listen 8000 >>/etc/httpd/conf.d/cvmfs.conf root@host # echo KeepAlive on >>/etc/httpd/conf.d/cvmfs.conf root@host # chkconfig httpd on root@host # service httpd start Check Firewalls Make sure that port 8000 is available to the Internet. Check the setting of the host- and site-level firewalls. The next steps will fail if the web server is not accessible. Creating a Repository \u00b6 Prior to creation, the repository administrator will need to make two decisions: Select a repository name ; typically, this is derived from the VO or project's name and ends in opensciencegrid.org . For example, the NoVA VO runs the repository nova.opensciencegrid.org . For this section, we will use <EXAMPLE.OPENSCIENCEGRID.ORG> . Select a repository owner : Software publication will need to run by a non- root Unix user account; for this document, we will use <LIBRARIAN> as the account name of the repository owner. The initial repository creation must be run as root : root@host # echo -e \"\\*\\\\t\\\\t-\\\\tnofile\\\\t\\\\t16384\" >>/etc/security/limits.conf root@host # ulimit -n 16384 root@host # cvmfs_server mkfs -o <LIBRARIAN> <EXAMPLE.OPENSCIENCEGRID.ORG> root@host # cat >/srv/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.htaccess <<xEOFx Order deny,allow Deny from all Allow from 127.0.0.1 Allow from ::1 Allow from 129.79.53.0/24 129.93.244.192/26 129.93.227.64/26 Allow from 2001:18e8:2:6::/56 2600:900:6::/48 xEOFx Here, we increase the number of open files allowed, create the repository using the mkfs command, and then limit the hosts that are allowed to access the repo to the OSG CDN. Next, adjust the configuration in the repository as follows. root@host # cat >>/etc/cvmfs/repositories.d/<EXAMPLE.OPENSCIENCEGRID.ORG>/server.conf <<xEOFx CVMFS_AUTO_TAG_TIMESPAN=\"2 weeks ago\" CVMFS_IGNORE_XDIR_HARDLINKS=true CVMFS_GENERATE_LEGACY_BULK_CHUNKS=false CVMFS_AUTOCATALOGS=true CVMFS_ENFORCE_LIMITS=true CVMFS_FORCE_REMOUNT_WARNING=false xEOFx Additionally, especially if files will be frequently deleted, enabling garbage collection is recommended in this way: root@host # cat >>/etc/cvmfs/repositories.d/<EXAMPLE.OPENSCIENCEGRID.ORG>/server.conf <<xEOFx CVMFS_GARBAGE_COLLECTION=true CVMFS_AUTO_GC=false xEOFx The above assumes that you have your own mechanism to run cvmfs_server gc regularly (typically daily) at a time when it won't interfere with publications, since garbage collection and publication can't be done at the same time. CVMFS_AUTO_GC=true will automatically run garbage collection periodically after publications, but those times are not always convenient. Also, check the cvmfs documentation for additional recommendations for special purpose repositories. Now verify that the repository is readable over HTTP: root@host # wget -qO- http://localhost:8000/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.cvmfswhitelist | cat -v That should print several lines including some gibberish at the end. Hosting a Repository on OASIS \u00b6 In order to host a repository on OASIS, perform the following steps: Verify your VO's registration is up-to-date . All repositories need to be associated with a VO; the VO needs to assign an OASIS manager in Topology who would be responsible for the contents of any of the VO's repositories and will be contacted in case of issues. To designate an OASIS manager, have the VO manager update the Topology registration . Create a support ticket using the following template: Please add a new CVMFS repository to OASIS for VO <VO NAME> using the URL http://<FQDN>:8000/cvmfs/<OASIS REPOSITORY> The VO responsible manager will be <OASIS MANAGER>. Replace the <ANGLE BRACKET TEXT> items with the appropriate values. If the repository name matches *.opensciencegrid.org or *.osgstorage.org , wait for the go-ahead from the OSG representative before continuing with the remaining instructions; for all other repositories (such as *.egi.eu ), you are done. When you are told in the ticket to proceed to the next step, first if the repository might be in a transaction abort it: root@host # su <LIBRARIAN> -c \"cvmfs_server abort <EXAMPLE.OPENSCIENCEGRID.ORG>\" Then execute the following commands: root@host # wget -O /srv/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.cvmfswhitelist \\ http://oasis.opensciencegrid.org/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.cvmfswhitelist root@host # cp /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub \\ /etc/cvmfs/keys/<EXAMPLE.OPENSCIENCEGRID.ORG>.pub Replace <EXAMPLE.OPENSCIENCEGRID.ORG> as appropriate. If the cp command prompts about overwriting an existing file, type 'y'. Verify that publishing operation succeeds: root@host # su <LIBRARIAN> -c \"cvmfs_server transaction <EXAMPLE.OPENSCIENCEGRID.ORG>\" root@host # su <LIBRARIAN> -c \"cvmfs_server publish <EXAMPLE.OPENSCIENCEGRID.ORG>\" Within an hour, the repository updates should appear at the OSG Operations and FNAL Stratum-1 servers. On success, make sure the whitelist update happens daily by creating /etc/cron.d/fetch-cvmfs-whitelist with the following contents: 5 4 * * * <LIBRARIAN> cd /srv/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG> && wget -qO .cvmfswhitelist.new http://oasis.opensciencegrid.org/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.cvmfswhitelist && mv .cvmfswhitelist.new .cvmfswhitelist Note This cronjob eliminates the need for the repository service administrator to periodically use cvmfs_server resign to update .cvmfswhitelist as described in the upstream CVMFS documentation. Update the open support ticket to indicate that the previous steps have been completed Once the repository is fully replicated on the OSG, the VO may proceed in publishing into CVMFS using the <LIBRARIAN> account on the repository server. Tip We strongly recommend the repository maintainer read through the upstream documentation on maintaining repositories and content limitations . If the repository ends in .opensciencegrid.org or .osgstorage.org , the VO may ask for it to be replicated outside the US. The VO should open a GGUS ticket following EGI's PROC20 . Replacing an Existing OASIS Repository Server \u00b6 If a need arises to replace a server for an existing *.opensciencegrid.org or *.osgstorage.org repository, there are two ways to do it: one without changing the DNS name and one with changing it. The latter can take longer because it requires OSG Operations intervention. Revision numbers must increase CVMFS does not allow repository revision numbers to decrease, so the instructions below make sure the revision numbers only go up. Without changing the server DNS name \u00b6 If you are recreating the repository on the same machine, use the following command to remove the repository configuration while preserving the data and keys: root@host # cvmfs_server rmfs -p <EXAMPLE.OPENSCIENCEGRID.ORG> Otherwise if it is a new machine, copy the keys from /etc/cvmfs/keys/ <EXAMPLE.OPENSCIENCEGRID.ORG> .* and the data from /srv/cvmfs/ <EXAMPLE.OPENSCIENCEGRID.ORG> from the old server to the new, making sure that no publish operations happen on the old server while you copy the data. Then in either case use cvmfs_server import instead of cvmfs_server mkfs in the above instructions for Creating the Repository , in order to reuse old data and keys. If you run an old and a new machine in parallel for a while, make sure that when you put the new machine into production (by moving the DNS name) that the new machine has had at least as many publishes as the old machine, so the revision number does not decrease. With changing the server DNS name \u00b6 Note If you create a repository from scratch, as opposed to copying the data and keys from an old server, it is in fact better to change the DNS name of the server because that causes the OSG Operations server to reinitialize the .cvmfswhitelist. If you create a replacement repository on a new machine from scratch, follow the normal instructions on this page above, but with the following differences in the Hosting a Repository on OASIS section: In step 2, instead of asking in the support ticket to create a new repository, give the new URL and ask them to change the repository registration to that URL. When you do the publish in step 5, add a -n NNNN option where NNNN is a revision number greater than the number on the existing repository. That number can be found by this command on a client machine: user@host $ attr -qg revision /cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG> Skip step 6; there is no need to tell OSG Operations when you are finished. After enough time has elapsed for the publish to propagate to clients, typically around 15 minutes, verify that the new chosen revision has reached a client. Removing a Repository from OASIS \u00b6 In order to remove a repository that is being hosted on OASIS, perform the following steps: If the repository has been replicated outside of the U.S., open a GGUS ticket asking that the replication be removed from EGI Stratum-1s. Wait until this ticket is resolved before proceeding. Open a support ticket asking to shut down the repository, giving the repository name (e.g., <EXAMPLE.OPENSCIENCEGRID.ORG> ), and the corresponding VO.","title":"Install an OASIS Repo"},{"location":"data/external-oasis-repos/#install-an-oasis-repository","text":"OASIS (the OSG A pplication S oftware I nstallation S ervice) is an infrastructure, based on CVMFS , for distributing software throughout the OSG. Once software is installed into an OASIS repository, the goal is to make it available across about 90% of the OSG within an hour. OASIS consists of keysigning infrastructure, a content distribution network (CDN), and a shared CVMFS repository that is hosted by the OSG. Many use cases will be covered by utilizing the shared repository ; this document covers how to install, configure, and host your own CVMFS repository server . This server will distribute software via OASIS, but will be hosted and operated externally from the OSG project. OASIS-based distribution and key signing is available to OSG VOs or repositories affiliated with an OSG VO. See the policy page for more information on what repositories OSG is willing to distribute.","title":"Install an OASIS Repository"},{"location":"data/external-oasis-repos/#before-starting","text":"CVMFS repositories work at the kernel filesystem layer, which adds more stringent host requirements than a typical OSG install. The host OS must meet ONE of the following: RHEL 7.3 (or equivalent) or later. This option is recommended . Additionally, User IDs: If it does not exist already, the installation will create the cvmfs Linux user Group IDs: If they do not exist already, the installation will create the Linux groups cvmfs and fuse Network ports: This page will configure the repository to distribute using Apache HTTPD on port 8000. At the minimum, the repository needs in-bound access from the OASIS CDN. Disk space: This host will need enough free disk space to host two copies of the software: one compressed and one uncompressed. /srv/cvmfs will hold all the published data (compressed and de-deuplicated). The /var/spool/cvmfs directory will contain all the data in all current transactions (uncompressed). Root access will be needed to install. Software install will be done as an unprivileged user. Yum will need to be configured to use the OSG repositories . Overlay-FS limitations CVMFS on RHEL7 only supports Overlay-FS if the underlying filesystem is ext3 or ext4 ; make sure /var/spool/cvmfs is one of these filesystem types. If this is not possible, add CVMFS_DONT_CHECK_OVERLAYFS_VERSION=yes to your CVMFS configuration. Using xfs will work if it was created with ftype=1","title":"Before Starting"},{"location":"data/external-oasis-repos/#installation","text":"Installation is a straightforward install via yum : root@host # yum install cvmfs-server osg-oasis","title":"Installation"},{"location":"data/external-oasis-repos/#apache-and-repository-mounts","text":"For all installs, we recommend mounting all the local repositories on startup: root@host # echo \"cvmfs_server mount -a\" >>/etc/rc.local root@host # chmod +x /etc/rc.local The Apache HTTPD service should be configured to listen on port 8000, have the KeepAlive option enabled, and be started: root@host # echo Listen 8000 >>/etc/httpd/conf.d/cvmfs.conf root@host # echo KeepAlive on >>/etc/httpd/conf.d/cvmfs.conf root@host # chkconfig httpd on root@host # service httpd start Check Firewalls Make sure that port 8000 is available to the Internet. Check the setting of the host- and site-level firewalls. The next steps will fail if the web server is not accessible.","title":"Apache and Repository Mounts"},{"location":"data/external-oasis-repos/#creating-a-repository","text":"Prior to creation, the repository administrator will need to make two decisions: Select a repository name ; typically, this is derived from the VO or project's name and ends in opensciencegrid.org . For example, the NoVA VO runs the repository nova.opensciencegrid.org . For this section, we will use <EXAMPLE.OPENSCIENCEGRID.ORG> . Select a repository owner : Software publication will need to run by a non- root Unix user account; for this document, we will use <LIBRARIAN> as the account name of the repository owner. The initial repository creation must be run as root : root@host # echo -e \"\\*\\\\t\\\\t-\\\\tnofile\\\\t\\\\t16384\" >>/etc/security/limits.conf root@host # ulimit -n 16384 root@host # cvmfs_server mkfs -o <LIBRARIAN> <EXAMPLE.OPENSCIENCEGRID.ORG> root@host # cat >/srv/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.htaccess <<xEOFx Order deny,allow Deny from all Allow from 127.0.0.1 Allow from ::1 Allow from 129.79.53.0/24 129.93.244.192/26 129.93.227.64/26 Allow from 2001:18e8:2:6::/56 2600:900:6::/48 xEOFx Here, we increase the number of open files allowed, create the repository using the mkfs command, and then limit the hosts that are allowed to access the repo to the OSG CDN. Next, adjust the configuration in the repository as follows. root@host # cat >>/etc/cvmfs/repositories.d/<EXAMPLE.OPENSCIENCEGRID.ORG>/server.conf <<xEOFx CVMFS_AUTO_TAG_TIMESPAN=\"2 weeks ago\" CVMFS_IGNORE_XDIR_HARDLINKS=true CVMFS_GENERATE_LEGACY_BULK_CHUNKS=false CVMFS_AUTOCATALOGS=true CVMFS_ENFORCE_LIMITS=true CVMFS_FORCE_REMOUNT_WARNING=false xEOFx Additionally, especially if files will be frequently deleted, enabling garbage collection is recommended in this way: root@host # cat >>/etc/cvmfs/repositories.d/<EXAMPLE.OPENSCIENCEGRID.ORG>/server.conf <<xEOFx CVMFS_GARBAGE_COLLECTION=true CVMFS_AUTO_GC=false xEOFx The above assumes that you have your own mechanism to run cvmfs_server gc regularly (typically daily) at a time when it won't interfere with publications, since garbage collection and publication can't be done at the same time. CVMFS_AUTO_GC=true will automatically run garbage collection periodically after publications, but those times are not always convenient. Also, check the cvmfs documentation for additional recommendations for special purpose repositories. Now verify that the repository is readable over HTTP: root@host # wget -qO- http://localhost:8000/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.cvmfswhitelist | cat -v That should print several lines including some gibberish at the end.","title":"Creating a Repository"},{"location":"data/external-oasis-repos/#hosting-a-repository-on-oasis","text":"In order to host a repository on OASIS, perform the following steps: Verify your VO's registration is up-to-date . All repositories need to be associated with a VO; the VO needs to assign an OASIS manager in Topology who would be responsible for the contents of any of the VO's repositories and will be contacted in case of issues. To designate an OASIS manager, have the VO manager update the Topology registration . Create a support ticket using the following template: Please add a new CVMFS repository to OASIS for VO <VO NAME> using the URL http://<FQDN>:8000/cvmfs/<OASIS REPOSITORY> The VO responsible manager will be <OASIS MANAGER>. Replace the <ANGLE BRACKET TEXT> items with the appropriate values. If the repository name matches *.opensciencegrid.org or *.osgstorage.org , wait for the go-ahead from the OSG representative before continuing with the remaining instructions; for all other repositories (such as *.egi.eu ), you are done. When you are told in the ticket to proceed to the next step, first if the repository might be in a transaction abort it: root@host # su <LIBRARIAN> -c \"cvmfs_server abort <EXAMPLE.OPENSCIENCEGRID.ORG>\" Then execute the following commands: root@host # wget -O /srv/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.cvmfswhitelist \\ http://oasis.opensciencegrid.org/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.cvmfswhitelist root@host # cp /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub \\ /etc/cvmfs/keys/<EXAMPLE.OPENSCIENCEGRID.ORG>.pub Replace <EXAMPLE.OPENSCIENCEGRID.ORG> as appropriate. If the cp command prompts about overwriting an existing file, type 'y'. Verify that publishing operation succeeds: root@host # su <LIBRARIAN> -c \"cvmfs_server transaction <EXAMPLE.OPENSCIENCEGRID.ORG>\" root@host # su <LIBRARIAN> -c \"cvmfs_server publish <EXAMPLE.OPENSCIENCEGRID.ORG>\" Within an hour, the repository updates should appear at the OSG Operations and FNAL Stratum-1 servers. On success, make sure the whitelist update happens daily by creating /etc/cron.d/fetch-cvmfs-whitelist with the following contents: 5 4 * * * <LIBRARIAN> cd /srv/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG> && wget -qO .cvmfswhitelist.new http://oasis.opensciencegrid.org/cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG>/.cvmfswhitelist && mv .cvmfswhitelist.new .cvmfswhitelist Note This cronjob eliminates the need for the repository service administrator to periodically use cvmfs_server resign to update .cvmfswhitelist as described in the upstream CVMFS documentation. Update the open support ticket to indicate that the previous steps have been completed Once the repository is fully replicated on the OSG, the VO may proceed in publishing into CVMFS using the <LIBRARIAN> account on the repository server. Tip We strongly recommend the repository maintainer read through the upstream documentation on maintaining repositories and content limitations . If the repository ends in .opensciencegrid.org or .osgstorage.org , the VO may ask for it to be replicated outside the US. The VO should open a GGUS ticket following EGI's PROC20 .","title":"Hosting a Repository on OASIS"},{"location":"data/external-oasis-repos/#replacing-an-existing-oasis-repository-server","text":"If a need arises to replace a server for an existing *.opensciencegrid.org or *.osgstorage.org repository, there are two ways to do it: one without changing the DNS name and one with changing it. The latter can take longer because it requires OSG Operations intervention. Revision numbers must increase CVMFS does not allow repository revision numbers to decrease, so the instructions below make sure the revision numbers only go up.","title":"Replacing an Existing OASIS Repository Server"},{"location":"data/external-oasis-repos/#without-changing-the-server-dns-name","text":"If you are recreating the repository on the same machine, use the following command to remove the repository configuration while preserving the data and keys: root@host # cvmfs_server rmfs -p <EXAMPLE.OPENSCIENCEGRID.ORG> Otherwise if it is a new machine, copy the keys from /etc/cvmfs/keys/ <EXAMPLE.OPENSCIENCEGRID.ORG> .* and the data from /srv/cvmfs/ <EXAMPLE.OPENSCIENCEGRID.ORG> from the old server to the new, making sure that no publish operations happen on the old server while you copy the data. Then in either case use cvmfs_server import instead of cvmfs_server mkfs in the above instructions for Creating the Repository , in order to reuse old data and keys. If you run an old and a new machine in parallel for a while, make sure that when you put the new machine into production (by moving the DNS name) that the new machine has had at least as many publishes as the old machine, so the revision number does not decrease.","title":"Without changing the server DNS name"},{"location":"data/external-oasis-repos/#with-changing-the-server-dns-name","text":"Note If you create a repository from scratch, as opposed to copying the data and keys from an old server, it is in fact better to change the DNS name of the server because that causes the OSG Operations server to reinitialize the .cvmfswhitelist. If you create a replacement repository on a new machine from scratch, follow the normal instructions on this page above, but with the following differences in the Hosting a Repository on OASIS section: In step 2, instead of asking in the support ticket to create a new repository, give the new URL and ask them to change the repository registration to that URL. When you do the publish in step 5, add a -n NNNN option where NNNN is a revision number greater than the number on the existing repository. That number can be found by this command on a client machine: user@host $ attr -qg revision /cvmfs/<EXAMPLE.OPENSCIENCEGRID.ORG> Skip step 6; there is no need to tell OSG Operations when you are finished. After enough time has elapsed for the publish to propagate to clients, typically around 15 minutes, verify that the new chosen revision has reached a client.","title":"With changing the server DNS name"},{"location":"data/external-oasis-repos/#removing-a-repository-from-oasis","text":"In order to remove a repository that is being hosted on OASIS, perform the following steps: If the repository has been replicated outside of the U.S., open a GGUS ticket asking that the replication be removed from EGI Stratum-1s. Wait until this ticket is resolved before proceeding. Open a support ticket asking to shut down the repository, giving the repository name (e.g., <EXAMPLE.OPENSCIENCEGRID.ORG> ), and the corresponding VO.","title":"Removing a Repository from OASIS"},{"location":"data/frontier-squid/","text":"Install the Frontier Squid HTTP Caching Proxy \u00b6 Frontier Squid is a distribution of the well-known squid HTTP caching proxy software that is optimized for use with applications on the Worldwide LHC Computing Grid (WLCG). It has many advantages over regular squid for common grid applications, especially Frontier and CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the upstream frontier-squid package for the convenience of OSG users. This document is intended for System Administrators who are installing frontier-squid , the OSG distribution of the Frontier Squid software. Frontier Squid Is Recommended \u00b6 OSG recommends that all sites run a caching proxy for HTTP and HTTPS to help reduce bandwidth and improve throughput. To that end, Compute Element (CE) installations include Frontier Squid automatically. We encourage all sites to configure and use this service, as described below. For large sites that expect heavy load on the proxy, it is best to run the proxy on its own host. If you are unsure if your site qualifies, we recommend initially running the proxy on your CE host and monitoring its bandwidth. If the network usage regularly peaks at over one third of the bandwidth capacity, move the proxy to a new host. Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If it does not exist already, the installation will create the squid Linux user Network ports: Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP) Host choice: If you will be supporting the Frontier application at your site, review the upstream documentation to determine how to size your equipment. As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Installing Frontier Squid \u00b6 To install Frontier Squid, make sure that your host is up to date before installing the required packages: Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install Frontier Squid: root@host # yum install frontier-squid Configuring Frontier Squid \u00b6 Configuring the Frontier Squid Service \u00b6 To configure the Frontier Squid service itself: Follow the Configuration section of the upstream Frontier Squid documentation . Enable, start, and test the service (as described below). Register the squid (also as described below ). Note An important difference between the standard Squid software and the Frontier Squid variant is that Frontier Squid changes are in /etc/squid/customize.sh instead of /etc/squid/squid.conf . Configuring the OSG CE \u00b6 To configure the OSG Compute Entrypoint (CE) to know about your Frontier Squid service: On your CE host (which may be different than your Frontier Squid host), edit /etc/osg/config.d/01-squid.ini Make sure that enabled is set to True Set location to the hostname and port of your Frontier Squid service (e.g., my.squid.host.edu:3128 ) Leave the other settings at DEFAULT unless you have specific reasons to change them Run osg-configure -c to propagate the changes on your CE. Note You may want to finish other CE configuration tasks before running osg-configure . Just be sure to run it once before starting CE services. Using Frontier-Squid \u00b6 Start the frontier-squid service and enable it to start at boot time. As a reminder, here are common service commands (all run as root ): To... Run the command... Start the service systemctl start frontier-squid Stop the service systemctl stop frontier-squid Enable the service to start on boot systemctl enable frontier-squid Disable the service from starting on boot systemctl disable frontier-squid Validating Frontier Squid \u00b6 As any user on another computer, do the following (where <MY.SQUID.HOST.EDU> is the fully qualified domain name of your squid server): user@host $ export http_proxy = http:// ` <MY.SQUID.HOST.EDU> ` :3128 user@host $ wget -qdO/dev/null http://frontier.cern.ch 2 > & 1 | grep X-Cache X-Cache: MISS from `<MY.SQUID.HOST.EDU>` user@host $ wget -qdO/dev/null http://frontier.cern.ch 2 > & 1 | grep X-Cache X-Cache: HIT from `<MY.SQUID.HOST.EDU>` If the grep doesn't print anything, try removing it from the pipeline to see if errors are obvious. If the second try says MISS again, something is probably wrong with the squid cache writes. Look at the squid access.log file to try to see what's wrong. If your squid will be supporting the Frontier application, it is also good to do the test in the upstream documentation Testing the installation section . Registering Frontier Squid \u00b6 To register your Frontier Squid host, follow the general registration instructions here with the following Frontier Squid-specific details. Alternatively, contact us for assistance with the registration process. Add a Squid: section to the Services: list, with any relevant fields for that service. This is a partial example: ... FQDN: <FULLY QUALIFIED DOMAIN NAME> Services: Squid: Description: Generic squid service ... Replacing <FULLY QUALIFIED DOMAIN NAME> with your Frontier Squid server's DNS entry or in the case of multiple Frontier Squid servers for a single resource, the round-robin DNS entry. See the BNL_ATLAS_Frontier_Squid for a complete example. If you are setting up a new resource, set Active: false . Only set Active: true for a resource when it is accepting requests and ready for production. Normally registered squids will be monitored by WLCG. This is strongly recommended even for non-WLCG sites so operations experts can help with diagnosing problems. However, if a site declines monitoring, that can be indicated by setting Monitored: false in a Details: section below Description: . Registration is still important for the sake of excluding squids from worker node failover monitors. The default if Details: Monitored: is not set is true . If you set Monitored to true, also enable monitoring as described in the upstream documentation on enabling monitoring . A few hours after a squid is registered and marked Active (and not marked Monitored: false ), verify that it is monitored by WLCG . Reference \u00b6 Users \u00b6 The frontier-squid installation will create one user account unless it already exists. User Comment squid Reduced privilege user that the squid process runs under. Set the default gid of the \"squid\" user to be a group that is also called \"squid\". The package can instead use another user name of your choice if you create a configuration file before installation. Details are in the upstream documentation Preparation section . Networking \u00b6 Service Name Protocol Port Number Inbound Outbound Comment Squid tcp 3128 \u2713 \u2713 Also limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously Squid monitor udp 3401 \u2713 Also limited in squid ACLs. Should be limited to monitoring server addresses The addresses of the WLCG monitoring servers for use in firewalls are listed in the upstream documentation Enabling monitoring section . Frontier Squid Log Files \u00b6 Log file contents are explained in the upstream documentation Log file contents section .","title":"Install Frontier Squid RPM"},{"location":"data/frontier-squid/#install-the-frontier-squid-http-caching-proxy","text":"Frontier Squid is a distribution of the well-known squid HTTP caching proxy software that is optimized for use with applications on the Worldwide LHC Computing Grid (WLCG). It has many advantages over regular squid for common grid applications, especially Frontier and CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the upstream frontier-squid package for the convenience of OSG users. This document is intended for System Administrators who are installing frontier-squid , the OSG distribution of the Frontier Squid software.","title":"Install the Frontier Squid HTTP Caching Proxy"},{"location":"data/frontier-squid/#frontier-squid-is-recommended","text":"OSG recommends that all sites run a caching proxy for HTTP and HTTPS to help reduce bandwidth and improve throughput. To that end, Compute Element (CE) installations include Frontier Squid automatically. We encourage all sites to configure and use this service, as described below. For large sites that expect heavy load on the proxy, it is best to run the proxy on its own host. If you are unsure if your site qualifies, we recommend initially running the proxy on your CE host and monitoring its bandwidth. If the network usage regularly peaks at over one third of the bandwidth capacity, move the proxy to a new host.","title":"Frontier Squid Is Recommended"},{"location":"data/frontier-squid/#before-starting","text":"Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If it does not exist already, the installation will create the squid Linux user Network ports: Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP) Host choice: If you will be supporting the Frontier application at your site, review the upstream documentation to determine how to size your equipment. As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories","title":"Before Starting"},{"location":"data/frontier-squid/#installing-frontier-squid","text":"To install Frontier Squid, make sure that your host is up to date before installing the required packages: Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install Frontier Squid: root@host # yum install frontier-squid","title":"Installing Frontier Squid"},{"location":"data/frontier-squid/#configuring-frontier-squid","text":"","title":"Configuring Frontier Squid"},{"location":"data/frontier-squid/#configuring-the-frontier-squid-service","text":"To configure the Frontier Squid service itself: Follow the Configuration section of the upstream Frontier Squid documentation . Enable, start, and test the service (as described below). Register the squid (also as described below ). Note An important difference between the standard Squid software and the Frontier Squid variant is that Frontier Squid changes are in /etc/squid/customize.sh instead of /etc/squid/squid.conf .","title":"Configuring the Frontier Squid Service"},{"location":"data/frontier-squid/#configuring-the-osg-ce","text":"To configure the OSG Compute Entrypoint (CE) to know about your Frontier Squid service: On your CE host (which may be different than your Frontier Squid host), edit /etc/osg/config.d/01-squid.ini Make sure that enabled is set to True Set location to the hostname and port of your Frontier Squid service (e.g., my.squid.host.edu:3128 ) Leave the other settings at DEFAULT unless you have specific reasons to change them Run osg-configure -c to propagate the changes on your CE. Note You may want to finish other CE configuration tasks before running osg-configure . Just be sure to run it once before starting CE services.","title":"Configuring the OSG CE"},{"location":"data/frontier-squid/#using-frontier-squid","text":"Start the frontier-squid service and enable it to start at boot time. As a reminder, here are common service commands (all run as root ): To... Run the command... Start the service systemctl start frontier-squid Stop the service systemctl stop frontier-squid Enable the service to start on boot systemctl enable frontier-squid Disable the service from starting on boot systemctl disable frontier-squid","title":"Using Frontier-Squid"},{"location":"data/frontier-squid/#validating-frontier-squid","text":"As any user on another computer, do the following (where <MY.SQUID.HOST.EDU> is the fully qualified domain name of your squid server): user@host $ export http_proxy = http:// ` <MY.SQUID.HOST.EDU> ` :3128 user@host $ wget -qdO/dev/null http://frontier.cern.ch 2 > & 1 | grep X-Cache X-Cache: MISS from `<MY.SQUID.HOST.EDU>` user@host $ wget -qdO/dev/null http://frontier.cern.ch 2 > & 1 | grep X-Cache X-Cache: HIT from `<MY.SQUID.HOST.EDU>` If the grep doesn't print anything, try removing it from the pipeline to see if errors are obvious. If the second try says MISS again, something is probably wrong with the squid cache writes. Look at the squid access.log file to try to see what's wrong. If your squid will be supporting the Frontier application, it is also good to do the test in the upstream documentation Testing the installation section .","title":"Validating Frontier Squid"},{"location":"data/frontier-squid/#registering-frontier-squid","text":"To register your Frontier Squid host, follow the general registration instructions here with the following Frontier Squid-specific details. Alternatively, contact us for assistance with the registration process. Add a Squid: section to the Services: list, with any relevant fields for that service. This is a partial example: ... FQDN: <FULLY QUALIFIED DOMAIN NAME> Services: Squid: Description: Generic squid service ... Replacing <FULLY QUALIFIED DOMAIN NAME> with your Frontier Squid server's DNS entry or in the case of multiple Frontier Squid servers for a single resource, the round-robin DNS entry. See the BNL_ATLAS_Frontier_Squid for a complete example. If you are setting up a new resource, set Active: false . Only set Active: true for a resource when it is accepting requests and ready for production. Normally registered squids will be monitored by WLCG. This is strongly recommended even for non-WLCG sites so operations experts can help with diagnosing problems. However, if a site declines monitoring, that can be indicated by setting Monitored: false in a Details: section below Description: . Registration is still important for the sake of excluding squids from worker node failover monitors. The default if Details: Monitored: is not set is true . If you set Monitored to true, also enable monitoring as described in the upstream documentation on enabling monitoring . A few hours after a squid is registered and marked Active (and not marked Monitored: false ), verify that it is monitored by WLCG .","title":"Registering Frontier Squid"},{"location":"data/frontier-squid/#reference","text":"","title":"Reference"},{"location":"data/frontier-squid/#users","text":"The frontier-squid installation will create one user account unless it already exists. User Comment squid Reduced privilege user that the squid process runs under. Set the default gid of the \"squid\" user to be a group that is also called \"squid\". The package can instead use another user name of your choice if you create a configuration file before installation. Details are in the upstream documentation Preparation section .","title":"Users"},{"location":"data/frontier-squid/#networking","text":"Service Name Protocol Port Number Inbound Outbound Comment Squid tcp 3128 \u2713 \u2713 Also limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously Squid monitor udp 3401 \u2713 Also limited in squid ACLs. Should be limited to monitoring server addresses The addresses of the WLCG monitoring servers for use in firewalls are listed in the upstream documentation Enabling monitoring section .","title":"Networking"},{"location":"data/frontier-squid/#frontier-squid-log-files","text":"Log file contents are explained in the upstream documentation Log file contents section .","title":"Frontier Squid Log Files"},{"location":"data/gridftp/","text":"Installing and Maintaining a GridFTP Server \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. About This Guide \u00b6 This page explains how to install the stand-alone Globus GridFTP server, which is an extension of the File Transfer Protocol (FTP) for grid computing. The aim of GridFTP is to provide a more reliable and high performance file transfer. The osg-gridftp package contains components necessary to set up a stand-alone GridFTP server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances: You are serving VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and your site has more than 250 cores Your site will be managing more than 50 TB of disk space You want a simple front-end to a filesystem allowing access over WAN - for example NFS. Note This document is for a standalone GridFTP server on top of POSIX storage. We have two specialized documents for Hadoop Distributed File System (HDFS) and XRootD based storage: Install and configure a GridFTP server on top of HDFS. Install and configure a GridFTP server on top of XRootD. Before Starting \u00b6 Before starting the installation process you will need to fulfill these prerequisites. Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates SSL Certificate: The GridFTP service uses a host certificate at /etc/grid-security/hostcert.pem and an accompanying key at /etc/grid-security/hostkey.pem Network ports: GridFTP listens on TCP port 2811 and the list of ports configured by the GLOBUS_TCP_SOURCE_RANGE environment variable. Installing GridFTP \u00b6 First, you will need to install the GridFTP meta-package: root@host # yum install osg-gridftp Configuring GridFTP \u00b6 Configuring authentication \u00b6 To configure which virtual organizations and users are allowed to use your GridFTP server, follow the instructions in the LCMAPS VOMS plugin document . Set port ranges \u00b6 As mentioned above, GridFTP uses port 2811 for control communication as well as a range of ports for the data transfer. This range of ports has to defined by setting the variable GLOBUS_TCP_PORT_RANGE within the configuration file: /etc/sysconfig/globus-gridftp-server as it is shown in the next example. This range has to be open within your firewall for inbound communication. $GLOBUS_TCP_PORT_RANGE 50000,51000 Optional configuration \u00b6 Setting transfer limits for GridFTP-HDFS \u00b6 To set a limit on the total or per-user number of transfers, create /etc/sysconfig/gridftp-hdfs and set the following configuration: export GRIDFTP_TRANSFER_LIMIT=\"80\" export GRIDFTP_DEFAULT_USER_TRANSFER_LIMIT=\"50\" export GRIDFTP_<UNIX USERNAME>_USER_TRANSFER_LIMIT=\"40\" In the above configuration: There would be no more than 80 transfers going at a time, across all users. By default, any single user can have no more than 50 transfers at a time. The <UNIX USERNAME> user has a more stringent limit of 40 transfers at a time. Note This limits are per gridftp server. If you have several gridftp servers you may want to have this limits divided by the number of gridftp servers at your site. Modifying the environment \u00b6 Environment variables are stored in /etc/sysconfig/globus-gridftp-server which is sourced on service startup. If you want to change LCMAPS log levels, or GridFTP port ranges, you can edit them there. #Uncomment and modify for firewalls #export GLOBUS_TCP_PORT_RANGE=min,max #export GLOBUS_TCP_SOURCE_RANGE=min,max Note that the variables GLOBUS_TCP_PORT_RANGE and GLOBUS_TCP_SOURCE_RANGE can be set here to allow GridFTP to navigate around firewall rules (these affect the inbound and outbound ports, respectively). To troubleshoot LCMAPS authorization, you can add the following to /etc/sysconfig/globus-gridftp-server and choose a higher debug level: # level 0: no messages, 1: errors, 2: also warnings, 3: also notices, # 4: also info, 5: maximum debug LCMAPS_DEBUG_LEVEL=2 Output goes to /var/log/messages by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive. Configuring a multi-homed server \u00b6 The GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file /etc/gridftp.conf . To use a single interface you can set hostname to the Hostname or IP address to use: hostname IP-TO-USE You can also set separately the control_interface , data_interface and ipc_interface . On systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting data_interface : control_interface IP-TO-USE data_interface IP-TO-USE ipc_interface IP-TO-USE For more options available for the GridFTP server, read the comments in the configuration file ( /etc/gridftp.conf ) or see the GridFTP manual . Monitoring \u00b6 Enabling GridFTP transfer probe \u00b6 The OSG monitoring of GridFTP is carried out by the GridFTP Gratia Probe which is installed by the package: gratia-probe-gridftp-transfer Assuming you installed GridFTP using the osg-gridftp or osg-se-hadoop-gridftp RPM, this package will already be installed. Here are the most relevant file and directory locations: Purpose Needs Editing? Location Probe Configuration Yes /etc/gratia/gridftp-transfer/ProbeConfig Probe Executables No /usr/share/gratia/gridftp-transfer Log files No /var/log/gratia Temporary files No /var/lib/gratia/tmp The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by the file /etc/gratia/gridftp-transfer/ProbeConfig This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following: Attribute Needs Editing Value ProbeName Maybe This should be set to \"gridftp-transfer: <HOSTNAME> \", where <HOSTNAME> is the fully-qualified domain name of your gridftp host. CollectorHost Maybe Set to the hostname and port of the central collector. By default it sends to the OSG collector. See below. SiteName Yes Set to the resource group name of your site as registered in OIM. GridftpLogDir Yes Set to /var/log, or wherever your current gridftp logs are located Grid Maybe Set to \"ITB\" if this is a test resource; otherwise, leave as OSG. UserVOMapFile No This should be set to /var/lib/osg/user-vo-map; see below for information about this file. SuppressUnknownVORecords Maybe Set to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended. SuppressNoDNRecords Maybe Set to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended. EnableProbe Yes Set to 1 to enable the probe. Selecting a collector host \u00b6 The collector is the central server which logs the GridFTP transfers into a database. There are usually two options: OSG Transfer Collector : This is the primary collector for transfers in the OSG. Use CollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\" . OSG-ITB Transfer Collector : This is the test collector for transfers in the OSG. Use CollectorHost=\" gratia-osg-itb.opensciencegrid.org:80\" . Managing GridFTP \u00b6 In addition to the GridFTP service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation for more info Gratia gratia-probes-cron Accounting software GridFTP globus-gridftp-server Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> Validation \u00b6 GridFTP \u00b6 Acquire a user certificate Find your subject DN: user@host # openssl x509 -in <CERITIFICATE_FILE.pem> -noout -subject Map your DN to a non-root user. As the non-root user, generate your proxy user@host # voms-proxy-init Create a test file to be transfered user@host # echo \"Hello World!\" > /tmp/hello_world Transfer the file we just created user@host # globus-url-copy file:///tmp/hello_world gsiftp://yourhost.yourdomain/tmp/hello_world To verify that the authentication is working, we could remove our proxy and execute the last command again, this time it should fail. user@host # voms-proxy-destroy user@host # globus-url-copy file:///tmp/hello_world gsiftp://yourhost.yourdomain/tmp/hello_world Warning Keep in mind that when invoked as root, globus-url-copy will attempt to use the host certificate instead of your user certificate, which could produce confusing results. Note If the binary globus-url-copy is not available on your system, you can get it by installing globus-gass-copy-progs : root@host # yum install globus-gass-copy-progs Gratia Probe \u00b6 Run the Gratia probe once by hand to check for functionality: root@host # /usr/share/gratia/gridftp-transfer/gridftp-transfer_meter Look in the log files in /var/log/gratia/<date>.log and make sure there are no error messages printed. Look for any abnormal termination and report it if it is a non-trivial site issue. Getting Help \u00b6 For assistance, please use this page . Reference \u00b6 GridFTP administration manual GridFTP tutorial Configuration and Log Files \u00b6 Service/Process Configuration File Description GridFTP /etc/sysconfig/globus-gridftp-server Environment variables for GridFTP and LCMAPS /usr/share/osg/sysconfig/globus-gridftp-server-plugin Where environment variables for GridFTP plugin are included Gratia Probe /etc/gratia/gridftp-transfer/ProbeConfig GridFTP Gratia Probe configuration Gratia Probe /etc/cron.d/gratia-probe-gridftp-transfer.cron Cron tab file Service/Process Log File Description GridFTP /var/log/gridftp.log GridFTP transfer log /var/log/gridftp-auth.log GridFTP authorization log Gratia probe /var/logs/gratia Certificates \u00b6 Certificate User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem Instructions to request a service certificate. Make sure you have installed the CA certificates Users \u00b6 For this package to function correctly, you will have to create the users needed for grid operation. Any Unix username that can be mapped by LCMAPS VOMS should be created on the GridFTP host. For example, VOs newly-added to the LCMAPS VOMS configuration will not be able to transfer files until the corresponding Unix user account is created. Networking \u00b6 Service Name Protocol Port Number Inbound Outbound Comment GridFTP data channels tcp GLOBUS_TCP_PORT_RANGE X contiguous range of ports is necessary. GridFTP data channels tcp GLOBUS_TCP_SOURCE_RANGE X contiguous range of ports is necessary. GridFTP control channel tcp 2811 X","title":"Install GridFTP Server"},{"location":"data/gridftp/#installing-and-maintaining-a-gridftp-server","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details.","title":"Installing and Maintaining a GridFTP Server"},{"location":"data/gridftp/#about-this-guide","text":"This page explains how to install the stand-alone Globus GridFTP server, which is an extension of the File Transfer Protocol (FTP) for grid computing. The aim of GridFTP is to provide a more reliable and high performance file transfer. The osg-gridftp package contains components necessary to set up a stand-alone GridFTP server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances: You are serving VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and your site has more than 250 cores Your site will be managing more than 50 TB of disk space You want a simple front-end to a filesystem allowing access over WAN - for example NFS. Note This document is for a standalone GridFTP server on top of POSIX storage. We have two specialized documents for Hadoop Distributed File System (HDFS) and XRootD based storage: Install and configure a GridFTP server on top of HDFS. Install and configure a GridFTP server on top of XRootD.","title":"About This Guide"},{"location":"data/gridftp/#before-starting","text":"Before starting the installation process you will need to fulfill these prerequisites. Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates SSL Certificate: The GridFTP service uses a host certificate at /etc/grid-security/hostcert.pem and an accompanying key at /etc/grid-security/hostkey.pem Network ports: GridFTP listens on TCP port 2811 and the list of ports configured by the GLOBUS_TCP_SOURCE_RANGE environment variable.","title":"Before Starting"},{"location":"data/gridftp/#installing-gridftp","text":"First, you will need to install the GridFTP meta-package: root@host # yum install osg-gridftp","title":"Installing GridFTP"},{"location":"data/gridftp/#configuring-gridftp","text":"","title":"Configuring GridFTP"},{"location":"data/gridftp/#configuring-authentication","text":"To configure which virtual organizations and users are allowed to use your GridFTP server, follow the instructions in the LCMAPS VOMS plugin document .","title":"Configuring authentication"},{"location":"data/gridftp/#set-port-ranges","text":"As mentioned above, GridFTP uses port 2811 for control communication as well as a range of ports for the data transfer. This range of ports has to defined by setting the variable GLOBUS_TCP_PORT_RANGE within the configuration file: /etc/sysconfig/globus-gridftp-server as it is shown in the next example. This range has to be open within your firewall for inbound communication. $GLOBUS_TCP_PORT_RANGE 50000,51000","title":"Set port ranges"},{"location":"data/gridftp/#optional-configuration","text":"","title":"Optional configuration"},{"location":"data/gridftp/#setting-transfer-limits-for-gridftp-hdfs","text":"To set a limit on the total or per-user number of transfers, create /etc/sysconfig/gridftp-hdfs and set the following configuration: export GRIDFTP_TRANSFER_LIMIT=\"80\" export GRIDFTP_DEFAULT_USER_TRANSFER_LIMIT=\"50\" export GRIDFTP_<UNIX USERNAME>_USER_TRANSFER_LIMIT=\"40\" In the above configuration: There would be no more than 80 transfers going at a time, across all users. By default, any single user can have no more than 50 transfers at a time. The <UNIX USERNAME> user has a more stringent limit of 40 transfers at a time. Note This limits are per gridftp server. If you have several gridftp servers you may want to have this limits divided by the number of gridftp servers at your site.","title":"Setting transfer limits for GridFTP-HDFS"},{"location":"data/gridftp/#modifying-the-environment","text":"Environment variables are stored in /etc/sysconfig/globus-gridftp-server which is sourced on service startup. If you want to change LCMAPS log levels, or GridFTP port ranges, you can edit them there. #Uncomment and modify for firewalls #export GLOBUS_TCP_PORT_RANGE=min,max #export GLOBUS_TCP_SOURCE_RANGE=min,max Note that the variables GLOBUS_TCP_PORT_RANGE and GLOBUS_TCP_SOURCE_RANGE can be set here to allow GridFTP to navigate around firewall rules (these affect the inbound and outbound ports, respectively). To troubleshoot LCMAPS authorization, you can add the following to /etc/sysconfig/globus-gridftp-server and choose a higher debug level: # level 0: no messages, 1: errors, 2: also warnings, 3: also notices, # 4: also info, 5: maximum debug LCMAPS_DEBUG_LEVEL=2 Output goes to /var/log/messages by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.","title":"Modifying the environment"},{"location":"data/gridftp/#configuring-a-multi-homed-server","text":"The GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file /etc/gridftp.conf . To use a single interface you can set hostname to the Hostname or IP address to use: hostname IP-TO-USE You can also set separately the control_interface , data_interface and ipc_interface . On systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting data_interface : control_interface IP-TO-USE data_interface IP-TO-USE ipc_interface IP-TO-USE For more options available for the GridFTP server, read the comments in the configuration file ( /etc/gridftp.conf ) or see the GridFTP manual .","title":"Configuring a multi-homed server"},{"location":"data/gridftp/#monitoring","text":"","title":"Monitoring"},{"location":"data/gridftp/#enabling-gridftp-transfer-probe","text":"The OSG monitoring of GridFTP is carried out by the GridFTP Gratia Probe which is installed by the package: gratia-probe-gridftp-transfer Assuming you installed GridFTP using the osg-gridftp or osg-se-hadoop-gridftp RPM, this package will already be installed. Here are the most relevant file and directory locations: Purpose Needs Editing? Location Probe Configuration Yes /etc/gratia/gridftp-transfer/ProbeConfig Probe Executables No /usr/share/gratia/gridftp-transfer Log files No /var/log/gratia Temporary files No /var/lib/gratia/tmp The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by the file /etc/gratia/gridftp-transfer/ProbeConfig This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following: Attribute Needs Editing Value ProbeName Maybe This should be set to \"gridftp-transfer: <HOSTNAME> \", where <HOSTNAME> is the fully-qualified domain name of your gridftp host. CollectorHost Maybe Set to the hostname and port of the central collector. By default it sends to the OSG collector. See below. SiteName Yes Set to the resource group name of your site as registered in OIM. GridftpLogDir Yes Set to /var/log, or wherever your current gridftp logs are located Grid Maybe Set to \"ITB\" if this is a test resource; otherwise, leave as OSG. UserVOMapFile No This should be set to /var/lib/osg/user-vo-map; see below for information about this file. SuppressUnknownVORecords Maybe Set to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended. SuppressNoDNRecords Maybe Set to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended. EnableProbe Yes Set to 1 to enable the probe.","title":"Enabling GridFTP transfer probe"},{"location":"data/gridftp/#selecting-a-collector-host","text":"The collector is the central server which logs the GridFTP transfers into a database. There are usually two options: OSG Transfer Collector : This is the primary collector for transfers in the OSG. Use CollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\" . OSG-ITB Transfer Collector : This is the test collector for transfers in the OSG. Use CollectorHost=\" gratia-osg-itb.opensciencegrid.org:80\" .","title":"Selecting a collector host"},{"location":"data/gridftp/#managing-gridftp","text":"In addition to the GridFTP service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation for more info Gratia gratia-probes-cron Accounting software GridFTP globus-gridftp-server Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME>","title":"Managing GridFTP"},{"location":"data/gridftp/#validation","text":"","title":"Validation"},{"location":"data/gridftp/#gridftp","text":"Acquire a user certificate Find your subject DN: user@host # openssl x509 -in <CERITIFICATE_FILE.pem> -noout -subject Map your DN to a non-root user. As the non-root user, generate your proxy user@host # voms-proxy-init Create a test file to be transfered user@host # echo \"Hello World!\" > /tmp/hello_world Transfer the file we just created user@host # globus-url-copy file:///tmp/hello_world gsiftp://yourhost.yourdomain/tmp/hello_world To verify that the authentication is working, we could remove our proxy and execute the last command again, this time it should fail. user@host # voms-proxy-destroy user@host # globus-url-copy file:///tmp/hello_world gsiftp://yourhost.yourdomain/tmp/hello_world Warning Keep in mind that when invoked as root, globus-url-copy will attempt to use the host certificate instead of your user certificate, which could produce confusing results. Note If the binary globus-url-copy is not available on your system, you can get it by installing globus-gass-copy-progs : root@host # yum install globus-gass-copy-progs","title":"GridFTP"},{"location":"data/gridftp/#gratia-probe","text":"Run the Gratia probe once by hand to check for functionality: root@host # /usr/share/gratia/gridftp-transfer/gridftp-transfer_meter Look in the log files in /var/log/gratia/<date>.log and make sure there are no error messages printed. Look for any abnormal termination and report it if it is a non-trivial site issue.","title":"Gratia Probe"},{"location":"data/gridftp/#getting-help","text":"For assistance, please use this page .","title":"Getting Help"},{"location":"data/gridftp/#reference","text":"GridFTP administration manual GridFTP tutorial","title":"Reference"},{"location":"data/gridftp/#configuration-and-log-files","text":"Service/Process Configuration File Description GridFTP /etc/sysconfig/globus-gridftp-server Environment variables for GridFTP and LCMAPS /usr/share/osg/sysconfig/globus-gridftp-server-plugin Where environment variables for GridFTP plugin are included Gratia Probe /etc/gratia/gridftp-transfer/ProbeConfig GridFTP Gratia Probe configuration Gratia Probe /etc/cron.d/gratia-probe-gridftp-transfer.cron Cron tab file Service/Process Log File Description GridFTP /var/log/gridftp.log GridFTP transfer log /var/log/gridftp-auth.log GridFTP authorization log Gratia probe /var/logs/gratia","title":"Configuration and Log Files"},{"location":"data/gridftp/#certificates","text":"Certificate User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem Instructions to request a service certificate. Make sure you have installed the CA certificates","title":"Certificates"},{"location":"data/gridftp/#users","text":"For this package to function correctly, you will have to create the users needed for grid operation. Any Unix username that can be mapped by LCMAPS VOMS should be created on the GridFTP host. For example, VOs newly-added to the LCMAPS VOMS configuration will not be able to transfer files until the corresponding Unix user account is created.","title":"Users"},{"location":"data/gridftp/#networking","text":"Service Name Protocol Port Number Inbound Outbound Comment GridFTP data channels tcp GLOBUS_TCP_PORT_RANGE X contiguous range of ports is necessary. GridFTP data channels tcp GLOBUS_TCP_SOURCE_RANGE X contiguous range of ports is necessary. GridFTP control channel tcp 2811 X","title":"Networking"},{"location":"data/hadoop-overview/","text":"Hadoop Overview \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. Hadoop Introduction \u00b6 Hadoop is a data processing framework. The framework has two main parts - job scheduling and a distributed file system, the Hadoop Distributed File System (HDFS). We currently utilize HDFS as a general-purpose file system. For this document, we'll use the words \"Hadoop\" and \"HDFS\" interchangeably, but it's nice to know the distinction. The HDFS file system has several features, some of which differ a bit from a typical file system: Each file is broken up into 64 MB or 128 MB chunks (user configurable) These chunks are stored on data nodes and served up from there; The central namenode manages block locations, the namespace information, and block placement policies. HDFS provides a subset of POSIX semantics: Random-access reads and non-random-access writes are fully supported. Fsync and appends (after the file has been initially closed) are experimental and not available to OSG-based installs. Rewriting closed files is not supported Hadoop SE Components \u00b6 We broadly break down the server components of the Hadoop SE into three categories: HDFS core, Grid extensions, and HDFS auxiliary. The components in each of these categories are outlined below: HDFS Core: Namenode: The core metadata server of Hadoop. This is the most critical piece of the system, and there can only be one of these. This stores both the file system image and the file system journal. The namenode keeps all of the filesystem layout information (files, blocks, directories, permissions, etc) and the block locations. The filesystem layout is persisted on disk and the block locations are kept solely in memory. When a client opens a file, the namenode tells the client the locations of all the blocks in the file; the client then no longer needs to communicate with the namenode for data transfer. Datanode: This node stores copies of the blocks in HDFS. They communicate with the namenode to perform \"housekeeping\" such as creating new replicas, transferring blocks between datanodes, and deleting excess blocks. They also communicate with the clients to transfer data. To reach the best scalability, there should be as many datanodes as possible. Grid extensions Globus GridFTP: The standard GridFTP from Globus. We use a plug-in module (using the Globus Direct Storage Interface) that allows the GridFTP process to use the HDFS C-bindings directly. Gratia probe: Gratia is an accounting system that records batch system and transfer records to a database. The records are collected by a client program called a \"probe\" which runs on the GridFTP or XRootD server. The probe parses the GridFTP or XRootD logs and generates transfer records. XRootD server plugin: XRootD is an extremely flexible and powerful data server popular in the high energy physics community. There exists a HDFS plugin for XRootD; integrating with XRootD provides a means to export HDFS securely outside the local cluster, as another XRootD plugin provides GSI-based authentication and authorization. HDFS auxiliary: \"Secondary Namenode\": Perhaps more aptly called a \"checkpoint server\". This server downloads the file system image and journal from the namenode, merges the two together, and uploads the new file system image up to the namenode. This is done on a different server in order to reduce the memory footprint of the namenode. Hadoop Balancer: This is a script (unlike the others, which are daemons) that runs on the namenode. It requests transfers of random blocks between the datanodes. This works until all datanodes have approximately the same percentage of free space. Well-balanced datanodes are necessary for having a healthy cluster. In addition to the server components, there are two client components: FUSE: This allows HDFS to be mounted as a filesystem on the worker nodes. FUSE is a Linux kernel module that allows kernel I/O calls to be translated into a call to a userspace program. In this case, a program called fuse_dfs translates the POSIX calls into HDFS C-binding calls. Hadoop Command Line Client: This command line client exposes a lot of the Unix-like calls without mounting FUSE, plus access to the non-POSIX calls (such as setting quotas and file replication levels). For example, \"hadoop fs -ls /\" is equivalent to \"ls /mnt/hadoop\" if /mnt/hadoop is the mount point of HDFS. Namenode: We recommend at least 8GB of RAM (minimum is 2GB RAM), preferably 16GB or more. A rough rule of thumb is 1GB per 100TB of raw disk space; the actual requirements is around 1GB per million objects (files, directories, and blocks). The CPU requirements are any modern multi-core server CPU. Typically, the namenode will only use 2-5% of your CPU. As this is a single point of failure, the most important requirement is reliable hardware rather than high performance hardware. We suggest a node with redundant power supplies and at least 2 hard drives. Secondary namenode: This node needs the same amount of RAM as the namenode for merging namespaces. It does not need to be high performance or high reliability. Datanode: Each datanode should plan to dedicate about 1-1.5 GB of RAM to HDFS. A general rule of thumb is to dedicate 1 CPU to HDFS per 5TB of disk capacity under heavily load; clusters with moderate load (i.e., mostly sequential workflows) will need less. At idle, HDFS will consume almost no CPU. Sizing Your Cluster \u00b6 The minimal installation would involve 5 nodes: hadoop-name: The namenode for the Hadoop system. hadoop-name2: This will run the HDFS secondary namenode. hadoop-data1, hadoop-data2: Two HDFS datanodes. They will hold data for the system, so they should have sizable hard drives. As the Hadoop installation grows to many terabytes, this will be the only class of nodes one adds. hadoop-grid: Runs the Globus GridFTP server. If desired, hadoop-name and hadoop-name2 may be virtualized. Prior to installation, DNS / host name resolution must work. That is, you should be able to resolve all the Hadoop servers either through DNS or /etc/hosts. Because of the grid software, hadoop-grid must have reverse DNS working. Larger clusters have the same basic components but with more HDFS datanodes and gridftp servers. Adding HDFS datanodes increases the capacity and number of IOPS the cluster can provide. Additional GridFTP servers will increase the data transfer rates to locations outside your data center. As your cluster increases in size, virtualized namenodes may need to be moved to physical hardware. Hadoop Security \u00b6 HDFS has Unix-like user/group authorization, but no strict authentication. HDFS should use a secure internal network which only non-malicious users are able to access . For users with access to the local cluster, it is not difficult to bypass authentication. The default ports are listed here . There are some ways to improve security of your cluster: Keep the namenode behind a firewall. One possibility is to run Hadoop entirely on the private subnet of a cluster. Use firewalls to protect the HDFS ports (default for the datanode is 50010 and 50075; for the namenode, 50070 and 9000). For clusters utilizing FUSE, one can block outgoing connections to the HDFS ports except for user root. This means that only root-owned processes (such as FUSE-DFS) will be able to access Hadoop. This is sufficient for grid environments, but does not protect one in the case where the attacker has physical access to the network switch. There exists another option, currently untested. It is possible to limit all HDFS socket connections to SSL-based sockets. Using this to only allow known hosts to connect to HDFS and only allowing FUSE-DFS to connect on those known hosts, one might be able to satisfy even fairly stringent security folks (but not paranoid ones). There are three options to export your data outside your cluster: Globus GridFTP. XRootD. HTTP and HTTPS. OSG utilizes the HTTP(S) protocol implementation built into the XRootD server. References \u00b6 Hadoop Architecture","title":"HDFS Overview"},{"location":"data/hadoop-overview/#hadoop-overview","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details.","title":"Hadoop Overview"},{"location":"data/hadoop-overview/#hadoop-introduction","text":"Hadoop is a data processing framework. The framework has two main parts - job scheduling and a distributed file system, the Hadoop Distributed File System (HDFS). We currently utilize HDFS as a general-purpose file system. For this document, we'll use the words \"Hadoop\" and \"HDFS\" interchangeably, but it's nice to know the distinction. The HDFS file system has several features, some of which differ a bit from a typical file system: Each file is broken up into 64 MB or 128 MB chunks (user configurable) These chunks are stored on data nodes and served up from there; The central namenode manages block locations, the namespace information, and block placement policies. HDFS provides a subset of POSIX semantics: Random-access reads and non-random-access writes are fully supported. Fsync and appends (after the file has been initially closed) are experimental and not available to OSG-based installs. Rewriting closed files is not supported","title":"Hadoop Introduction"},{"location":"data/hadoop-overview/#hadoop-se-components","text":"We broadly break down the server components of the Hadoop SE into three categories: HDFS core, Grid extensions, and HDFS auxiliary. The components in each of these categories are outlined below: HDFS Core: Namenode: The core metadata server of Hadoop. This is the most critical piece of the system, and there can only be one of these. This stores both the file system image and the file system journal. The namenode keeps all of the filesystem layout information (files, blocks, directories, permissions, etc) and the block locations. The filesystem layout is persisted on disk and the block locations are kept solely in memory. When a client opens a file, the namenode tells the client the locations of all the blocks in the file; the client then no longer needs to communicate with the namenode for data transfer. Datanode: This node stores copies of the blocks in HDFS. They communicate with the namenode to perform \"housekeeping\" such as creating new replicas, transferring blocks between datanodes, and deleting excess blocks. They also communicate with the clients to transfer data. To reach the best scalability, there should be as many datanodes as possible. Grid extensions Globus GridFTP: The standard GridFTP from Globus. We use a plug-in module (using the Globus Direct Storage Interface) that allows the GridFTP process to use the HDFS C-bindings directly. Gratia probe: Gratia is an accounting system that records batch system and transfer records to a database. The records are collected by a client program called a \"probe\" which runs on the GridFTP or XRootD server. The probe parses the GridFTP or XRootD logs and generates transfer records. XRootD server plugin: XRootD is an extremely flexible and powerful data server popular in the high energy physics community. There exists a HDFS plugin for XRootD; integrating with XRootD provides a means to export HDFS securely outside the local cluster, as another XRootD plugin provides GSI-based authentication and authorization. HDFS auxiliary: \"Secondary Namenode\": Perhaps more aptly called a \"checkpoint server\". This server downloads the file system image and journal from the namenode, merges the two together, and uploads the new file system image up to the namenode. This is done on a different server in order to reduce the memory footprint of the namenode. Hadoop Balancer: This is a script (unlike the others, which are daemons) that runs on the namenode. It requests transfers of random blocks between the datanodes. This works until all datanodes have approximately the same percentage of free space. Well-balanced datanodes are necessary for having a healthy cluster. In addition to the server components, there are two client components: FUSE: This allows HDFS to be mounted as a filesystem on the worker nodes. FUSE is a Linux kernel module that allows kernel I/O calls to be translated into a call to a userspace program. In this case, a program called fuse_dfs translates the POSIX calls into HDFS C-binding calls. Hadoop Command Line Client: This command line client exposes a lot of the Unix-like calls without mounting FUSE, plus access to the non-POSIX calls (such as setting quotas and file replication levels). For example, \"hadoop fs -ls /\" is equivalent to \"ls /mnt/hadoop\" if /mnt/hadoop is the mount point of HDFS. Namenode: We recommend at least 8GB of RAM (minimum is 2GB RAM), preferably 16GB or more. A rough rule of thumb is 1GB per 100TB of raw disk space; the actual requirements is around 1GB per million objects (files, directories, and blocks). The CPU requirements are any modern multi-core server CPU. Typically, the namenode will only use 2-5% of your CPU. As this is a single point of failure, the most important requirement is reliable hardware rather than high performance hardware. We suggest a node with redundant power supplies and at least 2 hard drives. Secondary namenode: This node needs the same amount of RAM as the namenode for merging namespaces. It does not need to be high performance or high reliability. Datanode: Each datanode should plan to dedicate about 1-1.5 GB of RAM to HDFS. A general rule of thumb is to dedicate 1 CPU to HDFS per 5TB of disk capacity under heavily load; clusters with moderate load (i.e., mostly sequential workflows) will need less. At idle, HDFS will consume almost no CPU.","title":"Hadoop SE Components"},{"location":"data/hadoop-overview/#sizing-your-cluster","text":"The minimal installation would involve 5 nodes: hadoop-name: The namenode for the Hadoop system. hadoop-name2: This will run the HDFS secondary namenode. hadoop-data1, hadoop-data2: Two HDFS datanodes. They will hold data for the system, so they should have sizable hard drives. As the Hadoop installation grows to many terabytes, this will be the only class of nodes one adds. hadoop-grid: Runs the Globus GridFTP server. If desired, hadoop-name and hadoop-name2 may be virtualized. Prior to installation, DNS / host name resolution must work. That is, you should be able to resolve all the Hadoop servers either through DNS or /etc/hosts. Because of the grid software, hadoop-grid must have reverse DNS working. Larger clusters have the same basic components but with more HDFS datanodes and gridftp servers. Adding HDFS datanodes increases the capacity and number of IOPS the cluster can provide. Additional GridFTP servers will increase the data transfer rates to locations outside your data center. As your cluster increases in size, virtualized namenodes may need to be moved to physical hardware.","title":"Sizing Your Cluster"},{"location":"data/hadoop-overview/#hadoop-security","text":"HDFS has Unix-like user/group authorization, but no strict authentication. HDFS should use a secure internal network which only non-malicious users are able to access . For users with access to the local cluster, it is not difficult to bypass authentication. The default ports are listed here . There are some ways to improve security of your cluster: Keep the namenode behind a firewall. One possibility is to run Hadoop entirely on the private subnet of a cluster. Use firewalls to protect the HDFS ports (default for the datanode is 50010 and 50075; for the namenode, 50070 and 9000). For clusters utilizing FUSE, one can block outgoing connections to the HDFS ports except for user root. This means that only root-owned processes (such as FUSE-DFS) will be able to access Hadoop. This is sufficient for grid environments, but does not protect one in the case where the attacker has physical access to the network switch. There exists another option, currently untested. It is possible to limit all HDFS socket connections to SSL-based sockets. Using this to only allow known hosts to connect to HDFS and only allowing FUSE-DFS to connect on those known hosts, one might be able to satisfy even fairly stringent security folks (but not paranoid ones). There are three options to export your data outside your cluster: Globus GridFTP. XRootD. HTTP and HTTPS. OSG utilizes the HTTP(S) protocol implementation built into the XRootD server.","title":"Hadoop Security"},{"location":"data/hadoop-overview/#references","text":"Hadoop Architecture","title":"References"},{"location":"data/install-hadoop/","text":"Installing and Maintaining HDFS \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. Hadoop Distributed File System (HDFS) is a scalable, reliable distributed file system developed in the Apache project. It is based on the map-reduce framework and design of the Google file system. The OSG distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. The purpose of this document is to provide Hadoop-based Storage Element administrators the information on how to prepare, install and validate OSG storage based on the Hadoop Distributed File System (HDFS). The OSG supports a patched version HDFS from Cloudera's CDH5 distribution of HDFS ( https://www.cloudera.com/products/open-source/apache-hadoop/key-cdh-components.html ). Note The OSG only supports HDFS on EL7 hosts Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If they do not exist already, the installation will create the Linux users hdfs and zookeeper on all nodes as well as hadoop and mapred on the NameNodes Firewall: In the OSG, HDFS is intended to run as an internal service without any direct, external access to any of the nodes. For more information on the ports used for communication between the various HDFS nodes, see the Cloudera documentation . As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Designing Your HDFS Cluster \u00b6 There are several important components to an HDFS installation: NameNode : The NameNode functions as the directory server and coordinator of the HDFS cluster. It houses all the meta-data for the hadoop cluster. Secondary NameNode (optional) : This is a secondary machine that periodically merges updates to the HDFS file system back into the fsimage . It must share a directory with the primary NameNode to exchange filesystem checkpoints. An HDFS installation with a Secondary NameNode dramatically improves startup and restart times. DataNode : You will have many DataNodes. Each DataNode stores large blocks of files to for the hadoop cluster. Client : This is a documentation shorthand that refers to any machine with the hadoop client commands or FUSE mount. Installing HDFS \u00b6 An OSG HDFS installation consists of HDFS and other support software (e.g., Gratia accounting). To simplify installation, OSG provides convenience RPMs that install all required software. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the relevant packages based on the node you are installing: If you are installing a(n)... Then run the following command... Primary NameNode yum install osg-se-hadoop-namenode Secondary NameNode yum install osg-se-hadoop-secondarynamenode DataNode yum install osg-se-hadoop-datanode Upgrading HDFS \u00b6 This section will guide you through the process to upgrade a HDFS 2.0.0 installation from OSG 3.3 to the HDFS 2.6.0 from OSG 3.5. Warning The upgrade process will involve downtime for your HDFS cluster. Please plan accordingly. Note The OSG only offers HDFS 2.6.0 for EL7 hosts. The upgrade process occurs in several steps: Preparing for the upgrade Updating to OSG 3.5 Upgrading the Primary NameNode Upgrading the DataNodes Upgrading the Secondary NameNode Finalizing the upgrade Preparing for the upgrade \u00b6 Before upgrading, backup your configuration data and HDFS metadata. Put your Primary NameNode into safe mode: root@primary-namenode # hdfs dfsadmin -safemode enter Safe mode is ON Save a clean copy of your HDFS namespace: root@primary-namenode # hdfs dfsadmin -saveNamespace Save namespace successful Shutdown the HDFS services on all of your HDFS nodes (see this section for instructions). On the Primary NameNode, verify that your NameNode service is off: root@primary-namenode # /etc/init.d/hadoop-hdfs-namenode status This command should indicate that your NameNode service is not running. Find the location of the directory with the HDFS metadata: root@primary-namenode # grep -C1 dfs.namenode.name.dir /etc/hadoop/conf/hdfs-site.xml And look for the value of dfs.namenode.name.dir : <property> <name> dfs.namenode.name.dir </name> <value> file:///var/lib/dfs/nn,file:///home/hadoop/dfs/nn </value> Backup the directory that appears in the output using your backup method of choice. If more than one directory appears in the list (as in the example above), choose the most convenient directory. All of the directories in the list will have the same contents. Updating to OSG 3.5 \u00b6 Once your HDFS services have been turned off and the HDFS metadata has been backed up, update each node to OSG 3.5 by following the instructions in this section . Upgrading the Primary NameNode \u00b6 To upgrade your Primary NameNode, update all relevant packages then run the upgrade command. Clear the yum cache: root@primary-namenode # yum clean all --enablerepo = * Update the HDFS RPMs: root@primary-namenode # yum update osg-se-hadoop-namenode --enablerepo-osg-upcoming Perform the upgrade command: root@primary-namenode # /etc/init.d/hadoop-hdfs-namenode upgrade This will start the upgrade process for the HDFS metadata on your primary namenode. You can follow the process by running root@primary-namenode # tail -f /var/log/hadoop-hdfs/hadoop-hdfs-namenode-<hostname>.log Upgrading the DataNodes \u00b6 Once the Primary NameNode has completed its upgrade process, start the process of upgrading each of your DataNodes. Clear the yum cache: root@datanode # yum clean all --enablerepo = * Update the HDFS RPMs: root@datanode # yum update osg-se-hadoop-datanode --enablerepo-osg-upcoming Start the DataNode service: root@datanode # /etc/init.d/hadoop-hdfs-datanode start After all the DataNodes have been brought back up, the Primary NameNode should exit safe mode automatically. On the Primary NameNode, run the following command to verify is no longer in safe mode: root@primary-namenode # hdfs dfsadmin -safemode get Safe mode is OFF Upgrading the Secondary NameNode \u00b6 Note This section only applies to sites with a Secondary NameNode. If you do not run a Secondary NameNode, skip to the next section . Once the Primary NameNode has exited safe mode, start the process of upgrading your Secondary NameNode. Clear the yum cache: root@secondary-namenode # yum clean all --enablerepo = * Update the HDFS RPMs: root@secondary-namenode # yum update osg-se-hadoop-secondarynamenode --enablerepo-osg-upcoming Start the Secondary NameNode service: root@secondary-namenode # /etc/init.d/hadoop-hdfs-secondarynamenode start Finalizing the upgrade \u00b6 Verify that the HDFS cluster is running correctly by following the instructions in this section . Finalize the upgrade from the Primary NameNode: root@primary-namenode # hdfs dfsadmin -finalizeUpgrade Finalize upgrade successful Configuring HDFS \u00b6 Note Needed by: Hadoop NameNode, Hadoop DataNodes, Hadoop client, GridFTP Hadoop configuration is needed by every node in the hadoop cluster. However, in most cases, you can do the configuration once and copy it to all nodes in the cluster (possibly using your favorite configuration management tool). Special configuration for various special components is given in the below sections. Hadoop configuration is stored in /etc/hadoop/conf . However, by default, these files are mostly blank. OSG provides a sample configuration in /etc/hadoop/conf.osg with most common values filled in. You will need to copy these into /etc/hadoop/conf before they become active. Please let us know if there are any common values that should be added/changed across the whole grid. You will likely need to modify hdfs-site.xml and core-site.xml . Review all the settings in these files, but listed below are common settings to modify: File Setting Example Comments core-site.xml fs.default.name hdfs://namenode.domain.tld.:9000 This is the address of the NameNode core-site.xml hadoop.tmp.dir /data/scratch Scratch temp directory used by Hadoop core-site.xml hadoop.log.dir /var/log/hadoop-hdfs Log directory used by Hadoop core-site.xml dfs.umaskmode 002 umask for permissions used by default hdfs-site.xml dfs.block.size 134217728 Block size: 128MB by default hdfs-site.xml dfs.replication 2 Default replication factor. Generally the same as dfs.replication.min/max hdfs-site.xml dfs.datanode.du.reserved 100000000 How much free space hadoop will reserve for non-Hadoop usage hdfs-site.xml dfs.datanode.handler.count 20 Number of server threads for DataNodes. Increase if you have many more client connections hdfs-site.xml dfs.namenode.handler.count 40 Number of server threads for NameNodes. Increase if you need more connections hdfs-site.xml dfs.http.address namenode.domain.tld.:50070 Web address for dfs health monitoring page See http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml for more parameters to configure. Note NameNodes must have a /etc/hosts_exclude present Special NameNode instructions for brand new installs \u00b6 If this is a new installation ( and only if this is a brand new installation ), you should run the following command as the hdfs user. (Otherwise, be sure to chown your storage directory to hdfs after running): hadoop namenode -format This will initialize the storage directory on your NameNode (optional) FUSE Client Configuration \u00b6 A FUSE mount is required on any node that you would like to use standard POSIX-like commands on the Hadoop filesystem. FUSE (or \"File system in User SpacE\") is a way to access HDFS using typical UNIX directory commands (i.e., POSIX-like access). Note that not all advanced functions of a full POSIX-compliant file system are necessarily available. FUSE is typically installed as part of this installation, but, if you are running a customized or non-standard system, make sure that the fuse kernel module is installed and loaded with modprobe fuse . You can add the FUSE to be mounted at boot time by adding the following line to /etc/fstab : hadoop-fuse-dfs# </MNT/HADOOP> fuse server=<NAMENODE.HOST>,port=9000,rdbuffer=131072,allow_other 0 0 Be sure to change the </MNT/HADOOP> mount point and <NAMENODE.HOST> to match your local configuration. To match the help documents, we recommend using </MNT/HADOOP> as your mountpoint. Once your /etc/fstab is updated, to mount FUSE run: root@host # mkdir /mnt/hadoop root@host # mount /mnt/hadoop When mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen: # mount /mnt/hadoop INFO fuse_options.c:162 Adding FUSE arg /mnt/hadoop INFO fuse_options.c:110 Ignoring option allow_other If you have troubles mounting FUSE refer to Running FUSE in Debug Mode in the Troubleshooting section. Creating VO and User Areas \u00b6 Note Grid Users are needed by GridFTP nodes. VO areas are common to all nodes. For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created. For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server. Note that these users must be kept in sync with the authentication method. Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. Create (and maintain) usernames and groups with UIDs and GIDs on all nodes . These are maintained in basic system files such as /etc/passwd and /etc/group . Note In the examples below It is assumed a FUSE mount is set to /mnt/hadoop . As an alternative hadoop fs commands could have been used. For clean HDFS operations and filesystem management: (a) Create top-level VO subdirectories under /mnt/hadoop . Example: root@host # mkdir /mnt/hadoop/cms root@host # mkdir /mnt/hadoop/dzero root@host # mkdir /mnt/hadoop/sbgrid root@host # mkdir /mnt/hadoop/fermigrid root@host # mkdir /mnt/hadoop/cmstest root@host # mkdir /mnt/hadoop/osg (b) Create individual top-level user areas, under each VO area, as needed. root@host # mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina root@host # mkdir -p /mnt/hadoop/cms/store/user/michaelthomas root@host # mkdir -p /mnt/hadoop/cms/store/user/brianbockelman root@host # mkdir -p /mnt/hadoop/cms/store/user/douglasstrain root@host # mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana (c) Adjust username:group ownership of each area. root@host # chown -R cms:cms /mnt/hadoop/cms root@host # chown -R sam:sam /mnt/hadoop/dzero root@host # chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas GridFTP Configuration \u00b6 gridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop. By now, you should have followed the instruction for installing hadoop as detailed in the previous section as well as created the proper users/directories. The default settings in /etc/gridftp.conf along with /etc/gridftp.d/gridftp-hdfs.conf are used by the init.d script and should be ok for most installations. The file /etc/gridftp-hdfs/gridftp-debug.conf is used by /usr/bin/gridftp-hdfs-standalone for starting up the GridFTP server in a testing mode. Any additional config files under /etc/gridftp.d will be used for both the init.d and standalone GridFTP server. /etc/sysconfig/gridftp-hdfs contains additional site-specific environment variables that are used by the gridftp-hdfs DSI module in both the init.d and standalone GridFTP server. Some of the environment variables that can be used in /etc/sysconfig/gridftp-hdfs include: Option Name Needs Editing? Suggested value GRIDFTP_HDFS_REPLICA_MAP No File containing a list of paths and replica values for setting the default # of replicas for specific file paths GRIDFTP_BUFFER_COUNT No The number of 1MB memory buffers used to reorder data streams before writing them to Hadoop GRIDFTP_FILE_BUFFER_COUNT No The number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop GRIDFTP_SYSLOG No Set this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application) GRIDFTP_HDFS_CHECKSUMS Maybe List of checksum calculations to perform on-the-fly (default: \"MD5,ADLER32,CRC32,CKSUM,CVMFS\" ) GRIDFTP_HDFS_MOUNT_POINT Maybe The location of the FUSE mount point used during the Hadoop installation. Defaults to /mnt/hadoop. This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths. Note: this does not imply you need FUSE mounted on GridFTP nodes! GRIDFTP_LOAD_LIMIT No GridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20. TMPDIR Maybe The temp directory where the file-based buffers are stored. Defaults to /tmp. /etc/sysconfig/gridftp-hdfs is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files ( ulimit -n ). Lastly, you will need to configure an authentication mechanism for GridFTP. Configuring authentication \u00b6 For information on how to configure authentication for your GridFTP installation, please refer to the configuring authentication section of the GridFTP guide . GridFTP Gratia Transfer Probe Configuration \u00b6 Note Needed by GridFTP node only. See the GridFTP documentation for configuration details. Hadoop Storage Probe Configuration \u00b6 Note This is only needed by the Hadoop NameNode Here are the most relevant file and directory locations: Purpose Needs Editing? Location Probe Configuration Yes /etc/gratia/hadoop-storage/ProbeConfig Probe Executable No /usr/share/gratia/hadoop-storage/hadoop_storage_probe Log files No /var/log/gratia Temporary files No /var/lib/gratia/tmp The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by two files /etc/gratia/hadoop-storage/ProbeConfig /etc/gratia/hadoop-storage/storage.cfg ProbeConfig \u00b6 This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following: Attribute Needs Editing Value CollectorHost Maybe Set to the hostname and port of the central collector. By default it sends to the OSG collector. You probably do not want to change it. SiteName Yes Set to the resource group name of your SE as registered in OIM. Grid Maybe Set to \"ITB\" if this is a test resource; otherwise, leave as OSG. EnableProbe Yes Set to 1 to enable the probe. storage.cfg \u00b6 This file controls which paths in HDFS should be monitored. This is in the Windows INI format. Note: for the current version of the storage.cfg, there is an error, and you may need to delete the \"probe/\" subdirectory for the ProbeConfig location ProbeConfig = /etc/gratia/probe/hadoop-storage/ProbeConfig For each logical \"area\" (arbitrarily defined by you), specify both a given name and a list of paths that belong to that area. Unix globs are accepted. To configure an area named \"CMS /store\" that monitors the space usage in the paths /user/cms/store/*, one would add the following to the storage.cfg file. [Area CMS /store] Name = CMS /store Path = /user/cms/store/* Trim = /user/cms For each such area, add a section to your configuration file. Example file \u00b6 Below is a configuration file that includes three distinct areas. Note that you shouldn't have to touch the [Gratia] section if you edited the ProbeConfig above: [Gratia] gratia_location = /opt/vdt/gratia ProbeConfig = %(gratia_location)s/probe/hadoop-storage/ProbeConfig [Area /store] Name = CMS /store Path = /store/* [Area /store/user] Name = CMS /store/user Path = /store/user/* [Area /user] Name = Hadoop /user Path = /user/* * NOTE These lines in the [gratia] section are wrong and need to be changed to the following by hand for now until the rpm is updated: gratia_location = /etc/gratia ProbeConfig = %(gratia_location)s/hadoop-storage/ProbeConfig Running Services \u00b6 Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> The relevant service for each node is as follows: Node Service Primary NameNode hadoop-hdfs-namenode Secondary NameNode hadoop-hdfs-secondarynamenode DataNode hadoop-hdfs-datanode GridFTP globus-gridftp-server Validation \u00b6 The first thing you may want to do after installing and starting your primary NameNode is to verify that the web interface works. In your web browser go to: http://<NAMENODE.HOSTNAME>:50070/dfshealth.jsp Change <NAMENODE.HOSTNAME> for the hostname of your Primary NameNode. Get familiar with Hadoop commands. Run hadoop with no arguments to see the list of commands. Show detailed ouput user$ hadoop Usage: hadoop [--config confdir] COMMAND where COMMAND is one of: namenode -format format the DFS filesystem secondarynamenode run the DFS secondary namenode namenode run the DFS namenode datanode run a DFS datanode dfsadmin run a DFS admin client mradmin run a Map-Reduce admin client fsck run a DFS filesystem checking utility fs run a generic filesystem user client balancer run a cluster balancing utility fetchdt fetch a delegation token from the NameNode jobtracker run the MapReduce job Tracker node pipes run a Pipes job tasktracker run a MapReduce task Tracker node job manipulate MapReduce jobs queue get information regarding JobQueues version print the version jar <jar> run a jar file distcp <srcurl> <desturl> copy file or directories recursively archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive oiv apply the offline fsimage viewer to an fsimage classpath prints the class path needed to get the Hadoop jar and the required libraries daemonlog get/set the log level for each daemon or CLASSNAME run the class named CLASSNAME Most commands print help when invoked w/o parameters. For a list of supported filesystem commands: Show 'hadoop fs' detailed ouput user$ hadoop fs Usage: java FsShell [-ls <path>] [-lsr <path>] [-df [<path>]] [-du <path>] [-dus <path>] [-count[-q] <path>] [-mv <src> <dst>] [-cp <src> <dst>] [-rm [-skipTrash] <path>] [-rmr [-skipTrash] <path>] [-expunge] [-put <localsrc> ... <dst>] [-copyFromLocal <localsrc> ... <dst>] [-moveFromLocal <localsrc> ... <dst>] [-get [-ignoreCrc] [-crc] <src> <localdst>] [-getmerge <src> <localdst> [addnl]] [-cat <src>] [-text <src>] [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>] [-moveToLocal [-crc] <src> <localdst>] [-mkdir <path>] [-setrep [-R] [-w] <rep> <path/file>] [-touchz <path>] [-test -[ezd] <path>] [-stat [format] <path>] [-tail [-f] <file>] [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-chgrp [-R] GROUP PATH...] [-help [cmd]] Generic options supported are -conf <configuration file> specify an application configuration file -D <property=value> use value for given property -fs <local|namenode:port> specify a namenode -jt <local|jobtracker:port> specify a job tracker -files <comma separated list of files> specify comma separated files to be copied to the map reduce cluster -libjars <comma separated list of jars> specify comma separated jar files to include in the classpath. -archives <comma separated list of archives> specify comma separated archives to be unarchived on the compute machines. The general command line syntax is bin/hadoop command [genericOptions] [commandOptions] An online guide is also available at Apache Hadoop commands manual . You can use Hadoop commands to perform filesystem operations with more consistency. Example, to look into the internal hadoop namespace: user$ hadoop fs -ls / Found 1 items drwxrwxr-x - engage engage 0 2011-07-25 06:32 /engage Example, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself /mnt/hadoop in Hadoop commands): root@host # hadoop fs -chown -R engage:engage /engage Example, compare hadoop fs command vs. using FUSE mount: user$ hadoop fs -ls /engage Found 3 items -rw-rw-r-- 2 engage engage 733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso -rw-rw-r-- 2 engage engage 215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz -rw-rw-r-- 2 engage engage 9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz user$ ls -l /mnt/hadoop/engage total 935855 -rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso -rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz -rw-rw-r-- 1 engage engage 9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz GridFTP Validation \u00b6 Note The commands used to verify GridFTP below assume you have access to a node where you can first generate a valid proxy using voms-proxy-init or grid-proxy-init . Obtaining grid credentials is beyond the scope of this document. user$ globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt If you are having troubles running GridFTP refer to Starting GridFTP in Standalone Mode in the Troubleshooting section. Troubleshooting \u00b6 Hadoop \u00b6 To view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser: http://<NAMENODE.HOSTNAME>:50070/conf Change <NAMENODE.HOSTNAME> for the hostname of your Primary NameNode. You will see the entire configuration in XML format, for example: Expand XML configuration <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?><configuration> <property><!--Loaded from core-default.xml--><name>fs.s3n.impl</name><value>org.apache.hadoop.fs.s3native.NativeS3FileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.cache.levels</name><value>2</value></property> <property><!--Loaded from mapred-default.xml--><name>map.sort.class</name><value>org.apache.hadoop.util.QuickSort</value></property> <property><!--Loaded from core-site.xml--><name>hadoop.tmp.dir</name><value>/data1/hadoop//scratch</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.native.lib</name><value>true</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.decommission.nodes.per.interval</name><value>5</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.need.client.auth</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.idlethreshold</name><value>4000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.system.dir</name><value>${hadoop.tmp.dir}/mapred/system</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.data.dir.perm</name><value>755</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.persist.jobstatus.hours</name><value>0</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.namenode.logging.level</name><value>all</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.address</name><value>0.0.0.0:50010</value></property> <property><!--Loaded from core-default.xml--><name>io.skip.checksum.errors</name><value>false</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.block.access.token.enable</name><value>false</value></property> <property><!--Loaded from Unknown--><name>fs.default.name</name><value>hdfs://nagios.t2.ucsd.edu:9000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.child.tmp</name><value>./tmp</value></property> <property><!--Loaded from core-default.xml--><name>fs.har.impl.disable.cache</name><value>true</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.reduce.max.skip.groups</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.safemode.threshold.pct</name><value>0.999f</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.heartbeats.in.second</name><value>100</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.namenode.handler.count</name><value>40</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.blockreport.initialDelay</name><value>0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.instrumentation</name><value>org.apache.hadoop.mapred.JobTrackerMetricsInst</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.dns.nameserver</name><value>default</value></property> <property><!--Loaded from mapred-default.xml--><name>io.sort.factor</name><value>10</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.timeout</name><value>600000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.max.tracker.failures</name><value>4</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.rpc.socket.factory.class.default</name><value>org.apache.hadoop.net.StandardSocketFactory</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.jobhistory.lru.cache.size</name><value>5</value></property> <property><!--Loaded from core-default.xml--><name>fs.hdfs.impl</name><value>org.apache.hadoop.hdfs.DistributedFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.map.auto.incr.proc.count</name><value>true</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.block.access.key.update.interval</name><value>600</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.job.complete.cancel.delegation.tokens</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>io.mapfile.bloom.size</name><value>1048576</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.reduce.shuffle.connect.timeout</name><value>180000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.safemode.extension</name><value>30000</value></property> <property><!--Loaded from mapred-site.xml--><name>tasktracker.http.threads</name><value>50</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.shuffle.merge.percent</name><value>0.66</value></property> <property><!--Loaded from core-default.xml--><name>fs.ftp.impl</name><value>org.apache.hadoop.fs.ftp.FTPFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.output.compress</name><value>false</value></property> <property><!--Loaded from core-site.xml--><name>io.bytes.per.checksum</name><value>4096</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.healthChecker.script.timeout</name><value>600000</value></property> <property><!--Loaded from core-default.xml--><name>topology.node.switch.mapping.impl</name><value>org.apache.hadoop.net.ScriptBasedMapping</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.server.keystore.resource</name><value>ssl-server.xml</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.slowstart.completed.maps</name><value>0.05</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.max.attempts</name><value>4</value></property> <property><!--Loaded from core-default.xml--><name>fs.ramfs.impl</name><value>org.apache.hadoop.fs.InMemoryFileSystem</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.block.access.token.lifetime</name><value>600</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.map.max.skip.records</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.name.edits.dir</name><value>${dfs.name.dir}</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.security.group.mapping</name><value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.persist.jobstatus.dir</name><value>/jobtracker/jobsInfo</value></property> <property><!--Loaded from core-site.xml--><name>hadoop.log.dir</name><value>/var/log/hadoop</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.buffer.dir</name><value>${hadoop.tmp.dir}/s3</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.block.size</name><value>134217728</value></property> <property><!--Loaded from mapred-default.xml--><name>job.end.retry.attempts</name><value>0</value></property> <property><!--Loaded from core-default.xml--><name>fs.file.impl</name><value>org.apache.hadoop.fs.LocalFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.output.compression.type</name><value>RECORD</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.local.dir.minspacestart</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.ipc.address</name><value>0.0.0.0:50020</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.permissions</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>topology.script.number.args</name><value>100</value></property> <property><!--Loaded from core-default.xml--><name>io.mapfile.bloom.error.rate</name><value>0.005</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.max.tracker.blacklists</name><value>4</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.profile.maps</name><value>0-2</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.https.address</name><value>0.0.0.0:50475</value></property> <property><!--Loaded from core-site.xml--><name>dfs.umaskmode</name><value>002</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.userlog.retain.hours</name><value>24</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.secondary.http.address</name><value>gratia-1:50090</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.replication.max</name><value>32</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.persist.jobstatus.active</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.security.authorization</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>local.cache.size</name><value>10737418240</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.min.split.size</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.delegation.token.renew-interval</name><value>86400000</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.map.tasks</name><value>7919</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.child.java.opts</name><value>-Xmx200m</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.client.keystore.resource</name><value>ssl-client.xml</value></property> <property><!--Loaded from Unknown--><name>dfs.namenode.startup</name><value>REGULAR</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.queue.name</name><value>default</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.retiredjobs.cache.size</name><value>1000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.address</name><value>0.0.0.0:50470</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.balance.bandwidthPerSec</name><value>2000000000</value></property> <property><!--Loaded from core-default.xml--><name>ipc.server.listen.queue.size</name><value>128</value></property> <property><!--Loaded from mapred-default.xml--><name>job.end.retry.interval</name><value>30000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.inmem.merge.threshold</name><value>1000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.attempts.to.start.skipping</name><value>2</value></property> <property><!--Loaded from hdfs-site.xml--><name>fs.checkpoint.dir</name><value>/var/hadoop/checkpoint-a</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.reduce.tasks</name><value>1543</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.merge.recordsBeforeProgress</name><value>10000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.userlog.limit.kb</name><value>0</value></property> <property><!--Loaded from core-default.xml--><name>webinterface.private.actions</name><value>false</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.max.objects</name><value>0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.shuffle.input.buffer.percent</name><value>0.70</value></property> <property><!--Loaded from mapred-default.xml--><name>io.sort.spill.percent</name><value>0.80</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.map.tasks.speculative.execution</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.util.hash.type</name><value>murmur</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.dns.nameserver</name><value>default</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.blockreport.intervalMsec</name><value>3600000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.map.max.attempts</name><value>4</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.job.acl-view-job</name><value> </value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.handler.count</name><value>10</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.client.block.write.retries</name><value>3</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.max.reduces.per.node</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.reduce.shuffle.read.timeout</name><value>180000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.expiry.interval</name><value>600000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.enable</name><value>false</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.maxtasks.per.job</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.job.history.block.size</name><value>3145728</value></property> <property><!--Loaded from mapred-default.xml--><name>keep.failed.task.files</name><value>false</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.failed.volumes.tolerated</name><value>0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.profile.reduces</name><value>0-2</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.tcpnodelay</name><value>false</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.output.compression.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value></property> <property><!--Loaded from mapred-default.xml--><name>io.map.index.skip</name><value>0</value></property> <property><!--Loaded from core-default.xml--><name>ipc.server.tcpnodelay</name><value>false</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.delegation.key.update-interval</name><value>86400000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.running.map.limit</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>jobclient.progress.monitor.poll.interval</name><value>1000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.default.chunk.view.size</name><value>32768</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.logfile.size</name><value>10000000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.tasks.speculative.execution</name><value>true</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.tasktracker.outofband.heartbeat</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3n.block.size</name><value>67108864</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.datanode.du.reserved</name><value>10000000000</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.security.authentication</name><value>simple</value></property> <property><!--Loaded from hdfs-site.xml--><name>fs.checkpoint.period</name><value>3600</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.running.reduce.limit</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.reuse.jvm.num.tasks</name><value>1</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.web.ugi</name><value>webuser,webgroup</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.completeuserjobs.maximum</name><value>100</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.df.interval</name><value>60000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.tracker.task-controller</name><value>org.apache.hadoop.mapred.DefaultTaskController</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.data.dir</name><value>/data1/hadoop//data</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.maxRetries</name><value>4</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.dns.interface</name><value>default</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.support.append</name><value>true</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.job.acl-modify-job</name><value> </value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.local.dir</name><value>${hadoop.tmp.dir}/mapred/local</value></property> <property><!--Loaded from core-default.xml--><name>fs.hftp.impl</name><value>org.apache.hadoop.hdfs.HftpFileSystem</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.permissions.supergroup</name><value>root</value></property> <property><!--Loaded from core-default.xml--><name>fs.trash.interval</name><value>0</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.sleepTimeSeconds</name><value>10</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.submit.replication</name><value>10</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.replication.min</name><value>1</value></property> <property><!--Loaded from core-default.xml--><name>fs.har.impl</name><value>org.apache.hadoop.fs.HarFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.map.output.compression.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.dns.interface</name><value>default</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.decommission.interval</name><value>30</value></property> <property><!--Loaded from Unknown--><name>dfs.http.address</name><value>nagios:50070</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.job.tracker</name><value>nagios:9000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.heartbeat.interval</name><value>3</value></property> <property><!--Loaded from core-default.xml--><name>io.seqfile.sorter.recordlimit</name><value>1000000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.name.dir</name><value>${hadoop.tmp.dir}/dfs/name</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.line.input.format.linespermap</name><value>1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.taskScheduler</name><value>org.apache.hadoop.mapred.JobQueueTaskScheduler</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.instrumentation</name><value>org.apache.hadoop.mapred.TaskTrackerMetricsInst</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.http.address</name><value>0.0.0.0:50075</value></property> <property><!--Loaded from mapred-default.xml--><name>jobclient.completion.poll.interval</name><value>5000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.max.maps.per.node</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.local.dir.minspacekill</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.replication.interval</name><value>3</value></property> <property><!--Loaded from mapred-default.xml--><name>io.sort.record.percent</name><value>0.05</value></property> <property><!--Loaded from core-default.xml--><name>fs.kfs.impl</name><value>org.apache.hadoop.fs.kfs.KosmosFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.temp.dir</name><value>${hadoop.tmp.dir}/mapred/temp</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.tasktracker.reduce.tasks.maximum</name><value>4</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.replication</name><value>2</value></property> <property><!--Loaded from core-default.xml--><name>fs.checkpoint.edits.dir</name><value>${fs.checkpoint.dir}</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.tasks.sleeptime-before-sigkill</name><value>5000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.reduce.input.buffer.percent</name><value>0.0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.indexcache.mb</name><value>10</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.job.split.metainfo.maxsize</name><value>10000000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.reduce.auto.incr.proc.count</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.logfile.count</name><value>10</value></property> <property><!--Loaded from core-default.xml--><name>fs.automatic.close</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>io.seqfile.compress.blocksize</name><value>1000000</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.hosts.exclude</name><value>/etc/hadoop-0.20/conf/hosts_exclude</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.block.size</name><value>67108864</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.taskmemorymanager.monitoring-interval</name><value>5000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.acls.enabled</name><value>false</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.jobtracker.staging.root.dir</name><value>${hadoop.tmp.dir}/mapred/staging</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.queue.names</name><value>default</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.access.time.precision</name><value>3600000</value></property> <property><!--Loaded from core-default.xml--><name>fs.hsftp.impl</name><value>org.apache.hadoop.hdfs.HsftpFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.tracker.http.address</name><value>0.0.0.0:50060</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.parallel.copies</name><value>5</value></property> <property><!--Loaded from core-default.xml--><name>io.seqfile.lazydecompress</name><value>true</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.safemode.min.datanodes</name><value>0</value></property> <property><!--Loaded from mapred-default.xml--><name>io.sort.mb</name><value>100</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.connection.maxidletime</name><value>10000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.compress.map.output</name><value>false</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.tracker.report.address</name><value>127.0.0.1:0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.healthChecker.interval</name><value>60000</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.kill.max</name><value>10</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.connect.max.retries</name><value>10</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.impl</name><value>org.apache.hadoop.fs.s3.S3FileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.http.address</name><value>0.0.0.0:50030</value></property> <property><!--Loaded from core-default.xml--><name>io.file.buffer.size</name><value>4096</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.restart.recover</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>io.serializations</name><value>org.apache.hadoop.io.serializer.WritableSerialization</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.profile</name><value>false</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.datanode.handler.count</name><value>10</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.copy.backoff</name><value>300</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.replication.considerLoad</name><value>true</value></property> <property><!--Loaded from mapred-default.xml--><name>jobclient.output.filter</name><value>FAILED</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.delegation.token.max-lifetime</name><value>604800000</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.tasktracker.map.tasks.maximum</name><value>4</value></property> <property><!--Loaded from core-default.xml--><name>io.compression.codecs</name><value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec</value></property> <property><!--Loaded from core-default.xml--><name>fs.checkpoint.size</name><value>67108864</value></property> </configuration> Please refer to the Apache Hadoop documentation for answers to common questions/concerns FUSE \u00b6 Notes on Building a FUSE Module \u00b6 If you are running a custom kernel, then be sure to enable the fuse module with CONFIG_FUSE_FS=m in your kernel config. Building and installing a fuse kernel module for your custom kernel is beyond the scope of this document. Running FUSE in Debug Mode \u00b6 To start the FUSE mount in debug mode, you can run the FUSE mount command by hand: root@host # /usr/bin/hadoop-fuse-dfs /mnt/hadoop -o rw,server = <NAMENODE.HOST>,port = 9000 ,rdbuffer = 131072 ,allow_other -d Change <NAMENODE.HOSTNAME> for the hostname of your Primary NameNode. Debug output will be printed to stderr, which you will probably want to redirect to a file. Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages. GridFTP \u00b6 Starting GridFTP in Standalone Mode \u00b6 If you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command: root@host # gridftp-hdfs-standalone The standalone server runs on port 5002, handles a single GridFTP request, and will log output to stdout/stderr. File Locations \u00b6 Component File Type Location Needs editing? Hadoop Log files /var/log/hadoop/* No PID files /var/run/hadoop/*.pid No init scripts /etc/init.d/hadoop No init script config file /etc/sysconfig/hadoop Yes runtime config files /etc/hadoop/conf/* Maybe System binaries /usr/bin/hadoop No JARs /usr/lib/hadoop/* No runtime config files /etc/hosts_exclude Yes, must be present on NameNodes Log files /var/log/gridftp-auth.log , /var/log/gridftp.log No GridFTP Transfer log /var/log/gridftp.log No Authentication log /var/log/gridftp-auth.log No LCMAPS auth error log /var/log/messages No init.d script /etc/init.d/globus-gridftp-server No runtime config files /etc/gridftp-hdfs/* , /etc/sysconfig/gridftp-hdfs Maybe System binaries /usr/bin/gridftp-hdfs-standalone , /usr/sbin/globus-gridftp-server No System libraries /usr/lib64/libglobus_gridftp_server_hdfs.so* No LCMAPS VOMS configuration /etc/lcmaps.db Yes CA certificates /etc/grid-security/certificates/* No Known Issues \u00b6 Replicas \u00b6 You may need to change the following line in /usr/share/gridftp-hdfs/gridftp-hdfs-environment : export GRIDFTP_HDFS_REPLICAS=2 copyFromLocal java IOException \u00b6 When trying to copy a local file into Hadoop you may come across the following java exception: Show detailed java exception 11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0] nodes == null 11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file \"/osg/ddd\" - Aborting... copyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0 nodes, instead of 1 11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd : org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only be replicated to 0 nodes, instead of 1 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415) at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313) This can occur if you try to install a DataNode on a machine with less than 10GB of disk space available. This can be changed by lowering the value of the following property in /usr/lib/hadoop-0.20/conf/hdfs-site.xml : <property> <name>dfs.datanode.du.reserved</name> <value>10000000000</value> </property> Hadoop always requires this amount of disk space to be available for non-hdfs usage on the machine. Getting Help \u00b6 To get assistance, please use the this page . References \u00b6 Using Hadoop as a Grid Storage Element , Journal of Physics Conference Series, 2009 . Hadoop Distributed File System for the Grid , IEEE Nuclear Science Symposium, 2009 . Users \u00b6 This installation will create following users unless they are already created. User Comment hadoop Runs the NameNode services hdfs Used by Hadoop to store data blocks and meta-data mapred zookeeper For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.","title":"Install HDFS"},{"location":"data/install-hadoop/#installing-and-maintaining-hdfs","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. Hadoop Distributed File System (HDFS) is a scalable, reliable distributed file system developed in the Apache project. It is based on the map-reduce framework and design of the Google file system. The OSG distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. The purpose of this document is to provide Hadoop-based Storage Element administrators the information on how to prepare, install and validate OSG storage based on the Hadoop Distributed File System (HDFS). The OSG supports a patched version HDFS from Cloudera's CDH5 distribution of HDFS ( https://www.cloudera.com/products/open-source/apache-hadoop/key-cdh-components.html ). Note The OSG only supports HDFS on EL7 hosts","title":"Installing and Maintaining HDFS"},{"location":"data/install-hadoop/#before-starting","text":"Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If they do not exist already, the installation will create the Linux users hdfs and zookeeper on all nodes as well as hadoop and mapred on the NameNodes Firewall: In the OSG, HDFS is intended to run as an internal service without any direct, external access to any of the nodes. For more information on the ports used for communication between the various HDFS nodes, see the Cloudera documentation . As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories","title":"Before Starting"},{"location":"data/install-hadoop/#designing-your-hdfs-cluster","text":"There are several important components to an HDFS installation: NameNode : The NameNode functions as the directory server and coordinator of the HDFS cluster. It houses all the meta-data for the hadoop cluster. Secondary NameNode (optional) : This is a secondary machine that periodically merges updates to the HDFS file system back into the fsimage . It must share a directory with the primary NameNode to exchange filesystem checkpoints. An HDFS installation with a Secondary NameNode dramatically improves startup and restart times. DataNode : You will have many DataNodes. Each DataNode stores large blocks of files to for the hadoop cluster. Client : This is a documentation shorthand that refers to any machine with the hadoop client commands or FUSE mount.","title":"Designing Your HDFS Cluster"},{"location":"data/install-hadoop/#installing-hdfs","text":"An OSG HDFS installation consists of HDFS and other support software (e.g., Gratia accounting). To simplify installation, OSG provides convenience RPMs that install all required software. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the relevant packages based on the node you are installing: If you are installing a(n)... Then run the following command... Primary NameNode yum install osg-se-hadoop-namenode Secondary NameNode yum install osg-se-hadoop-secondarynamenode DataNode yum install osg-se-hadoop-datanode","title":"Installing HDFS"},{"location":"data/install-hadoop/#upgrading-hdfs","text":"This section will guide you through the process to upgrade a HDFS 2.0.0 installation from OSG 3.3 to the HDFS 2.6.0 from OSG 3.5. Warning The upgrade process will involve downtime for your HDFS cluster. Please plan accordingly. Note The OSG only offers HDFS 2.6.0 for EL7 hosts. The upgrade process occurs in several steps: Preparing for the upgrade Updating to OSG 3.5 Upgrading the Primary NameNode Upgrading the DataNodes Upgrading the Secondary NameNode Finalizing the upgrade","title":"Upgrading HDFS"},{"location":"data/install-hadoop/#preparing-for-the-upgrade","text":"Before upgrading, backup your configuration data and HDFS metadata. Put your Primary NameNode into safe mode: root@primary-namenode # hdfs dfsadmin -safemode enter Safe mode is ON Save a clean copy of your HDFS namespace: root@primary-namenode # hdfs dfsadmin -saveNamespace Save namespace successful Shutdown the HDFS services on all of your HDFS nodes (see this section for instructions). On the Primary NameNode, verify that your NameNode service is off: root@primary-namenode # /etc/init.d/hadoop-hdfs-namenode status This command should indicate that your NameNode service is not running. Find the location of the directory with the HDFS metadata: root@primary-namenode # grep -C1 dfs.namenode.name.dir /etc/hadoop/conf/hdfs-site.xml And look for the value of dfs.namenode.name.dir : <property> <name> dfs.namenode.name.dir </name> <value> file:///var/lib/dfs/nn,file:///home/hadoop/dfs/nn </value> Backup the directory that appears in the output using your backup method of choice. If more than one directory appears in the list (as in the example above), choose the most convenient directory. All of the directories in the list will have the same contents.","title":"Preparing for the upgrade"},{"location":"data/install-hadoop/#updating-to-osg-35","text":"Once your HDFS services have been turned off and the HDFS metadata has been backed up, update each node to OSG 3.5 by following the instructions in this section .","title":"Updating to OSG 3.5"},{"location":"data/install-hadoop/#upgrading-the-primary-namenode","text":"To upgrade your Primary NameNode, update all relevant packages then run the upgrade command. Clear the yum cache: root@primary-namenode # yum clean all --enablerepo = * Update the HDFS RPMs: root@primary-namenode # yum update osg-se-hadoop-namenode --enablerepo-osg-upcoming Perform the upgrade command: root@primary-namenode # /etc/init.d/hadoop-hdfs-namenode upgrade This will start the upgrade process for the HDFS metadata on your primary namenode. You can follow the process by running root@primary-namenode # tail -f /var/log/hadoop-hdfs/hadoop-hdfs-namenode-<hostname>.log","title":"Upgrading the Primary NameNode"},{"location":"data/install-hadoop/#upgrading-the-datanodes","text":"Once the Primary NameNode has completed its upgrade process, start the process of upgrading each of your DataNodes. Clear the yum cache: root@datanode # yum clean all --enablerepo = * Update the HDFS RPMs: root@datanode # yum update osg-se-hadoop-datanode --enablerepo-osg-upcoming Start the DataNode service: root@datanode # /etc/init.d/hadoop-hdfs-datanode start After all the DataNodes have been brought back up, the Primary NameNode should exit safe mode automatically. On the Primary NameNode, run the following command to verify is no longer in safe mode: root@primary-namenode # hdfs dfsadmin -safemode get Safe mode is OFF","title":"Upgrading the DataNodes"},{"location":"data/install-hadoop/#upgrading-the-secondary-namenode","text":"Note This section only applies to sites with a Secondary NameNode. If you do not run a Secondary NameNode, skip to the next section . Once the Primary NameNode has exited safe mode, start the process of upgrading your Secondary NameNode. Clear the yum cache: root@secondary-namenode # yum clean all --enablerepo = * Update the HDFS RPMs: root@secondary-namenode # yum update osg-se-hadoop-secondarynamenode --enablerepo-osg-upcoming Start the Secondary NameNode service: root@secondary-namenode # /etc/init.d/hadoop-hdfs-secondarynamenode start","title":"Upgrading the Secondary NameNode"},{"location":"data/install-hadoop/#finalizing-the-upgrade","text":"Verify that the HDFS cluster is running correctly by following the instructions in this section . Finalize the upgrade from the Primary NameNode: root@primary-namenode # hdfs dfsadmin -finalizeUpgrade Finalize upgrade successful","title":"Finalizing the upgrade"},{"location":"data/install-hadoop/#configuring-hdfs","text":"Note Needed by: Hadoop NameNode, Hadoop DataNodes, Hadoop client, GridFTP Hadoop configuration is needed by every node in the hadoop cluster. However, in most cases, you can do the configuration once and copy it to all nodes in the cluster (possibly using your favorite configuration management tool). Special configuration for various special components is given in the below sections. Hadoop configuration is stored in /etc/hadoop/conf . However, by default, these files are mostly blank. OSG provides a sample configuration in /etc/hadoop/conf.osg with most common values filled in. You will need to copy these into /etc/hadoop/conf before they become active. Please let us know if there are any common values that should be added/changed across the whole grid. You will likely need to modify hdfs-site.xml and core-site.xml . Review all the settings in these files, but listed below are common settings to modify: File Setting Example Comments core-site.xml fs.default.name hdfs://namenode.domain.tld.:9000 This is the address of the NameNode core-site.xml hadoop.tmp.dir /data/scratch Scratch temp directory used by Hadoop core-site.xml hadoop.log.dir /var/log/hadoop-hdfs Log directory used by Hadoop core-site.xml dfs.umaskmode 002 umask for permissions used by default hdfs-site.xml dfs.block.size 134217728 Block size: 128MB by default hdfs-site.xml dfs.replication 2 Default replication factor. Generally the same as dfs.replication.min/max hdfs-site.xml dfs.datanode.du.reserved 100000000 How much free space hadoop will reserve for non-Hadoop usage hdfs-site.xml dfs.datanode.handler.count 20 Number of server threads for DataNodes. Increase if you have many more client connections hdfs-site.xml dfs.namenode.handler.count 40 Number of server threads for NameNodes. Increase if you need more connections hdfs-site.xml dfs.http.address namenode.domain.tld.:50070 Web address for dfs health monitoring page See http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml for more parameters to configure. Note NameNodes must have a /etc/hosts_exclude present","title":"Configuring HDFS"},{"location":"data/install-hadoop/#special-namenode-instructions-for-brand-new-installs","text":"If this is a new installation ( and only if this is a brand new installation ), you should run the following command as the hdfs user. (Otherwise, be sure to chown your storage directory to hdfs after running): hadoop namenode -format This will initialize the storage directory on your NameNode","title":"Special NameNode instructions for brand new installs"},{"location":"data/install-hadoop/#optional-fuse-client-configuration","text":"A FUSE mount is required on any node that you would like to use standard POSIX-like commands on the Hadoop filesystem. FUSE (or \"File system in User SpacE\") is a way to access HDFS using typical UNIX directory commands (i.e., POSIX-like access). Note that not all advanced functions of a full POSIX-compliant file system are necessarily available. FUSE is typically installed as part of this installation, but, if you are running a customized or non-standard system, make sure that the fuse kernel module is installed and loaded with modprobe fuse . You can add the FUSE to be mounted at boot time by adding the following line to /etc/fstab : hadoop-fuse-dfs# </MNT/HADOOP> fuse server=<NAMENODE.HOST>,port=9000,rdbuffer=131072,allow_other 0 0 Be sure to change the </MNT/HADOOP> mount point and <NAMENODE.HOST> to match your local configuration. To match the help documents, we recommend using </MNT/HADOOP> as your mountpoint. Once your /etc/fstab is updated, to mount FUSE run: root@host # mkdir /mnt/hadoop root@host # mount /mnt/hadoop When mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen: # mount /mnt/hadoop INFO fuse_options.c:162 Adding FUSE arg /mnt/hadoop INFO fuse_options.c:110 Ignoring option allow_other If you have troubles mounting FUSE refer to Running FUSE in Debug Mode in the Troubleshooting section.","title":"(optional) FUSE Client Configuration"},{"location":"data/install-hadoop/#creating-vo-and-user-areas","text":"Note Grid Users are needed by GridFTP nodes. VO areas are common to all nodes. For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created. For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server. Note that these users must be kept in sync with the authentication method. Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. Create (and maintain) usernames and groups with UIDs and GIDs on all nodes . These are maintained in basic system files such as /etc/passwd and /etc/group . Note In the examples below It is assumed a FUSE mount is set to /mnt/hadoop . As an alternative hadoop fs commands could have been used. For clean HDFS operations and filesystem management: (a) Create top-level VO subdirectories under /mnt/hadoop . Example: root@host # mkdir /mnt/hadoop/cms root@host # mkdir /mnt/hadoop/dzero root@host # mkdir /mnt/hadoop/sbgrid root@host # mkdir /mnt/hadoop/fermigrid root@host # mkdir /mnt/hadoop/cmstest root@host # mkdir /mnt/hadoop/osg (b) Create individual top-level user areas, under each VO area, as needed. root@host # mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina root@host # mkdir -p /mnt/hadoop/cms/store/user/michaelthomas root@host # mkdir -p /mnt/hadoop/cms/store/user/brianbockelman root@host # mkdir -p /mnt/hadoop/cms/store/user/douglasstrain root@host # mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana (c) Adjust username:group ownership of each area. root@host # chown -R cms:cms /mnt/hadoop/cms root@host # chown -R sam:sam /mnt/hadoop/dzero root@host # chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas","title":"Creating VO and User Areas"},{"location":"data/install-hadoop/#gridftp-configuration","text":"gridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop. By now, you should have followed the instruction for installing hadoop as detailed in the previous section as well as created the proper users/directories. The default settings in /etc/gridftp.conf along with /etc/gridftp.d/gridftp-hdfs.conf are used by the init.d script and should be ok for most installations. The file /etc/gridftp-hdfs/gridftp-debug.conf is used by /usr/bin/gridftp-hdfs-standalone for starting up the GridFTP server in a testing mode. Any additional config files under /etc/gridftp.d will be used for both the init.d and standalone GridFTP server. /etc/sysconfig/gridftp-hdfs contains additional site-specific environment variables that are used by the gridftp-hdfs DSI module in both the init.d and standalone GridFTP server. Some of the environment variables that can be used in /etc/sysconfig/gridftp-hdfs include: Option Name Needs Editing? Suggested value GRIDFTP_HDFS_REPLICA_MAP No File containing a list of paths and replica values for setting the default # of replicas for specific file paths GRIDFTP_BUFFER_COUNT No The number of 1MB memory buffers used to reorder data streams before writing them to Hadoop GRIDFTP_FILE_BUFFER_COUNT No The number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop GRIDFTP_SYSLOG No Set this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application) GRIDFTP_HDFS_CHECKSUMS Maybe List of checksum calculations to perform on-the-fly (default: \"MD5,ADLER32,CRC32,CKSUM,CVMFS\" ) GRIDFTP_HDFS_MOUNT_POINT Maybe The location of the FUSE mount point used during the Hadoop installation. Defaults to /mnt/hadoop. This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths. Note: this does not imply you need FUSE mounted on GridFTP nodes! GRIDFTP_LOAD_LIMIT No GridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20. TMPDIR Maybe The temp directory where the file-based buffers are stored. Defaults to /tmp. /etc/sysconfig/gridftp-hdfs is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files ( ulimit -n ). Lastly, you will need to configure an authentication mechanism for GridFTP.","title":"GridFTP Configuration"},{"location":"data/install-hadoop/#configuring-authentication","text":"For information on how to configure authentication for your GridFTP installation, please refer to the configuring authentication section of the GridFTP guide .","title":"Configuring authentication"},{"location":"data/install-hadoop/#gridftp-gratia-transfer-probe-configuration","text":"Note Needed by GridFTP node only. See the GridFTP documentation for configuration details.","title":"GridFTP Gratia Transfer Probe Configuration"},{"location":"data/install-hadoop/#hadoop-storage-probe-configuration","text":"Note This is only needed by the Hadoop NameNode Here are the most relevant file and directory locations: Purpose Needs Editing? Location Probe Configuration Yes /etc/gratia/hadoop-storage/ProbeConfig Probe Executable No /usr/share/gratia/hadoop-storage/hadoop_storage_probe Log files No /var/log/gratia Temporary files No /var/lib/gratia/tmp The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by two files /etc/gratia/hadoop-storage/ProbeConfig /etc/gratia/hadoop-storage/storage.cfg","title":"Hadoop Storage Probe Configuration"},{"location":"data/install-hadoop/#probeconfig","text":"This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following: Attribute Needs Editing Value CollectorHost Maybe Set to the hostname and port of the central collector. By default it sends to the OSG collector. You probably do not want to change it. SiteName Yes Set to the resource group name of your SE as registered in OIM. Grid Maybe Set to \"ITB\" if this is a test resource; otherwise, leave as OSG. EnableProbe Yes Set to 1 to enable the probe.","title":"ProbeConfig"},{"location":"data/install-hadoop/#storagecfg","text":"This file controls which paths in HDFS should be monitored. This is in the Windows INI format. Note: for the current version of the storage.cfg, there is an error, and you may need to delete the \"probe/\" subdirectory for the ProbeConfig location ProbeConfig = /etc/gratia/probe/hadoop-storage/ProbeConfig For each logical \"area\" (arbitrarily defined by you), specify both a given name and a list of paths that belong to that area. Unix globs are accepted. To configure an area named \"CMS /store\" that monitors the space usage in the paths /user/cms/store/*, one would add the following to the storage.cfg file. [Area CMS /store] Name = CMS /store Path = /user/cms/store/* Trim = /user/cms For each such area, add a section to your configuration file.","title":"storage.cfg"},{"location":"data/install-hadoop/#example-file","text":"Below is a configuration file that includes three distinct areas. Note that you shouldn't have to touch the [Gratia] section if you edited the ProbeConfig above: [Gratia] gratia_location = /opt/vdt/gratia ProbeConfig = %(gratia_location)s/probe/hadoop-storage/ProbeConfig [Area /store] Name = CMS /store Path = /store/* [Area /store/user] Name = CMS /store/user Path = /store/user/* [Area /user] Name = Hadoop /user Path = /user/* * NOTE These lines in the [gratia] section are wrong and need to be changed to the following by hand for now until the rpm is updated: gratia_location = /etc/gratia ProbeConfig = %(gratia_location)s/hadoop-storage/ProbeConfig","title":"Example file"},{"location":"data/install-hadoop/#running-services","text":"Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> The relevant service for each node is as follows: Node Service Primary NameNode hadoop-hdfs-namenode Secondary NameNode hadoop-hdfs-secondarynamenode DataNode hadoop-hdfs-datanode GridFTP globus-gridftp-server","title":"Running Services"},{"location":"data/install-hadoop/#validation","text":"The first thing you may want to do after installing and starting your primary NameNode is to verify that the web interface works. In your web browser go to: http://<NAMENODE.HOSTNAME>:50070/dfshealth.jsp Change <NAMENODE.HOSTNAME> for the hostname of your Primary NameNode. Get familiar with Hadoop commands. Run hadoop with no arguments to see the list of commands. Show detailed ouput user$ hadoop Usage: hadoop [--config confdir] COMMAND where COMMAND is one of: namenode -format format the DFS filesystem secondarynamenode run the DFS secondary namenode namenode run the DFS namenode datanode run a DFS datanode dfsadmin run a DFS admin client mradmin run a Map-Reduce admin client fsck run a DFS filesystem checking utility fs run a generic filesystem user client balancer run a cluster balancing utility fetchdt fetch a delegation token from the NameNode jobtracker run the MapReduce job Tracker node pipes run a Pipes job tasktracker run a MapReduce task Tracker node job manipulate MapReduce jobs queue get information regarding JobQueues version print the version jar <jar> run a jar file distcp <srcurl> <desturl> copy file or directories recursively archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive oiv apply the offline fsimage viewer to an fsimage classpath prints the class path needed to get the Hadoop jar and the required libraries daemonlog get/set the log level for each daemon or CLASSNAME run the class named CLASSNAME Most commands print help when invoked w/o parameters. For a list of supported filesystem commands: Show 'hadoop fs' detailed ouput user$ hadoop fs Usage: java FsShell [-ls <path>] [-lsr <path>] [-df [<path>]] [-du <path>] [-dus <path>] [-count[-q] <path>] [-mv <src> <dst>] [-cp <src> <dst>] [-rm [-skipTrash] <path>] [-rmr [-skipTrash] <path>] [-expunge] [-put <localsrc> ... <dst>] [-copyFromLocal <localsrc> ... <dst>] [-moveFromLocal <localsrc> ... <dst>] [-get [-ignoreCrc] [-crc] <src> <localdst>] [-getmerge <src> <localdst> [addnl]] [-cat <src>] [-text <src>] [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>] [-moveToLocal [-crc] <src> <localdst>] [-mkdir <path>] [-setrep [-R] [-w] <rep> <path/file>] [-touchz <path>] [-test -[ezd] <path>] [-stat [format] <path>] [-tail [-f] <file>] [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-chgrp [-R] GROUP PATH...] [-help [cmd]] Generic options supported are -conf <configuration file> specify an application configuration file -D <property=value> use value for given property -fs <local|namenode:port> specify a namenode -jt <local|jobtracker:port> specify a job tracker -files <comma separated list of files> specify comma separated files to be copied to the map reduce cluster -libjars <comma separated list of jars> specify comma separated jar files to include in the classpath. -archives <comma separated list of archives> specify comma separated archives to be unarchived on the compute machines. The general command line syntax is bin/hadoop command [genericOptions] [commandOptions] An online guide is also available at Apache Hadoop commands manual . You can use Hadoop commands to perform filesystem operations with more consistency. Example, to look into the internal hadoop namespace: user$ hadoop fs -ls / Found 1 items drwxrwxr-x - engage engage 0 2011-07-25 06:32 /engage Example, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself /mnt/hadoop in Hadoop commands): root@host # hadoop fs -chown -R engage:engage /engage Example, compare hadoop fs command vs. using FUSE mount: user$ hadoop fs -ls /engage Found 3 items -rw-rw-r-- 2 engage engage 733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso -rw-rw-r-- 2 engage engage 215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz -rw-rw-r-- 2 engage engage 9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz user$ ls -l /mnt/hadoop/engage total 935855 -rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso -rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz -rw-rw-r-- 1 engage engage 9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz","title":"Validation"},{"location":"data/install-hadoop/#gridftp-validation","text":"Note The commands used to verify GridFTP below assume you have access to a node where you can first generate a valid proxy using voms-proxy-init or grid-proxy-init . Obtaining grid credentials is beyond the scope of this document. user$ globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt If you are having troubles running GridFTP refer to Starting GridFTP in Standalone Mode in the Troubleshooting section.","title":"GridFTP Validation"},{"location":"data/install-hadoop/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"data/install-hadoop/#hadoop","text":"To view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser: http://<NAMENODE.HOSTNAME>:50070/conf Change <NAMENODE.HOSTNAME> for the hostname of your Primary NameNode. You will see the entire configuration in XML format, for example: Expand XML configuration <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?><configuration> <property><!--Loaded from core-default.xml--><name>fs.s3n.impl</name><value>org.apache.hadoop.fs.s3native.NativeS3FileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.cache.levels</name><value>2</value></property> <property><!--Loaded from mapred-default.xml--><name>map.sort.class</name><value>org.apache.hadoop.util.QuickSort</value></property> <property><!--Loaded from core-site.xml--><name>hadoop.tmp.dir</name><value>/data1/hadoop//scratch</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.native.lib</name><value>true</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.decommission.nodes.per.interval</name><value>5</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.need.client.auth</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.idlethreshold</name><value>4000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.system.dir</name><value>${hadoop.tmp.dir}/mapred/system</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.data.dir.perm</name><value>755</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.persist.jobstatus.hours</name><value>0</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.namenode.logging.level</name><value>all</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.address</name><value>0.0.0.0:50010</value></property> <property><!--Loaded from core-default.xml--><name>io.skip.checksum.errors</name><value>false</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.block.access.token.enable</name><value>false</value></property> <property><!--Loaded from Unknown--><name>fs.default.name</name><value>hdfs://nagios.t2.ucsd.edu:9000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.child.tmp</name><value>./tmp</value></property> <property><!--Loaded from core-default.xml--><name>fs.har.impl.disable.cache</name><value>true</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.reduce.max.skip.groups</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.safemode.threshold.pct</name><value>0.999f</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.heartbeats.in.second</name><value>100</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.namenode.handler.count</name><value>40</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.blockreport.initialDelay</name><value>0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.instrumentation</name><value>org.apache.hadoop.mapred.JobTrackerMetricsInst</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.dns.nameserver</name><value>default</value></property> <property><!--Loaded from mapred-default.xml--><name>io.sort.factor</name><value>10</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.timeout</name><value>600000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.max.tracker.failures</name><value>4</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.rpc.socket.factory.class.default</name><value>org.apache.hadoop.net.StandardSocketFactory</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.jobhistory.lru.cache.size</name><value>5</value></property> <property><!--Loaded from core-default.xml--><name>fs.hdfs.impl</name><value>org.apache.hadoop.hdfs.DistributedFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.map.auto.incr.proc.count</name><value>true</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.block.access.key.update.interval</name><value>600</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.job.complete.cancel.delegation.tokens</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>io.mapfile.bloom.size</name><value>1048576</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.reduce.shuffle.connect.timeout</name><value>180000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.safemode.extension</name><value>30000</value></property> <property><!--Loaded from mapred-site.xml--><name>tasktracker.http.threads</name><value>50</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.shuffle.merge.percent</name><value>0.66</value></property> <property><!--Loaded from core-default.xml--><name>fs.ftp.impl</name><value>org.apache.hadoop.fs.ftp.FTPFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.output.compress</name><value>false</value></property> <property><!--Loaded from core-site.xml--><name>io.bytes.per.checksum</name><value>4096</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.healthChecker.script.timeout</name><value>600000</value></property> <property><!--Loaded from core-default.xml--><name>topology.node.switch.mapping.impl</name><value>org.apache.hadoop.net.ScriptBasedMapping</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.server.keystore.resource</name><value>ssl-server.xml</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.slowstart.completed.maps</name><value>0.05</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.max.attempts</name><value>4</value></property> <property><!--Loaded from core-default.xml--><name>fs.ramfs.impl</name><value>org.apache.hadoop.fs.InMemoryFileSystem</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.block.access.token.lifetime</name><value>600</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.map.max.skip.records</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.name.edits.dir</name><value>${dfs.name.dir}</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.security.group.mapping</name><value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.persist.jobstatus.dir</name><value>/jobtracker/jobsInfo</value></property> <property><!--Loaded from core-site.xml--><name>hadoop.log.dir</name><value>/var/log/hadoop</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.buffer.dir</name><value>${hadoop.tmp.dir}/s3</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.block.size</name><value>134217728</value></property> <property><!--Loaded from mapred-default.xml--><name>job.end.retry.attempts</name><value>0</value></property> <property><!--Loaded from core-default.xml--><name>fs.file.impl</name><value>org.apache.hadoop.fs.LocalFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.output.compression.type</name><value>RECORD</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.local.dir.minspacestart</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.ipc.address</name><value>0.0.0.0:50020</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.permissions</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>topology.script.number.args</name><value>100</value></property> <property><!--Loaded from core-default.xml--><name>io.mapfile.bloom.error.rate</name><value>0.005</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.max.tracker.blacklists</name><value>4</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.profile.maps</name><value>0-2</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.https.address</name><value>0.0.0.0:50475</value></property> <property><!--Loaded from core-site.xml--><name>dfs.umaskmode</name><value>002</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.userlog.retain.hours</name><value>24</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.secondary.http.address</name><value>gratia-1:50090</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.replication.max</name><value>32</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.persist.jobstatus.active</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.security.authorization</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>local.cache.size</name><value>10737418240</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.min.split.size</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.delegation.token.renew-interval</name><value>86400000</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.map.tasks</name><value>7919</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.child.java.opts</name><value>-Xmx200m</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.client.keystore.resource</name><value>ssl-client.xml</value></property> <property><!--Loaded from Unknown--><name>dfs.namenode.startup</name><value>REGULAR</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.queue.name</name><value>default</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.retiredjobs.cache.size</name><value>1000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.address</name><value>0.0.0.0:50470</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.balance.bandwidthPerSec</name><value>2000000000</value></property> <property><!--Loaded from core-default.xml--><name>ipc.server.listen.queue.size</name><value>128</value></property> <property><!--Loaded from mapred-default.xml--><name>job.end.retry.interval</name><value>30000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.inmem.merge.threshold</name><value>1000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.attempts.to.start.skipping</name><value>2</value></property> <property><!--Loaded from hdfs-site.xml--><name>fs.checkpoint.dir</name><value>/var/hadoop/checkpoint-a</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.reduce.tasks</name><value>1543</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.merge.recordsBeforeProgress</name><value>10000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.userlog.limit.kb</name><value>0</value></property> <property><!--Loaded from core-default.xml--><name>webinterface.private.actions</name><value>false</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.max.objects</name><value>0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.shuffle.input.buffer.percent</name><value>0.70</value></property> <property><!--Loaded from mapred-default.xml--><name>io.sort.spill.percent</name><value>0.80</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.map.tasks.speculative.execution</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.util.hash.type</name><value>murmur</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.dns.nameserver</name><value>default</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.blockreport.intervalMsec</name><value>3600000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.map.max.attempts</name><value>4</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.job.acl-view-job</name><value> </value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.handler.count</name><value>10</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.client.block.write.retries</name><value>3</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.max.reduces.per.node</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.reduce.shuffle.read.timeout</name><value>180000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.expiry.interval</name><value>600000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.https.enable</name><value>false</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.maxtasks.per.job</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.job.history.block.size</name><value>3145728</value></property> <property><!--Loaded from mapred-default.xml--><name>keep.failed.task.files</name><value>false</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.failed.volumes.tolerated</name><value>0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.profile.reduces</name><value>0-2</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.tcpnodelay</name><value>false</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.output.compression.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value></property> <property><!--Loaded from mapred-default.xml--><name>io.map.index.skip</name><value>0</value></property> <property><!--Loaded from core-default.xml--><name>ipc.server.tcpnodelay</name><value>false</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.delegation.key.update-interval</name><value>86400000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.running.map.limit</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>jobclient.progress.monitor.poll.interval</name><value>1000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.default.chunk.view.size</name><value>32768</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.logfile.size</name><value>10000000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.tasks.speculative.execution</name><value>true</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.tasktracker.outofband.heartbeat</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3n.block.size</name><value>67108864</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.datanode.du.reserved</name><value>10000000000</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.security.authentication</name><value>simple</value></property> <property><!--Loaded from hdfs-site.xml--><name>fs.checkpoint.period</name><value>3600</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.running.reduce.limit</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.reuse.jvm.num.tasks</name><value>1</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.web.ugi</name><value>webuser,webgroup</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.completeuserjobs.maximum</name><value>100</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.df.interval</name><value>60000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.tracker.task-controller</name><value>org.apache.hadoop.mapred.DefaultTaskController</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.data.dir</name><value>/data1/hadoop//data</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.maxRetries</name><value>4</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.dns.interface</name><value>default</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.support.append</name><value>true</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.job.acl-modify-job</name><value> </value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.local.dir</name><value>${hadoop.tmp.dir}/mapred/local</value></property> <property><!--Loaded from core-default.xml--><name>fs.hftp.impl</name><value>org.apache.hadoop.hdfs.HftpFileSystem</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.permissions.supergroup</name><value>root</value></property> <property><!--Loaded from core-default.xml--><name>fs.trash.interval</name><value>0</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.sleepTimeSeconds</name><value>10</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.submit.replication</name><value>10</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.replication.min</name><value>1</value></property> <property><!--Loaded from core-default.xml--><name>fs.har.impl</name><value>org.apache.hadoop.fs.HarFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.map.output.compression.codec</name><value>org.apache.hadoop.io.compress.DefaultCodec</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.dns.interface</name><value>default</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.decommission.interval</name><value>30</value></property> <property><!--Loaded from Unknown--><name>dfs.http.address</name><value>nagios:50070</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.job.tracker</name><value>nagios:9000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.heartbeat.interval</name><value>3</value></property> <property><!--Loaded from core-default.xml--><name>io.seqfile.sorter.recordlimit</name><value>1000000</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.name.dir</name><value>${hadoop.tmp.dir}/dfs/name</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.line.input.format.linespermap</name><value>1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.taskScheduler</name><value>org.apache.hadoop.mapred.JobQueueTaskScheduler</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.instrumentation</name><value>org.apache.hadoop.mapred.TaskTrackerMetricsInst</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.datanode.http.address</name><value>0.0.0.0:50075</value></property> <property><!--Loaded from mapred-default.xml--><name>jobclient.completion.poll.interval</name><value>5000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.max.maps.per.node</name><value>-1</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.local.dir.minspacekill</name><value>0</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.replication.interval</name><value>3</value></property> <property><!--Loaded from mapred-default.xml--><name>io.sort.record.percent</name><value>0.05</value></property> <property><!--Loaded from core-default.xml--><name>fs.kfs.impl</name><value>org.apache.hadoop.fs.kfs.KosmosFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.temp.dir</name><value>${hadoop.tmp.dir}/mapred/temp</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.tasktracker.reduce.tasks.maximum</name><value>4</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.replication</name><value>2</value></property> <property><!--Loaded from core-default.xml--><name>fs.checkpoint.edits.dir</name><value>${fs.checkpoint.dir}</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.tasks.sleeptime-before-sigkill</name><value>5000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.reduce.input.buffer.percent</name><value>0.0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.indexcache.mb</name><value>10</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.job.split.metainfo.maxsize</name><value>10000000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.skip.reduce.auto.incr.proc.count</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>hadoop.logfile.count</name><value>10</value></property> <property><!--Loaded from core-default.xml--><name>fs.automatic.close</name><value>true</value></property> <property><!--Loaded from core-default.xml--><name>io.seqfile.compress.blocksize</name><value>1000000</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.hosts.exclude</name><value>/etc/hadoop-0.20/conf/hosts_exclude</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.block.size</name><value>67108864</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.tasktracker.taskmemorymanager.monitoring-interval</name><value>5000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.acls.enabled</name><value>false</value></property> <property><!--Loaded from mapred-default.xml--><name>mapreduce.jobtracker.staging.root.dir</name><value>${hadoop.tmp.dir}/mapred/staging</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.queue.names</name><value>default</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.access.time.precision</name><value>3600000</value></property> <property><!--Loaded from core-default.xml--><name>fs.hsftp.impl</name><value>org.apache.hadoop.hdfs.HsftpFileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.tracker.http.address</name><value>0.0.0.0:50060</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.parallel.copies</name><value>5</value></property> <property><!--Loaded from core-default.xml--><name>io.seqfile.lazydecompress</name><value>true</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.safemode.min.datanodes</name><value>0</value></property> <property><!--Loaded from mapred-default.xml--><name>io.sort.mb</name><value>100</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.connection.maxidletime</name><value>10000</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.compress.map.output</name><value>false</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.tracker.report.address</name><value>127.0.0.1:0</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.healthChecker.interval</name><value>60000</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.kill.max</name><value>10</value></property> <property><!--Loaded from core-default.xml--><name>ipc.client.connect.max.retries</name><value>10</value></property> <property><!--Loaded from core-default.xml--><name>fs.s3.impl</name><value>org.apache.hadoop.fs.s3.S3FileSystem</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.job.tracker.http.address</name><value>0.0.0.0:50030</value></property> <property><!--Loaded from core-default.xml--><name>io.file.buffer.size</name><value>4096</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.jobtracker.restart.recover</name><value>false</value></property> <property><!--Loaded from core-default.xml--><name>io.serializations</name><value>org.apache.hadoop.io.serializer.WritableSerialization</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.task.profile</name><value>false</value></property> <property><!--Loaded from hdfs-site.xml--><name>dfs.datanode.handler.count</name><value>10</value></property> <property><!--Loaded from mapred-default.xml--><name>mapred.reduce.copy.backoff</name><value>300</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.replication.considerLoad</name><value>true</value></property> <property><!--Loaded from mapred-default.xml--><name>jobclient.output.filter</name><value>FAILED</value></property> <property><!--Loaded from hdfs-default.xml--><name>dfs.namenode.delegation.token.max-lifetime</name><value>604800000</value></property> <property><!--Loaded from mapred-site.xml--><name>mapred.tasktracker.map.tasks.maximum</name><value>4</value></property> <property><!--Loaded from core-default.xml--><name>io.compression.codecs</name><value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec</value></property> <property><!--Loaded from core-default.xml--><name>fs.checkpoint.size</name><value>67108864</value></property> </configuration> Please refer to the Apache Hadoop documentation for answers to common questions/concerns","title":"Hadoop"},{"location":"data/install-hadoop/#fuse","text":"","title":"FUSE"},{"location":"data/install-hadoop/#notes-on-building-a-fuse-module","text":"If you are running a custom kernel, then be sure to enable the fuse module with CONFIG_FUSE_FS=m in your kernel config. Building and installing a fuse kernel module for your custom kernel is beyond the scope of this document.","title":"Notes on Building a FUSE Module"},{"location":"data/install-hadoop/#running-fuse-in-debug-mode","text":"To start the FUSE mount in debug mode, you can run the FUSE mount command by hand: root@host # /usr/bin/hadoop-fuse-dfs /mnt/hadoop -o rw,server = <NAMENODE.HOST>,port = 9000 ,rdbuffer = 131072 ,allow_other -d Change <NAMENODE.HOSTNAME> for the hostname of your Primary NameNode. Debug output will be printed to stderr, which you will probably want to redirect to a file. Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.","title":"Running FUSE in Debug Mode"},{"location":"data/install-hadoop/#gridftp","text":"","title":"GridFTP"},{"location":"data/install-hadoop/#starting-gridftp-in-standalone-mode","text":"If you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command: root@host # gridftp-hdfs-standalone The standalone server runs on port 5002, handles a single GridFTP request, and will log output to stdout/stderr.","title":"Starting GridFTP in Standalone Mode"},{"location":"data/install-hadoop/#file-locations","text":"Component File Type Location Needs editing? Hadoop Log files /var/log/hadoop/* No PID files /var/run/hadoop/*.pid No init scripts /etc/init.d/hadoop No init script config file /etc/sysconfig/hadoop Yes runtime config files /etc/hadoop/conf/* Maybe System binaries /usr/bin/hadoop No JARs /usr/lib/hadoop/* No runtime config files /etc/hosts_exclude Yes, must be present on NameNodes Log files /var/log/gridftp-auth.log , /var/log/gridftp.log No GridFTP Transfer log /var/log/gridftp.log No Authentication log /var/log/gridftp-auth.log No LCMAPS auth error log /var/log/messages No init.d script /etc/init.d/globus-gridftp-server No runtime config files /etc/gridftp-hdfs/* , /etc/sysconfig/gridftp-hdfs Maybe System binaries /usr/bin/gridftp-hdfs-standalone , /usr/sbin/globus-gridftp-server No System libraries /usr/lib64/libglobus_gridftp_server_hdfs.so* No LCMAPS VOMS configuration /etc/lcmaps.db Yes CA certificates /etc/grid-security/certificates/* No","title":"File Locations"},{"location":"data/install-hadoop/#known-issues","text":"","title":"Known Issues"},{"location":"data/install-hadoop/#replicas","text":"You may need to change the following line in /usr/share/gridftp-hdfs/gridftp-hdfs-environment : export GRIDFTP_HDFS_REPLICAS=2","title":"Replicas"},{"location":"data/install-hadoop/#copyfromlocal-java-ioexception","text":"When trying to copy a local file into Hadoop you may come across the following java exception: Show detailed java exception 11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0] nodes == null 11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file \"/osg/ddd\" - Aborting... copyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0 nodes, instead of 1 11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd : org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only be replicated to 0 nodes, instead of 1 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415) at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313) This can occur if you try to install a DataNode on a machine with less than 10GB of disk space available. This can be changed by lowering the value of the following property in /usr/lib/hadoop-0.20/conf/hdfs-site.xml : <property> <name>dfs.datanode.du.reserved</name> <value>10000000000</value> </property> Hadoop always requires this amount of disk space to be available for non-hdfs usage on the machine.","title":"copyFromLocal java IOException"},{"location":"data/install-hadoop/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"data/install-hadoop/#references","text":"Using Hadoop as a Grid Storage Element , Journal of Physics Conference Series, 2009 . Hadoop Distributed File System for the Grid , IEEE Nuclear Science Symposium, 2009 .","title":"References"},{"location":"data/install-hadoop/#users","text":"This installation will create following users unless they are already created. User Comment hadoop Runs the NameNode services hdfs Used by Hadoop to store data blocks and meta-data mapred zookeeper For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.","title":"Users"},{"location":"data/load-balanced-gridftp/","text":"Load Balancing GridFTP \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. GridFTP is designed for high throughput data transfers and in many cases can handle all of the transfers for a site. However, in some cases it may be useful to run multiple GridFTP servers to distribute the load. For such sites, we recommend using a load balancer to distribute requests and present the appearance of a single high-throughput GridFTP server. One general-purpose technology for implementing a load balancer on Linux is Linux Virtual Server (LVS). To use it with GridFTP, a single load balancer listens on a virtual IP address, monitors the health of the set of real GridFTP servers, and forwards requests to available ones. Optionally, there can be one or more inactive, backup load balancers that can activate and take over the virtual IP address in case the primary load balancer fails, resulting in a system that is more resilient to failure. LVS is implemented by the IP Virtual Server kernel module, which can be managed by userspace services on the load balancers such as keepalived . This guide explains how to install, configure, run, test, and troubleshoot the keepalived service on a load balancing host for a set of GridFTP servers. Before Starting \u00b6 Before starting the installation process, consider the following requirements: There must be a shared file system for file propagation across GridFTP servers You must have reserved a virtual IP address and associated virtual hostname As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to each host Designing Your Load-Balanced GridFTP System \u00b6 Before beginning the installation process, you will need to plan the overall architecture of your load-balanced GridFTP system: the number of GridFTP servers, the type of shared file system to run on the GridFTP servers, whether or not backup load balancers are required, and hardware requirements. GridFTP servers \u00b6 The number of GridFTP servers that you should run is determined first and foremost by the expected GridFTP transfer load at your site and the speed of the links available to each server. For example, if you expect a 20Gbps peak transfer load and have 10Gb links with 80\u201390% efficiency, you would need a minimum of 4 GridFTP servers: 3 to satisfy your desired throughput + 1 for failover or growth. Shared file system \u00b6 The number of GridFTP servers can also be determined by your hardware needs and by your choice of shared file system. If you choose a POSIX-based shared file system, plan for machines with more cores, or more GridFTP hosts to distribute the CPU load. If you are running GridFTP with Hadoop , plan for machines with more memory, or more GridFTP hosts to distribute the memory load. Note If you determine that you need only a single GridFTP host, you do not need load balancing. Instead, follow the standalone-GridFTP installation guide . Load balancer(s) \u00b6 In the recommended direct routing mode, load balancers simply rewrite initial packets from a given request so the hardware requirements are minimal. When choosing load balancer hosts, aim for stability. If your chosen host is unstable or if you do not want to introduce downtime for operating system or hardware updates, at least one additional load balancer will be needed as a backup. Preparing the GridFTP Servers \u00b6 Before adding your GridFTP hosts to the load-balanced system, each host requires the following: GridFTP software Special host certificates Load-balancing configuration Acquiring host certificate(s) \u00b6 When authenticating with a GridFTP server, clients verify that the server's host certificate matches the hostname of the server. In the case of a load-balanced GridFTP system, clients contact the GridFTP server through the virtual hostname, so the GridFTP server will have to present a certificate containing the virtual hostname as well as the GridFTP server's hostname. Use the OSG host certificate reference for more information on how to request these types of certificates. Additionally, a special procedure is available to acquire Let's Encrypt certificates with the load balanced gridftp. If your GridFTP servers are also running XRootD, you will need unique certificates for each GridFTP server. Otherwise, you can request a single certificate that can be shared among the GridFTP servers. Without XRootD \u00b6 The single shared certificate must have the hostname associated with the load-balanced GridFTP system as its common name and each GridFTP servers hostname listed as subject alternative names . Request and generate the shared certificate: user@host $ osg-cert-request --hostname <VIRTUAL-HOSTNAME> \\ --country <COUNTRY> \\ --state <STATE> \\ --locality <LOCALITY> \\ --organization <ORGANIZATION> --altname <GRIDFTP-SERVER-#1-HOSTNAME> \\ --altname <GRIDFTP-SERVER-#2-HOSTNAME> Take the resulting CSR and get it signed by the appropriate authority. Most institutions can use InCommon as outlined here . Create a directory to contain the shared certificate: root@host # mkdir /etc/grid-security/gridftp Place the shared certificate-key pair in the newly created directory: root@host # mv <PATH-TO-SERVICE-CERT> /etc/grid-security/gridftp/gridftp-hostcert.pem root@host # mv <PATH-TO-SERVICE-KEY> /etc/grid-security/gridftp/gridftp-hostkey.pem Edit /etc/sysconfig/globus-gridftp-server to identify the shared certificate-key pair: export X509_USER_CERT=/etc/grid-security/gridftp/gridftp-hostcert.pem export X509_USER_KEY=/etc/grid-security/gridftp/gridftp-hostkey.pem With XRootD \u00b6 XRootD requires that the certificate's common name refers specifically to the host it resides on. To ensure each GridFTP server can authenticate using the virtual hostname, add it as the subject alternative name for each certificate. Create a list of GridFTP server hostnames in load-balanced-hosts.txt : <GRIDFTP-SERVER-#1-HOSTNAME> <VIRTUAL-HOSTNAME> <GRIDFTP-SERVER-#2-HOSTNAME> <VIRTUAL-HOSTNAME> [...] Submit a batch request for the per-GridFTP server certificates: user@host $ osg-gridadmin-cert-request -f load-balanced-hosts.txt Copy the resulting certificates and keys to their corresponding GridFTP servers in /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem , respectively. With Let's Encrypt \u00b6 The certificate provided to the clients needs to have the virtual host address of the load balancer, as well as the hostname of each of the worker nodes. Additionally, LetsEncrypt contacts the requested domains to verify ownership. Therefore, each domain requested must be available to respond to HTTP requests at the same time. The easiest method for this is to use a shared directory for Let's Encrypt's certbot to install the secrets. The procedure to acquire Let's Encrypt certificates for multiple hosts is as follows: Create or use a shared directory that each of the data transfer nodes can read, for example a simple NFS share. The steps in creating a NFS shared directory is outside the scope of this guide. In this guide, the shared directory will be referred as /mnt/nfsshare . Install httpd on each of the data transfer nodes: root@host $ yum install httpd Create a webroot directory within the shared directory on one of the nodes: root@host $ mkdir /mnt/nfsshare/webroot Configure httpd to export the same webroot on each of the data transfer nodes: <VirtualHost *:80> DocumentRoot \"/mnt/nfsshare/webroot\" <Directory \"/mnt/nfsshare/webroot\"> Require all granted </Directory> </VirtualHost> Configure keepalived to virtualize port 80 to at least one of your data transfer nodes. Add to your configuration: virtual_server <VIRTUAL-IP-ADDRESS> 80 { delay_loop 10 lb_algo wlc lb_kind DR protocol tcp real_server <GRIDFTP-SERVER-#1-IP ADDRESS> { TCP_CHECK { connect_timeout 3 connect_port 80 } } } Run certbot with the webroot options on only 1 of the data nodes. The first domain in the command line should be the virtual hostname: root@host $ certbot certonly -w /mnt/nfsshare/webroot -d <VIRTUAL_HOSTNAME> -d <DATANODE_1> -d <DATANODE_N>... For XRootD certificates, the real hostname of the XRootD node is required to be the first hostname in the certbot command. You may run the certbot command several times on the same host, replacing the VIRTUAL_HOSTNAME with the real hostname of the XRootD servers, and placing the VIRTUAL_HOSTNAME in in the list of other domains in the certificate. Installing GridFTP \u00b6 Whether you are starting from scratch or adding more GridFTP servers to your load-balanced GridFTP system, follow the documentation for installing a standalone GridFTP server for each of your intended GridFTP servers (skip section 2.2, requesting a certificate). For hosts with GridFTP already installed, skip this section. Configuring your GridFTP servers \u00b6 Each GridFTP server requires changes to its IP configuration and potentially its arptables: Adding your virtual IP address Disabling ARP \u2212 if your GridFTP servers are on the same network segment as the virtual IP Adding your virtual IP address \u00b6 Use the virtual IP address of your load balancer(s) as the secondary IPs of each of your GridFTP servers. Add the virtual IP using the ip tool: root@host # ip addr add <VIRTUAL-IP-ADDRESS>/<SUBNET-MASK> dev <NETWORK-INTERFACE> To persist the virtual IP changes across reboots, edit /etc/rc.d/rc.local , and add the same command as used above. Make sure that /etc/rc.d/rc.local is executable: root@host # chmod u+x /etc/rc.d/rc.local Disabling ARP \u00b6 If your GridFTP servers and load balancer(s) are on the same network segment, you will have to disable ARP on the GridFTP servers to avoid ARP race conditions . Otherwise, skip to the section on preparing keepalived . Install the arptables software: root@host # yum install arptables Disable ARP: root@host # arptables -F root@host # arptables -A IN -d <VIRTUAL-IP-ADDRESS> -j DROP root@host # arptables -A OUT -s <VIRTUAL-IP-ADDRESS> -j mangle --mangle-ip-s <GRIDFTP-REAL-IP-ADDRESS> Save ARP tables to survive reboots: root@host # arptables-save > /etc/sysconfig/arptables Preparing Keepalived Load Balancer(s) \u00b6 Installing Keepalived \u00b6 Whether you run a single load balancer, or have one active load balancer and some inactive backups, each load balancer host must have the keepalived software installed, configured, and running. Note Do not install keepalived on the GridFTP servers themselves. The keepalived package is available from standard operating system repositories. Install it on each load balancer host using the following commands: Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the keepalived package: root@host # yum install keepalived Required configuration \u00b6 On the primary load balancer, edit /etc/keepalived/keepalived.conf : global_defs { router_id <STRING-LABEL-FOR-YOUR-LOAD-BALANCED-SYSTEM> } vrrp_instance VI_gridftp { state MASTER interface <NETWORK-INTERFACE> virtual_router_id <INTEGER-BETWEEN-0-AND-255> priority 100 virtual_ipaddress { <VIRTUAL-IP-ADDRESS>/<SUBNET-MASK> dev <NETWORK-INTERFACE> } } virtual_server <VIRTUAL-IP-ADDRESS> 2811 { delay_loop 10 lb_algo wlc lb_kind DR protocol tcp real_server <GRIDFTP-SERVER-#1-IP ADDRESS> { TCP_CHECK { connect_timeout 3 connect_port 2811 } } real_server <GRIDFTP-SERVER-#2-IP-ADDRESS> { TCP_CHECK { connect_timeout 3 connect_port 2811 } } [...] } Note Use the same VIRTUAL-IP-ADDRESS throughout the configuration of your load-balanced GridFTP system. Note In the virtual_server section, write one real_server subsection for each GridFTP server behind the load balancer. Optional configuration \u00b6 The following configuration steps are optional and will likely not be required for setting up a small cluster of GridFTP hosts. If you do not need any of the following special configurations, skip to the section on using keepalived . Adding backup load balancers Enabling e-mail notifications Adding backup load balancers \u00b6 If you need to add backup load balancers, copy /etc/keepalived/keepalived.conf from your primary load balancer and change the state and priority attributes under your vrrp_instance VI_gridftp section: Note Priority specifies the order of preferred load balancer fallback, where larger values corresponds to a higher preference. vrrp_instance VI_gridftp { state BACKUP interface <NETWORK-INTERFACE> virtual_router_id <SAME-ID-AS-MASTER-LOAD-BALANCER> priority <PRIORITY-INTEGER> virtual_ipaddress { <VIRTUAL-IP-ADDRESS>/<SUBNET-MASK> dev <NETWORK-INTERFACE> } } Enabling e-mail notifications \u00b6 To receive e-mails when the state of your load-balanced system changes, update the global_defs section of /etc/keepalived/keepalived.conf for each of your load balancer nodes: notification_email { <NOTIFY-EMAIL-ADDRESS-#1> <NOTIFY-EMAIL-ADDRESS-#2> [...] } notification_email_from <FROM-EMAIL-ADDRESS> smtp_server <SMTP-SERVER-IP-ADDRESS> smtp_connect_timeout 60 router_id <MACHINE-IDENTIFYING-STRING> Using Your Load Balanced GridFTP System \u00b6 Using GridFTP \u00b6 On the GridFTP servers, arptables is the only additional service required for running a load-balanced GridFTP system. Manage the service with the following commands: To ... Run the command... Start the service systemctl start arptables Stop theservice systemctl stop arptables Enable the service to start during boot systemctl enable arptables Disable the service from starting during boot systemctl disable arptables For information on how to use your individual GridFTP servers, please refer to the Managing GridFTP section of the GridFTP installation guide. Using Keepalived \u00b6 On the load balancer nodes, keepalived is the only additional service required for running a load-balanced GridFTP system. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start keepalived Stop a service systemctl stop keepalived Enable a service to start during boot systemctl enable keepalived Disable a service from starting during boot systemctl disable keepalived Getting Help \u00b6 To get assistance with keepalived in front of OSG Software services, please use the this page . Linux Virtual Server homepage Keepalived homepage RHEL 7 Load Balancer Administration Guide T2 Nebraska LVS installation notes","title":"Install Load Balanced GridFTP"},{"location":"data/load-balanced-gridftp/#load-balancing-gridftp","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. GridFTP is designed for high throughput data transfers and in many cases can handle all of the transfers for a site. However, in some cases it may be useful to run multiple GridFTP servers to distribute the load. For such sites, we recommend using a load balancer to distribute requests and present the appearance of a single high-throughput GridFTP server. One general-purpose technology for implementing a load balancer on Linux is Linux Virtual Server (LVS). To use it with GridFTP, a single load balancer listens on a virtual IP address, monitors the health of the set of real GridFTP servers, and forwards requests to available ones. Optionally, there can be one or more inactive, backup load balancers that can activate and take over the virtual IP address in case the primary load balancer fails, resulting in a system that is more resilient to failure. LVS is implemented by the IP Virtual Server kernel module, which can be managed by userspace services on the load balancers such as keepalived . This guide explains how to install, configure, run, test, and troubleshoot the keepalived service on a load balancing host for a set of GridFTP servers.","title":"Load Balancing GridFTP"},{"location":"data/load-balanced-gridftp/#before-starting","text":"Before starting the installation process, consider the following requirements: There must be a shared file system for file propagation across GridFTP servers You must have reserved a virtual IP address and associated virtual hostname As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to each host","title":"Before Starting"},{"location":"data/load-balanced-gridftp/#designing-your-load-balanced-gridftp-system","text":"Before beginning the installation process, you will need to plan the overall architecture of your load-balanced GridFTP system: the number of GridFTP servers, the type of shared file system to run on the GridFTP servers, whether or not backup load balancers are required, and hardware requirements.","title":"Designing Your Load-Balanced GridFTP System"},{"location":"data/load-balanced-gridftp/#gridftp-servers","text":"The number of GridFTP servers that you should run is determined first and foremost by the expected GridFTP transfer load at your site and the speed of the links available to each server. For example, if you expect a 20Gbps peak transfer load and have 10Gb links with 80\u201390% efficiency, you would need a minimum of 4 GridFTP servers: 3 to satisfy your desired throughput + 1 for failover or growth.","title":"GridFTP servers"},{"location":"data/load-balanced-gridftp/#shared-file-system","text":"The number of GridFTP servers can also be determined by your hardware needs and by your choice of shared file system. If you choose a POSIX-based shared file system, plan for machines with more cores, or more GridFTP hosts to distribute the CPU load. If you are running GridFTP with Hadoop , plan for machines with more memory, or more GridFTP hosts to distribute the memory load. Note If you determine that you need only a single GridFTP host, you do not need load balancing. Instead, follow the standalone-GridFTP installation guide .","title":"Shared file system"},{"location":"data/load-balanced-gridftp/#load-balancers","text":"In the recommended direct routing mode, load balancers simply rewrite initial packets from a given request so the hardware requirements are minimal. When choosing load balancer hosts, aim for stability. If your chosen host is unstable or if you do not want to introduce downtime for operating system or hardware updates, at least one additional load balancer will be needed as a backup.","title":"Load balancer(s)"},{"location":"data/load-balanced-gridftp/#preparing-the-gridftp-servers","text":"Before adding your GridFTP hosts to the load-balanced system, each host requires the following: GridFTP software Special host certificates Load-balancing configuration","title":"Preparing the GridFTP Servers"},{"location":"data/load-balanced-gridftp/#acquiring-host-certificates","text":"When authenticating with a GridFTP server, clients verify that the server's host certificate matches the hostname of the server. In the case of a load-balanced GridFTP system, clients contact the GridFTP server through the virtual hostname, so the GridFTP server will have to present a certificate containing the virtual hostname as well as the GridFTP server's hostname. Use the OSG host certificate reference for more information on how to request these types of certificates. Additionally, a special procedure is available to acquire Let's Encrypt certificates with the load balanced gridftp. If your GridFTP servers are also running XRootD, you will need unique certificates for each GridFTP server. Otherwise, you can request a single certificate that can be shared among the GridFTP servers.","title":"Acquiring host certificate(s)"},{"location":"data/load-balanced-gridftp/#without-xrootd","text":"The single shared certificate must have the hostname associated with the load-balanced GridFTP system as its common name and each GridFTP servers hostname listed as subject alternative names . Request and generate the shared certificate: user@host $ osg-cert-request --hostname <VIRTUAL-HOSTNAME> \\ --country <COUNTRY> \\ --state <STATE> \\ --locality <LOCALITY> \\ --organization <ORGANIZATION> --altname <GRIDFTP-SERVER-#1-HOSTNAME> \\ --altname <GRIDFTP-SERVER-#2-HOSTNAME> Take the resulting CSR and get it signed by the appropriate authority. Most institutions can use InCommon as outlined here . Create a directory to contain the shared certificate: root@host # mkdir /etc/grid-security/gridftp Place the shared certificate-key pair in the newly created directory: root@host # mv <PATH-TO-SERVICE-CERT> /etc/grid-security/gridftp/gridftp-hostcert.pem root@host # mv <PATH-TO-SERVICE-KEY> /etc/grid-security/gridftp/gridftp-hostkey.pem Edit /etc/sysconfig/globus-gridftp-server to identify the shared certificate-key pair: export X509_USER_CERT=/etc/grid-security/gridftp/gridftp-hostcert.pem export X509_USER_KEY=/etc/grid-security/gridftp/gridftp-hostkey.pem","title":"Without XRootD"},{"location":"data/load-balanced-gridftp/#with-xrootd","text":"XRootD requires that the certificate's common name refers specifically to the host it resides on. To ensure each GridFTP server can authenticate using the virtual hostname, add it as the subject alternative name for each certificate. Create a list of GridFTP server hostnames in load-balanced-hosts.txt : <GRIDFTP-SERVER-#1-HOSTNAME> <VIRTUAL-HOSTNAME> <GRIDFTP-SERVER-#2-HOSTNAME> <VIRTUAL-HOSTNAME> [...] Submit a batch request for the per-GridFTP server certificates: user@host $ osg-gridadmin-cert-request -f load-balanced-hosts.txt Copy the resulting certificates and keys to their corresponding GridFTP servers in /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem , respectively.","title":"With XRootD"},{"location":"data/load-balanced-gridftp/#with-lets-encrypt","text":"The certificate provided to the clients needs to have the virtual host address of the load balancer, as well as the hostname of each of the worker nodes. Additionally, LetsEncrypt contacts the requested domains to verify ownership. Therefore, each domain requested must be available to respond to HTTP requests at the same time. The easiest method for this is to use a shared directory for Let's Encrypt's certbot to install the secrets. The procedure to acquire Let's Encrypt certificates for multiple hosts is as follows: Create or use a shared directory that each of the data transfer nodes can read, for example a simple NFS share. The steps in creating a NFS shared directory is outside the scope of this guide. In this guide, the shared directory will be referred as /mnt/nfsshare . Install httpd on each of the data transfer nodes: root@host $ yum install httpd Create a webroot directory within the shared directory on one of the nodes: root@host $ mkdir /mnt/nfsshare/webroot Configure httpd to export the same webroot on each of the data transfer nodes: <VirtualHost *:80> DocumentRoot \"/mnt/nfsshare/webroot\" <Directory \"/mnt/nfsshare/webroot\"> Require all granted </Directory> </VirtualHost> Configure keepalived to virtualize port 80 to at least one of your data transfer nodes. Add to your configuration: virtual_server <VIRTUAL-IP-ADDRESS> 80 { delay_loop 10 lb_algo wlc lb_kind DR protocol tcp real_server <GRIDFTP-SERVER-#1-IP ADDRESS> { TCP_CHECK { connect_timeout 3 connect_port 80 } } } Run certbot with the webroot options on only 1 of the data nodes. The first domain in the command line should be the virtual hostname: root@host $ certbot certonly -w /mnt/nfsshare/webroot -d <VIRTUAL_HOSTNAME> -d <DATANODE_1> -d <DATANODE_N>... For XRootD certificates, the real hostname of the XRootD node is required to be the first hostname in the certbot command. You may run the certbot command several times on the same host, replacing the VIRTUAL_HOSTNAME with the real hostname of the XRootD servers, and placing the VIRTUAL_HOSTNAME in in the list of other domains in the certificate.","title":"With Let's Encrypt"},{"location":"data/load-balanced-gridftp/#installing-gridftp","text":"Whether you are starting from scratch or adding more GridFTP servers to your load-balanced GridFTP system, follow the documentation for installing a standalone GridFTP server for each of your intended GridFTP servers (skip section 2.2, requesting a certificate). For hosts with GridFTP already installed, skip this section.","title":"Installing GridFTP"},{"location":"data/load-balanced-gridftp/#configuring-your-gridftp-servers","text":"Each GridFTP server requires changes to its IP configuration and potentially its arptables: Adding your virtual IP address Disabling ARP \u2212 if your GridFTP servers are on the same network segment as the virtual IP","title":"Configuring your GridFTP servers"},{"location":"data/load-balanced-gridftp/#adding-your-virtual-ip-address","text":"Use the virtual IP address of your load balancer(s) as the secondary IPs of each of your GridFTP servers. Add the virtual IP using the ip tool: root@host # ip addr add <VIRTUAL-IP-ADDRESS>/<SUBNET-MASK> dev <NETWORK-INTERFACE> To persist the virtual IP changes across reboots, edit /etc/rc.d/rc.local , and add the same command as used above. Make sure that /etc/rc.d/rc.local is executable: root@host # chmod u+x /etc/rc.d/rc.local","title":"Adding your virtual IP address"},{"location":"data/load-balanced-gridftp/#disabling-arp","text":"If your GridFTP servers and load balancer(s) are on the same network segment, you will have to disable ARP on the GridFTP servers to avoid ARP race conditions . Otherwise, skip to the section on preparing keepalived . Install the arptables software: root@host # yum install arptables Disable ARP: root@host # arptables -F root@host # arptables -A IN -d <VIRTUAL-IP-ADDRESS> -j DROP root@host # arptables -A OUT -s <VIRTUAL-IP-ADDRESS> -j mangle --mangle-ip-s <GRIDFTP-REAL-IP-ADDRESS> Save ARP tables to survive reboots: root@host # arptables-save > /etc/sysconfig/arptables","title":"Disabling ARP"},{"location":"data/load-balanced-gridftp/#preparing-keepalived-load-balancers","text":"","title":"Preparing Keepalived Load Balancer(s)"},{"location":"data/load-balanced-gridftp/#installing-keepalived","text":"Whether you run a single load balancer, or have one active load balancer and some inactive backups, each load balancer host must have the keepalived software installed, configured, and running. Note Do not install keepalived on the GridFTP servers themselves. The keepalived package is available from standard operating system repositories. Install it on each load balancer host using the following commands: Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the keepalived package: root@host # yum install keepalived","title":"Installing Keepalived"},{"location":"data/load-balanced-gridftp/#required-configuration","text":"On the primary load balancer, edit /etc/keepalived/keepalived.conf : global_defs { router_id <STRING-LABEL-FOR-YOUR-LOAD-BALANCED-SYSTEM> } vrrp_instance VI_gridftp { state MASTER interface <NETWORK-INTERFACE> virtual_router_id <INTEGER-BETWEEN-0-AND-255> priority 100 virtual_ipaddress { <VIRTUAL-IP-ADDRESS>/<SUBNET-MASK> dev <NETWORK-INTERFACE> } } virtual_server <VIRTUAL-IP-ADDRESS> 2811 { delay_loop 10 lb_algo wlc lb_kind DR protocol tcp real_server <GRIDFTP-SERVER-#1-IP ADDRESS> { TCP_CHECK { connect_timeout 3 connect_port 2811 } } real_server <GRIDFTP-SERVER-#2-IP-ADDRESS> { TCP_CHECK { connect_timeout 3 connect_port 2811 } } [...] } Note Use the same VIRTUAL-IP-ADDRESS throughout the configuration of your load-balanced GridFTP system. Note In the virtual_server section, write one real_server subsection for each GridFTP server behind the load balancer.","title":"Required configuration"},{"location":"data/load-balanced-gridftp/#optional-configuration","text":"The following configuration steps are optional and will likely not be required for setting up a small cluster of GridFTP hosts. If you do not need any of the following special configurations, skip to the section on using keepalived . Adding backup load balancers Enabling e-mail notifications","title":"Optional configuration"},{"location":"data/load-balanced-gridftp/#adding-backup-load-balancers","text":"If you need to add backup load balancers, copy /etc/keepalived/keepalived.conf from your primary load balancer and change the state and priority attributes under your vrrp_instance VI_gridftp section: Note Priority specifies the order of preferred load balancer fallback, where larger values corresponds to a higher preference. vrrp_instance VI_gridftp { state BACKUP interface <NETWORK-INTERFACE> virtual_router_id <SAME-ID-AS-MASTER-LOAD-BALANCER> priority <PRIORITY-INTEGER> virtual_ipaddress { <VIRTUAL-IP-ADDRESS>/<SUBNET-MASK> dev <NETWORK-INTERFACE> } }","title":"Adding backup load balancers"},{"location":"data/load-balanced-gridftp/#enabling-e-mail-notifications","text":"To receive e-mails when the state of your load-balanced system changes, update the global_defs section of /etc/keepalived/keepalived.conf for each of your load balancer nodes: notification_email { <NOTIFY-EMAIL-ADDRESS-#1> <NOTIFY-EMAIL-ADDRESS-#2> [...] } notification_email_from <FROM-EMAIL-ADDRESS> smtp_server <SMTP-SERVER-IP-ADDRESS> smtp_connect_timeout 60 router_id <MACHINE-IDENTIFYING-STRING>","title":"Enabling e-mail notifications"},{"location":"data/load-balanced-gridftp/#using-your-load-balanced-gridftp-system","text":"","title":"Using Your Load Balanced GridFTP System"},{"location":"data/load-balanced-gridftp/#using-gridftp","text":"On the GridFTP servers, arptables is the only additional service required for running a load-balanced GridFTP system. Manage the service with the following commands: To ... Run the command... Start the service systemctl start arptables Stop theservice systemctl stop arptables Enable the service to start during boot systemctl enable arptables Disable the service from starting during boot systemctl disable arptables For information on how to use your individual GridFTP servers, please refer to the Managing GridFTP section of the GridFTP installation guide.","title":"Using GridFTP"},{"location":"data/load-balanced-gridftp/#using-keepalived","text":"On the load balancer nodes, keepalived is the only additional service required for running a load-balanced GridFTP system. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start keepalived Stop a service systemctl stop keepalived Enable a service to start during boot systemctl enable keepalived Disable a service from starting during boot systemctl disable keepalived","title":"Using Keepalived"},{"location":"data/load-balanced-gridftp/#getting-help","text":"To get assistance with keepalived in front of OSG Software services, please use the this page . Linux Virtual Server homepage Keepalived homepage RHEL 7 Load Balancer Administration Guide T2 Nebraska LVS installation notes","title":"Getting Help"},{"location":"data/run-frontier-squid-container/","text":"Running Frontier Squid in a Container \u00b6 Frontier Squid is a distribution of the well-known squid HTTP caching proxy software that is optimized for use with applications on the Worldwide LHC Computing Grid (WLCG). It has many advantages over regular squid for common grid applications, especially Frontier and CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the upstream frontier-squid package for the convenience of OSG users. Tip OSG recommends that all sites run a caching proxy for HTTP to help reduce bandwidth and improve throughput. This document outlines how to run Frontier Squid in a Docker container. Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the Frontier Squid Reference section as needed): Docker: For the purpose of this guide, the host must have a running docker service and you must have the ability to start containers (i.e., belong to the docker Unix group). Network ports: Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP). We encourage sites to allow monitoring on port 3401 via UDP from CERN IP address ranges, 128.142.0.0/16, 188.184.128.0/17, 188.185.48.0/20 and 188.185.128.0/17. See the CERN monitoring documentation for additional details. If outgoing connections are filtered, note that CVMFS always uses ports 8000, 80, or 8080. Host choice: If you will be supporting the Frontier application at your site, review the upstream documentation to determine how to size your equipment. Configuring Squid \u00b6 Environment variables (optional) \u00b6 In addition to the required configuration above (ports and file systems), you may also configure the behavior of your cache with the following environment variables: Variable name Description Defaults SQUID_IPRANGE Limits the incoming connections to the provided whitelist. By default only standard private network addresses are whitelisted. SQUID_CACHE_DISK Sets the cache_dir option which determines the disk size squid uses. Must be an integer value, and its unit is MBs. Note: The cache disk area is located at /var/cache/squid. Defaults to 10000. SQUID_CACHE_MEM Sets the cache_mem option which regulates the size squid reserves for caching small objects in memory. Defaults to \"128 MB\". Mount points \u00b6 In order to preserve the cache between redeployments, you should map the following areas to persistent storage outside the container: Mountpoint Description Example docker mount /var/cache/squid This directory contains the cache for squid. See also SQUID_CACHE_DISK above. -v /tmp/squid:/var/cache/squid /var/log/squid This directory contains the squid logs. -v /tmp/log:/var/log/squid For more details, see the Frontier Squid documentation . Configuration customization (optional) \u00b6 More complicated configuration customization can be done by mounting .sh and .awk files into /etc/squid/customize.d. For details on the names and content of those files see the comments in the customization script and see the upstream documentation on configuration customization. Running a Frontier Squid Container \u00b6 To run a Frontier Squid container with the defaults: user@host $ docker run --rm --name frontier-squid \\ -v <HOST CACHE PARTITION>:/var/cache/squid \\ -v <HOST LOG PARTITION>:/var/log/squid \\ -p <HOST PORT>:3128 opensciencegrid/frontier-squid:release You may pass configuration variables in KEY=VALUE format with either docker -e options or in a file specified with --env-file=<FILENAME> . Running a Frontier Squid container with systemd \u00b6 An example systemd service file for Frontier Squid. This will require creating the environment file in the directory /opt/xcache/.env . Note This example systemd file assumes <HOST PORT> is 3128 and <HOST CACHE PARTITION> is /tmp/squid and <HOST LOG PARTITION> is /tmp/log . Create the systemd service file /etc/systemd/system/docker.frontier-squid.service as follows: [Unit] Description=Stash Cache Container After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 Restart=always ExecStartPre=-/usr/bin/docker stop %n ExecStartPre=-/usr/bin/docker rm %n ExecStartPre=/usr/bin/docker pull opensciencegrid/frontier-squid:release ExecStart=/usr/bin/docker run --rm --name %n --publish 3128:3128 -v /tmp/squid:/var/cache/squid -v /tmp/log:/var/log/squid --env-file /opt/xcache/.env opensciencegrid/frontier-squid:release [Install] WantedBy=multi-user.target Enable and start the service with: root@host $ systemctl enable docker.frontier-squid root@host $ systemctl start docker.frontier-squid Validating the Frontier Squid Cache \u00b6 The cache server functions as a normal HTTP server and can interact with typical HTTP clients, such as curl or wget . Here, <HOST PORT> is the port chosen in the docker run command, 3128 by default. user@host $ export http_proxy = http://localhost:<HOST PORT> user@host $ wget -qdO/dev/null http://frontier.cern.ch 2 > & 1 | grep X-Cache X-Cache: MISS from 797a56e426cf user@host $ wget -qdO/dev/null http://frontier.cern.ch 2 > & 1 | grep X-Cache X-Cache: HIT from 797a56e426cf Registering Frontier Squid \u00b6 See the Registering Frontier Squid instructions to register your Frontier Squid host. Getting Help \u00b6 To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Running Frontier Squid in a Container"},{"location":"data/run-frontier-squid-container/#running-frontier-squid-in-a-container","text":"Frontier Squid is a distribution of the well-known squid HTTP caching proxy software that is optimized for use with applications on the Worldwide LHC Computing Grid (WLCG). It has many advantages over regular squid for common grid applications, especially Frontier and CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the upstream frontier-squid package for the convenience of OSG users. Tip OSG recommends that all sites run a caching proxy for HTTP to help reduce bandwidth and improve throughput. This document outlines how to run Frontier Squid in a Docker container.","title":"Running Frontier Squid in a Container"},{"location":"data/run-frontier-squid-container/#before-starting","text":"Before starting the installation process, consider the following points (consulting the Frontier Squid Reference section as needed): Docker: For the purpose of this guide, the host must have a running docker service and you must have the ability to start containers (i.e., belong to the docker Unix group). Network ports: Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP). We encourage sites to allow monitoring on port 3401 via UDP from CERN IP address ranges, 128.142.0.0/16, 188.184.128.0/17, 188.185.48.0/20 and 188.185.128.0/17. See the CERN monitoring documentation for additional details. If outgoing connections are filtered, note that CVMFS always uses ports 8000, 80, or 8080. Host choice: If you will be supporting the Frontier application at your site, review the upstream documentation to determine how to size your equipment.","title":"Before Starting"},{"location":"data/run-frontier-squid-container/#configuring-squid","text":"","title":"Configuring Squid"},{"location":"data/run-frontier-squid-container/#environment-variables-optional","text":"In addition to the required configuration above (ports and file systems), you may also configure the behavior of your cache with the following environment variables: Variable name Description Defaults SQUID_IPRANGE Limits the incoming connections to the provided whitelist. By default only standard private network addresses are whitelisted. SQUID_CACHE_DISK Sets the cache_dir option which determines the disk size squid uses. Must be an integer value, and its unit is MBs. Note: The cache disk area is located at /var/cache/squid. Defaults to 10000. SQUID_CACHE_MEM Sets the cache_mem option which regulates the size squid reserves for caching small objects in memory. Defaults to \"128 MB\".","title":"Environment variables (optional)"},{"location":"data/run-frontier-squid-container/#mount-points","text":"In order to preserve the cache between redeployments, you should map the following areas to persistent storage outside the container: Mountpoint Description Example docker mount /var/cache/squid This directory contains the cache for squid. See also SQUID_CACHE_DISK above. -v /tmp/squid:/var/cache/squid /var/log/squid This directory contains the squid logs. -v /tmp/log:/var/log/squid For more details, see the Frontier Squid documentation .","title":"Mount points"},{"location":"data/run-frontier-squid-container/#configuration-customization-optional","text":"More complicated configuration customization can be done by mounting .sh and .awk files into /etc/squid/customize.d. For details on the names and content of those files see the comments in the customization script and see the upstream documentation on configuration customization.","title":"Configuration customization (optional)"},{"location":"data/run-frontier-squid-container/#running-a-frontier-squid-container","text":"To run a Frontier Squid container with the defaults: user@host $ docker run --rm --name frontier-squid \\ -v <HOST CACHE PARTITION>:/var/cache/squid \\ -v <HOST LOG PARTITION>:/var/log/squid \\ -p <HOST PORT>:3128 opensciencegrid/frontier-squid:release You may pass configuration variables in KEY=VALUE format with either docker -e options or in a file specified with --env-file=<FILENAME> .","title":"Running a Frontier Squid Container"},{"location":"data/run-frontier-squid-container/#running-a-frontier-squid-container-with-systemd","text":"An example systemd service file for Frontier Squid. This will require creating the environment file in the directory /opt/xcache/.env . Note This example systemd file assumes <HOST PORT> is 3128 and <HOST CACHE PARTITION> is /tmp/squid and <HOST LOG PARTITION> is /tmp/log . Create the systemd service file /etc/systemd/system/docker.frontier-squid.service as follows: [Unit] Description=Stash Cache Container After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 Restart=always ExecStartPre=-/usr/bin/docker stop %n ExecStartPre=-/usr/bin/docker rm %n ExecStartPre=/usr/bin/docker pull opensciencegrid/frontier-squid:release ExecStart=/usr/bin/docker run --rm --name %n --publish 3128:3128 -v /tmp/squid:/var/cache/squid -v /tmp/log:/var/log/squid --env-file /opt/xcache/.env opensciencegrid/frontier-squid:release [Install] WantedBy=multi-user.target Enable and start the service with: root@host $ systemctl enable docker.frontier-squid root@host $ systemctl start docker.frontier-squid","title":"Running a Frontier Squid container with systemd"},{"location":"data/run-frontier-squid-container/#validating-the-frontier-squid-cache","text":"The cache server functions as a normal HTTP server and can interact with typical HTTP clients, such as curl or wget . Here, <HOST PORT> is the port chosen in the docker run command, 3128 by default. user@host $ export http_proxy = http://localhost:<HOST PORT> user@host $ wget -qdO/dev/null http://frontier.cern.ch 2 > & 1 | grep X-Cache X-Cache: MISS from 797a56e426cf user@host $ wget -qdO/dev/null http://frontier.cern.ch 2 > & 1 | grep X-Cache X-Cache: HIT from 797a56e426cf","title":"Validating the Frontier Squid Cache"},{"location":"data/run-frontier-squid-container/#registering-frontier-squid","text":"See the Registering Frontier Squid instructions to register your Frontier Squid host.","title":"Registering Frontier Squid"},{"location":"data/run-frontier-squid-container/#getting-help","text":"To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Getting Help"},{"location":"data/update-oasis/","text":"Updating Software in OASIS \u00b6 OASIS is the OSG Application Software Installation Service that can be used to publish and update software on OSG Worker Nodes under /cvmfs/oasis.opensciencegrid.org . It is implemented using CernVM FileSystem (CVMFS) technology and is the recommended method to install software on the Open Science Grid. This document is a step by step explanation of how a member of a Virtual Organization (VO) can become an OASIS manager for their VO and gain access to the shared OASIS service for software management. The shared OASIS service is especially appropropriate for VOs that have a relatively small number of members and a relatively small amount of software to distribute. Larger VOs should consider hosting their own separate repositories . Note For information on how to configure an OASIS client see the CVMFS installation documentation . Requirements \u00b6 To begin the process to distribute software on OASIS using the service, you must: Register as an OSG contact and upload your SSH Key . Submit a request to help@opensciencegrid.org to become an OASIS manager with the following: The names of the VO(s) whose software that you would like to manage with the shared OASIS login host The names of any other VO members that should be OASIS managers The name of a member of the VO(s) that can verify your affiliation, and Cc that person on your emailed request How to use OASIS \u00b6 Log in with SSH \u00b6 The shared OASIS login server is accessible via SSH for all OASIS managers with registered SSH keys: user@host $ ssh -i <PATH TO SSH KEY> ouser.<VO>@oasis-login.opensciencegrid.org Change <VO> for the name of the Virtual Organization you are trying to access and <PATH TO SSH KEY> with the path to the private part of the SSH key whose public part you registered with the OSG . Instead of putting -i <PATH TO SSH KEY> or ouser.<VO>@ on the command line, you can put it in your ~/.ssh/config : Host oasis-login.opensciencegrid.org User ouser.<VO> IdentityFile <PATH TO SSH KEY> Install and update software \u00b6 Once you log in, you can add/modify/remove content on a staging area at /stage/oasis/$VO where $VO is the name of the VO represented by the manager. Files here are visible to both oasis-login and the Stratum 0 server (oasis.opensciencegrid.org). There is a symbolic link at /cvmfs/oasis.opensciencegrid.org/$VO that points to the same staging area. Request an oasis publish with this command: user@host $ osg-oasis-update This command queues a process to sync the content of OASIS with the content of /stage/oasis/$VO osg-oasis-update returns immediately, but only one update can run at a time (across all VOs); your request may be queued behind a different VO. If you encounter severe delays before the update is finished being published (more than 4 hours), please file a support ticket . Limitations on repository content \u00b6 Although CVMFS provides a POSIX filesystem, it does not work well with all types of content. Content in OASIS is expected to adhere to the CVMFS repository content limitations so please review those guidelines carefully. Testing \u00b6 After osg-oasis-update completes and the changes have been propagated to the CVMFS stratum 1 servers (typically between 0 and 60 minutes, but possibly longer if the servers are busy with updates of other repositories) then the changes can be visible under /cvmfs/oasis.opensciencegrid.org on a computer that has the CVMFS client installed . A client normally only checks for updates if at least an hour has passed since it last checked, but people who have superuser access on the client machine can force it to check again with root@host # cvmfs_talk -i oasis.opensciencegrid.org remount This can be done while the filesystem is mounted (despite the name, it does not do an OS-level umount/mount of the filesystem). If the filesystem is not mounted, it will automatically check for new updates the next time it is mounted. In order to find out if an update has reached the CVMFS stratum 1 server, you can find out the latest osg-oasis-update time seen by the stratum 1 most favored by your CVMFS client with the following long command on your client machine: user@host $ date -d \"1970-1-1 GMT + $( wget -qO- $( attr -qg host /cvmfs/oasis.opensciencegrid.org ) /.cvmfspublished | \\ cat -v|sed -n '/^T/{s/^T//p;q;}') sec\" References \u00b6 CVMFS Documentation","title":"Update OASIS Shared Repo"},{"location":"data/update-oasis/#updating-software-in-oasis","text":"OASIS is the OSG Application Software Installation Service that can be used to publish and update software on OSG Worker Nodes under /cvmfs/oasis.opensciencegrid.org . It is implemented using CernVM FileSystem (CVMFS) technology and is the recommended method to install software on the Open Science Grid. This document is a step by step explanation of how a member of a Virtual Organization (VO) can become an OASIS manager for their VO and gain access to the shared OASIS service for software management. The shared OASIS service is especially appropropriate for VOs that have a relatively small number of members and a relatively small amount of software to distribute. Larger VOs should consider hosting their own separate repositories . Note For information on how to configure an OASIS client see the CVMFS installation documentation .","title":"Updating Software in OASIS"},{"location":"data/update-oasis/#requirements","text":"To begin the process to distribute software on OASIS using the service, you must: Register as an OSG contact and upload your SSH Key . Submit a request to help@opensciencegrid.org to become an OASIS manager with the following: The names of the VO(s) whose software that you would like to manage with the shared OASIS login host The names of any other VO members that should be OASIS managers The name of a member of the VO(s) that can verify your affiliation, and Cc that person on your emailed request","title":"Requirements"},{"location":"data/update-oasis/#how-to-use-oasis","text":"","title":"How to use OASIS"},{"location":"data/update-oasis/#log-in-with-ssh","text":"The shared OASIS login server is accessible via SSH for all OASIS managers with registered SSH keys: user@host $ ssh -i <PATH TO SSH KEY> ouser.<VO>@oasis-login.opensciencegrid.org Change <VO> for the name of the Virtual Organization you are trying to access and <PATH TO SSH KEY> with the path to the private part of the SSH key whose public part you registered with the OSG . Instead of putting -i <PATH TO SSH KEY> or ouser.<VO>@ on the command line, you can put it in your ~/.ssh/config : Host oasis-login.opensciencegrid.org User ouser.<VO> IdentityFile <PATH TO SSH KEY>","title":"Log in with SSH"},{"location":"data/update-oasis/#install-and-update-software","text":"Once you log in, you can add/modify/remove content on a staging area at /stage/oasis/$VO where $VO is the name of the VO represented by the manager. Files here are visible to both oasis-login and the Stratum 0 server (oasis.opensciencegrid.org). There is a symbolic link at /cvmfs/oasis.opensciencegrid.org/$VO that points to the same staging area. Request an oasis publish with this command: user@host $ osg-oasis-update This command queues a process to sync the content of OASIS with the content of /stage/oasis/$VO osg-oasis-update returns immediately, but only one update can run at a time (across all VOs); your request may be queued behind a different VO. If you encounter severe delays before the update is finished being published (more than 4 hours), please file a support ticket .","title":"Install and update software"},{"location":"data/update-oasis/#limitations-on-repository-content","text":"Although CVMFS provides a POSIX filesystem, it does not work well with all types of content. Content in OASIS is expected to adhere to the CVMFS repository content limitations so please review those guidelines carefully.","title":"Limitations on repository content"},{"location":"data/update-oasis/#testing","text":"After osg-oasis-update completes and the changes have been propagated to the CVMFS stratum 1 servers (typically between 0 and 60 minutes, but possibly longer if the servers are busy with updates of other repositories) then the changes can be visible under /cvmfs/oasis.opensciencegrid.org on a computer that has the CVMFS client installed . A client normally only checks for updates if at least an hour has passed since it last checked, but people who have superuser access on the client machine can force it to check again with root@host # cvmfs_talk -i oasis.opensciencegrid.org remount This can be done while the filesystem is mounted (despite the name, it does not do an OS-level umount/mount of the filesystem). If the filesystem is not mounted, it will automatically check for new updates the next time it is mounted. In order to find out if an update has reached the CVMFS stratum 1 server, you can find out the latest osg-oasis-update time seen by the stratum 1 most favored by your CVMFS client with the following long command on your client machine: user@host $ date -d \"1970-1-1 GMT + $( wget -qO- $( attr -qg host /cvmfs/oasis.opensciencegrid.org ) /.cvmfspublished | \\ cat -v|sed -n '/^T/{s/^T//p;q;}') sec\"","title":"Testing"},{"location":"data/update-oasis/#references","text":"CVMFS Documentation","title":"References"},{"location":"data/stashcache/install-cache/","text":"Installing the StashCache Cache \u00b6 This document describes how to install a StashCache cache service. This service allows a site or regional network to cache data frequently used on the OSG, reducing data transfer over the wide-area network and decreasing access latency. Before Starting \u00b6 Before starting the installation process, consider the following requirements: Operating system: A RHEL 7 or compatible operating systems. User IDs: If they do not exist already, the installation will create the Linux user IDs condor and xrootd Host certificate: Required for reporting and authenticated StashCache. Authenticated StashCache is an optional feature. See our documentation for instructions on how to request and install host certificates. Network ports: The cache service requires the following ports open: Inbound TCP port 1094 for file access via the XRootD protocol Inbound TCP port 8000 for file access via HTTP Inbound TCP port 8443 for authenticated file access via HTTPS (optional) Outbound UDP port 9930 for reporting to xrd-report.osgstorage.org and xrd-mon.osgstorage.org for monitoring Hardware requirements: We recommend that a cache has at least 10Gbps connectivity, 1TB of disk space for the cache directory, and 8GB of RAM. As with all OSG software installations, there are some one-time steps to prepare in advance: Obtain root access to the host Prepare the required Yum repositories Install CA certificates Registering the Cache \u00b6 To be part of the OSG StashCache Federation, your cache must be registered with the OSG. You will need basic information like the resource name and hostname, and the administrative and security contacts. Initial registration \u00b6 To register your cache host, follow the general registration instructions here . The service type is XRootD cache server . Info This step must be completed before installation. In your registration, you must specify which VOs your cache will serve by adding an AllowedVOs list, with each line specifying a VO whose data you are willing to cache. There are special values you may use in AllowedVOs : ANY_PUBLIC indicates that the cache is willing to serve public data from any VO. ANY indicates that the cache is willing to serve data from any VO, both public and non-public. ANY implies ANY_PUBLIC . There are extra requirements for serving non-public data: In addition to the cache allowing a VO in the AllowedVOs list, that VO must also allow the cache in its AllowedCaches list. See the page on getting your VO's data into StashCache . There must be an authenticated XRootD instance on the cache server. There must be a DN attribute in the resource registration with the subject DN of the host certificate This is an example registration for a cache server that serves all public data: MY_STASHCACHE_CACHE : FQDN : my-cache.example.net Service : XRootD cache server Description : StashCache cache server AllowedVOs : - ANY_PUBLIC This is an example registration for a cache server that only serves authenticated data from the OSG VO: MY_AUTH_STASHCACHE_CACHE : FQDN : my-auth-cache.example.net Service : XRootD cache server Description : StashCache cache server AllowedVOs : - OSG DN : /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=my-auth-cache.example.net This is an example registration for a cache server that serves all public data and authenticated data from the OSG VO: MY_COMBO_STASHCACHE_CACHE : FQDN : my-combo-cache.example.net Service : XRootD cache server Description : StashCache cache server AllowedVOs : - OSG - ANY_PUBLIC DN : /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=my-combo-cache.example.net Finalizing registration \u00b6 Once initial registration is complete, you may start the installation process. In the meantime, open a help ticket with your cache name. Mention in your ticket that you would like to \"Finalize the cache registration.\" Installing the Cache \u00b6 The StashCache software consists of an XRootD server with special configuration and supporting services. To simplify installation, OSG provides convenience RPMs that install all required packages with a single command: root@host # yum install stash-cache Configuring the Cache \u00b6 First, you must create a \"cache directory\", which will be used to store downloaded files. By default this is /mnt/stash . We recommend using a separate file system for the cache directory, with at least 1 TB of storage available. Note The cache directory must be writable by the xrootd:xrootd user and group. The stash-cache package provides default configuration files in /etc/xrootd/xrootd-stash-cache.cfg and /etc/xrootd/config.d/ . Administrators may provide additional configuration by placing files in /etc/xrootd/config.d/1*.cfg (for files that need to be processed BEFORE the OSG configuration) or /etc/xrootd/config.d/9*.cfg (for files that need to be processed AFTER the OSG configuration). You must configure every variable in /etc/xrootd/config.d/10-common-site-local.cfg . The mandatory variables to configure are: set rootdir = /mnt/stash : the mounted filesystem path to export. This document refers to this as /mnt/stash . set resourcename = YOUR_RESOURCE_NAME : the resource name registered with the OSG. Ensure the xrootd service has a certificate \u00b6 The service will need a certificate for reporting and to authenticate to StashCache origins. The easiest solution for this is to use your host certificate and key as follows: Copy the host certificate to /etc/grid-security/xrd/xrd{cert,key}.pem Set the owner of the directory and contents /etc/grid-security/xrd/ to xrootd:xrootd : root@host # chown -R xrootd:xrootd /etc/grid-security/xrd/ Note You must repeat the above steps whenever you renew your host certificate. If you automate certificate renewal, you should automate copying as well. For example, if you are using Certbot for Let's Encrypt, you should write a \"deploy hook\" as documented on the Certbot site . Configuring Optional Features \u00b6 Adjust disk utilization \u00b6 To adjust the disk utilization of your cache, create or edit a file named /etc/xrootd/config.d/90-local.cfg and set the values of pfc.diskusage . pfc.diskusage 0.90 0.95 The two values correspond to the low and high usage water marks, respectively. When usage goes above the high water mark, the XRootD service will delete cached files until usage goes below the low water mark. Enable remote debugging \u00b6 XRootD provides remote debugging via a read-only file system named digFS. This feature is disabled by default, but you may enable it if you need help troubleshooting your server. To enable remote debugging, edit /etc/xrootd/digauth.cfg and specify the authorizations for reading digFS. An example of authorizations: all allow gsi g=/glow h=*.cs.wisc.edu This gives access to the config file, log files, core files, and process information to anyone from *.cs.wisc.edu in the /glow VOMS group. See the XRootD manual for the full syntax. Remote debugging should only be enabled for as long as you need assistance. As soon as your issue has been resolved, revert any changes you have made to /etc/xrootd/digauth.cfg . Enable HTTPS on the unauthenticated cache \u00b6 By default, the unauthenticated stash-cache instance uses plain HTTP, not HTTPS. To use HTTPS: Add a certificate according to the instructions above Create a file named /etc/xrootd/config.d/11-cache-https.cfg with the following contents: # Support HTTPS access to unauthenticated cache if named stash-cache http.cadir /etc/grid-security/certificates http.cert /etc/grid-security/xrd/xrdcert.pem http.key /etc/grid-security/xrd/xrdkey.pem http.secxtractor /usr/lib64/libXrdLcmaps.so fi Manually Setting the FQDN (optional) \u00b6 The FQDN of the cache server that you registered in Topology may be different than its internal hostname (as reported by hostname -f ). For example, this may be the case if your cache is behind a load balancer such as LVS or MetalLB. In this case, you must manually tell the cache services which FQDN to use for topology lookups. Create the file /etc/systemd/system/stash-cache-authfile.service.d/override.conf with the following contents: [Service] Environment = CACHE_FQDN=<Topology-registered FQDN> Managing StashCache and associated services \u00b6 These services must be managed by systemctl and may start additional services as dependencies. As a reminder, here are common service commands (all run as root ) for EL7: To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> Public cache services \u00b6 Software Service name Notes XRootD xrootd@stash-cache.service The XRootD daemon, which performs the data transfers XCache xcache-reporter.timer Reports usage information to collector.opensciencegrid.org Fetch CRL fetch-crl-boot and fetch-crl-cron Required to authenticate monitoring services. See CA documentation for more info Authenticated cache services (optional) \u00b6 In addition to the public cache services, there are three systemd units specific to the authenticated cache. Software Service name Notes XRootD xrootd-renew-proxy.service Renew a proxy for authenticated downloads to the cache xrootd@stash-cache-auth.service The xrootd daemon which performs authenticated data transfers xrootd-renew-proxy.timer Trigger daily proxy renewal stash-cache-authfile.service Generate the Authentication configuration file for XRootD stash-cache-authfile.timer Periodically generate the Authentication configuration file for XRootD Validating the Cache \u00b6 The cache server functions as a normal HTTP server and can interact with typical HTTP clients, such as curl . user@host $ curl -O http://cache_host:8000/osgconnect/public/rynge/test.data curl may not correctly report a failure, so verify that the contents of the file are: hello world! Test cache server reporting to the central collector \u00b6 To verify the cache is reporting to the central collector, run the following command from the cache server: user@host $ condor_status -any -pool collector.opensciencegrid.org:9619 \\ -l -const \"Name==\\\"xrootd@`hostname`\\\"\" The output of the above command should detail what the collector knows about the status of your cache. Here is an example snippet of the output: AuthenticatedIdentity = \"sc-cache.chtc.wisc.edu@daemon.opensciencegrid.org\" AuthenticationMethod = \"GSI\" free_cache_bytes = 868104454144 free_cache_fraction = 0.8022261674321525 LastHeardFrom = 1552002482 most_recent_access_time = 1551997049 MyType = \"Machine\" Name = \"xrootd@sc-cache.chtc.wisc.edu\" ping_elapsed_time = 0.00763392448425293 ping_response_code = 0 ping_response_message = \"[SUCCESS] \" ping_response_status = \"ok\" STASHCACHE_DaemonVersion = \"1.0.0\" ... Getting Help \u00b6 To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Install from RPM"},{"location":"data/stashcache/install-cache/#installing-the-stashcache-cache","text":"This document describes how to install a StashCache cache service. This service allows a site or regional network to cache data frequently used on the OSG, reducing data transfer over the wide-area network and decreasing access latency.","title":"Installing the StashCache Cache"},{"location":"data/stashcache/install-cache/#before-starting","text":"Before starting the installation process, consider the following requirements: Operating system: A RHEL 7 or compatible operating systems. User IDs: If they do not exist already, the installation will create the Linux user IDs condor and xrootd Host certificate: Required for reporting and authenticated StashCache. Authenticated StashCache is an optional feature. See our documentation for instructions on how to request and install host certificates. Network ports: The cache service requires the following ports open: Inbound TCP port 1094 for file access via the XRootD protocol Inbound TCP port 8000 for file access via HTTP Inbound TCP port 8443 for authenticated file access via HTTPS (optional) Outbound UDP port 9930 for reporting to xrd-report.osgstorage.org and xrd-mon.osgstorage.org for monitoring Hardware requirements: We recommend that a cache has at least 10Gbps connectivity, 1TB of disk space for the cache directory, and 8GB of RAM. As with all OSG software installations, there are some one-time steps to prepare in advance: Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"data/stashcache/install-cache/#registering-the-cache","text":"To be part of the OSG StashCache Federation, your cache must be registered with the OSG. You will need basic information like the resource name and hostname, and the administrative and security contacts.","title":"Registering the Cache"},{"location":"data/stashcache/install-cache/#initial-registration","text":"To register your cache host, follow the general registration instructions here . The service type is XRootD cache server . Info This step must be completed before installation. In your registration, you must specify which VOs your cache will serve by adding an AllowedVOs list, with each line specifying a VO whose data you are willing to cache. There are special values you may use in AllowedVOs : ANY_PUBLIC indicates that the cache is willing to serve public data from any VO. ANY indicates that the cache is willing to serve data from any VO, both public and non-public. ANY implies ANY_PUBLIC . There are extra requirements for serving non-public data: In addition to the cache allowing a VO in the AllowedVOs list, that VO must also allow the cache in its AllowedCaches list. See the page on getting your VO's data into StashCache . There must be an authenticated XRootD instance on the cache server. There must be a DN attribute in the resource registration with the subject DN of the host certificate This is an example registration for a cache server that serves all public data: MY_STASHCACHE_CACHE : FQDN : my-cache.example.net Service : XRootD cache server Description : StashCache cache server AllowedVOs : - ANY_PUBLIC This is an example registration for a cache server that only serves authenticated data from the OSG VO: MY_AUTH_STASHCACHE_CACHE : FQDN : my-auth-cache.example.net Service : XRootD cache server Description : StashCache cache server AllowedVOs : - OSG DN : /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=my-auth-cache.example.net This is an example registration for a cache server that serves all public data and authenticated data from the OSG VO: MY_COMBO_STASHCACHE_CACHE : FQDN : my-combo-cache.example.net Service : XRootD cache server Description : StashCache cache server AllowedVOs : - OSG - ANY_PUBLIC DN : /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=my-combo-cache.example.net","title":"Initial registration"},{"location":"data/stashcache/install-cache/#finalizing-registration","text":"Once initial registration is complete, you may start the installation process. In the meantime, open a help ticket with your cache name. Mention in your ticket that you would like to \"Finalize the cache registration.\"","title":"Finalizing registration"},{"location":"data/stashcache/install-cache/#installing-the-cache","text":"The StashCache software consists of an XRootD server with special configuration and supporting services. To simplify installation, OSG provides convenience RPMs that install all required packages with a single command: root@host # yum install stash-cache","title":"Installing the Cache"},{"location":"data/stashcache/install-cache/#configuring-the-cache","text":"First, you must create a \"cache directory\", which will be used to store downloaded files. By default this is /mnt/stash . We recommend using a separate file system for the cache directory, with at least 1 TB of storage available. Note The cache directory must be writable by the xrootd:xrootd user and group. The stash-cache package provides default configuration files in /etc/xrootd/xrootd-stash-cache.cfg and /etc/xrootd/config.d/ . Administrators may provide additional configuration by placing files in /etc/xrootd/config.d/1*.cfg (for files that need to be processed BEFORE the OSG configuration) or /etc/xrootd/config.d/9*.cfg (for files that need to be processed AFTER the OSG configuration). You must configure every variable in /etc/xrootd/config.d/10-common-site-local.cfg . The mandatory variables to configure are: set rootdir = /mnt/stash : the mounted filesystem path to export. This document refers to this as /mnt/stash . set resourcename = YOUR_RESOURCE_NAME : the resource name registered with the OSG.","title":"Configuring the Cache"},{"location":"data/stashcache/install-cache/#ensure-the-xrootd-service-has-a-certificate","text":"The service will need a certificate for reporting and to authenticate to StashCache origins. The easiest solution for this is to use your host certificate and key as follows: Copy the host certificate to /etc/grid-security/xrd/xrd{cert,key}.pem Set the owner of the directory and contents /etc/grid-security/xrd/ to xrootd:xrootd : root@host # chown -R xrootd:xrootd /etc/grid-security/xrd/ Note You must repeat the above steps whenever you renew your host certificate. If you automate certificate renewal, you should automate copying as well. For example, if you are using Certbot for Let's Encrypt, you should write a \"deploy hook\" as documented on the Certbot site .","title":"Ensure the xrootd service has a certificate"},{"location":"data/stashcache/install-cache/#configuring-optional-features","text":"","title":"Configuring Optional Features"},{"location":"data/stashcache/install-cache/#adjust-disk-utilization","text":"To adjust the disk utilization of your cache, create or edit a file named /etc/xrootd/config.d/90-local.cfg and set the values of pfc.diskusage . pfc.diskusage 0.90 0.95 The two values correspond to the low and high usage water marks, respectively. When usage goes above the high water mark, the XRootD service will delete cached files until usage goes below the low water mark.","title":"Adjust disk utilization"},{"location":"data/stashcache/install-cache/#enable-remote-debugging","text":"XRootD provides remote debugging via a read-only file system named digFS. This feature is disabled by default, but you may enable it if you need help troubleshooting your server. To enable remote debugging, edit /etc/xrootd/digauth.cfg and specify the authorizations for reading digFS. An example of authorizations: all allow gsi g=/glow h=*.cs.wisc.edu This gives access to the config file, log files, core files, and process information to anyone from *.cs.wisc.edu in the /glow VOMS group. See the XRootD manual for the full syntax. Remote debugging should only be enabled for as long as you need assistance. As soon as your issue has been resolved, revert any changes you have made to /etc/xrootd/digauth.cfg .","title":"Enable remote debugging"},{"location":"data/stashcache/install-cache/#enable-https-on-the-unauthenticated-cache","text":"By default, the unauthenticated stash-cache instance uses plain HTTP, not HTTPS. To use HTTPS: Add a certificate according to the instructions above Create a file named /etc/xrootd/config.d/11-cache-https.cfg with the following contents: # Support HTTPS access to unauthenticated cache if named stash-cache http.cadir /etc/grid-security/certificates http.cert /etc/grid-security/xrd/xrdcert.pem http.key /etc/grid-security/xrd/xrdkey.pem http.secxtractor /usr/lib64/libXrdLcmaps.so fi","title":"Enable HTTPS on the unauthenticated cache"},{"location":"data/stashcache/install-cache/#manually-setting-the-fqdn-optional","text":"The FQDN of the cache server that you registered in Topology may be different than its internal hostname (as reported by hostname -f ). For example, this may be the case if your cache is behind a load balancer such as LVS or MetalLB. In this case, you must manually tell the cache services which FQDN to use for topology lookups. Create the file /etc/systemd/system/stash-cache-authfile.service.d/override.conf with the following contents: [Service] Environment = CACHE_FQDN=<Topology-registered FQDN>","title":"Manually Setting the FQDN (optional)"},{"location":"data/stashcache/install-cache/#managing-stashcache-and-associated-services","text":"These services must be managed by systemctl and may start additional services as dependencies. As a reminder, here are common service commands (all run as root ) for EL7: To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME>","title":"Managing StashCache and associated services"},{"location":"data/stashcache/install-cache/#public-cache-services","text":"Software Service name Notes XRootD xrootd@stash-cache.service The XRootD daemon, which performs the data transfers XCache xcache-reporter.timer Reports usage information to collector.opensciencegrid.org Fetch CRL fetch-crl-boot and fetch-crl-cron Required to authenticate monitoring services. See CA documentation for more info","title":"Public cache services"},{"location":"data/stashcache/install-cache/#authenticated-cache-services-optional","text":"In addition to the public cache services, there are three systemd units specific to the authenticated cache. Software Service name Notes XRootD xrootd-renew-proxy.service Renew a proxy for authenticated downloads to the cache xrootd@stash-cache-auth.service The xrootd daemon which performs authenticated data transfers xrootd-renew-proxy.timer Trigger daily proxy renewal stash-cache-authfile.service Generate the Authentication configuration file for XRootD stash-cache-authfile.timer Periodically generate the Authentication configuration file for XRootD","title":"Authenticated cache services (optional)"},{"location":"data/stashcache/install-cache/#validating-the-cache","text":"The cache server functions as a normal HTTP server and can interact with typical HTTP clients, such as curl . user@host $ curl -O http://cache_host:8000/osgconnect/public/rynge/test.data curl may not correctly report a failure, so verify that the contents of the file are: hello world!","title":"Validating the Cache"},{"location":"data/stashcache/install-cache/#test-cache-server-reporting-to-the-central-collector","text":"To verify the cache is reporting to the central collector, run the following command from the cache server: user@host $ condor_status -any -pool collector.opensciencegrid.org:9619 \\ -l -const \"Name==\\\"xrootd@`hostname`\\\"\" The output of the above command should detail what the collector knows about the status of your cache. Here is an example snippet of the output: AuthenticatedIdentity = \"sc-cache.chtc.wisc.edu@daemon.opensciencegrid.org\" AuthenticationMethod = \"GSI\" free_cache_bytes = 868104454144 free_cache_fraction = 0.8022261674321525 LastHeardFrom = 1552002482 most_recent_access_time = 1551997049 MyType = \"Machine\" Name = \"xrootd@sc-cache.chtc.wisc.edu\" ping_elapsed_time = 0.00763392448425293 ping_response_code = 0 ping_response_message = \"[SUCCESS] \" ping_response_status = \"ok\" STASHCACHE_DaemonVersion = \"1.0.0\" ...","title":"Test cache server reporting to the central collector"},{"location":"data/stashcache/install-cache/#getting-help","text":"To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Getting Help"},{"location":"data/stashcache/install-origin/","text":"Installing the StashCache Origin \u00b6 Warning If you want to run origins for authenticated and unauthenticated data, you must run them on separate hosts. This requires registering a resource for each host. This requirement will be removed in a future version of StashCache. This document describes how to install a StashCache origin service. This service allows an organization to export its data to the StashCache data federation. Note The origin must be registered with the OSG prior to joining the data federation. You may start the registration process prior to finishing the installation by using this link along with information like: Resource name and hostname VO associated with this origin server (which will be used to determine the origin's namespace prefix) Administrative and security contact(s) Who (or what) will be allowed to access the VO's data Which caches will be allowed to cache the VO data Before Starting \u00b6 Before starting the installation process, consider the following points: Operating system: A RHEL 7 or compatible operating system. User IDs: If they do not exist already, the installation will create the Linux user IDs condor and xrootd ; only the xrootd user is utilized for the running daemons. Host certificate: The origin service uses a host certificate to authenticate with the caches it serves. The host certificate documentation provides more information on setting up host certificates. Network ports: The origin service requires the following ports open: Inbound TCP port 1094 for file access via the XRootD protocol Outbound TCP port 1213 to redirector.osgstorage.org for connecting to the data federation Outbound UDP port 9930 for reporting to xrd-report.osgstorage.org and xrd-mon.osgstorage.org for monitoring. Hardware requirements: We recommend that an origin has at least 1Gbps connectivity and 8GB of RAM. We suggest that several gigabytes of local disk space be available for log files, although some logging verbosity can be reduced. As with all OSG software installations, there are some one-time steps to prepare in advance: Obtain root access to the host Prepare the required Yum repositories Install CA certificates Installing the Origin \u00b6 The origin service consists of one or more XRootD daemons and their dependencies for the authentication infrastructure. To simplify installation, OSG provides convenience RPMs that install all required software with a single command: root@host # yum install stash-origin For this installation guide, we assume that the data to be exported to the federation is mounted at /mnt/stash and owned by the xrootd:xrootd user. Configuring the Origin Server \u00b6 The stash-origin package provides a default configuration files in /etc/xrootd/xrootd-stash-origin.cfg and /etc/xrootd/config.d . Administrators may provide additional configuration by placing files in /etc/xrootd/config.d of the form /etc/xrootd/config.d/1*.cfg (for directives that need to be processed BEFORE the OSG configuration) or /etc/xrootd/config.d/9*.cfg (for directives that are processed AFTER the OSG configuration). You must configure every variable in /etc/xrootd/config.d/10-common-site-local.cfg and /etc/xrootd/config.d/10-origin-site-local.cfg . The mandatory variables to configure are: File Config line Description 10-common-site-local.cfg set rootdir = /mnt/stash The mounted filesystem path to export; this document calls it /mnt/stash 10-common-site-local.cfg set resourcename = YOUR_RESOURCE_NAME The resource name registered with OSG 10-origin-site-local.cfg set originexport = /VO The directory relative to rootdir that is the top of the exported namespace for the origin services For example, if the HCC VO would like to set up an origin server exporting from the mount point /mnt/stash , and HCC's registered namespace is /hcc , then the following would be set in 10-common-site-local.cfg : set rootdir = /mnt/stash set resourcename = HCC_STASH_ORIGIN And the following would be set in 10-origin-site-local.cfg : set originexport = /hcc With this configuration, the data under /mnt/stash/hcc/bio/datasets would be available under the StashCache path /hcc/bio/datasets and the data under /mnt/stash/hcc/hep/generators would be available under the StashCache path /hcc/hep/generators . Warning If you want to run origins for authenticated and unauthenticated data, you must run them on separate hosts. This requires registering a resource for each host. This requirement will be removed in a future version of StashCache. Warning The StashCache namespace is global within a data federation. Directories you export must not collide with directories provided by other origin servers; this is why the explicit registration is required. Manually Setting the FQDN (optional) \u00b6 The FQDN of the origin server that you registered in Topology may be different than its internal hostname (as reported by hostname -f ). For example, this may be the case if your origin is behind a load balancer such as LVS or MetalLB. In this case, you must manually tell the origin services which FQDN to use for topology lookups. Create the file /etc/systemd/system/stash-origin-authfile.service.d/override.conf with the following contents: [Service] Environment = ORIGIN_FQDN=<Topology-registered FQDN> Managing the Origin Service \u00b6 The origin service consists of the following SystemD units that you must directly manage: Service name Notes xrootd@stash-origin.service Performs data transfers (unauthenticated instance) xrootd@stash-origin-auth.service Performs data transfers (authenticated instance) These services must be managed with systemctl and may start additional services as dependencies. As a reminder, here are common service commands (all run as root ): To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> In addition, the origin service automatically uses the following SystemD units: Service name Notes cmsd@stash-origin.service Integrates the origin into the data federation (unauthenticated instance) cmsd@stash-origin-auth.service Integrates the origin into the data federation (authenticated instance) stash-origin-authfile.timer Updates the authorization files periodically Verifying the Origin Server \u00b6 Once your server has been registered with the OSG and started, perform the following steps to verify that it is functional. Testing availability \u00b6 To verify that your origin is correctly advertising its availability, run the following command from the origin server: [user@server ~]$ xrdmapc -r --list s redirector.osgstorage.org:1094 0**** redirector.osgstorage.org:1094 Srv ceph-gridftp1.grid.uchicago.edu:1094 Srv stashcache.fnal.gov:1094 Srv stash.osgconnect.net:1094 Srv origin.ligo.caltech.edu:1094 Srv csiu.grid.iu.edu:1094 The output should list the hostname of your origin server. Testing directory export \u00b6 To verify that the directories you are exporting are visible from the redirector, run the following command from the origin server: [user@server ~]$ xrdmapc -r --verify --list s redirector.osgstorage.org:1094 <EXPORTED DIR> 0*rv* redirector.osgstorage.org:1094 >+ Srv ceph-gridftp1.grid.uchicago.edu:1094 ? Srv stashcache.fnal.gov:1094 [not authorized] >+ Srv stash.osgconnect.net:1094 - Srv origin.ligo.caltech.edu:1094 ? Srv csiu.grid.iu.edu:1094 [connect error] Change <EXPORTED_DIR> for the directory the service is suppose to export. Your server should be marked with a >+ to indicate that it contains the given path and the path was accessible. Testing file access \u00b6 To verify that you can download a file from the origin server, use the stashcp tool. Place a <TEST FILE> in <EXPORTED DIR> . Where <TEST FILE> can be any file. The stashcp tool is available in the stashcache-client RPM. Run the following command: [user@host]$ stashcp <TEST FILE> /tmp/testfile If successful, there should be a file at /tmp/testfile with the contents of the test file on your origin server. If unsuccessful, you can pass the -d flag to stashcp for debug info. You can also test directly downloading from the origin via xrdcp , which is available in the xrootd-client RPM. Run the following command: [user@host]$ xrdcp xroot://<origin server>:1094/<TEST FILE> /tmp/testfile Registering the Origin \u00b6 To be part of the OSG StashCache Federation, your origin must be registered with the OSG . The service type is XRootD origin server . The resource must also specify which VOs it will serve data from. To do this, add an AllowedVOs list, with each line specifying a VO whose StashCache data the resource is willing to host. For example: MY_STASHCACHE_ORIGIN : Service : XRootD origin server Description : StashCache origin server AllowedVOs : - GLOW - OSG You can use the special value ANY to indicate that the origin will serve data from any VO that puts data on it. In addition to the origin allowing a VOs via the AllowedVOs list, that VO must also allow the origin in its DataFederations/StashCache/AllowedOrigins list. See the page on getting your VO's data into StashCache . Getting Help \u00b6 To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Install from RPM"},{"location":"data/stashcache/install-origin/#installing-the-stashcache-origin","text":"Warning If you want to run origins for authenticated and unauthenticated data, you must run them on separate hosts. This requires registering a resource for each host. This requirement will be removed in a future version of StashCache. This document describes how to install a StashCache origin service. This service allows an organization to export its data to the StashCache data federation. Note The origin must be registered with the OSG prior to joining the data federation. You may start the registration process prior to finishing the installation by using this link along with information like: Resource name and hostname VO associated with this origin server (which will be used to determine the origin's namespace prefix) Administrative and security contact(s) Who (or what) will be allowed to access the VO's data Which caches will be allowed to cache the VO data","title":"Installing the StashCache Origin"},{"location":"data/stashcache/install-origin/#before-starting","text":"Before starting the installation process, consider the following points: Operating system: A RHEL 7 or compatible operating system. User IDs: If they do not exist already, the installation will create the Linux user IDs condor and xrootd ; only the xrootd user is utilized for the running daemons. Host certificate: The origin service uses a host certificate to authenticate with the caches it serves. The host certificate documentation provides more information on setting up host certificates. Network ports: The origin service requires the following ports open: Inbound TCP port 1094 for file access via the XRootD protocol Outbound TCP port 1213 to redirector.osgstorage.org for connecting to the data federation Outbound UDP port 9930 for reporting to xrd-report.osgstorage.org and xrd-mon.osgstorage.org for monitoring. Hardware requirements: We recommend that an origin has at least 1Gbps connectivity and 8GB of RAM. We suggest that several gigabytes of local disk space be available for log files, although some logging verbosity can be reduced. As with all OSG software installations, there are some one-time steps to prepare in advance: Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"data/stashcache/install-origin/#installing-the-origin","text":"The origin service consists of one or more XRootD daemons and their dependencies for the authentication infrastructure. To simplify installation, OSG provides convenience RPMs that install all required software with a single command: root@host # yum install stash-origin For this installation guide, we assume that the data to be exported to the federation is mounted at /mnt/stash and owned by the xrootd:xrootd user.","title":"Installing the Origin"},{"location":"data/stashcache/install-origin/#configuring-the-origin-server","text":"The stash-origin package provides a default configuration files in /etc/xrootd/xrootd-stash-origin.cfg and /etc/xrootd/config.d . Administrators may provide additional configuration by placing files in /etc/xrootd/config.d of the form /etc/xrootd/config.d/1*.cfg (for directives that need to be processed BEFORE the OSG configuration) or /etc/xrootd/config.d/9*.cfg (for directives that are processed AFTER the OSG configuration). You must configure every variable in /etc/xrootd/config.d/10-common-site-local.cfg and /etc/xrootd/config.d/10-origin-site-local.cfg . The mandatory variables to configure are: File Config line Description 10-common-site-local.cfg set rootdir = /mnt/stash The mounted filesystem path to export; this document calls it /mnt/stash 10-common-site-local.cfg set resourcename = YOUR_RESOURCE_NAME The resource name registered with OSG 10-origin-site-local.cfg set originexport = /VO The directory relative to rootdir that is the top of the exported namespace for the origin services For example, if the HCC VO would like to set up an origin server exporting from the mount point /mnt/stash , and HCC's registered namespace is /hcc , then the following would be set in 10-common-site-local.cfg : set rootdir = /mnt/stash set resourcename = HCC_STASH_ORIGIN And the following would be set in 10-origin-site-local.cfg : set originexport = /hcc With this configuration, the data under /mnt/stash/hcc/bio/datasets would be available under the StashCache path /hcc/bio/datasets and the data under /mnt/stash/hcc/hep/generators would be available under the StashCache path /hcc/hep/generators . Warning If you want to run origins for authenticated and unauthenticated data, you must run them on separate hosts. This requires registering a resource for each host. This requirement will be removed in a future version of StashCache. Warning The StashCache namespace is global within a data federation. Directories you export must not collide with directories provided by other origin servers; this is why the explicit registration is required.","title":"Configuring the Origin Server"},{"location":"data/stashcache/install-origin/#manually-setting-the-fqdn-optional","text":"The FQDN of the origin server that you registered in Topology may be different than its internal hostname (as reported by hostname -f ). For example, this may be the case if your origin is behind a load balancer such as LVS or MetalLB. In this case, you must manually tell the origin services which FQDN to use for topology lookups. Create the file /etc/systemd/system/stash-origin-authfile.service.d/override.conf with the following contents: [Service] Environment = ORIGIN_FQDN=<Topology-registered FQDN>","title":"Manually Setting the FQDN (optional)"},{"location":"data/stashcache/install-origin/#managing-the-origin-service","text":"The origin service consists of the following SystemD units that you must directly manage: Service name Notes xrootd@stash-origin.service Performs data transfers (unauthenticated instance) xrootd@stash-origin-auth.service Performs data transfers (authenticated instance) These services must be managed with systemctl and may start additional services as dependencies. As a reminder, here are common service commands (all run as root ): To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> In addition, the origin service automatically uses the following SystemD units: Service name Notes cmsd@stash-origin.service Integrates the origin into the data federation (unauthenticated instance) cmsd@stash-origin-auth.service Integrates the origin into the data federation (authenticated instance) stash-origin-authfile.timer Updates the authorization files periodically","title":"Managing the Origin Service"},{"location":"data/stashcache/install-origin/#verifying-the-origin-server","text":"Once your server has been registered with the OSG and started, perform the following steps to verify that it is functional.","title":"Verifying the Origin Server"},{"location":"data/stashcache/install-origin/#testing-availability","text":"To verify that your origin is correctly advertising its availability, run the following command from the origin server: [user@server ~]$ xrdmapc -r --list s redirector.osgstorage.org:1094 0**** redirector.osgstorage.org:1094 Srv ceph-gridftp1.grid.uchicago.edu:1094 Srv stashcache.fnal.gov:1094 Srv stash.osgconnect.net:1094 Srv origin.ligo.caltech.edu:1094 Srv csiu.grid.iu.edu:1094 The output should list the hostname of your origin server.","title":"Testing availability"},{"location":"data/stashcache/install-origin/#testing-directory-export","text":"To verify that the directories you are exporting are visible from the redirector, run the following command from the origin server: [user@server ~]$ xrdmapc -r --verify --list s redirector.osgstorage.org:1094 <EXPORTED DIR> 0*rv* redirector.osgstorage.org:1094 >+ Srv ceph-gridftp1.grid.uchicago.edu:1094 ? Srv stashcache.fnal.gov:1094 [not authorized] >+ Srv stash.osgconnect.net:1094 - Srv origin.ligo.caltech.edu:1094 ? Srv csiu.grid.iu.edu:1094 [connect error] Change <EXPORTED_DIR> for the directory the service is suppose to export. Your server should be marked with a >+ to indicate that it contains the given path and the path was accessible.","title":"Testing directory export"},{"location":"data/stashcache/install-origin/#testing-file-access","text":"To verify that you can download a file from the origin server, use the stashcp tool. Place a <TEST FILE> in <EXPORTED DIR> . Where <TEST FILE> can be any file. The stashcp tool is available in the stashcache-client RPM. Run the following command: [user@host]$ stashcp <TEST FILE> /tmp/testfile If successful, there should be a file at /tmp/testfile with the contents of the test file on your origin server. If unsuccessful, you can pass the -d flag to stashcp for debug info. You can also test directly downloading from the origin via xrdcp , which is available in the xrootd-client RPM. Run the following command: [user@host]$ xrdcp xroot://<origin server>:1094/<TEST FILE> /tmp/testfile","title":"Testing file access"},{"location":"data/stashcache/install-origin/#registering-the-origin","text":"To be part of the OSG StashCache Federation, your origin must be registered with the OSG . The service type is XRootD origin server . The resource must also specify which VOs it will serve data from. To do this, add an AllowedVOs list, with each line specifying a VO whose StashCache data the resource is willing to host. For example: MY_STASHCACHE_ORIGIN : Service : XRootD origin server Description : StashCache origin server AllowedVOs : - GLOW - OSG You can use the special value ANY to indicate that the origin will serve data from any VO that puts data on it. In addition to the origin allowing a VOs via the AllowedVOs list, that VO must also allow the origin in its DataFederations/StashCache/AllowedOrigins list. See the page on getting your VO's data into StashCache .","title":"Registering the Origin"},{"location":"data/stashcache/install-origin/#getting-help","text":"To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Getting Help"},{"location":"data/stashcache/overview/","text":"Data Federation Overview \u00b6 The OSG operates the StashCache data federation , which provides organizations with a method to distribute their data in a scalable manner to thousands of jobs without needing to pre-stage data at each site. The StashCache data federation is best suited for per-job data set sizes between 1 and 50 GB, with no more than 1 TB for a complete workflow. Organizations export their data from an origin server , and the data is streamed to jobs from a set of cache servers distributed throughout the OSG and Internet2. Jobs can achieve lower latency and higher throughput for data transfer by using a nearby cache instead of accessing the origin directly. The map below shows the location of the current caches in the StashCache federation: Architecture \u00b6 The StashCache federation consists of three service types: Origin : Keeps the authoritative copy of an organization's data. Each origin is operated by the organization that wants to distribute its data within the StashCache federation. Cache : Transfers data to clients such as jobs or users. A set of caches are operated across the OSG for the benefit of nearby sites; in addition, each site may run its own cache in order to reduce the amount of data transferred over the WAN. Redirector : Tells a cache service which origin to download data from. The redirectors are run centrally by OSG Operations. The structure of the federation is illustrated below: A job will request data from a cache. The cache will serve the requested data from local disk if possible. Otherwise, it will query a redirector for the origin server that provides the data, download the data from that origin server, and then serve the data to the job. A copy of the data will be kept on the cache for use by future jobs. Joining and Using StashCache \u00b6 Organizations can export their data to StashCache by installing the StashCache Origin Sites can reduce data transfer via the WAN by installing the StashCache Cache Users can access StashCache via the stashcp client or from CVMFS via the /cvmfs/stash.osgstorage.org directory tree.","title":"Overview"},{"location":"data/stashcache/overview/#data-federation-overview","text":"The OSG operates the StashCache data federation , which provides organizations with a method to distribute their data in a scalable manner to thousands of jobs without needing to pre-stage data at each site. The StashCache data federation is best suited for per-job data set sizes between 1 and 50 GB, with no more than 1 TB for a complete workflow. Organizations export their data from an origin server , and the data is streamed to jobs from a set of cache servers distributed throughout the OSG and Internet2. Jobs can achieve lower latency and higher throughput for data transfer by using a nearby cache instead of accessing the origin directly. The map below shows the location of the current caches in the StashCache federation:","title":"Data Federation Overview"},{"location":"data/stashcache/overview/#architecture","text":"The StashCache federation consists of three service types: Origin : Keeps the authoritative copy of an organization's data. Each origin is operated by the organization that wants to distribute its data within the StashCache federation. Cache : Transfers data to clients such as jobs or users. A set of caches are operated across the OSG for the benefit of nearby sites; in addition, each site may run its own cache in order to reduce the amount of data transferred over the WAN. Redirector : Tells a cache service which origin to download data from. The redirectors are run centrally by OSG Operations. The structure of the federation is illustrated below: A job will request data from a cache. The cache will serve the requested data from local disk if possible. Otherwise, it will query a redirector for the origin server that provides the data, download the data from that origin server, and then serve the data to the job. A copy of the data will be kept on the cache for use by future jobs.","title":"Architecture"},{"location":"data/stashcache/overview/#joining-and-using-stashcache","text":"Organizations can export their data to StashCache by installing the StashCache Origin Sites can reduce data transfer via the WAN by installing the StashCache Cache Users can access StashCache via the stashcp client or from CVMFS via the /cvmfs/stash.osgstorage.org directory tree.","title":"Joining and Using StashCache"},{"location":"data/stashcache/run-stash-origin-container/","text":"Running StashCache Origin in a Container \u00b6 Note Currently, the StashCache Origin container only supports distribution of public data. If you would like to distribute private data requiring authentication, see the RPM installation guide . The OSG operates the StashCache data federation , which provides organizations with a method to distribute their data in a scalable manner to thousands of jobs without needing to pre-stage data across sites or operate their own scalable infrastructure. Stash Origins store copies of users' data. Each community (or experiment) needs to run one origin to export its data via the StashCache federation. This document outlines how to run such an origin in a Docker container. Before Starting \u00b6 Before starting the installation process, consider the following points: Docker: For the purpose of this guide, the host must have a running docker service and you must have the ability to start containers (i.e., belong to the docker Unix group). Network ports: The Stash Origin listens for incoming HTTP/S and XRootD connections on ports 1094 and 1095 (by default). File Systems: Stash Origin needs a host partition to store user data. Registration: Before deploying an origin, you must registered the service in the OSG Topology Configuring the Origin \u00b6 In addition to the required configuration above (ports and file systems), you may also configure the behavior of your origin with the following variables using an environment variable file: Where the environment file on the docker host, /opt/origin/.env , has (at least) the following contents, replacing <YOUR_RESOURCE_NAME> with the resource name of your origin as registered in Topology and <FQDN> with the public DNS name that should be used to contact your origin: XC_RESOURCENAME=YOUR_SITE_NAME ORIGIN_FQDN=<FQDN> Populating Origin Data \u00b6 The Stash Cache data federation namespace is shared by multiple VOs so you must choose a namespace for your own VO's data. When running an origin container, your chosen namespace must be reflected in your host partition. For example, if your host partition is /srv/origin-puliic and the name of your VO is ASTRO, you should store the Astro VO's public data in /srv/origin-public/astro/ . Then, when starting container, you will mount /srv/origin-public/ into /xcache/namespace` in the container. Running the Origin \u00b6 It is recommended to use a container orchestration service such as docker-compose or kubernetes whose details are beyond the scope of this document. The following sections provide examples for starting origin containers from the command-line as well as a more production-appropriate method using systemd. user@host $ docker run --rm --publish 1094 :1094 \\ --publish 1095:1095 \\ --volume <HOST PARTITION>:/xcache/namespace \\ --env-file=/opt/origin/.env \\ opensciencegrid/stash-origin:release Replacing <HOST PARTITION> with the host directory containing data that your origin should serve. See this section for details. Warning A container deployed this way will serve the entire contents of <HOST PARTITION> . Running on origin container with systemd \u00b6 An example systemd service file for StashCache. This will require creating the environment file in the directory /opt/origin/.env . Note This example systemd file assumes <HOST PARTITION> is /srv/origin-public . Create the systemd service file /etc/systemd/system/docker.stash-origin.service as follows: [Unit] Description=Stash Origin Container After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 Restart=always ExecStartPre=-/usr/bin/docker stop %n ExecStartPre=-/usr/bin/docker rm %n ExecStartPre=/usr/bin/docker pull opensciencegrid/stash-origin:release ExecStart=/usr/bin/docker run --rm --name %n -p 1094:1094 -p 1095:1095 -v /srv/origin-public:/xcache/namespace --env-file /opt/origin/.env opensciencegrid/stash-origin:release [Install] WantedBy=multi-user.target Enable and start the service with: root@host $ systemctl enable docker.stash-origin root@host $ systemctl start docker.stash-origin Warning You must register the origin before considering it a production service. Validating Origin \u00b6 To validate the origin please follow the validating origin instructions . Getting Help \u00b6 To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Install from container"},{"location":"data/stashcache/run-stash-origin-container/#running-stashcache-origin-in-a-container","text":"Note Currently, the StashCache Origin container only supports distribution of public data. If you would like to distribute private data requiring authentication, see the RPM installation guide . The OSG operates the StashCache data federation , which provides organizations with a method to distribute their data in a scalable manner to thousands of jobs without needing to pre-stage data across sites or operate their own scalable infrastructure. Stash Origins store copies of users' data. Each community (or experiment) needs to run one origin to export its data via the StashCache federation. This document outlines how to run such an origin in a Docker container.","title":"Running StashCache Origin in a Container"},{"location":"data/stashcache/run-stash-origin-container/#before-starting","text":"Before starting the installation process, consider the following points: Docker: For the purpose of this guide, the host must have a running docker service and you must have the ability to start containers (i.e., belong to the docker Unix group). Network ports: The Stash Origin listens for incoming HTTP/S and XRootD connections on ports 1094 and 1095 (by default). File Systems: Stash Origin needs a host partition to store user data. Registration: Before deploying an origin, you must registered the service in the OSG Topology","title":"Before Starting"},{"location":"data/stashcache/run-stash-origin-container/#configuring-the-origin","text":"In addition to the required configuration above (ports and file systems), you may also configure the behavior of your origin with the following variables using an environment variable file: Where the environment file on the docker host, /opt/origin/.env , has (at least) the following contents, replacing <YOUR_RESOURCE_NAME> with the resource name of your origin as registered in Topology and <FQDN> with the public DNS name that should be used to contact your origin: XC_RESOURCENAME=YOUR_SITE_NAME ORIGIN_FQDN=<FQDN>","title":"Configuring the Origin"},{"location":"data/stashcache/run-stash-origin-container/#populating-origin-data","text":"The Stash Cache data federation namespace is shared by multiple VOs so you must choose a namespace for your own VO's data. When running an origin container, your chosen namespace must be reflected in your host partition. For example, if your host partition is /srv/origin-puliic and the name of your VO is ASTRO, you should store the Astro VO's public data in /srv/origin-public/astro/ . Then, when starting container, you will mount /srv/origin-public/ into /xcache/namespace` in the container.","title":"Populating Origin Data"},{"location":"data/stashcache/run-stash-origin-container/#running-the-origin","text":"It is recommended to use a container orchestration service such as docker-compose or kubernetes whose details are beyond the scope of this document. The following sections provide examples for starting origin containers from the command-line as well as a more production-appropriate method using systemd. user@host $ docker run --rm --publish 1094 :1094 \\ --publish 1095:1095 \\ --volume <HOST PARTITION>:/xcache/namespace \\ --env-file=/opt/origin/.env \\ opensciencegrid/stash-origin:release Replacing <HOST PARTITION> with the host directory containing data that your origin should serve. See this section for details. Warning A container deployed this way will serve the entire contents of <HOST PARTITION> .","title":"Running the Origin"},{"location":"data/stashcache/run-stash-origin-container/#running-on-origin-container-with-systemd","text":"An example systemd service file for StashCache. This will require creating the environment file in the directory /opt/origin/.env . Note This example systemd file assumes <HOST PARTITION> is /srv/origin-public . Create the systemd service file /etc/systemd/system/docker.stash-origin.service as follows: [Unit] Description=Stash Origin Container After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 Restart=always ExecStartPre=-/usr/bin/docker stop %n ExecStartPre=-/usr/bin/docker rm %n ExecStartPre=/usr/bin/docker pull opensciencegrid/stash-origin:release ExecStart=/usr/bin/docker run --rm --name %n -p 1094:1094 -p 1095:1095 -v /srv/origin-public:/xcache/namespace --env-file /opt/origin/.env opensciencegrid/stash-origin:release [Install] WantedBy=multi-user.target Enable and start the service with: root@host $ systemctl enable docker.stash-origin root@host $ systemctl start docker.stash-origin Warning You must register the origin before considering it a production service.","title":"Running on origin container with systemd"},{"location":"data/stashcache/run-stash-origin-container/#validating-origin","text":"To validate the origin please follow the validating origin instructions .","title":"Validating Origin"},{"location":"data/stashcache/run-stash-origin-container/#getting-help","text":"To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Getting Help"},{"location":"data/stashcache/run-stashcache-container/","text":"Running StashCache Cache in a Container \u00b6 The OSG operates the StashCache data federation , which provides organizations with a method to distribute their data in a scalable manner to thousands of jobs without needing to pre-stage data across sites or operate their own scalable infrastructure. Stash Caches transfer data to clients such as jobs or users. A set of caches are operated across the OSG for the benefit of nearby sites; in addition, each site may run its own cache in order to reduce the amount of data transferred over the WAN. This document outlines how to run StashCache in a Docker container. Before Starting \u00b6 Before starting the installation process, consider the following points: Docker: For the purpose of this guide, the host must have a running docker service and you must have the ability to start containers (i.e., belong to the docker Unix group). Network ports: Stash Cache listens for incoming HTTP/S connections on port 8000 (by default) File Systems: Stash Cache needs host partitions to store user data. For improved performance and storage, we recommend multiple partitions for handling namespaces (HDD), data (HDDs), and metadata (SSDs). Configuring Stash Cache \u00b6 In addition to the required configuration above (ports and file systems), you may also configure the behavior of your cache with the following variables using an environment variable file: Where the environment file on the docker host, /opt/xcache/.env , has (at least) the following contents, replacing <YOUR_RESOURCE_NAME> with the name of your resource as registered in Topology and <FQDN> with the public DNS name that should be used to contact your cache: XC_RESOURCENAME=<YOUR_RESOURCE_NAME> CACHE_FQDN=<FQDN> Optional configuration \u00b6 Further behavior of the cache can be configured by setting the following in the environment variable file: XC_SPACE_HIGH_WM , XC_SPACE_LOW_WM : High-water and low-water marks for disk usage; when usage goes above the high-water mark, the cache will delete files until it hits the low-water mark. XC_PORT : TCP port that XCache listens on XC_RAMSIZE : Amount of memory to use for storing blocks before writting them to disk. (Use higher for slower disks). XC_BLOCKSIZE : Size of the blocks in the cache. XC_PREFETCH : Number of blocks to prefetch from a file at once. This controls how aggressive the cache is to request portions of a file. If you set it to 0 , prefetching will be disabled, but that is not recommended. Running a Cache \u00b6 StashCache cache containers may be run with either multiple mounted host partitions (recommended) or a single host partition. It is recommended to use a container orchestration service such as docker-compose or kubernetes whose details are beyond the scope of this document. The following sections provide examples for starting cache containers from the command-line as well as a more production-appropriate method using systemd. Multiple host partitions (recommended) \u00b6 For improved performance and storage, especially if your cache is serving over 10 TB of data, we recommend multiple partitions for handling namespaces (HDD, SSD, or NVME), data (HDDs), and metadata (SSDs or NVME). Note Under this configuration the <NAMESPACE PARTITION> is not used to store the files. Instead, the partition stores symlinks to the files in the metadata and data partitions. user@host $ docker run --rm --publish <HOST PORT>:8000 \\ --volume <NAMESPACE PARTITION>:/xcache/namespace \\ --volume <METADATA PARTITION 1>:/xcache/meta1 ... --volume <METADATA PARTITION N>:/xcache/metaN --volume <DATA PARTITION 1>:/xcache/data1 ... --volume <DATA PARTITION N>:/xcache/dataN --env-file=/opt/xcache/.env \\ opensciencegrid/stash-cache:release Warning For over 100 TB of assigned space we highly encourage to use this setup and mount <NAMESPACE PARTITION> in solid state disks or NVME. Single host partition \u00b6 For a simpler installation, you may use a single host partition mounted to /xcache/ : user@host $ docker run --rm --publish <HOST PORT>:8000 \\ --volume <HOST PARTITION>:/xcache \\ --env-file=/opt/xcache/.env \\ opensciencegrid/stash-cache:release Running Stash Cache on container with systemd \u00b6 An example systemd service file for Stash Cache. This will require creating the environment file in the directory /opt/xcache/.env . Note This example systemd file assumes <HOST PORT> is 8000 and <HOST PARTITION> is /srv/cache . Create the systemd service file /etc/systemd/system/docker.stash-cache.service as follows: [Unit] Description=Stash Cache Container After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 Restart=always ExecStartPre=-/usr/bin/docker stop %n ExecStartPre=-/usr/bin/docker rm %n ExecStartPre=/usr/bin/docker pull opensciencegrid/stash-cache:release ExecStart=/usr/bin/docker run --rm --name %n --publish 8000:8000 --volume /srv/cache:/xcache --env-file /opt/xcache/.env opensciencegrid/stash-cache:release [Install] WantedBy=multi-user.target Enable and start the service with: root@host $ systemctl enable docker.stash-cache root@host $ systemctl start docker.stash-cache Warning You must register the cache before considering it a production service. Network optimization \u00b6 For caches that are connected to NIC's over 40Gbps we recommend that you disable the virtualized network and \"bind\" the container to the host network: user@host $ docker run --rm \\ --network=\"host\" \\ --volume <HOST PARTITION>:/cache \\ --env-file=/opt/xcache/.env \\ opensciencegrid/stash-cache:release Memory optimization \u00b6 The cache uses the host's memory for two purposes: Caching files recently read from disk (via the kernel page cache). Buffering files recently received from the network before writing them to disk (to compensate for slow disks). An easy way to increase the performance of the cache is to assign it more memory. If you set a limit on the container's memory usage via the docker option --memory or Kubernetes resource limits, make sure it is at least twice the value of XC_RAMSIZE . Validating the Cache \u00b6 The cache server functions as a normal HTTP server and can interact with typical HTTP clients, such as curl . Here, <HOST PORT> is the port chosen in the docker run command, 8000 by default. user@host $ curl -O http://cache_host:<HOST PORT>/osgconnect/public/rynge/test.data curl may not correctly report a failure, so verify that the contents of the file are: hello world! Getting Help \u00b6 To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Install from container"},{"location":"data/stashcache/run-stashcache-container/#running-stashcache-cache-in-a-container","text":"The OSG operates the StashCache data federation , which provides organizations with a method to distribute their data in a scalable manner to thousands of jobs without needing to pre-stage data across sites or operate their own scalable infrastructure. Stash Caches transfer data to clients such as jobs or users. A set of caches are operated across the OSG for the benefit of nearby sites; in addition, each site may run its own cache in order to reduce the amount of data transferred over the WAN. This document outlines how to run StashCache in a Docker container.","title":"Running StashCache Cache in a Container"},{"location":"data/stashcache/run-stashcache-container/#before-starting","text":"Before starting the installation process, consider the following points: Docker: For the purpose of this guide, the host must have a running docker service and you must have the ability to start containers (i.e., belong to the docker Unix group). Network ports: Stash Cache listens for incoming HTTP/S connections on port 8000 (by default) File Systems: Stash Cache needs host partitions to store user data. For improved performance and storage, we recommend multiple partitions for handling namespaces (HDD), data (HDDs), and metadata (SSDs).","title":"Before Starting"},{"location":"data/stashcache/run-stashcache-container/#configuring-stash-cache","text":"In addition to the required configuration above (ports and file systems), you may also configure the behavior of your cache with the following variables using an environment variable file: Where the environment file on the docker host, /opt/xcache/.env , has (at least) the following contents, replacing <YOUR_RESOURCE_NAME> with the name of your resource as registered in Topology and <FQDN> with the public DNS name that should be used to contact your cache: XC_RESOURCENAME=<YOUR_RESOURCE_NAME> CACHE_FQDN=<FQDN>","title":"Configuring Stash Cache"},{"location":"data/stashcache/run-stashcache-container/#optional-configuration","text":"Further behavior of the cache can be configured by setting the following in the environment variable file: XC_SPACE_HIGH_WM , XC_SPACE_LOW_WM : High-water and low-water marks for disk usage; when usage goes above the high-water mark, the cache will delete files until it hits the low-water mark. XC_PORT : TCP port that XCache listens on XC_RAMSIZE : Amount of memory to use for storing blocks before writting them to disk. (Use higher for slower disks). XC_BLOCKSIZE : Size of the blocks in the cache. XC_PREFETCH : Number of blocks to prefetch from a file at once. This controls how aggressive the cache is to request portions of a file. If you set it to 0 , prefetching will be disabled, but that is not recommended.","title":"Optional configuration"},{"location":"data/stashcache/run-stashcache-container/#running-a-cache","text":"StashCache cache containers may be run with either multiple mounted host partitions (recommended) or a single host partition. It is recommended to use a container orchestration service such as docker-compose or kubernetes whose details are beyond the scope of this document. The following sections provide examples for starting cache containers from the command-line as well as a more production-appropriate method using systemd.","title":"Running a Cache"},{"location":"data/stashcache/run-stashcache-container/#multiple-host-partitions-recommended","text":"For improved performance and storage, especially if your cache is serving over 10 TB of data, we recommend multiple partitions for handling namespaces (HDD, SSD, or NVME), data (HDDs), and metadata (SSDs or NVME). Note Under this configuration the <NAMESPACE PARTITION> is not used to store the files. Instead, the partition stores symlinks to the files in the metadata and data partitions. user@host $ docker run --rm --publish <HOST PORT>:8000 \\ --volume <NAMESPACE PARTITION>:/xcache/namespace \\ --volume <METADATA PARTITION 1>:/xcache/meta1 ... --volume <METADATA PARTITION N>:/xcache/metaN --volume <DATA PARTITION 1>:/xcache/data1 ... --volume <DATA PARTITION N>:/xcache/dataN --env-file=/opt/xcache/.env \\ opensciencegrid/stash-cache:release Warning For over 100 TB of assigned space we highly encourage to use this setup and mount <NAMESPACE PARTITION> in solid state disks or NVME.","title":"Multiple host partitions (recommended)"},{"location":"data/stashcache/run-stashcache-container/#single-host-partition","text":"For a simpler installation, you may use a single host partition mounted to /xcache/ : user@host $ docker run --rm --publish <HOST PORT>:8000 \\ --volume <HOST PARTITION>:/xcache \\ --env-file=/opt/xcache/.env \\ opensciencegrid/stash-cache:release","title":"Single host partition"},{"location":"data/stashcache/run-stashcache-container/#running-stash-cache-on-container-with-systemd","text":"An example systemd service file for Stash Cache. This will require creating the environment file in the directory /opt/xcache/.env . Note This example systemd file assumes <HOST PORT> is 8000 and <HOST PARTITION> is /srv/cache . Create the systemd service file /etc/systemd/system/docker.stash-cache.service as follows: [Unit] Description=Stash Cache Container After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 Restart=always ExecStartPre=-/usr/bin/docker stop %n ExecStartPre=-/usr/bin/docker rm %n ExecStartPre=/usr/bin/docker pull opensciencegrid/stash-cache:release ExecStart=/usr/bin/docker run --rm --name %n --publish 8000:8000 --volume /srv/cache:/xcache --env-file /opt/xcache/.env opensciencegrid/stash-cache:release [Install] WantedBy=multi-user.target Enable and start the service with: root@host $ systemctl enable docker.stash-cache root@host $ systemctl start docker.stash-cache Warning You must register the cache before considering it a production service.","title":"Running Stash Cache on container with systemd"},{"location":"data/stashcache/run-stashcache-container/#network-optimization","text":"For caches that are connected to NIC's over 40Gbps we recommend that you disable the virtualized network and \"bind\" the container to the host network: user@host $ docker run --rm \\ --network=\"host\" \\ --volume <HOST PARTITION>:/cache \\ --env-file=/opt/xcache/.env \\ opensciencegrid/stash-cache:release","title":"Network optimization"},{"location":"data/stashcache/run-stashcache-container/#memory-optimization","text":"The cache uses the host's memory for two purposes: Caching files recently read from disk (via the kernel page cache). Buffering files recently received from the network before writing them to disk (to compensate for slow disks). An easy way to increase the performance of the cache is to assign it more memory. If you set a limit on the container's memory usage via the docker option --memory or Kubernetes resource limits, make sure it is at least twice the value of XC_RAMSIZE .","title":"Memory optimization"},{"location":"data/stashcache/run-stashcache-container/#validating-the-cache","text":"The cache server functions as a normal HTTP server and can interact with typical HTTP clients, such as curl . Here, <HOST PORT> is the port chosen in the docker run command, 8000 by default. user@host $ curl -O http://cache_host:<HOST PORT>/osgconnect/public/rynge/test.data curl may not correctly report a failure, so verify that the contents of the file are: hello world!","title":"Validating the Cache"},{"location":"data/stashcache/run-stashcache-container/#getting-help","text":"To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Getting Help"},{"location":"data/stashcache/vo-data/","text":"Getting VO Data into StashCache \u00b6 This document describes the steps required to manage a VO's role in the StashCache Data Federation including selecting a namespace, registration, and selecting which resources are allowed to host or cache your data. For general information about StashCache, see the overview document . Site admins should work together with VO managers in order to perform these steps. Definitions \u00b6 Namespace: a directory tree in the federation that is used to find VO data. Public data: data that can be read by anyone. Protected data: data that requires authentication to read. Requirements \u00b6 In order for a Virtual Organization to join the StashCache Federation, the VO must already be registered in OSG Topology. See the registration document . Choosing Namespaces \u00b6 The VO must pick one or more \"namespaces\" for their data. A namespace is a directory tree in the federation where VO data is found. Note Namespaces are global across the federation, so you must work with the StashCache Operations team to ensure that your VO's namespaces do not collide with those of another VO. Send an email to help@opensciencegrid.org with the following subject: \"Requesting StashCache namespaces for VO \" and put the desired namespaces in the body of the email. A namespace should be easy for your users to remember but not so generic that it collides with other VOs. We recommend using the lowercase version of your VO as the top-level directory. In addition, public data, if any, should be stored in a subdirectory named PUBLIC , and protected data, if any, should be stored in a subdirectory named PROTECTED . Putting this together, if your VO is named Astro , you should have: /astro/PUBLIC for public data /astro/PROTECTED for protected data Separating the public and protected data in separate directory trees is preferred for technical reasons. Registering Data Federation Information \u00b6 The VO must allow one or more StashCache origins to host their data. An origin will typically be hosted on a site owned by the VO. For information about setting up an origin, see the installation document . In order to declare your VO's role in the StashCache federation, you must add StashCache information to your VO's YAML file in the OSG Topology repository. For example, the full registration for the Astro VO may look something like the following: DataFederations : StashCache : Namespaces : /astro/PUBLIC : - PUBLIC AllowedCaches : - ANY AllowedOrigins : - CHTC_STASHCACHE_ORIGIN The sections are described below. Namespaces section \u00b6 In the namespaces section, you will declare one or more namespaces and, for each namespace, list who is allowed to access that namespace. The list will contain one or more of these: FQAN:<VOMS FQAN> allows someone using a proxy with the specified VOMS FQAN DN:<DN> allows someone using a proxy with that specific DN PUBLIC allows anyone; this is used for public data A complete declaration looks like: Namespaces : /astro/PUBLIC : - PUBLIC /astro/PROTECTED : - FQAN:/Astro - DN:/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Matyas Selmeci This declares two namespaces: /astro/PUBLIC for public data, and /astro/PROTECTED which can only be read by someone with the /Astro FQAN or by Matyas Selmeci. AllowedCaches list \u00b6 The VO must allow one or more StashCache Caches to cache their data. The more places a VO's data can be cached in, the bigger the data transfer benefit for the VO. The majority of caches across OSG will automatically cache all \"public\" VO data. Caching \"protected\" VO data will often be done on a site owned by the VO. For information about setting up a cache, see the installation document . AllowedCaches is a list of which caches are allowed to host copies of your data. There are two cases: If you only have public data, your AllowedCaches list can look like: AllowedCaches : - ANY This allows any cache to host a copy of your data. If you have some protected data, then AllowedCaches is a list of resources that are allowed to cache your data. A resource is an entry in a /topology/<FACILITY>/<SITE>/<RESOURCEGROUP>.yaml file, for example \"CHTC_STASHCACHE_CACHE\". The following requirements must be met for the resource: It must have an \"XRootD cache server\" service It must have an AllowedVOs list that includes either your VO, \"ANY\", or \"ANY_PUBLIC\" It must have a DN attribute with the DN of its host cert AllowedOrigins list \u00b6 AllowedOrigins is a list of which origins are allowed to host your data. This is a list of resources . A resource is an entry in a /topology/<FACILITY>/<SITE>/<RESOURCEGROUP>.yaml file, for example \"CHTC_STASHCACHE_ORIGIN\". The following requirements must be met for the resource: It must have an \"XRootD origin server\" service It must have an AllowedVOs list that includes either your VO or \"ANY\"","title":"Publishing VO data"},{"location":"data/stashcache/vo-data/#getting-vo-data-into-stashcache","text":"This document describes the steps required to manage a VO's role in the StashCache Data Federation including selecting a namespace, registration, and selecting which resources are allowed to host or cache your data. For general information about StashCache, see the overview document . Site admins should work together with VO managers in order to perform these steps.","title":"Getting VO Data into StashCache"},{"location":"data/stashcache/vo-data/#definitions","text":"Namespace: a directory tree in the federation that is used to find VO data. Public data: data that can be read by anyone. Protected data: data that requires authentication to read.","title":"Definitions"},{"location":"data/stashcache/vo-data/#requirements","text":"In order for a Virtual Organization to join the StashCache Federation, the VO must already be registered in OSG Topology. See the registration document .","title":"Requirements"},{"location":"data/stashcache/vo-data/#choosing-namespaces","text":"The VO must pick one or more \"namespaces\" for their data. A namespace is a directory tree in the federation where VO data is found. Note Namespaces are global across the federation, so you must work with the StashCache Operations team to ensure that your VO's namespaces do not collide with those of another VO. Send an email to help@opensciencegrid.org with the following subject: \"Requesting StashCache namespaces for VO \" and put the desired namespaces in the body of the email. A namespace should be easy for your users to remember but not so generic that it collides with other VOs. We recommend using the lowercase version of your VO as the top-level directory. In addition, public data, if any, should be stored in a subdirectory named PUBLIC , and protected data, if any, should be stored in a subdirectory named PROTECTED . Putting this together, if your VO is named Astro , you should have: /astro/PUBLIC for public data /astro/PROTECTED for protected data Separating the public and protected data in separate directory trees is preferred for technical reasons.","title":"Choosing Namespaces"},{"location":"data/stashcache/vo-data/#registering-data-federation-information","text":"The VO must allow one or more StashCache origins to host their data. An origin will typically be hosted on a site owned by the VO. For information about setting up an origin, see the installation document . In order to declare your VO's role in the StashCache federation, you must add StashCache information to your VO's YAML file in the OSG Topology repository. For example, the full registration for the Astro VO may look something like the following: DataFederations : StashCache : Namespaces : /astro/PUBLIC : - PUBLIC AllowedCaches : - ANY AllowedOrigins : - CHTC_STASHCACHE_ORIGIN The sections are described below.","title":"Registering Data Federation Information"},{"location":"data/stashcache/vo-data/#namespaces-section","text":"In the namespaces section, you will declare one or more namespaces and, for each namespace, list who is allowed to access that namespace. The list will contain one or more of these: FQAN:<VOMS FQAN> allows someone using a proxy with the specified VOMS FQAN DN:<DN> allows someone using a proxy with that specific DN PUBLIC allows anyone; this is used for public data A complete declaration looks like: Namespaces : /astro/PUBLIC : - PUBLIC /astro/PROTECTED : - FQAN:/Astro - DN:/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Matyas Selmeci This declares two namespaces: /astro/PUBLIC for public data, and /astro/PROTECTED which can only be read by someone with the /Astro FQAN or by Matyas Selmeci.","title":"Namespaces section"},{"location":"data/stashcache/vo-data/#allowedcaches-list","text":"The VO must allow one or more StashCache Caches to cache their data. The more places a VO's data can be cached in, the bigger the data transfer benefit for the VO. The majority of caches across OSG will automatically cache all \"public\" VO data. Caching \"protected\" VO data will often be done on a site owned by the VO. For information about setting up a cache, see the installation document . AllowedCaches is a list of which caches are allowed to host copies of your data. There are two cases: If you only have public data, your AllowedCaches list can look like: AllowedCaches : - ANY This allows any cache to host a copy of your data. If you have some protected data, then AllowedCaches is a list of resources that are allowed to cache your data. A resource is an entry in a /topology/<FACILITY>/<SITE>/<RESOURCEGROUP>.yaml file, for example \"CHTC_STASHCACHE_CACHE\". The following requirements must be met for the resource: It must have an \"XRootD cache server\" service It must have an AllowedVOs list that includes either your VO, \"ANY\", or \"ANY_PUBLIC\" It must have a DN attribute with the DN of its host cert","title":"AllowedCaches list"},{"location":"data/stashcache/vo-data/#allowedorigins-list","text":"AllowedOrigins is a list of which origins are allowed to host your data. This is a list of resources . A resource is an entry in a /topology/<FACILITY>/<SITE>/<RESOURCEGROUP>.yaml file, for example \"CHTC_STASHCACHE_ORIGIN\". The following requirements must be met for the resource: It must have an \"XRootD origin server\" service It must have an AllowedVOs list that includes either your VO or \"ANY\"","title":"AllowedOrigins list"},{"location":"data/xrootd/install-client/","text":"Using XRootD \u00b6 EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an issue with the globus-gsi-proxy-core package XRootD is a high performance data system widely used by several science VOs on OSG to store and to distribute data to jobs. It can be used to create a data store from distributed data nodes or to serve data to systems using a distributed caching architecture. Either mode of operation requires you to install the XRootD client software. This page provides instructions for accessing data on XRootD data systems using a variety of methods. As a user you have three different ways to interact with XRootD: Using the XRootD clients Using a XRootDFS FUSE mount to access a local XRootD data store Using LD_PRELOAD to use XRootD libraries with Unix tools We'll show how to install the XRootD client software and use all three mechanisms to access data. Note Only the client tools method should be used to access XRootD systems across a WAN link. Before Starting \u00b6 As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates If you are using the FUSE mount, you should also consider the following requirement: User IDs: If it does not exist already, you will need to create a xrootd user Using the XRootD client software \u00b6 Installing the XRootD Client \u00b6 If you are planning on interacting with XRootD using the XRootD client, then you'll need to install the XRootD client RPM. Installing the XRootD Client RPM \u00b6 The following steps will install the rpm on your system. Clean yum cache: root@client $ yum clean all --enablerepo = \\* Update software: root@client $ yum update This command will update all packages Install XRootD Client rpm: root@client $ yum install xrootd-client Using the XRootD Client \u00b6 Once the xrootd-client rpm is installed, you should be able to use the xrdcp command to copy files to and from XRootD systems and the local file system. For example: user@client $ echo \"This is a test\" >/tmp/test user@client $ xrdcp /tmp/test xroot://redirector.domain.org:1094//storage/path/test user@client $ xrdcp xroot://redirector.domain.org:1094//storage/path/test /tmp/test1 user@client $ diff /tmp/test1 /tmp/test For other operations, you'll need to use the xrdfs command. This command allows you to do file operations such as creating directories, removing directories, deleting files, and moving files on a XRootD system, provided you have the appropriate authorization. The xrdfs command can be used interactively by running xrdfs xroot://redirector.domain.org:1094/ . Alternatively, you can use it in batch mode by adding the xrdfs command after the xroot URI. For example: user@client $ echo \"This is a test\" >/tmp/test user@client $ xrdfs xroot://redirector.domain.org:1094/ mkdir /storage/path/test user@client $ xrdcp xroot://redirector.domain.org:1094//storage/path/test/test1 /tmp/test1 user@client $ xrdfs xroot://redirector.domain.org:1094/ ls /storage/path/test/test1 user@client $ xrdfs xroot://redirector.domain.org:1094/ rm /storage/path/test/test1 user@client $ xrdfs xroot://redirector.domain.org:1094/ rmdir /storage/path/test Note To access remote XRootD resources, you will may need to use a VOMS proxy in order to authenticate successfully. The XRootD client tools will automatically locate your proxy if you generate it using voms-proxy-init , otherwise you can set the X509_USER_PROXY environment variable to the location of the proxy XRootD should use. Validation \u00b6 Assuming that there is a file called test_file in your XRootD data store, you can do the following to validate your installation. Here we assume that there is a file on your XRootD system at /storage/path/test_file . user@client $ xrdcp xroot://redirector.yourdomain.org:1094//storage/path/test_file /tmp/test1 Using XRootDFS FUSE mount \u00b6 This section will explain how to install, setup, and interact with XRootD using a FUSE mount. This method of accessing XRootD only works when accessing a local XRootD system. Installing the XRootD FUSE RPM \u00b6 If you are planning on using a FUSE mount, you'll need to install the xrootd-fuse rpm by running the following commands: Clean yum cache: root@client $ yum clean all --enablerepo = \\* Update software: root@client $ yum update Install XRootD FUSE rpm: root@client $ yum install xrootd-fuse Configuring the FUSE Mount \u00b6 Once the appropriate rpms are installed, the FUSE setup will need further configuration. See this for instructions on updating your fstab file. Using the XRootDFS FUSE Mount \u00b6 The directory mounted using XRootDFS can be used as any other directory mounted on your file system. All the normal Unix commands should work out of the box. Try using cp , rm , mv , mkdir , rmdir . Assuming your mount is /mnt/xrootd : user@client $ echo \"This is a new test\" >/tmp/test user@client $ mkdir -p /mnt/xrootd/subdir/sub2 user@client $ cp /tmp/test /mnt/xrootd/subdir/sub2/test user@client $ cp /mnt/xrootd/subdir/sub2/test /mnt/xrootd/subdir/sub2/test1 user@client $ cp /mnt/xuserd/subdir/sub2/test1 /tmp/test1 user@client $ diff /tmp/test1 /tmp/test user@client $ rm -r /mnt/xrootd/subdir Validation \u00b6 Assuming your mount is /mnt/xrootd and that there is a file called test_file in your XRootD data store: user@client $ cp /mnt/xrootd/test_file /tmp/test1 Using LD_PRELOAD to access XRootD \u00b6 Installing XRootD Libraries For LD_PRELOAD \u00b6 In order to use LD_PRELOAD to access XRootD, you'll need to install the XRootD client libraries. The following steps will install them on your system: Clean yum cache: root@client $ yum clean all --enablerepo = \\* Update software: root@client $ yum update This command will update all packages Install XRootD Client rpm: root@client $ yum install xrootd-client Using LD_PRELOAD method \u00b6 In order to use the LD_PRELOAD method to access a XRootD data store, you'll need to change your environment to use the XRootD libraries in conjunction with the standard Unix binaries. This is done by setting the LD_PRELOAD environment variable. Once this is done, the standard unix commands like mkdir , rm , cp , etc. will work with xroot URIs. For example: user@client $ export LD_PRELOAD = /usr/lib64/libXrdPosixPreload.so user@client $ echo \"This is a new test\" >/tmp/test user@client $ mkdir xroot://redirector.yourdomain.org:1094//storage/path/subdir user@client $ cp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/subdir/test user@client $ cp xuser://redirector.yourdomain.org:1094//storage/path/subdir/test /tmp/test1 user@client $ diff /tmp/test1 /tmp/test user@client $ rm xroot://redirector.yourdomain.org:1094//storage/path/subdir/test user@client $ rmdir xroot://redirector.yourdomain.org:1094//storage/path/subdir Validation \u00b6 Assuming that there is a file called test_file in your XRootD data store, the following steps will validate your installation: user@client $ export LD_PRELOAD = /usr/lib64/libXrdPosixPreload.so user@client $ cp xroot://redirector.yourdomain.org:1094//storage/path/test_file /tmp/test1 How to get Help? \u00b6 If you cannot resolve the problem, the best way to get help is by contacting osg-software@opensciencegrid.org . For a full set of help options, see Help Procedure .","title":"Using XRootD"},{"location":"data/xrootd/install-client/#using-xrootd","text":"EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an issue with the globus-gsi-proxy-core package XRootD is a high performance data system widely used by several science VOs on OSG to store and to distribute data to jobs. It can be used to create a data store from distributed data nodes or to serve data to systems using a distributed caching architecture. Either mode of operation requires you to install the XRootD client software. This page provides instructions for accessing data on XRootD data systems using a variety of methods. As a user you have three different ways to interact with XRootD: Using the XRootD clients Using a XRootDFS FUSE mount to access a local XRootD data store Using LD_PRELOAD to use XRootD libraries with Unix tools We'll show how to install the XRootD client software and use all three mechanisms to access data. Note Only the client tools method should be used to access XRootD systems across a WAN link.","title":"Using XRootD"},{"location":"data/xrootd/install-client/#before-starting","text":"As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates If you are using the FUSE mount, you should also consider the following requirement: User IDs: If it does not exist already, you will need to create a xrootd user","title":"Before Starting"},{"location":"data/xrootd/install-client/#using-the-xrootd-client-software","text":"","title":"Using the XRootD client software"},{"location":"data/xrootd/install-client/#installing-the-xrootd-client","text":"If you are planning on interacting with XRootD using the XRootD client, then you'll need to install the XRootD client RPM.","title":"Installing the XRootD Client"},{"location":"data/xrootd/install-client/#installing-the-xrootd-client-rpm","text":"The following steps will install the rpm on your system. Clean yum cache: root@client $ yum clean all --enablerepo = \\* Update software: root@client $ yum update This command will update all packages Install XRootD Client rpm: root@client $ yum install xrootd-client","title":"Installing the XRootD Client RPM"},{"location":"data/xrootd/install-client/#using-the-xrootd-client","text":"Once the xrootd-client rpm is installed, you should be able to use the xrdcp command to copy files to and from XRootD systems and the local file system. For example: user@client $ echo \"This is a test\" >/tmp/test user@client $ xrdcp /tmp/test xroot://redirector.domain.org:1094//storage/path/test user@client $ xrdcp xroot://redirector.domain.org:1094//storage/path/test /tmp/test1 user@client $ diff /tmp/test1 /tmp/test For other operations, you'll need to use the xrdfs command. This command allows you to do file operations such as creating directories, removing directories, deleting files, and moving files on a XRootD system, provided you have the appropriate authorization. The xrdfs command can be used interactively by running xrdfs xroot://redirector.domain.org:1094/ . Alternatively, you can use it in batch mode by adding the xrdfs command after the xroot URI. For example: user@client $ echo \"This is a test\" >/tmp/test user@client $ xrdfs xroot://redirector.domain.org:1094/ mkdir /storage/path/test user@client $ xrdcp xroot://redirector.domain.org:1094//storage/path/test/test1 /tmp/test1 user@client $ xrdfs xroot://redirector.domain.org:1094/ ls /storage/path/test/test1 user@client $ xrdfs xroot://redirector.domain.org:1094/ rm /storage/path/test/test1 user@client $ xrdfs xroot://redirector.domain.org:1094/ rmdir /storage/path/test Note To access remote XRootD resources, you will may need to use a VOMS proxy in order to authenticate successfully. The XRootD client tools will automatically locate your proxy if you generate it using voms-proxy-init , otherwise you can set the X509_USER_PROXY environment variable to the location of the proxy XRootD should use.","title":"Using the XRootD Client"},{"location":"data/xrootd/install-client/#validation","text":"Assuming that there is a file called test_file in your XRootD data store, you can do the following to validate your installation. Here we assume that there is a file on your XRootD system at /storage/path/test_file . user@client $ xrdcp xroot://redirector.yourdomain.org:1094//storage/path/test_file /tmp/test1","title":"Validation"},{"location":"data/xrootd/install-client/#using-xrootdfs-fuse-mount","text":"This section will explain how to install, setup, and interact with XRootD using a FUSE mount. This method of accessing XRootD only works when accessing a local XRootD system.","title":"Using XRootDFS FUSE mount"},{"location":"data/xrootd/install-client/#installing-the-xrootd-fuse-rpm","text":"If you are planning on using a FUSE mount, you'll need to install the xrootd-fuse rpm by running the following commands: Clean yum cache: root@client $ yum clean all --enablerepo = \\* Update software: root@client $ yum update Install XRootD FUSE rpm: root@client $ yum install xrootd-fuse","title":"Installing the XRootD FUSE RPM"},{"location":"data/xrootd/install-client/#configuring-the-fuse-mount","text":"Once the appropriate rpms are installed, the FUSE setup will need further configuration. See this for instructions on updating your fstab file.","title":"Configuring the FUSE Mount"},{"location":"data/xrootd/install-client/#using-the-xrootdfs-fuse-mount","text":"The directory mounted using XRootDFS can be used as any other directory mounted on your file system. All the normal Unix commands should work out of the box. Try using cp , rm , mv , mkdir , rmdir . Assuming your mount is /mnt/xrootd : user@client $ echo \"This is a new test\" >/tmp/test user@client $ mkdir -p /mnt/xrootd/subdir/sub2 user@client $ cp /tmp/test /mnt/xrootd/subdir/sub2/test user@client $ cp /mnt/xrootd/subdir/sub2/test /mnt/xrootd/subdir/sub2/test1 user@client $ cp /mnt/xuserd/subdir/sub2/test1 /tmp/test1 user@client $ diff /tmp/test1 /tmp/test user@client $ rm -r /mnt/xrootd/subdir","title":"Using the XRootDFS FUSE Mount"},{"location":"data/xrootd/install-client/#validation_1","text":"Assuming your mount is /mnt/xrootd and that there is a file called test_file in your XRootD data store: user@client $ cp /mnt/xrootd/test_file /tmp/test1","title":"Validation"},{"location":"data/xrootd/install-client/#using-ld_preload-to-access-xrootd","text":"","title":"Using LD_PRELOAD to access XRootD"},{"location":"data/xrootd/install-client/#installing-xrootd-libraries-for-ld_preload","text":"In order to use LD_PRELOAD to access XRootD, you'll need to install the XRootD client libraries. The following steps will install them on your system: Clean yum cache: root@client $ yum clean all --enablerepo = \\* Update software: root@client $ yum update This command will update all packages Install XRootD Client rpm: root@client $ yum install xrootd-client","title":"Installing XRootD Libraries For LD_PRELOAD"},{"location":"data/xrootd/install-client/#using-ld_preload-method","text":"In order to use the LD_PRELOAD method to access a XRootD data store, you'll need to change your environment to use the XRootD libraries in conjunction with the standard Unix binaries. This is done by setting the LD_PRELOAD environment variable. Once this is done, the standard unix commands like mkdir , rm , cp , etc. will work with xroot URIs. For example: user@client $ export LD_PRELOAD = /usr/lib64/libXrdPosixPreload.so user@client $ echo \"This is a new test\" >/tmp/test user@client $ mkdir xroot://redirector.yourdomain.org:1094//storage/path/subdir user@client $ cp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/subdir/test user@client $ cp xuser://redirector.yourdomain.org:1094//storage/path/subdir/test /tmp/test1 user@client $ diff /tmp/test1 /tmp/test user@client $ rm xroot://redirector.yourdomain.org:1094//storage/path/subdir/test user@client $ rmdir xroot://redirector.yourdomain.org:1094//storage/path/subdir","title":"Using LD_PRELOAD method"},{"location":"data/xrootd/install-client/#validation_2","text":"Assuming that there is a file called test_file in your XRootD data store, the following steps will validate your installation: user@client $ export LD_PRELOAD = /usr/lib64/libXrdPosixPreload.so user@client $ cp xroot://redirector.yourdomain.org:1094//storage/path/test_file /tmp/test1","title":"Validation"},{"location":"data/xrootd/install-client/#how-to-get-help","text":"If you cannot resolve the problem, the best way to get help is by contacting osg-software@opensciencegrid.org . For a full set of help options, see Help Procedure .","title":"How to get Help?"},{"location":"data/xrootd/install-cms-xcache/","text":"Installing the CMS XCache \u00b6 This document describes how to install a CMS XCache. This service allows a site or regional network to cache data frequently used by the CMS experiment , reducing data transfer over the wide-area network and decreasing access latency. The are two types of installations described in this document: single or multinode cache. The difference might be based on the total disk that your cache needs. Before Starting \u00b6 Before starting the installation process, consider the following requirements: Operating system: A RHEL 7 or compatible operating systems. User IDs: If they do not exist already, the installation will create the Linux user IDs xrootd Host certificate: Required for client authentication and authentication with CMS VOMS Server See our documentation for instructions on how to request and install host certificates. Network ports: The cache service requires the following ports open: Inbound TCP port 1094 for file access via the XRootD protocol Outbound UDP port 9930 for reporting to xrd-report.osgstorage.org and xrd-mon.osgstorage.org for monitoring Hardware requirements: We recommend that a cache has at least 10Gbps connectivity, 100TB of disk space for the whole cache (can be divided among several caches), and 8GB of RAM. As with all OSG software installations, there are some one-time steps to prepare in advance: Obtain root access to the host Prepare the required Yum repositories Install CA certificates Installing the Cache \u00b6 The CMS XCache ROM software consists of an XRootD server with special configuration and supporting services. To simplify installation, OSG provides convenience RPMs that install all required packages with a single command: root@host # yum install cms-xcache Configuring the Cache \u00b6 First, you must create a \"cache directory\", which will be used to store downloaded files. By default this is /mnt/stash . We recommend using a separate file system for the cache directory, with at least 1 TB of storage available. Note The cache directory must be writable by the xrootd:xrootd user and group. The cms-xcache package provides default configuration files in /etc/xrootd/xrootd-cms-xcache.cfg and /etc/xrootd/config.d/ . Administrators may provide additional configuration by placing files in /etc/xrootd/config.d/1*.cfg (for files that need to be processed BEFORE the OSG configuration) or /etc/xrootd/config.d/9*.cfg (for files that need to be processed AFTER the OSG configuration). You must configure every variable in /etc/xrootd/config.d/10-common-site-local.cfg . The mandatory variables to configure are: set rootdir = /mnt/stash : the mounted filesystem path to export. This document refers to this as /mnt/stash . set resourcename = YOUR_RESOURCE_NAME : the resource name registered with the OSG for example (\"T2_US_UCSD\") Note XRootD can manage a set of independent disk for the cache. So you can modify file 90-cms-xcache-disks.cfg and add the disks there then rootdir just becomes a place to hold symlinks. Ensure the xrootd service has a certificate \u00b6 The service will need a certificate for reporting and to authenticate to CMS AAA. The easiest solution for this is to use your host certificate and key as follows: Copy the host certificate to /etc/grid-security/xrd/xrd{cert,key}.pem Set the owner of the directory and contents /etc/grid-security/xrd/ to xrootd:xrootd : root@host # chown -R xrootd:xrootd /etc/grid-security/xrd/ Note You must repeat the above steps whenever you renew your host certificate. If you automate certificate renewal, you should automate copying as well. For example, if you are using Certbot for Let's Encrypt, you should write a \"deploy hook\" as documented here . Note You must also register this certificate with the CMS VOMS (https://voms2.cern.ch:8443/voms/cms/) Configuring Optional Features \u00b6 Adjust disk utilization \u00b6 To adjust the disk utilization of your cache, create or edit a file named /etc/xrootd/config.d/90-local.cfg and set the values of pfc.diskusage . pfc.diskusage 0.90 0.95 The two values correspond to the low and high usage water marks, respectively. When usage goes above the high water mark, the XRootD service will delete cached files until usage goes below the low water mark. Modify the storage access settings at a site \u00b6 In order for CMSSW jobs to use the cache at your site you need to modify the storage.xml and create the following rules # Portions of /store in xcache <lfn-to-pfn protocol=\"direct\" destination-match=\".*\" path-match=\"/+store/(data/.*/.*/NANOAOD/.*)\" result=\"root://yourlocalcache:1094//store/$1\"/> <lfn-to-pfn protocol=\"direct\" destination-match=\".*\" path-match=\"/+store/(mc/.*/.*/NANOAODSIM/.*)\" result=\"root://yourlocalcache:1094//store/$1\"/> Note If you are installing a multinode cache then instead of yourlocalcache:1094 url should be changed for yourcacheredirector:2040 Enable remote debugging \u00b6 XRootD provides remote debugging via a read-only file system named digFS. This feature is disabled by default, but you may enable it if you need help troubleshooting your server. To enable remote debugging, edit /etc/xrootd/digauth.cfg and specify the authorizations for reading digFS. An example of authorizations: all allow gsi g=/glow h=*.cs.wisc.edu This gives access to the config file, log files, core files, and process information to anyone from *.cs.wisc.edu in the /glow VOMS group. See the XRootD manual for the full syntax. Remote debugging should only be enabled for as long as you need assistance. As soon as your issue has been resolved, revert any changes you have made to /etc/xrootd/digauth.cfg . Installing a Multinode Cache (optional) \u00b6 Some sites would like to have a single logical cache composed of several nodes as shown below: This can be achieved by following the next steps Install an XCache redirector \u00b6 This can be a simple lightweight virtual machine and will be the single point of contact from jobs to the caches. Install the redirector package root@host # yum install xcache-redirector Create file named /etc/xrootd/config.d/04-local-redir.cfg with contents: all.manager yourlocalredir:2041 You must configure every variable in /etc/xrootd/config.d/10-common-site-local.cfg . The mandatory variables to configure are: set rootdir = /mnt/stash : the mounted filesystem path to export. This document refers to this as /mnt/stash . set resourcename = YOUR_RESOURCE_NAME : the resource name registered with the OSG for example (\"T2_US_UCSD\") Start and enable the cmsd and xrootd proccess: Software Service name Notes XRootD cmsd@xcache-redir.service The cmsd daemon that interact with the different xrootd servers XRootD xrootd@xcache-redir.service The xrootd daemon which performs authenticated data transfers Configuring each of your cache nodes \u00b6 Create a config file in the nodes where you installed your caches /etc/xrootd/config.d/94-xrootd-manager.cfg with the following contents: all.manager yourlocalredir:2041 Start and enable the cmsd service: Software Service name Notes XRootD cmsd@cms-xcache.service The xrootd daemon which performs authenticated data transfers Managing CMS XCache and associated services \u00b6 These services must be managed by systemctl and may start additional services as dependencies. As a reminder, here are common service commands (all run as root ) for EL7: To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> CMS XCache services \u00b6 Software Service name Notes XRootD xrootd@cms-xcache.service The XRootD daemon, which performs the data transfers XRootD (Optional) cmsd@cms-xcache.service The cmsd daemon that interact with the different xrootd servers Fetch CRL fetch-crl-boot and fetch-crl-cron Required to authenticate monitoring services. See CA documentation for more info xrootd-renew-proxy.service Renew a proxy for downloads to the cache xrootd-renew-proxy.timer Trigger daily proxy renewal XCache redirector services (Optional) \u00b6 In the node where the cache redirector is installed these are the list of services: Software Service name Notes XRootD (Optional) xrootd@xcache-redir.service The xrootd daemon which performs authenticated data transfers XRootD (Optional) cmsd@xcache-redir.service The xrootd daemon which performs authenticated data transfers Validating the Cache \u00b6 The cache server functions as a normal CMS XRootD server so first verify it with a personal CMS X.509 proxy: === VO cms extension information === VO : cms subject : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=efajardo/CN=722781/CN=Edgar Fajardo Hernandez issuer : /DC=ch/DC=cern/OU=computers/CN=lcg-voms2.cern.ch attribute : /cms/Role=NULL/Capability=NULL attribute : /cms/uscms/Role=NULL/Capability=NULL timeleft : 71:59:46 uri : lcg-voms2.cern.ch:15002 Then test using xrdcp directly in your cache: user@host $ xrdcp -vf -d 1 root://cache_host:1094//store/data/Run2017B/SingleElectron/MINIAOD/31Mar2018-v1/60000/9E0F8458-EA37-E811-93F1-008CFAC919F0.root /dev/null Getting Help \u00b6 To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Install CMS XCache"},{"location":"data/xrootd/install-cms-xcache/#installing-the-cms-xcache","text":"This document describes how to install a CMS XCache. This service allows a site or regional network to cache data frequently used by the CMS experiment , reducing data transfer over the wide-area network and decreasing access latency. The are two types of installations described in this document: single or multinode cache. The difference might be based on the total disk that your cache needs.","title":"Installing the CMS XCache"},{"location":"data/xrootd/install-cms-xcache/#before-starting","text":"Before starting the installation process, consider the following requirements: Operating system: A RHEL 7 or compatible operating systems. User IDs: If they do not exist already, the installation will create the Linux user IDs xrootd Host certificate: Required for client authentication and authentication with CMS VOMS Server See our documentation for instructions on how to request and install host certificates. Network ports: The cache service requires the following ports open: Inbound TCP port 1094 for file access via the XRootD protocol Outbound UDP port 9930 for reporting to xrd-report.osgstorage.org and xrd-mon.osgstorage.org for monitoring Hardware requirements: We recommend that a cache has at least 10Gbps connectivity, 100TB of disk space for the whole cache (can be divided among several caches), and 8GB of RAM. As with all OSG software installations, there are some one-time steps to prepare in advance: Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"data/xrootd/install-cms-xcache/#installing-the-cache","text":"The CMS XCache ROM software consists of an XRootD server with special configuration and supporting services. To simplify installation, OSG provides convenience RPMs that install all required packages with a single command: root@host # yum install cms-xcache","title":"Installing the Cache"},{"location":"data/xrootd/install-cms-xcache/#configuring-the-cache","text":"First, you must create a \"cache directory\", which will be used to store downloaded files. By default this is /mnt/stash . We recommend using a separate file system for the cache directory, with at least 1 TB of storage available. Note The cache directory must be writable by the xrootd:xrootd user and group. The cms-xcache package provides default configuration files in /etc/xrootd/xrootd-cms-xcache.cfg and /etc/xrootd/config.d/ . Administrators may provide additional configuration by placing files in /etc/xrootd/config.d/1*.cfg (for files that need to be processed BEFORE the OSG configuration) or /etc/xrootd/config.d/9*.cfg (for files that need to be processed AFTER the OSG configuration). You must configure every variable in /etc/xrootd/config.d/10-common-site-local.cfg . The mandatory variables to configure are: set rootdir = /mnt/stash : the mounted filesystem path to export. This document refers to this as /mnt/stash . set resourcename = YOUR_RESOURCE_NAME : the resource name registered with the OSG for example (\"T2_US_UCSD\") Note XRootD can manage a set of independent disk for the cache. So you can modify file 90-cms-xcache-disks.cfg and add the disks there then rootdir just becomes a place to hold symlinks.","title":"Configuring the Cache"},{"location":"data/xrootd/install-cms-xcache/#ensure-the-xrootd-service-has-a-certificate","text":"The service will need a certificate for reporting and to authenticate to CMS AAA. The easiest solution for this is to use your host certificate and key as follows: Copy the host certificate to /etc/grid-security/xrd/xrd{cert,key}.pem Set the owner of the directory and contents /etc/grid-security/xrd/ to xrootd:xrootd : root@host # chown -R xrootd:xrootd /etc/grid-security/xrd/ Note You must repeat the above steps whenever you renew your host certificate. If you automate certificate renewal, you should automate copying as well. For example, if you are using Certbot for Let's Encrypt, you should write a \"deploy hook\" as documented here . Note You must also register this certificate with the CMS VOMS (https://voms2.cern.ch:8443/voms/cms/)","title":"Ensure the xrootd service has a certificate"},{"location":"data/xrootd/install-cms-xcache/#configuring-optional-features","text":"","title":"Configuring Optional Features"},{"location":"data/xrootd/install-cms-xcache/#adjust-disk-utilization","text":"To adjust the disk utilization of your cache, create or edit a file named /etc/xrootd/config.d/90-local.cfg and set the values of pfc.diskusage . pfc.diskusage 0.90 0.95 The two values correspond to the low and high usage water marks, respectively. When usage goes above the high water mark, the XRootD service will delete cached files until usage goes below the low water mark.","title":"Adjust disk utilization"},{"location":"data/xrootd/install-cms-xcache/#modify-the-storage-access-settings-at-a-site","text":"In order for CMSSW jobs to use the cache at your site you need to modify the storage.xml and create the following rules # Portions of /store in xcache <lfn-to-pfn protocol=\"direct\" destination-match=\".*\" path-match=\"/+store/(data/.*/.*/NANOAOD/.*)\" result=\"root://yourlocalcache:1094//store/$1\"/> <lfn-to-pfn protocol=\"direct\" destination-match=\".*\" path-match=\"/+store/(mc/.*/.*/NANOAODSIM/.*)\" result=\"root://yourlocalcache:1094//store/$1\"/> Note If you are installing a multinode cache then instead of yourlocalcache:1094 url should be changed for yourcacheredirector:2040","title":"Modify the storage access settings at a site"},{"location":"data/xrootd/install-cms-xcache/#enable-remote-debugging","text":"XRootD provides remote debugging via a read-only file system named digFS. This feature is disabled by default, but you may enable it if you need help troubleshooting your server. To enable remote debugging, edit /etc/xrootd/digauth.cfg and specify the authorizations for reading digFS. An example of authorizations: all allow gsi g=/glow h=*.cs.wisc.edu This gives access to the config file, log files, core files, and process information to anyone from *.cs.wisc.edu in the /glow VOMS group. See the XRootD manual for the full syntax. Remote debugging should only be enabled for as long as you need assistance. As soon as your issue has been resolved, revert any changes you have made to /etc/xrootd/digauth.cfg .","title":"Enable remote debugging"},{"location":"data/xrootd/install-cms-xcache/#installing-a-multinode-cache-optional","text":"Some sites would like to have a single logical cache composed of several nodes as shown below: This can be achieved by following the next steps","title":"Installing a Multinode Cache (optional)"},{"location":"data/xrootd/install-cms-xcache/#install-an-xcache-redirector","text":"This can be a simple lightweight virtual machine and will be the single point of contact from jobs to the caches. Install the redirector package root@host # yum install xcache-redirector Create file named /etc/xrootd/config.d/04-local-redir.cfg with contents: all.manager yourlocalredir:2041 You must configure every variable in /etc/xrootd/config.d/10-common-site-local.cfg . The mandatory variables to configure are: set rootdir = /mnt/stash : the mounted filesystem path to export. This document refers to this as /mnt/stash . set resourcename = YOUR_RESOURCE_NAME : the resource name registered with the OSG for example (\"T2_US_UCSD\") Start and enable the cmsd and xrootd proccess: Software Service name Notes XRootD cmsd@xcache-redir.service The cmsd daemon that interact with the different xrootd servers XRootD xrootd@xcache-redir.service The xrootd daemon which performs authenticated data transfers","title":"Install an XCache redirector"},{"location":"data/xrootd/install-cms-xcache/#configuring-each-of-your-cache-nodes","text":"Create a config file in the nodes where you installed your caches /etc/xrootd/config.d/94-xrootd-manager.cfg with the following contents: all.manager yourlocalredir:2041 Start and enable the cmsd service: Software Service name Notes XRootD cmsd@cms-xcache.service The xrootd daemon which performs authenticated data transfers","title":"Configuring each of your cache nodes"},{"location":"data/xrootd/install-cms-xcache/#managing-cms-xcache-and-associated-services","text":"These services must be managed by systemctl and may start additional services as dependencies. As a reminder, here are common service commands (all run as root ) for EL7: To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME>","title":"Managing CMS XCache and associated services"},{"location":"data/xrootd/install-cms-xcache/#cms-xcache-services","text":"Software Service name Notes XRootD xrootd@cms-xcache.service The XRootD daemon, which performs the data transfers XRootD (Optional) cmsd@cms-xcache.service The cmsd daemon that interact with the different xrootd servers Fetch CRL fetch-crl-boot and fetch-crl-cron Required to authenticate monitoring services. See CA documentation for more info xrootd-renew-proxy.service Renew a proxy for downloads to the cache xrootd-renew-proxy.timer Trigger daily proxy renewal","title":"CMS XCache services"},{"location":"data/xrootd/install-cms-xcache/#xcache-redirector-services-optional","text":"In the node where the cache redirector is installed these are the list of services: Software Service name Notes XRootD (Optional) xrootd@xcache-redir.service The xrootd daemon which performs authenticated data transfers XRootD (Optional) cmsd@xcache-redir.service The xrootd daemon which performs authenticated data transfers","title":"XCache redirector services (Optional)"},{"location":"data/xrootd/install-cms-xcache/#validating-the-cache","text":"The cache server functions as a normal CMS XRootD server so first verify it with a personal CMS X.509 proxy: === VO cms extension information === VO : cms subject : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=efajardo/CN=722781/CN=Edgar Fajardo Hernandez issuer : /DC=ch/DC=cern/OU=computers/CN=lcg-voms2.cern.ch attribute : /cms/Role=NULL/Capability=NULL attribute : /cms/uscms/Role=NULL/Capability=NULL timeleft : 71:59:46 uri : lcg-voms2.cern.ch:15002 Then test using xrdcp directly in your cache: user@host $ xrdcp -vf -d 1 root://cache_host:1094//store/data/Run2017B/SingleElectron/MINIAOD/31Mar2018-v1/60000/9E0F8458-EA37-E811-93F1-008CFAC919F0.root /dev/null","title":"Validating the Cache"},{"location":"data/xrootd/install-cms-xcache/#getting-help","text":"To get assistance, please use the this page or contact help@opensciencegrid.org directly.","title":"Getting Help"},{"location":"data/xrootd/install-standalone/","text":"Install XRootD Standalone \u00b6 EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an issue with the globus-gsi-proxy-core package XRootD is a hierarchical storage system that can be used in a variety of ways to access data, typically distributed among actual storage resources. In this document we focus on using XRootD as a simple layer exporting an underlying storage system (e.g., HDFS ) to the outside world. Before Starting \u00b6 Before starting the installation process, consider the following points: User IDs: If it does not exist already, the installation will create the Linux user ID xrootd Service certificate: The XRootD service uses a host certificate and key pair at /etc/grid-security/xrd/xrdcert.pem and /etc/grid-security/xrd/xrdkey.pem Networking: The XRootD service uses port 1094 by default As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates Installing XRootD \u00b6 To install the XRootD Standalone server, run the following Yum command: root@xrootd-standalone # yum install osg-xrootd-standalone Configuring XRootD \u00b6 To configure XRootD as a standalone server, you will modify /etc/xrootd/xrootd-standalone.cfg and the config files under /etc/xrootd/config.d/ as follows: Configure a rootdir in /etc/xrootd/config.d/10-common-site-local.cfg , to point to the top of the directory hierarchy which you wish to serve via XRootD. set rootdir = <DIRECTORY> Warning Do not set rootdir to / . This might result in serving private information. If you want to limit the sub-directories to serve under your configured rootdir , comment out the all.export / directive in /etc/xrootd/config.d/90-osg-standalone-paths.cfg , and add an all.export directive for each directory under rootdir that you wish to serve via XRootD. This is useful if you have a mixture of files under your rootdir , for example from multiple users, but only want to expose a subset of them to the world. For example, to serve the contents of /data/store and /data/public (with rootdir configured to /data ): all.export /store/ all.export /public/ If you want to serve everything under your configured rootdir , you don't have to change anything. Note The directories specified this way are writable by default. Access controls should be managed via authorization configuration . In /etc/xrootd/config.d/10-common-site-local.cfg , add a line to set the resourcename variable to the resource name of your XRootD service. For example, the XRootD service registered at the University of Florida site should set the following configuration: set resourcename = UFlorida-XRD Note CMS sites should follow CMS policy for resourcename Configuring authorization \u00b6 To configure XRootD authorization please follow the documentation here . Optional configuration \u00b6 The following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to the section on using XRootD . Enabling Hadoop support (EL 7 Only) \u00b6 Hadoop File System (HDFS) based sites should utilize the xrootd-hdfs plugin to allow XRootD to access their storage: Install the XRootD HDFS plugin package: root@host # yum install xrootd-hdfs Add the following configuration to /etc/xrootd/xrootd-clustered.cfg : ofs.osslib /usr/lib64/libXrdHdfs.so For more information, see the HDFS installation documents . Enabling multi-user support \u00b6 By default XRootD servers write files on the storage system aa the Unix user xrootd instead of the authenticated user. The xrootd-multiuser plugins changes this behaviour: Install the XRootD multi-user plugin: root@host # yum install xrootd-multiuser Start the XRootD process in privileged mode: root@host # systemctl xrootd-privileged@standalone Bug The multiuser support is incompatible with checksums. Note This is not necessary when XRootD is used for read-only access Enabling CMS TFC support (CMS sites only) \u00b6 For CMS users, there is a package available to integrate rule-based name lookup using a storage.xml file. If you are not setting up a CMS site, you can skip this section. yum install --enablerepo=osg-contrib xrootd-cmstfc You will need to add your storage.xml to /etc/xrootd/storage.xml and then add the following line to your XRootD configuration: # Integrate with CMS TFC, placed in /etc/xrootd/storage.xml oss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml?protocol=hadoop Add the orange text only if you are running hadoop (see below). See the CMS TWiki for more information: https://twiki.cern.ch/twiki/bin/view/Main/XrootdTfcChanges https://twiki.cern.ch/twiki/bin/view/Main/HdfsXrootdInstall Using XRootD \u00b6 In addition to the XRootD service itself, there are a number of supporting services in your installation. The specific services are: Software Service Name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation for more info XRootD xrootd@standalone XRootD Multiuser xrootd-privileged@standalone See XRootD multiuser for more info Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To \u2026 Run the command\u2026 Start a service systemctl start SERVICE-NAME Stop a service systemctl stop SERVICE-NAME Enable a service to start during boot systemctl enable SERVICE-NAME Disable a service from starting during boot systemctl disable SERVICE-NAME Validating XRootD \u00b6 To validate an XRootD installation, perform the following verification steps: Note If you have configured authentication/authorization for XRootD, be sure you have given yourself the necessary permissions to run these tests. For example, if you are using a grid proxy, make sure your DN is mapped to a user in /etc/grid-security/grid-mapfile , and make sure you have a valid proxy on your local machine. Also, ensure that the Authfile on the XRootD server gives write access to the Unix user you will get mapped to. Verify file transfer over the XRootD protocol using XRootD client tools: Install the client tools: root@xrootd-standalone # yum install xrootd-client Copy a file to a directory for which you have write access: root@xrootd-standalone # xrdcp /bin/sh root://localhost:1094//tmp/first_test [xrootd] Total 0.76 MB [====================] 100.00 % [inf MB/s] Verify that the file has been copied over: root@xrootd-standalone # ls -l /tmp/first_test -rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test Verify file transfer over HTTP using GFAL2 client tools: Install the GFAL2 client tools: root@xrootd-standalone # yum install gfal2-util gfal2-plugin-http Copy a file to a directory for which you have write access: root@xrootd-standalone # gfal-copy /bin/sh http://localhost:1094//tmp/first_test Verify that the file has been copied over: root@xrootd-standalone # ls -l /tmp/first_test -rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test Registering an XRootD Standalone Server \u00b6 To register your XRootD server, follow the general registration instructions here with the following XRootD-specific details: Add an XRootD component: section to the Services: list, with any relevant fields for that service. This is a partial example: ... FQDN: <FULLY QUALIFIED DOMAIN NAME> Services: XRootD component: Description: Standalone XRootD server ... Replacing <FULLY QUALIFIED DOMAIN NAME> with your XRootD server's DNS entry. If you are setting up a new resource, set Active: false . Only set Active: true for a resource when it is accepting requests and ready for production. Getting Help \u00b6 To get assistance. please use the Help Procedure page. Reference \u00b6 XRootD documentation Export directive in the XRootD configuration and relevant options Service Configuration \u00b6 The configuration that your XRootD service uses is determined by the service name given to systemctl . To use the standalone config, you would start XRootD with the following command: root@host # systemctl start xrootd@standalone File locations \u00b6 Service/Process Configuration File Description xrootd /etc/xrootd/xrootd-standalone.cfg Main XRootD configuration /etc/xrootd/config.d/ Drop-in configuration dir /etc/xrootd/auth_file Authorized users file Service/Process Log File Description xrootd /var/log/xrootd/server/xrootd.log XRootD server daemon log cmsd /var/log/xrootd/server/cmsd.log Cluster management log","title":"Install XRootD Standalone"},{"location":"data/xrootd/install-standalone/#install-xrootd-standalone","text":"EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an issue with the globus-gsi-proxy-core package XRootD is a hierarchical storage system that can be used in a variety of ways to access data, typically distributed among actual storage resources. In this document we focus on using XRootD as a simple layer exporting an underlying storage system (e.g., HDFS ) to the outside world.","title":"Install XRootD Standalone"},{"location":"data/xrootd/install-standalone/#before-starting","text":"Before starting the installation process, consider the following points: User IDs: If it does not exist already, the installation will create the Linux user ID xrootd Service certificate: The XRootD service uses a host certificate and key pair at /etc/grid-security/xrd/xrdcert.pem and /etc/grid-security/xrd/xrdkey.pem Networking: The XRootD service uses port 1094 by default As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"data/xrootd/install-standalone/#installing-xrootd","text":"To install the XRootD Standalone server, run the following Yum command: root@xrootd-standalone # yum install osg-xrootd-standalone","title":"Installing XRootD"},{"location":"data/xrootd/install-standalone/#configuring-xrootd","text":"To configure XRootD as a standalone server, you will modify /etc/xrootd/xrootd-standalone.cfg and the config files under /etc/xrootd/config.d/ as follows: Configure a rootdir in /etc/xrootd/config.d/10-common-site-local.cfg , to point to the top of the directory hierarchy which you wish to serve via XRootD. set rootdir = <DIRECTORY> Warning Do not set rootdir to / . This might result in serving private information. If you want to limit the sub-directories to serve under your configured rootdir , comment out the all.export / directive in /etc/xrootd/config.d/90-osg-standalone-paths.cfg , and add an all.export directive for each directory under rootdir that you wish to serve via XRootD. This is useful if you have a mixture of files under your rootdir , for example from multiple users, but only want to expose a subset of them to the world. For example, to serve the contents of /data/store and /data/public (with rootdir configured to /data ): all.export /store/ all.export /public/ If you want to serve everything under your configured rootdir , you don't have to change anything. Note The directories specified this way are writable by default. Access controls should be managed via authorization configuration . In /etc/xrootd/config.d/10-common-site-local.cfg , add a line to set the resourcename variable to the resource name of your XRootD service. For example, the XRootD service registered at the University of Florida site should set the following configuration: set resourcename = UFlorida-XRD Note CMS sites should follow CMS policy for resourcename","title":"Configuring XRootD"},{"location":"data/xrootd/install-standalone/#configuring-authorization","text":"To configure XRootD authorization please follow the documentation here .","title":"Configuring authorization"},{"location":"data/xrootd/install-standalone/#optional-configuration","text":"The following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to the section on using XRootD .","title":"Optional configuration"},{"location":"data/xrootd/install-standalone/#enabling-hadoop-support-el-7-only","text":"Hadoop File System (HDFS) based sites should utilize the xrootd-hdfs plugin to allow XRootD to access their storage: Install the XRootD HDFS plugin package: root@host # yum install xrootd-hdfs Add the following configuration to /etc/xrootd/xrootd-clustered.cfg : ofs.osslib /usr/lib64/libXrdHdfs.so For more information, see the HDFS installation documents .","title":"Enabling Hadoop support (EL 7 Only)"},{"location":"data/xrootd/install-standalone/#enabling-multi-user-support","text":"By default XRootD servers write files on the storage system aa the Unix user xrootd instead of the authenticated user. The xrootd-multiuser plugins changes this behaviour: Install the XRootD multi-user plugin: root@host # yum install xrootd-multiuser Start the XRootD process in privileged mode: root@host # systemctl xrootd-privileged@standalone Bug The multiuser support is incompatible with checksums. Note This is not necessary when XRootD is used for read-only access","title":"Enabling multi-user support"},{"location":"data/xrootd/install-standalone/#enabling-cms-tfc-support-cms-sites-only","text":"For CMS users, there is a package available to integrate rule-based name lookup using a storage.xml file. If you are not setting up a CMS site, you can skip this section. yum install --enablerepo=osg-contrib xrootd-cmstfc You will need to add your storage.xml to /etc/xrootd/storage.xml and then add the following line to your XRootD configuration: # Integrate with CMS TFC, placed in /etc/xrootd/storage.xml oss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml?protocol=hadoop Add the orange text only if you are running hadoop (see below). See the CMS TWiki for more information: https://twiki.cern.ch/twiki/bin/view/Main/XrootdTfcChanges https://twiki.cern.ch/twiki/bin/view/Main/HdfsXrootdInstall","title":"Enabling CMS TFC support (CMS sites only)"},{"location":"data/xrootd/install-standalone/#using-xrootd","text":"In addition to the XRootD service itself, there are a number of supporting services in your installation. The specific services are: Software Service Name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation for more info XRootD xrootd@standalone XRootD Multiuser xrootd-privileged@standalone See XRootD multiuser for more info Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To \u2026 Run the command\u2026 Start a service systemctl start SERVICE-NAME Stop a service systemctl stop SERVICE-NAME Enable a service to start during boot systemctl enable SERVICE-NAME Disable a service from starting during boot systemctl disable SERVICE-NAME","title":"Using XRootD"},{"location":"data/xrootd/install-standalone/#validating-xrootd","text":"To validate an XRootD installation, perform the following verification steps: Note If you have configured authentication/authorization for XRootD, be sure you have given yourself the necessary permissions to run these tests. For example, if you are using a grid proxy, make sure your DN is mapped to a user in /etc/grid-security/grid-mapfile , and make sure you have a valid proxy on your local machine. Also, ensure that the Authfile on the XRootD server gives write access to the Unix user you will get mapped to. Verify file transfer over the XRootD protocol using XRootD client tools: Install the client tools: root@xrootd-standalone # yum install xrootd-client Copy a file to a directory for which you have write access: root@xrootd-standalone # xrdcp /bin/sh root://localhost:1094//tmp/first_test [xrootd] Total 0.76 MB [====================] 100.00 % [inf MB/s] Verify that the file has been copied over: root@xrootd-standalone # ls -l /tmp/first_test -rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test Verify file transfer over HTTP using GFAL2 client tools: Install the GFAL2 client tools: root@xrootd-standalone # yum install gfal2-util gfal2-plugin-http Copy a file to a directory for which you have write access: root@xrootd-standalone # gfal-copy /bin/sh http://localhost:1094//tmp/first_test Verify that the file has been copied over: root@xrootd-standalone # ls -l /tmp/first_test -rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test","title":"Validating XRootD"},{"location":"data/xrootd/install-standalone/#registering-an-xrootd-standalone-server","text":"To register your XRootD server, follow the general registration instructions here with the following XRootD-specific details: Add an XRootD component: section to the Services: list, with any relevant fields for that service. This is a partial example: ... FQDN: <FULLY QUALIFIED DOMAIN NAME> Services: XRootD component: Description: Standalone XRootD server ... Replacing <FULLY QUALIFIED DOMAIN NAME> with your XRootD server's DNS entry. If you are setting up a new resource, set Active: false . Only set Active: true for a resource when it is accepting requests and ready for production.","title":"Registering an XRootD Standalone Server"},{"location":"data/xrootd/install-standalone/#getting-help","text":"To get assistance. please use the Help Procedure page.","title":"Getting Help"},{"location":"data/xrootd/install-standalone/#reference","text":"XRootD documentation Export directive in the XRootD configuration and relevant options","title":"Reference"},{"location":"data/xrootd/install-standalone/#service-configuration","text":"The configuration that your XRootD service uses is determined by the service name given to systemctl . To use the standalone config, you would start XRootD with the following command: root@host # systemctl start xrootd@standalone","title":"Service Configuration"},{"location":"data/xrootd/install-standalone/#file-locations","text":"Service/Process Configuration File Description xrootd /etc/xrootd/xrootd-standalone.cfg Main XRootD configuration /etc/xrootd/config.d/ Drop-in configuration dir /etc/xrootd/auth_file Authorized users file Service/Process Log File Description xrootd /var/log/xrootd/server/xrootd.log XRootD server daemon log cmsd /var/log/xrootd/server/cmsd.log Cluster management log","title":"File locations"},{"location":"data/xrootd/install-storage-element/","text":"Installing an XRootD Storage Element \u00b6 EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an issue with the globus-gsi-proxy-core package XRootD is a hierarchical storage system that can be used in a variety of ways to access data, typically distributed among actual storage resources. One way to use XRootD is to have it refer to many data resources at a single site, and another way to use it is to refer to many storage systems, most likely distributed among sites. An XRootD system includes a redirector , which accepts requests for data and finds a storage repository \u2014 locally or otherwise \u2014 that can provide the data to the requestor. Use this page to learn how to install, configure, and use an XRootD redirector as part of a Storage Element (SE) or as part of a global namespace. Before Starting \u00b6 Before starting the installation process, consider the following points: User IDs: If it does not exist already, the installation will create the Linux user ID xrootd Service certificate: The XRootD service uses a host certificate at /etc/grid-security/host*.pem Networking: The XRootD service uses port 1094 by default As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates Installing an XRootD Server \u00b6 An installation of the XRootD server consists of the server itself and its dependencies. Install these with Yum: root@host # yum install osg-xrootd Configuring an XRootD Server \u00b6 An advanced XRootD setup has multiple components; it is important to validate that each additional component that you set up is working before moving on to the next component. We have included validation instructions after each component below. Creating an XRootD cluster \u00b6 If your storage is spread out over multiple hosts, you will need to set up an XRootD cluster . The cluster uses one \"redirector\" node as a frontend for user accesses, and multiple data nodes that have the data that users request. Two daemons will run on each node: xrootd The eXtended Root Daemon controls file access and storage. cmsd The Cluster Management Services Daemon controls communication between nodes. Note that for large virtual organizations, a site-level redirector may actually also communicate upwards to a regional or global redirector that handles access to a multi-level hierarchy. This section will only cover handling one level of XRootD hierarchy. In the instructions below, <RDRNODE> will refer to the redirector host and <DATANODE> will refer to the data node host. These should be replaced with the fully-qualified domain name of the host in question. Modify /etc/xrootd/xrootd-clustered.cfg \u00b6 You will need to modify the xrootd-clustered.cfg on the redirector node and each data node. The following example should serve as a base configuration for clustering. Further customizations are detailed below. all.export /mnt/xrootd stage set xrdr = <RDRNODE> all.manager $(xrdr):3121 if $(xrdr) # Lines in this block are only executed on the redirector node all.role manager else # Lines in this block are executed on all nodes but the redirector node all.role server cms.space min 2g 5g fi You will need to customize the following lines: Configuration Line Changes Needed all.export /mnt/xrootd stage Change /mnt/xrootd to the directory to allow XRootD access to set xrdr=<RDRNODE> Change to the hostname of the redirector cms.space min 2g 5g Reserve this amount of free space on the node. For this example, if space falls below 2GB, xrootd will not store further files on this node until space climbs above 5GB. You can use k , m , g , or t to indicate kilobyte, megabytes, gigabytes, or terabytes, respectively. Further information can be found at https://xrootd.slac.stanford.edu/docs.html Verifying the clustered config \u00b6 Start both xrootd and cmsd on all nodes according to the instructions in the Using XRootD section . Verify that you can copy a file such as /bin/sh to /mnt/xrootd on the server data via the redirector: root@host # xrdcp /bin/sh root://<RDRNODE>:1094///mnt/xrootd/second_test [xrootd] Total 0.76 MB [====================] 100.00 % [inf MB/s] Check that the /mnt/xrootd/second_test is located on data server <DATANODE> . (Optional) Adding High Availability (HA) redirectors \u00b6 It is possible to have an XRootD clustered setup with more than one redirector to ensure high availability service. To do this: In the /etc/xrootd/xrootd-clustered.cfg on each data node follow the instructions in this section with: set xrdr1 = <RDRNODE1> set xrdr2 = <RDRNODE2> all.manager $(xrdr1):3121 all.manager $(xrdr2):3121 Create DNS ALIAS records for <RDRNODE> pointing to <RDNODE1> and <RDRNODE2> Advertise the <RDRNODE> FQDN to users interacting with the XRootD cluster should be <RDRNODE> . (Optional) Adding Simple Server Inventory to your cluster \u00b6 The Simple Server Inventory (SSI) provide means to have an inventory for each data server. SSI requires: A second instance of the xrootd daemon on the redirector A \"composite name space daemon\" ( XrdCnsd ) on each data server; this daemon handles the inventory As an example, we will set up a two-node XRootD cluster with SSI. Host A is a redirector node that is running the following daemons: xrootd redirector cmsd xrootd - second instance that required for SSI Host B is a data server that is running the following daemons: xrootd data server cmsd XrdCnsd - started automatically by xrootd We will need to create a directory on the redirector node for Inventory files. root@host # mkdir -p /data/inventory root@host # chown xrootd:xrootd /data/inventory On the data server (host B) let's use a storage cache that will be at a different location from /mnt/xrootd . root@host # mkdir -p /local/xrootd root@host # chown xrootd:xrootd /local/xrootd We will be running two instances of XRootD on <HOST A> . Modify /etc/xrootd/xrootd-clustered.cfg to give the two instances different behavior, as such: all.export /data/xrootdfs set xrdr=<HOST A> all.manager $(xrdr):3121 if $(xrdr) && named cns all.export /data/inventory xrd.port 1095 else if $(xrdr) all.role manager xrd.port 1094 else all.role server oss.localroot /local/xrootd ofs.notify closew create mkdir mv rm rmdir trunc | /usr/bin/XrdCnsd -d -D 2 -i 90 -b $(xrdr):1095:/data/inventory #add cms.space if you have less the 11GB # cms.space options https://xrootd.slac.stanford.edu/doc/dev410/cms_config.htm cms.space min 2g 5g fi The value of oss.localroot will be prepended to any file access. E.g. accessing root://<RDRNODE>:1094//data/xrootdfs/test1 will actually go to /local/xrootd/data/xrootdfs/test1 . Starting a second instance of XRootD \u00b6 Create a symlink pointing to /etc/xrootd/xrootd-clustered.cfg at /etc/xrootd/xrootd-cns.cfg : root@host # ln -s /etc/xrootd/xrootd-clustered.cfg /etc/xrootd/xrootd-cns.cfg Start an instance of the xrootd service named cns using the syntax in the managing services section : root@host # systemctl start xrootd@cns Testing an XRootD cluster with SSI \u00b6 Copy file to redirector node specifying storage path (/data/xrootdfs instead of /mnt/xrootd): root@host # xrdcp /bin/sh root://<RDRNODE>:1094//data/xrootdfs/test1 [xrootd] Total 0.00 MB [================] 100.00 % [inf MB/s] To verify that SSI is working execute cns_ssi command on the redirector node: root@host # cns_ssi list /data/inventory fermicloud054.fnal.gov incomplete inventory as of Mon Apr 11 17:28:11 2011 root@host # cns_ssi updt /data/inventory cns_ssi: fermicloud054.fnal.gov inventory with 1 directory and 1 file updated with 0 errors. root@host # cns_ssi list /data/inventory fermicloud054.fnal.gov complete inventory as of Tue Apr 12 07:38:29 2011 /data/xrootdfs/test1 Note : In this example, fermicloud53.fnal.gov is a redirector node and fermicloud054.fnal.gov is a data node. (Optional) Enabling Xrootd over HTTP \u00b6 XRootD can be accessed using the HTTP protocol. To do that: Configure LCMAPS authorization . Uncomment the following line in /etc/xrootd/config.d/10-xrootd-lcmaps.cfg : set EnableLcmaps = 1 Additionally, add the following line to the same file: set EnableHttp = 1 Testing the configuration From the terminal, generate a proxy and attempt to use davix-get to copy from your XRootD host (the XRootD service needs running; see the services section ). For example, if your server has a file named /store/user/test.root : davix-get https://<YOUR FQDN>:1094/store/user/test.root -E /mnt/xrootd/x509up_u`id -u` --capath /etc/grid-security/certificates Note For clients to successfully read from the regional redirector, HTTPS must be enabled for the data servers and the site-level redirector. Warning If you have u * in your Authfile, recall this provides an authorization to ALL users, including unauthenticated. This includes random web spiders! (Optional) Enable HTTP based Writes \u00b6 No changes to the HTTP module is needed to enable HTTP-based writes. The HTTP protocol uses the same authorization setup as the XRootD protocol. For example, you may need to provide a (all) style authorizations to allow users authorization to write. See the Authentication File section for more details. (Optional) Enabling a FUSE mount \u00b6 XRootD storage can be mounted as a standard POSIX filesystem via FUSE, providing users with a more familiar interface.. Modify /etc/fstab by adding the following entries: .... xrootdfs /mnt/xrootd fuse rdr=xroot://<REDIRECTOR FQDN>:1094/<PATH TO FILE>,uid=xrootd 0 0 Replace /mnt/xrootd with the path that you would like to access with. This should also match the GridFTP settings for the XROOTD_VMP local path. Create /mnt/xrootd directory. Make sure the xrootd user exists on the system. Once you are finished, you can mount it: mount /mnt/xrootd You should now be able to run UNIX commands such as ls /mnt/xrootd to see the contents of the XRootD server. (Optional) Authorization \u00b6 For information on how to configure xrootd-lcmaps authorization, please refer to the Configuring XRootD Authorization guide . (Optional) Adding CMS TFC support to XRootD (CMS sites only) \u00b6 For CMS users, there is a package available to integrate rule-based name lookup using a storage.xml file. See this documentation . (Optional) Adding Hadoop support to XRootD \u00b6 For documentation on how to export your Hadoop storage using XRootD please see this documentation (Optional) Adding Multi user support for an XRootd server \u00b6 For documentation how to enable multi-user support using XRootD see this documentation . (Optional) Adding File Residency Manager (FRM) to an XRootd cluster \u00b6 If you have a multi-tiered storage system (e.g. some data is stored on SSDs and some on disks or tapes), then install the File Residency Manager (FRM), so you can move data between tiers more easily. If you do not have a multi-tiered storage system, then you do not need FRM and you can skip this section. The FRM deals with two major mechanisms: local disk remote servers The description of fully functional multiple XRootD clusters is beyond the scope of this document. In order to have this fully functional system you will need a global redirector and at least one remote XRootD cluster from where files could be moved to the local cluster. Below are the modifications you should make in order to enable FRM on your local cluster: Make sure that FRM is enabled in /etc/sysconfig/xrootd on your data sever: ROOTD_USER=xrootd XROOTD_GROUP=xrootd XROOTD_DEFAULT_OPTIONS=\"-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg\" CMSD_DEFAULT_OPTIONS=\"-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg\" FRMD_DEFAULT_OPTIONS=\"-l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg\" XROOTD_INSTANCES=\"default\" CMSD_INSTANCES=\"default\" FRMD_INSTANCES=\"default\" Modify /etc/xrootd/xrootd-clustered.cfg on both nodes to specify options for frm_xfrd (File Transfer Daemon) and frm_purged (File Purging Daemon). For more information, you can visit the FRM Documentation Start frm daemons on data server: root@host # service frm_xfrd start root@host # service frm_purged start (Optional) Installing a GridFTP Server \u00b6 The Globus GridFTP server can be installed alongside an XRootD storage element to provide GridFTP-based access to the storage. See Also OSG has extensive documentation on setting up a GridFTP server; this section is an abbreviated version documenting the special steps needed for XRootD integration. You may also find the following useful: Basic GridFTP Install . Additionally covers service planning topics. Load-balanced GridFTP Install . Covers the creation of a load-balanced GridFTP service using multiple servers. Prior to following this installation guide, verify the host certificates and networking is configured correctly as in the basic GridFTP install . Installation \u00b6 GridFTP support for XRootD-based storage is provided by the osg-gridftp-xrootd meta-package: root@host # yum install osg-gridftp-xrootd Configuration \u00b6 For information on how to configure authentication for your GridFTP installation, please refer to the configuring authentication section of the GridFTP guide . Edit /etc/sysconfig/globus-gridftp-server to set XROOTD_VMP to use your XRootD redirector. export XROOTD_VMP = \"redirector:1094:/local_path=/remote_path\" Warning The syntax of XROOTD_VMP is tricky; make sure to use the following guidance: Redirector : The hostname and domain of the local XRootD redirector server. local_path : The full local path exported by the GridFTP server. For example /mystorage/export/data/store remote_path : The XRootD path that will be mounted at local_path . When xrootd-dsi is enabled, GridFTP configuration changes should go into /etc/xrootd-dsi/gridftp-xrootd.conf , not /etc/gridftp.conf . Sites should review any customizations made in the latter and copy them as necessary. You can use the FUSE mount in order to test POSIX access to xrootd in the GridFTP server. You should be able to run Unix commands such as ls /mnt/xrootd and see the contents of the XRootD server. For log / config file locations and system services to run, see the basic GridFTP install . Using XRootD \u00b6 Managing XRootD services \u00b6 Start services on the redirector node before starting any services on the data nodes. If you installed only XRootD itself, you will only need to start the xrootd service. However, if you installed cluster management services, you will need to start cmsd as well. XRootD determines which configuration to use based on the service name specified by systemctl . For example, to have xrootd use the clustered config, you would start up xrootd with this line: root@host # systemctl start xrootd@clustered To use the standalone config instead, you would use: root@host # systemctl start xrootd@standalone The services are: Service EL 6 service name EL 7 service name XRootD (standalone config) xrootd xrootd@standalone XRootD (clustered config) xrootd xrootd@clustered XRootD (multiuser) xrootd-privileged@clustered CMSD (clustered config) cmsd cmsd@clustered As a reminder, here are common service commands (all run as root ): To \u2026 On EL 6, run the command\u2026 On EL 7, run the command\u2026 Start a service service SERVICE-NAME start systemctl start SERVICE-NAME Stop a service service SERVICE-NAME stop systemctl stop SERVICE-NAME Enable a service to start during boot chkconfig SERVICE-NAME on systemctl enable SERVICE-NAME Disable a service from starting during boot chkconfig SERVICE-NAME off systemctl disable SERVICE-NAME Getting Help \u00b6 To get assistance. please use the Help Procedure page. Reference \u00b6 File locations \u00b6 Service/Process Configuration File Description xrootd /etc/xrootd/xrootd-clustered.cfg Main clustered mode XRootD configuration /etc/xrootd/auth_file Authorized users file Service/Process Log File Description xrootd /var/log/xrootd/xrootd.log XRootD server daemon log cmsd /var/log/xrootd/cmsd.log Cluster management log cns /var/log/xrootd/cns/xrootd.log Server inventory (composite name space) log frm_xfrd , frm_purged /var/log/xrootd/frmd.log File Residency Manager log Links \u00b6 XRootD documentation","title":"Install XRootD SE"},{"location":"data/xrootd/install-storage-element/#installing-an-xrootd-storage-element","text":"EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an issue with the globus-gsi-proxy-core package XRootD is a hierarchical storage system that can be used in a variety of ways to access data, typically distributed among actual storage resources. One way to use XRootD is to have it refer to many data resources at a single site, and another way to use it is to refer to many storage systems, most likely distributed among sites. An XRootD system includes a redirector , which accepts requests for data and finds a storage repository \u2014 locally or otherwise \u2014 that can provide the data to the requestor. Use this page to learn how to install, configure, and use an XRootD redirector as part of a Storage Element (SE) or as part of a global namespace.","title":"Installing an XRootD Storage Element"},{"location":"data/xrootd/install-storage-element/#before-starting","text":"Before starting the installation process, consider the following points: User IDs: If it does not exist already, the installation will create the Linux user ID xrootd Service certificate: The XRootD service uses a host certificate at /etc/grid-security/host*.pem Networking: The XRootD service uses port 1094 by default As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"data/xrootd/install-storage-element/#installing-an-xrootd-server","text":"An installation of the XRootD server consists of the server itself and its dependencies. Install these with Yum: root@host # yum install osg-xrootd","title":"Installing an XRootD Server"},{"location":"data/xrootd/install-storage-element/#configuring-an-xrootd-server","text":"An advanced XRootD setup has multiple components; it is important to validate that each additional component that you set up is working before moving on to the next component. We have included validation instructions after each component below.","title":"Configuring an XRootD Server"},{"location":"data/xrootd/install-storage-element/#creating-an-xrootd-cluster","text":"If your storage is spread out over multiple hosts, you will need to set up an XRootD cluster . The cluster uses one \"redirector\" node as a frontend for user accesses, and multiple data nodes that have the data that users request. Two daemons will run on each node: xrootd The eXtended Root Daemon controls file access and storage. cmsd The Cluster Management Services Daemon controls communication between nodes. Note that for large virtual organizations, a site-level redirector may actually also communicate upwards to a regional or global redirector that handles access to a multi-level hierarchy. This section will only cover handling one level of XRootD hierarchy. In the instructions below, <RDRNODE> will refer to the redirector host and <DATANODE> will refer to the data node host. These should be replaced with the fully-qualified domain name of the host in question.","title":"Creating an XRootD cluster"},{"location":"data/xrootd/install-storage-element/#modify-etcxrootdxrootd-clusteredcfg","text":"You will need to modify the xrootd-clustered.cfg on the redirector node and each data node. The following example should serve as a base configuration for clustering. Further customizations are detailed below. all.export /mnt/xrootd stage set xrdr = <RDRNODE> all.manager $(xrdr):3121 if $(xrdr) # Lines in this block are only executed on the redirector node all.role manager else # Lines in this block are executed on all nodes but the redirector node all.role server cms.space min 2g 5g fi You will need to customize the following lines: Configuration Line Changes Needed all.export /mnt/xrootd stage Change /mnt/xrootd to the directory to allow XRootD access to set xrdr=<RDRNODE> Change to the hostname of the redirector cms.space min 2g 5g Reserve this amount of free space on the node. For this example, if space falls below 2GB, xrootd will not store further files on this node until space climbs above 5GB. You can use k , m , g , or t to indicate kilobyte, megabytes, gigabytes, or terabytes, respectively. Further information can be found at https://xrootd.slac.stanford.edu/docs.html","title":"Modify /etc/xrootd/xrootd-clustered.cfg"},{"location":"data/xrootd/install-storage-element/#verifying-the-clustered-config","text":"Start both xrootd and cmsd on all nodes according to the instructions in the Using XRootD section . Verify that you can copy a file such as /bin/sh to /mnt/xrootd on the server data via the redirector: root@host # xrdcp /bin/sh root://<RDRNODE>:1094///mnt/xrootd/second_test [xrootd] Total 0.76 MB [====================] 100.00 % [inf MB/s] Check that the /mnt/xrootd/second_test is located on data server <DATANODE> .","title":"Verifying the clustered config"},{"location":"data/xrootd/install-storage-element/#optional-adding-high-availability-ha-redirectors","text":"It is possible to have an XRootD clustered setup with more than one redirector to ensure high availability service. To do this: In the /etc/xrootd/xrootd-clustered.cfg on each data node follow the instructions in this section with: set xrdr1 = <RDRNODE1> set xrdr2 = <RDRNODE2> all.manager $(xrdr1):3121 all.manager $(xrdr2):3121 Create DNS ALIAS records for <RDRNODE> pointing to <RDNODE1> and <RDRNODE2> Advertise the <RDRNODE> FQDN to users interacting with the XRootD cluster should be <RDRNODE> .","title":"(Optional) Adding High Availability (HA) redirectors"},{"location":"data/xrootd/install-storage-element/#optional-adding-simple-server-inventory-to-your-cluster","text":"The Simple Server Inventory (SSI) provide means to have an inventory for each data server. SSI requires: A second instance of the xrootd daemon on the redirector A \"composite name space daemon\" ( XrdCnsd ) on each data server; this daemon handles the inventory As an example, we will set up a two-node XRootD cluster with SSI. Host A is a redirector node that is running the following daemons: xrootd redirector cmsd xrootd - second instance that required for SSI Host B is a data server that is running the following daemons: xrootd data server cmsd XrdCnsd - started automatically by xrootd We will need to create a directory on the redirector node for Inventory files. root@host # mkdir -p /data/inventory root@host # chown xrootd:xrootd /data/inventory On the data server (host B) let's use a storage cache that will be at a different location from /mnt/xrootd . root@host # mkdir -p /local/xrootd root@host # chown xrootd:xrootd /local/xrootd We will be running two instances of XRootD on <HOST A> . Modify /etc/xrootd/xrootd-clustered.cfg to give the two instances different behavior, as such: all.export /data/xrootdfs set xrdr=<HOST A> all.manager $(xrdr):3121 if $(xrdr) && named cns all.export /data/inventory xrd.port 1095 else if $(xrdr) all.role manager xrd.port 1094 else all.role server oss.localroot /local/xrootd ofs.notify closew create mkdir mv rm rmdir trunc | /usr/bin/XrdCnsd -d -D 2 -i 90 -b $(xrdr):1095:/data/inventory #add cms.space if you have less the 11GB # cms.space options https://xrootd.slac.stanford.edu/doc/dev410/cms_config.htm cms.space min 2g 5g fi The value of oss.localroot will be prepended to any file access. E.g. accessing root://<RDRNODE>:1094//data/xrootdfs/test1 will actually go to /local/xrootd/data/xrootdfs/test1 .","title":"(Optional) Adding Simple Server Inventory to your cluster"},{"location":"data/xrootd/install-storage-element/#starting-a-second-instance-of-xrootd","text":"Create a symlink pointing to /etc/xrootd/xrootd-clustered.cfg at /etc/xrootd/xrootd-cns.cfg : root@host # ln -s /etc/xrootd/xrootd-clustered.cfg /etc/xrootd/xrootd-cns.cfg Start an instance of the xrootd service named cns using the syntax in the managing services section : root@host # systemctl start xrootd@cns","title":"Starting a second instance of XRootD"},{"location":"data/xrootd/install-storage-element/#testing-an-xrootd-cluster-with-ssi","text":"Copy file to redirector node specifying storage path (/data/xrootdfs instead of /mnt/xrootd): root@host # xrdcp /bin/sh root://<RDRNODE>:1094//data/xrootdfs/test1 [xrootd] Total 0.00 MB [================] 100.00 % [inf MB/s] To verify that SSI is working execute cns_ssi command on the redirector node: root@host # cns_ssi list /data/inventory fermicloud054.fnal.gov incomplete inventory as of Mon Apr 11 17:28:11 2011 root@host # cns_ssi updt /data/inventory cns_ssi: fermicloud054.fnal.gov inventory with 1 directory and 1 file updated with 0 errors. root@host # cns_ssi list /data/inventory fermicloud054.fnal.gov complete inventory as of Tue Apr 12 07:38:29 2011 /data/xrootdfs/test1 Note : In this example, fermicloud53.fnal.gov is a redirector node and fermicloud054.fnal.gov is a data node.","title":"Testing an XRootD cluster with SSI"},{"location":"data/xrootd/install-storage-element/#optional-enabling-xrootd-over-http","text":"XRootD can be accessed using the HTTP protocol. To do that: Configure LCMAPS authorization . Uncomment the following line in /etc/xrootd/config.d/10-xrootd-lcmaps.cfg : set EnableLcmaps = 1 Additionally, add the following line to the same file: set EnableHttp = 1 Testing the configuration From the terminal, generate a proxy and attempt to use davix-get to copy from your XRootD host (the XRootD service needs running; see the services section ). For example, if your server has a file named /store/user/test.root : davix-get https://<YOUR FQDN>:1094/store/user/test.root -E /mnt/xrootd/x509up_u`id -u` --capath /etc/grid-security/certificates Note For clients to successfully read from the regional redirector, HTTPS must be enabled for the data servers and the site-level redirector. Warning If you have u * in your Authfile, recall this provides an authorization to ALL users, including unauthenticated. This includes random web spiders!","title":"(Optional) Enabling Xrootd over HTTP"},{"location":"data/xrootd/install-storage-element/#optional-enable-http-based-writes","text":"No changes to the HTTP module is needed to enable HTTP-based writes. The HTTP protocol uses the same authorization setup as the XRootD protocol. For example, you may need to provide a (all) style authorizations to allow users authorization to write. See the Authentication File section for more details.","title":"(Optional) Enable HTTP based Writes"},{"location":"data/xrootd/install-storage-element/#optional-enabling-a-fuse-mount","text":"XRootD storage can be mounted as a standard POSIX filesystem via FUSE, providing users with a more familiar interface.. Modify /etc/fstab by adding the following entries: .... xrootdfs /mnt/xrootd fuse rdr=xroot://<REDIRECTOR FQDN>:1094/<PATH TO FILE>,uid=xrootd 0 0 Replace /mnt/xrootd with the path that you would like to access with. This should also match the GridFTP settings for the XROOTD_VMP local path. Create /mnt/xrootd directory. Make sure the xrootd user exists on the system. Once you are finished, you can mount it: mount /mnt/xrootd You should now be able to run UNIX commands such as ls /mnt/xrootd to see the contents of the XRootD server.","title":"(Optional) Enabling a FUSE mount"},{"location":"data/xrootd/install-storage-element/#optional-authorization","text":"For information on how to configure xrootd-lcmaps authorization, please refer to the Configuring XRootD Authorization guide .","title":"(Optional) Authorization"},{"location":"data/xrootd/install-storage-element/#optional-adding-cms-tfc-support-to-xrootd-cms-sites-only","text":"For CMS users, there is a package available to integrate rule-based name lookup using a storage.xml file. See this documentation .","title":"(Optional) Adding CMS TFC support to XRootD (CMS sites only)"},{"location":"data/xrootd/install-storage-element/#optional-adding-hadoop-support-to-xrootd","text":"For documentation on how to export your Hadoop storage using XRootD please see this documentation","title":"(Optional) Adding Hadoop support to XRootD"},{"location":"data/xrootd/install-storage-element/#optional-adding-multi-user-support-for-an-xrootd-server","text":"For documentation how to enable multi-user support using XRootD see this documentation .","title":"(Optional) Adding Multi user support for an XRootd server"},{"location":"data/xrootd/install-storage-element/#optional-adding-file-residency-manager-frm-to-an-xrootd-cluster","text":"If you have a multi-tiered storage system (e.g. some data is stored on SSDs and some on disks or tapes), then install the File Residency Manager (FRM), so you can move data between tiers more easily. If you do not have a multi-tiered storage system, then you do not need FRM and you can skip this section. The FRM deals with two major mechanisms: local disk remote servers The description of fully functional multiple XRootD clusters is beyond the scope of this document. In order to have this fully functional system you will need a global redirector and at least one remote XRootD cluster from where files could be moved to the local cluster. Below are the modifications you should make in order to enable FRM on your local cluster: Make sure that FRM is enabled in /etc/sysconfig/xrootd on your data sever: ROOTD_USER=xrootd XROOTD_GROUP=xrootd XROOTD_DEFAULT_OPTIONS=\"-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg\" CMSD_DEFAULT_OPTIONS=\"-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg\" FRMD_DEFAULT_OPTIONS=\"-l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg\" XROOTD_INSTANCES=\"default\" CMSD_INSTANCES=\"default\" FRMD_INSTANCES=\"default\" Modify /etc/xrootd/xrootd-clustered.cfg on both nodes to specify options for frm_xfrd (File Transfer Daemon) and frm_purged (File Purging Daemon). For more information, you can visit the FRM Documentation Start frm daemons on data server: root@host # service frm_xfrd start root@host # service frm_purged start","title":"(Optional) Adding File Residency Manager (FRM) to an XRootd cluster"},{"location":"data/xrootd/install-storage-element/#optional-installing-a-gridftp-server","text":"The Globus GridFTP server can be installed alongside an XRootD storage element to provide GridFTP-based access to the storage. See Also OSG has extensive documentation on setting up a GridFTP server; this section is an abbreviated version documenting the special steps needed for XRootD integration. You may also find the following useful: Basic GridFTP Install . Additionally covers service planning topics. Load-balanced GridFTP Install . Covers the creation of a load-balanced GridFTP service using multiple servers. Prior to following this installation guide, verify the host certificates and networking is configured correctly as in the basic GridFTP install .","title":"(Optional) Installing a GridFTP Server"},{"location":"data/xrootd/install-storage-element/#installation","text":"GridFTP support for XRootD-based storage is provided by the osg-gridftp-xrootd meta-package: root@host # yum install osg-gridftp-xrootd","title":"Installation"},{"location":"data/xrootd/install-storage-element/#configuration","text":"For information on how to configure authentication for your GridFTP installation, please refer to the configuring authentication section of the GridFTP guide . Edit /etc/sysconfig/globus-gridftp-server to set XROOTD_VMP to use your XRootD redirector. export XROOTD_VMP = \"redirector:1094:/local_path=/remote_path\" Warning The syntax of XROOTD_VMP is tricky; make sure to use the following guidance: Redirector : The hostname and domain of the local XRootD redirector server. local_path : The full local path exported by the GridFTP server. For example /mystorage/export/data/store remote_path : The XRootD path that will be mounted at local_path . When xrootd-dsi is enabled, GridFTP configuration changes should go into /etc/xrootd-dsi/gridftp-xrootd.conf , not /etc/gridftp.conf . Sites should review any customizations made in the latter and copy them as necessary. You can use the FUSE mount in order to test POSIX access to xrootd in the GridFTP server. You should be able to run Unix commands such as ls /mnt/xrootd and see the contents of the XRootD server. For log / config file locations and system services to run, see the basic GridFTP install .","title":"Configuration"},{"location":"data/xrootd/install-storage-element/#using-xrootd","text":"","title":"Using XRootD"},{"location":"data/xrootd/install-storage-element/#managing-xrootd-services","text":"Start services on the redirector node before starting any services on the data nodes. If you installed only XRootD itself, you will only need to start the xrootd service. However, if you installed cluster management services, you will need to start cmsd as well. XRootD determines which configuration to use based on the service name specified by systemctl . For example, to have xrootd use the clustered config, you would start up xrootd with this line: root@host # systemctl start xrootd@clustered To use the standalone config instead, you would use: root@host # systemctl start xrootd@standalone The services are: Service EL 6 service name EL 7 service name XRootD (standalone config) xrootd xrootd@standalone XRootD (clustered config) xrootd xrootd@clustered XRootD (multiuser) xrootd-privileged@clustered CMSD (clustered config) cmsd cmsd@clustered As a reminder, here are common service commands (all run as root ): To \u2026 On EL 6, run the command\u2026 On EL 7, run the command\u2026 Start a service service SERVICE-NAME start systemctl start SERVICE-NAME Stop a service service SERVICE-NAME stop systemctl stop SERVICE-NAME Enable a service to start during boot chkconfig SERVICE-NAME on systemctl enable SERVICE-NAME Disable a service from starting during boot chkconfig SERVICE-NAME off systemctl disable SERVICE-NAME","title":"Managing XRootD services"},{"location":"data/xrootd/install-storage-element/#getting-help","text":"To get assistance. please use the Help Procedure page.","title":"Getting Help"},{"location":"data/xrootd/install-storage-element/#reference","text":"","title":"Reference"},{"location":"data/xrootd/install-storage-element/#file-locations","text":"Service/Process Configuration File Description xrootd /etc/xrootd/xrootd-clustered.cfg Main clustered mode XRootD configuration /etc/xrootd/auth_file Authorized users file Service/Process Log File Description xrootd /var/log/xrootd/xrootd.log XRootD server daemon log cmsd /var/log/xrootd/cmsd.log Cluster management log cns /var/log/xrootd/cns/xrootd.log Server inventory (composite name space) log frm_xfrd , frm_purged /var/log/xrootd/frmd.log File Residency Manager log","title":"File locations"},{"location":"data/xrootd/install-storage-element/#links","text":"XRootD documentation","title":"Links"},{"location":"data/xrootd/overview/","text":"XRootD Overview \u00b6 XRootD is a highly-configurable data server used by sites in the OSG to support VO-specific storage needs. The software can be used to create an export of an existing file system through multiple protocols, participate in a data federation, or act as a caching service. XRootD data servers can stream data directly to client applications or support experiment-wide data management by performing bulk data transfer via \"third-party-copy\" between distinct sites. The OSG supports multiple different configurations of XRootD: XCache \u00b6 Previously known as the \"XRootD proxy cache\", XCache provides a caching service for data federations that serve one or more VOs. If your site contributes large amounts of computing resources to the OSG, a site XCache could be part of a solution to help reduce incoming WAN usage. In the OSG, there are three data federations based on XCache: ATLAS XCache, CMS XCache, and StashCache for all other VOs. If you are affiliated with a site or VO interested in contributing to a data federation, contact us at help@opensciencegrid.org . XRootD Standalone \u00b6 An XRootD standalone server exports data from an existing network storage solution, such as HDFS or Lustre, using both the XRootD and WebDAV protocols. Generally, only sites affiliated with large VOs would need to install an XRootD standalone server so consult your VO if you are interested in contributing storage. XRootD Storage Element \u00b6 For an XRootD storage element (SE) , the XRootD software acts as the network storage technology, exporting data from multiple, distributed hosts using both the XRootD and WebDAV protocols. Generally, only sites affiliated with large VOs would need to install an XRootD SE so consult your VO if you are interested in contributing storage.","title":"XRootD Overview"},{"location":"data/xrootd/overview/#xrootd-overview","text":"XRootD is a highly-configurable data server used by sites in the OSG to support VO-specific storage needs. The software can be used to create an export of an existing file system through multiple protocols, participate in a data federation, or act as a caching service. XRootD data servers can stream data directly to client applications or support experiment-wide data management by performing bulk data transfer via \"third-party-copy\" between distinct sites. The OSG supports multiple different configurations of XRootD:","title":"XRootD Overview"},{"location":"data/xrootd/overview/#xcache","text":"Previously known as the \"XRootD proxy cache\", XCache provides a caching service for data federations that serve one or more VOs. If your site contributes large amounts of computing resources to the OSG, a site XCache could be part of a solution to help reduce incoming WAN usage. In the OSG, there are three data federations based on XCache: ATLAS XCache, CMS XCache, and StashCache for all other VOs. If you are affiliated with a site or VO interested in contributing to a data federation, contact us at help@opensciencegrid.org .","title":"XCache"},{"location":"data/xrootd/overview/#xrootd-standalone","text":"An XRootD standalone server exports data from an existing network storage solution, such as HDFS or Lustre, using both the XRootD and WebDAV protocols. Generally, only sites affiliated with large VOs would need to install an XRootD standalone server so consult your VO if you are interested in contributing storage.","title":"XRootD Standalone"},{"location":"data/xrootd/overview/#xrootd-storage-element","text":"For an XRootD storage element (SE) , the XRootD software acts as the network storage technology, exporting data from multiple, distributed hosts using both the XRootD and WebDAV protocols. Generally, only sites affiliated with large VOs would need to install an XRootD SE so consult your VO if you are interested in contributing storage.","title":"XRootD Storage Element"},{"location":"data/xrootd/xrootd-authorization/","text":"Configuring XRootD Authorization \u00b6 EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an issue with the globus-gsi-proxy-core package There are several authorization options in XRootD available through its security plugins. In this document, we will cover the xrootd-lcmaps security option supported in the OSG. The XRootD LCMAPS authorization method depends on configuring LCMAPS with the VOMS plugin . LCMAPS maps an incoming user's grid credentials to a Unix account name; the permissions listed in the authorization file all reference Unix account names. Note On the data nodes, the files will actually be owned by Unix user xrootd (or other daemon user), not as the user authenticated to, under most circumstances. XRootD will verify the permissions and authorization based on the user that the security plugin authenticates you to, but, internally, the data node files will be owned by the xrootd user. If this behaviour is not desired, enable XRootD multi-user support . Authorization file \u00b6 XRootD allows configuring fine-grained file access permissions based on usernames and paths. This is configured in the authorization file /etc/xrootd/auth_file on the data server node, which should be writable only by the xrootd user, optionally readable by others. (The path /etc/xrootd/auth_file corresponds to the acc.authdb parameter in your xrootd config.) Here is an example /etc/xrootd/auth_file : # This means that all the users have read access to the datasets, _except_ under /private u * <STORAGE PATH>/private -rl <STORAGE PATH> rl # Or the following, without a restricted /private dir # u * <STORAGE PATH> rl # This means that all the users have full access to their private home dirs u = <STORAGE PATH>/home/@=/ a # This means that the privileged 'xrootd' user can do everything # There must be at least one such user in order to create the # private dirs for users willing to store their data in the facility u xrootd <STORAGE PATH> a # This means that users in group 'biology' can do anything under the 'genomics' directory g biology <STORAGE PATH>/genomics a Replacing <STORAGE PATH> with the path to the directory that will contain data served by XRootD, e.g. /data/xrootdfs . This path is relative to the oss.localroot or all.localroot configuration values, if either one is defined in the xrootd config file. Note Specific paths need to be specified before generic paths; e.g., this does not work: u * rl /data/xrootdfs -rl /data/xrootdfs/private More generally, each configuration line of the auth file has the following form: idtype id path privs Field Description idtype Type of id. Use u for username, g for group, etc. id Username (or groupname). Use * for all users or = for user-specific capabilities, like home directories path The path prefix to be used for matching purposes. @= expands to the current user name before a path prefix match is attempted privs Letter list of privileges: a - all ; l - lookup ; d - delete ; n - rename ; i - insert ; r - read ; k - lock (not used) ; w - write ; - - prefix to remove specified privileges For more details or examples on how to use templated user options, see XRootd Authorization Database File . Ensure the auth file is owned by xrootd (if you have created file as root), and that it is not writable by others. root@host # chown xrootd:xrootd /etc/xrootd/auth_file root@host # chmod 0640 /etc/xrootd/auth_file # or 0644 Enabling xrootd-lcmaps authorization \u00b6 The xrootd-lcmaps security plugin uses the lcmaps library and the LCMAPS VOMS plugin to authenticate and authorize users based on X509 certificates and VOMS attributes. Perform the following instructions on all data nodes: Install CA certificates and manage CRLs Copy your host certificate and key to /etc/grid-security/xrd/xrdcert.pem and /etc/grid-security/xrd/xrdkey.pem , respectively. Install and configure the LCMAPS VOMS plugin Install xrootd-lcmaps and necessary configuration: root@host # yum install xrootd-lcmaps vo-client Configure access rights for mapped users by creating and modifying the XRootD authorization file Restart the relevant services Verifying XRootD Authorization \u00b6 To verify the LCMAPS security, run the following commands from a machine with your user certificate/key pair, xrootd-client , and voms-clients-cpp installed: Destroy any pre-existing proxies and attempt a copy to a directory (which we will refer to as <DESTINATION PATH> ) on the <XROOTD HOST> to verify failure: user@client $ voms-proxy-destroy user@client $ xrdcp /bin/bash root://<XROOTD HOST>/<DESTINATION PATH> 180213 13:56:49 396570 cryptossl_X509CreateProxy: EEC certificate has expired [0B/0B][100%][==================================================][0B/s] Run: [FATAL] Auth failed On the XRootD host, add your DN to /etc/grid-security/grid-mapfile Add a line to /etc/xrootd/auth_file to ensure the mapped user can write to <DESTINATION PATH> Restart the xrootd service. (See this section for more information of managing XRootD services.) Generate your proxy and verify that you can successfully transfer files: user@client $ voms-proxy-init user@client $ xrdcp /bin/sh root://<XROOTD HOST>/<DESTINATION PATH> [938.1kB/938.1kB][100%][==================================================][938.1kB/s] If your transfer does not succeed, run the previous command with --debug 2 for more information.","title":"Configure Authorization"},{"location":"data/xrootd/xrootd-authorization/#configuring-xrootd-authorization","text":"EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an issue with the globus-gsi-proxy-core package There are several authorization options in XRootD available through its security plugins. In this document, we will cover the xrootd-lcmaps security option supported in the OSG. The XRootD LCMAPS authorization method depends on configuring LCMAPS with the VOMS plugin . LCMAPS maps an incoming user's grid credentials to a Unix account name; the permissions listed in the authorization file all reference Unix account names. Note On the data nodes, the files will actually be owned by Unix user xrootd (or other daemon user), not as the user authenticated to, under most circumstances. XRootD will verify the permissions and authorization based on the user that the security plugin authenticates you to, but, internally, the data node files will be owned by the xrootd user. If this behaviour is not desired, enable XRootD multi-user support .","title":"Configuring XRootD Authorization"},{"location":"data/xrootd/xrootd-authorization/#authorization-file","text":"XRootD allows configuring fine-grained file access permissions based on usernames and paths. This is configured in the authorization file /etc/xrootd/auth_file on the data server node, which should be writable only by the xrootd user, optionally readable by others. (The path /etc/xrootd/auth_file corresponds to the acc.authdb parameter in your xrootd config.) Here is an example /etc/xrootd/auth_file : # This means that all the users have read access to the datasets, _except_ under /private u * <STORAGE PATH>/private -rl <STORAGE PATH> rl # Or the following, without a restricted /private dir # u * <STORAGE PATH> rl # This means that all the users have full access to their private home dirs u = <STORAGE PATH>/home/@=/ a # This means that the privileged 'xrootd' user can do everything # There must be at least one such user in order to create the # private dirs for users willing to store their data in the facility u xrootd <STORAGE PATH> a # This means that users in group 'biology' can do anything under the 'genomics' directory g biology <STORAGE PATH>/genomics a Replacing <STORAGE PATH> with the path to the directory that will contain data served by XRootD, e.g. /data/xrootdfs . This path is relative to the oss.localroot or all.localroot configuration values, if either one is defined in the xrootd config file. Note Specific paths need to be specified before generic paths; e.g., this does not work: u * rl /data/xrootdfs -rl /data/xrootdfs/private More generally, each configuration line of the auth file has the following form: idtype id path privs Field Description idtype Type of id. Use u for username, g for group, etc. id Username (or groupname). Use * for all users or = for user-specific capabilities, like home directories path The path prefix to be used for matching purposes. @= expands to the current user name before a path prefix match is attempted privs Letter list of privileges: a - all ; l - lookup ; d - delete ; n - rename ; i - insert ; r - read ; k - lock (not used) ; w - write ; - - prefix to remove specified privileges For more details or examples on how to use templated user options, see XRootd Authorization Database File . Ensure the auth file is owned by xrootd (if you have created file as root), and that it is not writable by others. root@host # chown xrootd:xrootd /etc/xrootd/auth_file root@host # chmod 0640 /etc/xrootd/auth_file # or 0644","title":"Authorization file"},{"location":"data/xrootd/xrootd-authorization/#enabling-xrootd-lcmaps-authorization","text":"The xrootd-lcmaps security plugin uses the lcmaps library and the LCMAPS VOMS plugin to authenticate and authorize users based on X509 certificates and VOMS attributes. Perform the following instructions on all data nodes: Install CA certificates and manage CRLs Copy your host certificate and key to /etc/grid-security/xrd/xrdcert.pem and /etc/grid-security/xrd/xrdkey.pem , respectively. Install and configure the LCMAPS VOMS plugin Install xrootd-lcmaps and necessary configuration: root@host # yum install xrootd-lcmaps vo-client Configure access rights for mapped users by creating and modifying the XRootD authorization file Restart the relevant services","title":"Enabling xrootd-lcmaps authorization"},{"location":"data/xrootd/xrootd-authorization/#verifying-xrootd-authorization","text":"To verify the LCMAPS security, run the following commands from a machine with your user certificate/key pair, xrootd-client , and voms-clients-cpp installed: Destroy any pre-existing proxies and attempt a copy to a directory (which we will refer to as <DESTINATION PATH> ) on the <XROOTD HOST> to verify failure: user@client $ voms-proxy-destroy user@client $ xrdcp /bin/bash root://<XROOTD HOST>/<DESTINATION PATH> 180213 13:56:49 396570 cryptossl_X509CreateProxy: EEC certificate has expired [0B/0B][100%][==================================================][0B/s] Run: [FATAL] Auth failed On the XRootD host, add your DN to /etc/grid-security/grid-mapfile Add a line to /etc/xrootd/auth_file to ensure the mapped user can write to <DESTINATION PATH> Restart the xrootd service. (See this section for more information of managing XRootD services.) Generate your proxy and verify that you can successfully transfer files: user@client $ voms-proxy-init user@client $ xrdcp /bin/sh root://<XROOTD HOST>/<DESTINATION PATH> [938.1kB/938.1kB][100%][==================================================][938.1kB/s] If your transfer does not succeed, run the previous command with --debug 2 for more information.","title":"Verifying XRootD Authorization"},{"location":"monitoring/advanced-rsv-configuration/","text":"Advanced RSV Configuration \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. About This Document \u00b6 Most site administrators will be able to configure RSV by editing /etc/osg/config.d/30-rsv.ini and running osg-configure as described in the RSV installation document . This document provides instructions for configuration beyond what osg-configure is able to do. Configuring metrics \u00b6 If you need to change the behavior of a metric you can edit the metric configuration files. These replace the spec files from previous versions of RSV. /etc/rsv/metrics - changes made to conf file in this directory named after a metric will affect the metric when run against all hosts /etc/rsv/metrics/<HOST> - changes made to conf files in this directory (named as the host FQDN) will affect the metric when run against the specific host The configuration files are in INI format and have two sections: a first one named after the metric with execution options a second one with the name including the \"args\" keyword, including parameters sent to the probe at invokation Changing the times a metric runs \u00b6 To change the time a metric runs set the cron-interval setting in the metric's conf file. Use man 5 crontab for a description of the format. For example, to change the org.osg.general.ping-host to run at a different time: [org.osg.general.ping-host] cron-interval = 45 * * * * [org.osg.general.ping-host args] #ping-count = #ping-timeout = Note Be sure to put the cron-interval setting in the [org.osg.general.ping-host] section, and not the [org.osg.general.ping-host args] section! The purpose of the \"args\" section is described in the \"passing extra parameters to a metric\" section below. After modifying the cron time of a metric you must restart RSV for the change to take effect. To see what times each of the metrics is running you can use rsv-control as follows: root@host# rsv-control -l --cron-times Metrics enabled for host: osg-edu.cs.wisc.edu:10443 | Cron times ----------------------------------------------------+-------------------- org.osg.srm.srmcp-readwrite | 28 * * * * org.osg.srm.srmping | 13,33,53 * * * * ... Passing extra parameters to a metric \u00b6 Any key=value pairs in the \"args\" section of the metric's conf file will be turned into command line parameters to the probe. For example, for this file: [org.osg.certificates.cacert-expiry args] warning-hours = 6 error-hours = 12 This would lead to the probe getting called with the command-line parameters --warning-hours 6 --error-hours 12 . Configure consumers \u00b6 There is a configuration file common to all consumers: /etc/rsv/consumers.conf . It is a file in INI format and possible entries are: Setting Values Details enabled <consumers> Comma-separated list of consumers to be enabled timestamp local If this is set to local, a record with a local timestamp will be supplied to the consumer. If this is set to any other value, or is not set, a record with the GMT will be created. Each consumer has a configuration file in /etc/rsv/consumers named after it. This allows to specify command lines and environment for the consumers. Some consumers may have also their own configuration file, usually in /etc/rsv/ . Below is an example for the Nagios consumer. Sending RSV records to Nagios \u00b6 Edit your /etc/rsv/rsv-nagios.conf file and fill in the appropriate information. The path of the configuration file is specified in /etc/rsv/consumers/nagios-consumer.conf . If your Nagios config file contains password information you will want to lock down the permissions. Here is a suggested way to do this (replace <RSVUSER> with the group of your RSV user ( rsvuser by default)): root@host# chown root:<RSVUSER> /etc/rsv/rsv-nagios.conf root@host# chmod 0440 /etc/rsv/rsv-nagios.conf In the configuration file at /etc/rsv/consumers/nagios-consumer.conf , check the following two settings: Make sure that the path to your config file is correct. It may be referencing a directory config instead of etc If you want to use rsv2nsca add the string \"--send-nsca\" to the args line. Enable and start the Nagios consumer by editing consumers.conf or by using rsv-control as follows: root@host# rsv-control --enable nagios-consumer The Nagios consumer will be started the next time that you start RSV. If you are already running RSV you can turn on the Nagios consumer immediately by running: root@host# rsv-control --on nagios-consumer To verify that the Nagios consumer is running you can run rsv-control -j . The log information for the Nagios consumer can be found in these files: /var/log/rsv/consumers/nagios-consumer.log /var/log/rsv/consumers/nagios-consumer.out /var/log/rsv/consumers/nagios-consumer.err General RSV configuration options \u00b6 You can configure the RSV framework using /etc/rsv/rsv.conf . It is a file in INI format and possible entries are: Setting Values Details user <username> The UNIX username that owns RSV. This is mandatory service-cert <path> Absolute path to the service certificate file. If this is set service-key and service-proxy must also be set. service-key <path> Absolute path to the service key file. This must be used with service-cert. service-proxy <path> Absolute path where the service proxy will be generated. This must be used with service-cert. proxy-file <path> Alternative to service-cert. The absolute path where the user proxy file is located. This will not be auto-regenerated. details-data-trim-length <integer> The number of bytes to trim the detailsData section to. If set to 0 no trimming will occur. job-timeout <integer> Time in seconds before a metric is killed. A metric that times out will return a CRITICAL status. Troubleshooting \u00b6 Important files locations \u00b6 Configuration files: File Description Location Comment RSV configuration directory /etc/rsv RSV configuration /etc/rsv/rsv.conf RSV framework configuration Consumers configuration in RSV /etc/rsv/consumers.conf Select the consumers and change generic options Consumers configuration /etc/rsv/consumers/<CONSUMERNAME> To change arguments and environment Generic metrics configuration /etc/rsv/metrics/<METRICNAME>.conf To change arguments and environment Host specific metrics configuration /etc/rsv/metrics/<HOSTNAME>/<METRICNAME>.conf To change arguments and environment when running on HOSTNAME Other files: File Description Location Comment Metric log files /var/log/rsv/metrics Consumer log files /var/log/rsv/consumers Initial configuration /etc/osg/config.d/30-rsv.ini Read by osg-configure Web files output /usr/share/rsv/www/ To find the metrics and the other files in RSV you can use also the RPM commands: rpm -ql rsv-metrics and rpm -ql rsv .","title":"Advanced RSV Configuration"},{"location":"monitoring/advanced-rsv-configuration/#advanced-rsv-configuration","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details.","title":"Advanced RSV Configuration"},{"location":"monitoring/advanced-rsv-configuration/#about-this-document","text":"Most site administrators will be able to configure RSV by editing /etc/osg/config.d/30-rsv.ini and running osg-configure as described in the RSV installation document . This document provides instructions for configuration beyond what osg-configure is able to do.","title":"About This Document"},{"location":"monitoring/advanced-rsv-configuration/#configuring-metrics","text":"If you need to change the behavior of a metric you can edit the metric configuration files. These replace the spec files from previous versions of RSV. /etc/rsv/metrics - changes made to conf file in this directory named after a metric will affect the metric when run against all hosts /etc/rsv/metrics/<HOST> - changes made to conf files in this directory (named as the host FQDN) will affect the metric when run against the specific host The configuration files are in INI format and have two sections: a first one named after the metric with execution options a second one with the name including the \"args\" keyword, including parameters sent to the probe at invokation","title":"Configuring metrics"},{"location":"monitoring/advanced-rsv-configuration/#changing-the-times-a-metric-runs","text":"To change the time a metric runs set the cron-interval setting in the metric's conf file. Use man 5 crontab for a description of the format. For example, to change the org.osg.general.ping-host to run at a different time: [org.osg.general.ping-host] cron-interval = 45 * * * * [org.osg.general.ping-host args] #ping-count = #ping-timeout = Note Be sure to put the cron-interval setting in the [org.osg.general.ping-host] section, and not the [org.osg.general.ping-host args] section! The purpose of the \"args\" section is described in the \"passing extra parameters to a metric\" section below. After modifying the cron time of a metric you must restart RSV for the change to take effect. To see what times each of the metrics is running you can use rsv-control as follows: root@host# rsv-control -l --cron-times Metrics enabled for host: osg-edu.cs.wisc.edu:10443 | Cron times ----------------------------------------------------+-------------------- org.osg.srm.srmcp-readwrite | 28 * * * * org.osg.srm.srmping | 13,33,53 * * * * ...","title":"Changing the times a metric runs"},{"location":"monitoring/advanced-rsv-configuration/#passing-extra-parameters-to-a-metric","text":"Any key=value pairs in the \"args\" section of the metric's conf file will be turned into command line parameters to the probe. For example, for this file: [org.osg.certificates.cacert-expiry args] warning-hours = 6 error-hours = 12 This would lead to the probe getting called with the command-line parameters --warning-hours 6 --error-hours 12 .","title":"Passing extra parameters to a metric"},{"location":"monitoring/advanced-rsv-configuration/#configure-consumers","text":"There is a configuration file common to all consumers: /etc/rsv/consumers.conf . It is a file in INI format and possible entries are: Setting Values Details enabled <consumers> Comma-separated list of consumers to be enabled timestamp local If this is set to local, a record with a local timestamp will be supplied to the consumer. If this is set to any other value, or is not set, a record with the GMT will be created. Each consumer has a configuration file in /etc/rsv/consumers named after it. This allows to specify command lines and environment for the consumers. Some consumers may have also their own configuration file, usually in /etc/rsv/ . Below is an example for the Nagios consumer.","title":"Configure consumers"},{"location":"monitoring/advanced-rsv-configuration/#sending-rsv-records-to-nagios","text":"Edit your /etc/rsv/rsv-nagios.conf file and fill in the appropriate information. The path of the configuration file is specified in /etc/rsv/consumers/nagios-consumer.conf . If your Nagios config file contains password information you will want to lock down the permissions. Here is a suggested way to do this (replace <RSVUSER> with the group of your RSV user ( rsvuser by default)): root@host# chown root:<RSVUSER> /etc/rsv/rsv-nagios.conf root@host# chmod 0440 /etc/rsv/rsv-nagios.conf In the configuration file at /etc/rsv/consumers/nagios-consumer.conf , check the following two settings: Make sure that the path to your config file is correct. It may be referencing a directory config instead of etc If you want to use rsv2nsca add the string \"--send-nsca\" to the args line. Enable and start the Nagios consumer by editing consumers.conf or by using rsv-control as follows: root@host# rsv-control --enable nagios-consumer The Nagios consumer will be started the next time that you start RSV. If you are already running RSV you can turn on the Nagios consumer immediately by running: root@host# rsv-control --on nagios-consumer To verify that the Nagios consumer is running you can run rsv-control -j . The log information for the Nagios consumer can be found in these files: /var/log/rsv/consumers/nagios-consumer.log /var/log/rsv/consumers/nagios-consumer.out /var/log/rsv/consumers/nagios-consumer.err","title":"Sending RSV records to Nagios"},{"location":"monitoring/advanced-rsv-configuration/#general-rsv-configuration-options","text":"You can configure the RSV framework using /etc/rsv/rsv.conf . It is a file in INI format and possible entries are: Setting Values Details user <username> The UNIX username that owns RSV. This is mandatory service-cert <path> Absolute path to the service certificate file. If this is set service-key and service-proxy must also be set. service-key <path> Absolute path to the service key file. This must be used with service-cert. service-proxy <path> Absolute path where the service proxy will be generated. This must be used with service-cert. proxy-file <path> Alternative to service-cert. The absolute path where the user proxy file is located. This will not be auto-regenerated. details-data-trim-length <integer> The number of bytes to trim the detailsData section to. If set to 0 no trimming will occur. job-timeout <integer> Time in seconds before a metric is killed. A metric that times out will return a CRITICAL status.","title":"General RSV configuration options"},{"location":"monitoring/advanced-rsv-configuration/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"monitoring/advanced-rsv-configuration/#important-files-locations","text":"Configuration files: File Description Location Comment RSV configuration directory /etc/rsv RSV configuration /etc/rsv/rsv.conf RSV framework configuration Consumers configuration in RSV /etc/rsv/consumers.conf Select the consumers and change generic options Consumers configuration /etc/rsv/consumers/<CONSUMERNAME> To change arguments and environment Generic metrics configuration /etc/rsv/metrics/<METRICNAME>.conf To change arguments and environment Host specific metrics configuration /etc/rsv/metrics/<HOSTNAME>/<METRICNAME>.conf To change arguments and environment when running on HOSTNAME Other files: File Description Location Comment Metric log files /var/log/rsv/metrics Consumer log files /var/log/rsv/consumers Initial configuration /etc/osg/config.d/30-rsv.ini Read by osg-configure Web files output /usr/share/rsv/www/ To find the metrics and the other files in RSV you can use also the RPM commands: rpm -ql rsv-metrics and rpm -ql rsv .","title":"Important files locations"},{"location":"monitoring/install-rsv-gwms-tester/","text":"Installing and Using the RSV GlideinWMS Tester \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. About This Guide \u00b6 The RSV GlideinWMS Tester (or Tester , in this document) is a tool that a VO front-end administrator can use to test remote sites for the ability to run the VO\u2019s jobs. It is particularly useful when setting up a VO for the first time or when changing the sites at which a VO\u2019s jobs can run. For a site to pass the test, it must successfully run a simple test job via the normal GlideinWMS mechanisms, in much the same way as a real VO job. Use this page to learn how to install, configure, and use the Tester for your VO front-end. Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the Reference section below as needed): Software: You must have a GlideinWMS Front-end installed Configuration: The GlideinWMS Front-end must be configured (a) to have at least one group that matches pilots to sites using DESIRED_SITES , and (b) to support the is_itb user job attribute Host choice: The Tester should be installed on its own host; a small Virtual Machine (VM) is ideal Service certificate: The Tester requires a host certificate at /etc/grid-security/hostcert.pem and an accompanying key at /etc/grid-security/hostkey.pem Network ports: Test jobs must be able to contact the tester using the HTCondor Shared Port on port 9615 (TCP), and you must be able to contact a web server on port 80 (TCP) to view test results. Installing the Tester \u00b6 The Tester software takes advantage of several other OSG software components, so the installation will also include OSG\u2019s site validation system (RSV), HTCondor, and the GlideinWMS pilot submission software. root@host # yum install rsv-gwms-tester Configuring the Tester \u00b6 Before you use the Tester, there are some one-time configuration steps to complete, one set on your GlideinWMS Front-end Central Manager host and one set on the Tester host. Configuring the GlideinWMS Front-end Central Manager \u00b6 Complete these steps on your GlideinWMS Front-end Central Manager host : Authorize the Tester host to connect to your Central Manager: root@host # glidecondor_addDN -allow-others -daemon <COMMENT> <TESTER_DN> condor Where COMMENT is a human-readable label for the Tester host (e.g., \u201cRSV GWMS Tester at myhost\u201d), and TESTER_DN is the Distinguished Name (DN) of the host certificate of your Tester host. Most likely, you will need to quote both of these values to protect them from the shell. For example: root@host # glidecondor_addDN -allow-others -daemon 'RSV GWMS Tester on Fermicloud' '/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov' condor Restart HTCondor to apply the changes On EL 6 systems: root@host # service condor restart On EL 7 systems: root@host # systemctl restart condor Add the new Tester to your GlideinWMS front-end configuration. Edit the file /etc/gwms-frontend/frontend.xml and add a line as follows within the <schedds> element <schedd DN=\"<TESTER_DN>\" fullname=\"<TESTER_HOSTNAME>\"> Where TESTER_DN is the Distinguished Name (DN) of the host certificate of your Tester host (as above), and TESTER_HOSTNAME is the fully qualified hostname of the Tester host. For example: <schedd DN=\"/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov\" fullname=\"fermicloud357.fnal.gov\"> Reconfigure your GlideinWMS front-end to apply the changes: root@host # service gwms-frontend reconfig Configuring the Tester host \u00b6 Complete the following steps on your Tester host : Configure the Tester for the VOs that your Front-end supports Edit the file /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf . The constraint line is an HTCondor ClassAd expression containing one stringListMember function per VO that your Front-end supports. If there is more than one VO, the function invocations are joined by the \u201clogical or\u201d operator, || . Edit the constraint line for your Front-end. For example, for a single VO named Foo , the line would be: constraint = stringListMember(\"Foo\", GLIDEIN_Supported_VOs) For two VOs named Foo and Bar , the line would be: constraint = stringListMember(\"Foo\", GLIDEIN_Supported_VOs) || stringListMember(\"Bar\", GLIDEIN_Supported_VOs) Do not change the other settings in this file, unless you have clear and specific reasons to do so. Authorize the central manager of your Front-end to connect to the tester host: root@host # glidecondor_addDN -allow-others -daemon <COMMENT> <CENTRAL_MGR> condor Where COMMENT is a human-readable identifier for the Central Manager, and CENTRAL_MGR is the Distinguished Name (DN) of the host certificate of your GlideinWMS Front-end\u2019s Central Manager host. Most likely, you will need to quote both of these values to protect them from the shell. For example: root@host # glidecondor_addDN -allow-others -daemon 'UCSD central manager DN' '/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=osg-ligo-1.t2.ucsd.edu' condor Configure the special HTCondor-RSV instance with your host IP address. Create the file /etc/condor/config.d/98_public_interface.config with this content: NETWORK_INTERFACE = <ADDRESS> CONDOR_HOST = <CENTRAL_MGR> Where ADDRESS is the IP address of your Tester host, and CENTRAL_MGR is the hostname of your GlideinWMS Front-end Central Manager. Enable the Tester\u2019s RSV probe: root@host # rsv-control --enable org.osg.local-gfactory-site-querying-local --host localhost Using the Tester \u00b6 There are at least two aspects of using the Tester: Managing the services that are associated with the Tester software Viewing results from the Tester Managing Tester services \u00b6 Because the Tester is built on other OSG software, there are a number of services in your installation. The specific services are: Software Service name Notes Apache HTTP Server httpd Web server for results HTCondor-Cron condor-cron cron-like jobs in HTCondor RSV rsv OSG site validator Viewing Tester results \u00b6 Once the Tester RSV probe is enabled and active, and the services listed above have been started, there are two kinds of RSV probes that run periodically: One probe asks the GlideinWMS factory for the up-to-date list of sites supported by your VO(s) \u2014 runs every 30 minutes One probe submits and monitors one test job to each site supported by your VO(s) \u2014 run every 60 minutes You can view the latest results of both probe types on an RSV results web page, or you can manually run the first probe to see the full list of sites. Viewing RSV results online \u00b6 To see the latest results, access https://<HOSTNAME> (where HOSTNAME is the name of your Tester host). There should be one result row per site supported by your VO(s), using the \u201corg.osg.general.dummy-vanilla-probe\u201d probe (aka metric ) There should be exactly one result row for the probe that fetches the list of sites, which is the \u201corg.osg.local-gfactory-site-querying-local\u201d probe (aka metric ) There is a legend for the background colors at the end of the page Ideally, each site supported by your VO(s) should be shown with a green background, which indicates that a Tester job ran at that site recently and successfully. There may be transient failures but if you notice a site in the failed state over multiple days, contact OSG Factory Operations ( osg-gfactory-support@physics.ucsd.edu ) about the failing site, including a link to your Tester RSV results page. To see detailed information from each probe, click on the probe name in the Metric column. To see the list of sites that are supported by your VO(s) and are being tested, click the \u201corg.osg.local-gfactory-site-querying-local\u201d link at the bottom of the list of probes. You can also run the probe manually, as described next. Listing supported sites manually \u00b6 To manually run the probe that fetches the list of sites supported by your VO(s), run the following command on your Tester host: root@host # rsv-control --run org.osg.local-gfactory-site-querying-local --host localhost The probe produces many lines of output, some of which are just about the probe execution itself. But look for lines like this: MSG: Updating configuration for host <SITE_NAME> Where <SITE_NAME> is the name of the site, and there should be one such line per site supported by your VO(s). Troubleshooting RSV-GWMS-Tester \u00b6 You can find more information on troubleshooting in the RSV troubleshooting section Logs and configuration: File Description Location Comment Condor Cron log files /var/log/condor-cron File Description Location Comment Metric configuration /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf To change arguments and environment Getting Help \u00b6 To get assistance, please use the this page . Reference \u00b6 Certificates \u00b6 Certificate User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem Host key root /etc/grid-security/hostkey.pem Find instructions to request a host certificate here .","title":"RSV GlideinWMS Tester"},{"location":"monitoring/install-rsv-gwms-tester/#installing-and-using-the-rsv-glideinwms-tester","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details.","title":"Installing and Using the RSV GlideinWMS Tester"},{"location":"monitoring/install-rsv-gwms-tester/#about-this-guide","text":"The RSV GlideinWMS Tester (or Tester , in this document) is a tool that a VO front-end administrator can use to test remote sites for the ability to run the VO\u2019s jobs. It is particularly useful when setting up a VO for the first time or when changing the sites at which a VO\u2019s jobs can run. For a site to pass the test, it must successfully run a simple test job via the normal GlideinWMS mechanisms, in much the same way as a real VO job. Use this page to learn how to install, configure, and use the Tester for your VO front-end.","title":"About This Guide"},{"location":"monitoring/install-rsv-gwms-tester/#before-starting","text":"Before starting the installation process, consider the following points (consulting the Reference section below as needed): Software: You must have a GlideinWMS Front-end installed Configuration: The GlideinWMS Front-end must be configured (a) to have at least one group that matches pilots to sites using DESIRED_SITES , and (b) to support the is_itb user job attribute Host choice: The Tester should be installed on its own host; a small Virtual Machine (VM) is ideal Service certificate: The Tester requires a host certificate at /etc/grid-security/hostcert.pem and an accompanying key at /etc/grid-security/hostkey.pem Network ports: Test jobs must be able to contact the tester using the HTCondor Shared Port on port 9615 (TCP), and you must be able to contact a web server on port 80 (TCP) to view test results.","title":"Before Starting"},{"location":"monitoring/install-rsv-gwms-tester/#installing-the-tester","text":"The Tester software takes advantage of several other OSG software components, so the installation will also include OSG\u2019s site validation system (RSV), HTCondor, and the GlideinWMS pilot submission software. root@host # yum install rsv-gwms-tester","title":"Installing the Tester"},{"location":"monitoring/install-rsv-gwms-tester/#configuring-the-tester","text":"Before you use the Tester, there are some one-time configuration steps to complete, one set on your GlideinWMS Front-end Central Manager host and one set on the Tester host.","title":"Configuring the Tester"},{"location":"monitoring/install-rsv-gwms-tester/#configuring-the-glideinwms-front-end-central-manager","text":"Complete these steps on your GlideinWMS Front-end Central Manager host : Authorize the Tester host to connect to your Central Manager: root@host # glidecondor_addDN -allow-others -daemon <COMMENT> <TESTER_DN> condor Where COMMENT is a human-readable label for the Tester host (e.g., \u201cRSV GWMS Tester at myhost\u201d), and TESTER_DN is the Distinguished Name (DN) of the host certificate of your Tester host. Most likely, you will need to quote both of these values to protect them from the shell. For example: root@host # glidecondor_addDN -allow-others -daemon 'RSV GWMS Tester on Fermicloud' '/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov' condor Restart HTCondor to apply the changes On EL 6 systems: root@host # service condor restart On EL 7 systems: root@host # systemctl restart condor Add the new Tester to your GlideinWMS front-end configuration. Edit the file /etc/gwms-frontend/frontend.xml and add a line as follows within the <schedds> element <schedd DN=\"<TESTER_DN>\" fullname=\"<TESTER_HOSTNAME>\"> Where TESTER_DN is the Distinguished Name (DN) of the host certificate of your Tester host (as above), and TESTER_HOSTNAME is the fully qualified hostname of the Tester host. For example: <schedd DN=\"/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov\" fullname=\"fermicloud357.fnal.gov\"> Reconfigure your GlideinWMS front-end to apply the changes: root@host # service gwms-frontend reconfig","title":"Configuring the GlideinWMS Front-end Central Manager"},{"location":"monitoring/install-rsv-gwms-tester/#configuring-the-tester-host","text":"Complete the following steps on your Tester host : Configure the Tester for the VOs that your Front-end supports Edit the file /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf . The constraint line is an HTCondor ClassAd expression containing one stringListMember function per VO that your Front-end supports. If there is more than one VO, the function invocations are joined by the \u201clogical or\u201d operator, || . Edit the constraint line for your Front-end. For example, for a single VO named Foo , the line would be: constraint = stringListMember(\"Foo\", GLIDEIN_Supported_VOs) For two VOs named Foo and Bar , the line would be: constraint = stringListMember(\"Foo\", GLIDEIN_Supported_VOs) || stringListMember(\"Bar\", GLIDEIN_Supported_VOs) Do not change the other settings in this file, unless you have clear and specific reasons to do so. Authorize the central manager of your Front-end to connect to the tester host: root@host # glidecondor_addDN -allow-others -daemon <COMMENT> <CENTRAL_MGR> condor Where COMMENT is a human-readable identifier for the Central Manager, and CENTRAL_MGR is the Distinguished Name (DN) of the host certificate of your GlideinWMS Front-end\u2019s Central Manager host. Most likely, you will need to quote both of these values to protect them from the shell. For example: root@host # glidecondor_addDN -allow-others -daemon 'UCSD central manager DN' '/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=osg-ligo-1.t2.ucsd.edu' condor Configure the special HTCondor-RSV instance with your host IP address. Create the file /etc/condor/config.d/98_public_interface.config with this content: NETWORK_INTERFACE = <ADDRESS> CONDOR_HOST = <CENTRAL_MGR> Where ADDRESS is the IP address of your Tester host, and CENTRAL_MGR is the hostname of your GlideinWMS Front-end Central Manager. Enable the Tester\u2019s RSV probe: root@host # rsv-control --enable org.osg.local-gfactory-site-querying-local --host localhost","title":"Configuring the Tester host"},{"location":"monitoring/install-rsv-gwms-tester/#using-the-tester","text":"There are at least two aspects of using the Tester: Managing the services that are associated with the Tester software Viewing results from the Tester","title":"Using the Tester"},{"location":"monitoring/install-rsv-gwms-tester/#managing-tester-services","text":"Because the Tester is built on other OSG software, there are a number of services in your installation. The specific services are: Software Service name Notes Apache HTTP Server httpd Web server for results HTCondor-Cron condor-cron cron-like jobs in HTCondor RSV rsv OSG site validator","title":"Managing Tester services"},{"location":"monitoring/install-rsv-gwms-tester/#viewing-tester-results","text":"Once the Tester RSV probe is enabled and active, and the services listed above have been started, there are two kinds of RSV probes that run periodically: One probe asks the GlideinWMS factory for the up-to-date list of sites supported by your VO(s) \u2014 runs every 30 minutes One probe submits and monitors one test job to each site supported by your VO(s) \u2014 run every 60 minutes You can view the latest results of both probe types on an RSV results web page, or you can manually run the first probe to see the full list of sites.","title":"Viewing Tester results"},{"location":"monitoring/install-rsv-gwms-tester/#viewing-rsv-results-online","text":"To see the latest results, access https://<HOSTNAME> (where HOSTNAME is the name of your Tester host). There should be one result row per site supported by your VO(s), using the \u201corg.osg.general.dummy-vanilla-probe\u201d probe (aka metric ) There should be exactly one result row for the probe that fetches the list of sites, which is the \u201corg.osg.local-gfactory-site-querying-local\u201d probe (aka metric ) There is a legend for the background colors at the end of the page Ideally, each site supported by your VO(s) should be shown with a green background, which indicates that a Tester job ran at that site recently and successfully. There may be transient failures but if you notice a site in the failed state over multiple days, contact OSG Factory Operations ( osg-gfactory-support@physics.ucsd.edu ) about the failing site, including a link to your Tester RSV results page. To see detailed information from each probe, click on the probe name in the Metric column. To see the list of sites that are supported by your VO(s) and are being tested, click the \u201corg.osg.local-gfactory-site-querying-local\u201d link at the bottom of the list of probes. You can also run the probe manually, as described next.","title":"Viewing RSV results online"},{"location":"monitoring/install-rsv-gwms-tester/#listing-supported-sites-manually","text":"To manually run the probe that fetches the list of sites supported by your VO(s), run the following command on your Tester host: root@host # rsv-control --run org.osg.local-gfactory-site-querying-local --host localhost The probe produces many lines of output, some of which are just about the probe execution itself. But look for lines like this: MSG: Updating configuration for host <SITE_NAME> Where <SITE_NAME> is the name of the site, and there should be one such line per site supported by your VO(s).","title":"Listing supported sites manually"},{"location":"monitoring/install-rsv-gwms-tester/#troubleshooting-rsv-gwms-tester","text":"You can find more information on troubleshooting in the RSV troubleshooting section Logs and configuration: File Description Location Comment Condor Cron log files /var/log/condor-cron File Description Location Comment Metric configuration /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf To change arguments and environment","title":"Troubleshooting RSV-GWMS-Tester"},{"location":"monitoring/install-rsv-gwms-tester/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"monitoring/install-rsv-gwms-tester/#reference","text":"","title":"Reference"},{"location":"monitoring/install-rsv-gwms-tester/#certificates","text":"Certificate User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem Host key root /etc/grid-security/hostkey.pem Find instructions to request a host certificate here .","title":"Certificates"},{"location":"monitoring/install-rsv/","text":"Installing, Configuring, Using, and Troubleshooting RSV \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. About This Guide \u00b6 The Resource and Service Validation (RSV) software helps a site administrator verify that certain site resources and services are working as expected. OSG recommends that sites install and run RSV, but it is optional; further, each site selects which specific tests (called probes ) to run. Use this page to learn more about RSV in general, and how to install, configure, run, test, and troubleshoot RSV from the OSG software repositories. For documentation on specific probes or on how to write your own probes, please check the Reference section . Introduction to RSV \u00b6 The Resource and Service Validation (RSV) software provides OSG site administrators a scalable and easy-to-maintain resource and service monitoring infrastructure. It consists of client tools that allow a site administrator to run tests against their site by providing a set of tests (which can run on the same or other hosts within a site), HTCondor-Cron for scheduling, and tools for collecting and storing the results (using Gratia). The client package is not installed by default and may be installed on a CE or other host. Generally, you configure the RSV client to run tests at scheduled time intervals and then it makes results available on a local website. Also, the client can upload test results to a central collector (see next item). Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If they do not exist already, the installation will create the Linux user IDs rsv and cndrcron Service certificate: The RSV service requires a service certificate ( /etc/grid-security/rsv/rsvcert.pem ) and matching key ( /etc/grid-security/rsv/rsvkey.pem ) Network ports: To view results, port 80 must accept incoming requests; outbound connectivity to tested services must work, too Host choice: Install RSV on your site CE unless you have specific reasons (e.g., performance) for installing on a separate host As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the RSV host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates Installing RSV \u00b6 An installation of RSV at a site consists of the RSV client software, the Apache web server, parts of HTCondor (for its cron-like scheduling capabilities), and various other small tools. To simplify installation, OSG provides a convenience RPM that installs all required software with a single command. Consider updating your local cache of Yum repository data and your existing RPM packages: root@host # yum clean all --enablerepo = \\* root@host # yum update Note The update command will update all packages on your system. If you have installed HTCondor already but not by RPM, install a special empty RPM to make RSV happy: root@host # yum install empty-condor --enablerepo = osg-empty Install RSV and related software: root@host # yum install rsv Configuring RSV \u00b6 After installation, there are some one-time configuration steps to tell RSV how to operate at your site. Edit /etc/osg/config.d/30-rsv.ini and follow the instructions in the file. There are detailed comments for each setting. In the simplest case \u2014 to monitor only your CE \u2014 set the htcondor_ce_hosts variable to the fully qualified hostname of your CE. If you have installed HTCondor already but not by RPM, specify the location of the Condor installation in 30-rsv.ini in the condor_location setting. If an HTCondor RPM is installed, you do not need to set condor_location . Complete the configuration using the osg-configure tool: root@host # osg-configure -v root@host # osg-configure -c Optional configuration \u00b6 The following configuration steps are optional and will likely not be required for setting up a small or typical site. If you do not need any of the following special configurations, skip to the section on using RSV . Generally speaking, read the ConfigureRsv page for more advanced configuration options. Configuring RSV to run probes using a remote server \u00b6 RSV monitors systems by running probes, which can run on the RSV host itself (the default case), via a separate batch system like HTCondor, or via a remote batch system using a Globus gatekeeper and its job manager. The last two options both can count those jobs and report them to, for example, Gratia. In this case, remember to: Add the RSV user rsv on all the systems where the probes may run, and Map the RSV service certificate to the user you intend to use for RSV. This should be a local user used exclusively for RSV and not belonging to an institutional VO to avoid for the RSV probes to be accounted as regular VO jobs in Gratia. This can be done in the configuration of the LCMAPS VOMS plugin on your CE. Configuring the RSV web server to use HTTPS instead of HTTP \u00b6 If you would like your local RSV web server to use HTTPS instead of the default HTTP (for compatibility or security reasons), complete the steps below. This procedure assumes that you already have an HTTP service certificate (or a copy of the host certificate) in /etc/grid-security/http/ . If not, omit the SSLCertificate* modifications below, and your web server will start with its own, self-signed certificate. Install mod_ssl : root@host # yum install mod_ssl Make an alternate set of HTTP service certificate files: root@host # cp -p /etc/grid-security/http/httpcert.pem /etc/grid-security/http/httpcert2.pem root@host # cp -p /etc/grid-security/http/httpkey.pem /etc/grid-security/http/httpkey2.pem root@host # chown apache:apache /etc/grid-security/http/http*2.pem Back up existing Apache configuration files: root@host # cp -p /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.orig root@host # cp -p /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.orig Change the default port for HTTP connections to 8000 by editing /etc/httpd/conf/httpd.conf Listen 8000 Set up HTTPS access by editing /etc/httpd/conf.d/ssl.conf : Listen 8443 <VirtualHost _default_:8443> SSLCertificateFile /etc/grid-security/http/httpcert2.pem SSLCertificateKeyFile /etc/grid-security/http/httpkey2.pem After these changes, when you start the Apache service, it will listening on ports 8000 (for HTTP) and 8443 (for HTTPS), rather than the default port 80 (for HTTP only). Warning if you make the changes above, you must restart the Apache server after each CA certificate update to pick up the changes. Using RSV \u00b6 Managing RSV and associated services \u00b6 In addition to the RSV service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation Apache httpd HTCondor-Cron condor-cron RSV rsv Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To \u2026 Run the command \u2026 Start a service service <SERVICE-NAME> start Stop a service service <SERVICE-NAME> stop Enable a service to start during boot chkconfig <SERVICE-NAME> on Disable a service from starting during boot chkconfig <SERVICE-NAME> off Where <SERVICE-NAME> is the name of the service on the table above. Running RSV manually \u00b6 Normally, the HTCondor-Cron scheduler runs RSV periodically. However, you can run RSV probes manually at any time: root@host # rsv-control --run --all-enabled If successful, results will be available from your local RSV web server (e.g., http://localhost/rsv ) and, if enabled (which is the default) on MyOSG . You can also run the metrics individually or pass special parameters as explained in the rsv-control document . Troubleshooting RSV \u00b6 To get assistance, use the help procedure . RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff. To use it: root@host# rsv-control --profile Running the rsv-profiler... OSG-RSV Profiler Analyzing... Making tarball (rsv-profiler.tar.gz) You can find more information on troubleshooting RSV in the rsv-control documentation . Note If you are getting assistance via the trouble ticket system, you must add a .txt extension to the tarball so it can be uploaded: Failed to send via Gratia \u00b6 If you see Failed to send record Failed to send via Gratia: Server unable to receive data: in /var/log/rsv/consumers/gratia-consumer.output you should process to disable the gratia consumer using the following commands root@host# rsv-control --disable --host <YOUR RSV HOST> gratia-consumer root@host# rsv-control --off --host <YOUR RSV HOST> gratia-consumer Change <YOUR RSV HOST> for the hostname of the server where you are installing RSV. Important file locations \u00b6 Logs and configuration: File Description Location Comment Metric log files /var/log/rsv/metrics Consumer log files /var/log/rsv/consumers HTML files /usr/share/rsv/www/ Available at http://your.host.example.com/rsv File Description Location Comment Initial configuration /etc/osg/config.d/30-rsv.ini Read by osg-configure RSV configuration /etc/rsv Generally files in this directory should not be edited directly. Use osg-configure instead. Metric configuration /etc/rsv/metrics/HOSTNAME/METRICNAME.conf To change arguments and environment To find the metrics and the other files in RSV you can use also the RPM commands: rpm -ql rsv-metrics and rpm -ql rsv . Getting more information from rsv-control \u00b6 The first step to getting more information is to run rsv-control with more verbosity. Use the --verbose ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are: 0 = print nothing 1 = print warnings and errors along with usual output of command being run (1 is the default level) 2 = adds informational messages 3 = full debugging output For example, here is the output when running a metric with -v2. Show detailed ouput [root@fermicloud016 condor]# rsv-control -r org.osg.general.osg-version -v 2 -u osg-edu.cs.wisc.edu INFO: Reading configuration file /etc/rsv/rsv.conf INFO: Reading configuration file /etc/rsv/consumers.conf INFO: Validating configuration: INFO: Validating user: INFO: Invoked as root. Switching to 'rsv' user (uid: 100 - gid: 102) INFO: Registered consumers: html-consumer, gratia-consumer INFO: Loading config file '/etc/rsv/meta/metrics/org.osg.general.osg-version.meta' INFO: Loading config file '/etc/rsv/metrics/org.osg.general.osg-version.conf' INFO: Optional config file '/etc/rsv/metrics/osg-edu.cs.wisc.edu/org.osg.general.osg-version.conf' does not exist INFO: Checking proxy: INFO: Using service certificate proxy INFO: Running command with timeout (1200 seconds): /usr/bin/openssl x509 -in /tmp/rsvproxy -noout -enddate -checkend 21600 INFO: Exit code of job: 0 INFO: Service certificate valid for at least 6 hours. INFO: Pinging host osg-edu.cs.wisc.edu: INFO: Running command with timeout (1200 seconds): /bin/ping -W 3 -c 1 osg-edu.cs.wisc.edu INFO: Exit code of job: 0 INFO: Ping successful Running metric org.osg.general.osg-version: INFO: Executing job remotely using Condor-G INFO: Setting up job environment: INFO: No environment setup declared INFO: Condor-G working directory: /var/tmp/rsv/condor_g-JiQthF INFO: Forming arguments: INFO: Arguments: '' INFO: List of files to transfer: /usr/libexec/rsv/probes/RSVMetric.pm INFO: Condor submission: Submitting job(s). 1 job(s) submitted to cluster 2. INFO: Trimming data to 10000 bytes because details-data-trim-length is set INFO: Creating record for html-consumer consumer at '/var/spool/rsv/html-consumer/org.osg.general.osg-version.7rgLfn' INFO: Creating record for gratia-consumer consumer at '/var/spool/rsv/gratia-consumer/org.osg.general.osg-version.-qelnL' INFO: Result: metricName: org.osg.general.osg-version metricType: status timestamp: 2012-01-25 16:12:40 CST metricStatus: OK serviceType: OSG-CE serviceURI: osg-edu.cs.wisc.edu gatheredAt: fermicloud016.fnal.gov summaryData: OK detailsData: OSG 1.2.26 EOT Getting Help \u00b6 To get assistance, please use this page and attach the output of rsv-control --profile : root@host # rsv-control --profile Running the rsv-profiler... OSG-RSV Profiler Analyzing... Making tarball (rsv-profiler.tar.gz) Reference \u00b6 Users \u00b6 The RSV installation will create two users unless they are already created. The users are created when the rsv and condor-cron packages are installed. User Comment rsv Runs the RSV tests; the RSV certificate (below) will need to be owned by this user cndrcron Runs the Condor Cron processes to schedule the running of the tests Note if you pre-create the RSV user, it should have a working shell. That is, it shouldn't have a default shell of /sbin/nologin . Warning If you manage your /etc/passwd file with configuration management software such as Puppet, CFEngine or 411, make sure the UID and GID in /etc/condor-cron/config.d/condor_ids matches the UID and GID of the cndrcron user and group in /etc/passwd . If it does not, create a file named /etc/condor-cron/config.d/condor_ids_override with the contents: CONDOR_IDS=UID.GID where UID and GID are the UID and GID of the cndrcron user and group. Certificates \u00b6 Certificate User that owns certificate Path to certificate RSV service certificate rsv /etc/grid-security/rsv/rsvcert.pem /etc/grid-security/rsv/rsvkey.pem Ensure an RSV service certificate is installed in /etc/grid-security/rsv/ and the certificate files are owned by the rsv user. Adjust the permissions if necessary (cert needs to be readable by all, key needs to be readable by nobody but owner). You may need another certificate owned by apache if you'd like an authenticated web server; see Configuring the RSV web server to use HTTPS instead of HTTP above. See instructions to request a service certificate. Networking \u00b6 Service Name Protocol Port Number Inbound Outbound Comment HTTP tcp 80 YES RSV runs an HTTP server (Apache) that publishes a page with the RSV testing results HTTP tcp 80 YES RSV pushes testing results to the OSG Gratia Collectors at opensciencegrid.org various various various YES Allow outbound network connection to all services that you want to test Or, if you'd rather have your RSV web page appear as https://...:8443/rsv/ like it used to in OSG 1.2, the first column above would be HTTPS / tcp / 8443 . See above for how to configure this.","title":"Install RSV"},{"location":"monitoring/install-rsv/#installing-configuring-using-and-troubleshooting-rsv","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details.","title":"Installing, Configuring, Using, and Troubleshooting RSV"},{"location":"monitoring/install-rsv/#about-this-guide","text":"The Resource and Service Validation (RSV) software helps a site administrator verify that certain site resources and services are working as expected. OSG recommends that sites install and run RSV, but it is optional; further, each site selects which specific tests (called probes ) to run. Use this page to learn more about RSV in general, and how to install, configure, run, test, and troubleshoot RSV from the OSG software repositories. For documentation on specific probes or on how to write your own probes, please check the Reference section .","title":"About This Guide"},{"location":"monitoring/install-rsv/#introduction-to-rsv","text":"The Resource and Service Validation (RSV) software provides OSG site administrators a scalable and easy-to-maintain resource and service monitoring infrastructure. It consists of client tools that allow a site administrator to run tests against their site by providing a set of tests (which can run on the same or other hosts within a site), HTCondor-Cron for scheduling, and tools for collecting and storing the results (using Gratia). The client package is not installed by default and may be installed on a CE or other host. Generally, you configure the RSV client to run tests at scheduled time intervals and then it makes results available on a local website. Also, the client can upload test results to a central collector (see next item).","title":"Introduction to RSV"},{"location":"monitoring/install-rsv/#before-starting","text":"Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If they do not exist already, the installation will create the Linux user IDs rsv and cndrcron Service certificate: The RSV service requires a service certificate ( /etc/grid-security/rsv/rsvcert.pem ) and matching key ( /etc/grid-security/rsv/rsvkey.pem ) Network ports: To view results, port 80 must accept incoming requests; outbound connectivity to tested services must work, too Host choice: Install RSV on your site CE unless you have specific reasons (e.g., performance) for installing on a separate host As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the RSV host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"monitoring/install-rsv/#installing-rsv","text":"An installation of RSV at a site consists of the RSV client software, the Apache web server, parts of HTCondor (for its cron-like scheduling capabilities), and various other small tools. To simplify installation, OSG provides a convenience RPM that installs all required software with a single command. Consider updating your local cache of Yum repository data and your existing RPM packages: root@host # yum clean all --enablerepo = \\* root@host # yum update Note The update command will update all packages on your system. If you have installed HTCondor already but not by RPM, install a special empty RPM to make RSV happy: root@host # yum install empty-condor --enablerepo = osg-empty Install RSV and related software: root@host # yum install rsv","title":"Installing RSV"},{"location":"monitoring/install-rsv/#configuring-rsv","text":"After installation, there are some one-time configuration steps to tell RSV how to operate at your site. Edit /etc/osg/config.d/30-rsv.ini and follow the instructions in the file. There are detailed comments for each setting. In the simplest case \u2014 to monitor only your CE \u2014 set the htcondor_ce_hosts variable to the fully qualified hostname of your CE. If you have installed HTCondor already but not by RPM, specify the location of the Condor installation in 30-rsv.ini in the condor_location setting. If an HTCondor RPM is installed, you do not need to set condor_location . Complete the configuration using the osg-configure tool: root@host # osg-configure -v root@host # osg-configure -c","title":"Configuring RSV"},{"location":"monitoring/install-rsv/#optional-configuration","text":"The following configuration steps are optional and will likely not be required for setting up a small or typical site. If you do not need any of the following special configurations, skip to the section on using RSV . Generally speaking, read the ConfigureRsv page for more advanced configuration options.","title":"Optional configuration"},{"location":"monitoring/install-rsv/#configuring-rsv-to-run-probes-using-a-remote-server","text":"RSV monitors systems by running probes, which can run on the RSV host itself (the default case), via a separate batch system like HTCondor, or via a remote batch system using a Globus gatekeeper and its job manager. The last two options both can count those jobs and report them to, for example, Gratia. In this case, remember to: Add the RSV user rsv on all the systems where the probes may run, and Map the RSV service certificate to the user you intend to use for RSV. This should be a local user used exclusively for RSV and not belonging to an institutional VO to avoid for the RSV probes to be accounted as regular VO jobs in Gratia. This can be done in the configuration of the LCMAPS VOMS plugin on your CE.","title":"Configuring RSV to run probes using a remote server"},{"location":"monitoring/install-rsv/#configuring-the-rsv-web-server-to-use-https-instead-of-http","text":"If you would like your local RSV web server to use HTTPS instead of the default HTTP (for compatibility or security reasons), complete the steps below. This procedure assumes that you already have an HTTP service certificate (or a copy of the host certificate) in /etc/grid-security/http/ . If not, omit the SSLCertificate* modifications below, and your web server will start with its own, self-signed certificate. Install mod_ssl : root@host # yum install mod_ssl Make an alternate set of HTTP service certificate files: root@host # cp -p /etc/grid-security/http/httpcert.pem /etc/grid-security/http/httpcert2.pem root@host # cp -p /etc/grid-security/http/httpkey.pem /etc/grid-security/http/httpkey2.pem root@host # chown apache:apache /etc/grid-security/http/http*2.pem Back up existing Apache configuration files: root@host # cp -p /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.orig root@host # cp -p /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.orig Change the default port for HTTP connections to 8000 by editing /etc/httpd/conf/httpd.conf Listen 8000 Set up HTTPS access by editing /etc/httpd/conf.d/ssl.conf : Listen 8443 <VirtualHost _default_:8443> SSLCertificateFile /etc/grid-security/http/httpcert2.pem SSLCertificateKeyFile /etc/grid-security/http/httpkey2.pem After these changes, when you start the Apache service, it will listening on ports 8000 (for HTTP) and 8443 (for HTTPS), rather than the default port 80 (for HTTP only). Warning if you make the changes above, you must restart the Apache server after each CA certificate update to pick up the changes.","title":"Configuring the RSV web server to use HTTPS instead of HTTP"},{"location":"monitoring/install-rsv/#using-rsv","text":"","title":"Using RSV"},{"location":"monitoring/install-rsv/#managing-rsv-and-associated-services","text":"In addition to the RSV service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation Apache httpd HTCondor-Cron condor-cron RSV rsv Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To \u2026 Run the command \u2026 Start a service service <SERVICE-NAME> start Stop a service service <SERVICE-NAME> stop Enable a service to start during boot chkconfig <SERVICE-NAME> on Disable a service from starting during boot chkconfig <SERVICE-NAME> off Where <SERVICE-NAME> is the name of the service on the table above.","title":"Managing RSV and associated services"},{"location":"monitoring/install-rsv/#running-rsv-manually","text":"Normally, the HTCondor-Cron scheduler runs RSV periodically. However, you can run RSV probes manually at any time: root@host # rsv-control --run --all-enabled If successful, results will be available from your local RSV web server (e.g., http://localhost/rsv ) and, if enabled (which is the default) on MyOSG . You can also run the metrics individually or pass special parameters as explained in the rsv-control document .","title":"Running RSV manually"},{"location":"monitoring/install-rsv/#troubleshooting-rsv","text":"To get assistance, use the help procedure . RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff. To use it: root@host# rsv-control --profile Running the rsv-profiler... OSG-RSV Profiler Analyzing... Making tarball (rsv-profiler.tar.gz) You can find more information on troubleshooting RSV in the rsv-control documentation . Note If you are getting assistance via the trouble ticket system, you must add a .txt extension to the tarball so it can be uploaded:","title":"Troubleshooting RSV"},{"location":"monitoring/install-rsv/#failed-to-send-via-gratia","text":"If you see Failed to send record Failed to send via Gratia: Server unable to receive data: in /var/log/rsv/consumers/gratia-consumer.output you should process to disable the gratia consumer using the following commands root@host# rsv-control --disable --host <YOUR RSV HOST> gratia-consumer root@host# rsv-control --off --host <YOUR RSV HOST> gratia-consumer Change <YOUR RSV HOST> for the hostname of the server where you are installing RSV.","title":"Failed to send via Gratia"},{"location":"monitoring/install-rsv/#important-file-locations","text":"Logs and configuration: File Description Location Comment Metric log files /var/log/rsv/metrics Consumer log files /var/log/rsv/consumers HTML files /usr/share/rsv/www/ Available at http://your.host.example.com/rsv File Description Location Comment Initial configuration /etc/osg/config.d/30-rsv.ini Read by osg-configure RSV configuration /etc/rsv Generally files in this directory should not be edited directly. Use osg-configure instead. Metric configuration /etc/rsv/metrics/HOSTNAME/METRICNAME.conf To change arguments and environment To find the metrics and the other files in RSV you can use also the RPM commands: rpm -ql rsv-metrics and rpm -ql rsv .","title":"Important file locations"},{"location":"monitoring/install-rsv/#getting-more-information-from-rsv-control","text":"The first step to getting more information is to run rsv-control with more verbosity. Use the --verbose ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are: 0 = print nothing 1 = print warnings and errors along with usual output of command being run (1 is the default level) 2 = adds informational messages 3 = full debugging output For example, here is the output when running a metric with -v2. Show detailed ouput [root@fermicloud016 condor]# rsv-control -r org.osg.general.osg-version -v 2 -u osg-edu.cs.wisc.edu INFO: Reading configuration file /etc/rsv/rsv.conf INFO: Reading configuration file /etc/rsv/consumers.conf INFO: Validating configuration: INFO: Validating user: INFO: Invoked as root. Switching to 'rsv' user (uid: 100 - gid: 102) INFO: Registered consumers: html-consumer, gratia-consumer INFO: Loading config file '/etc/rsv/meta/metrics/org.osg.general.osg-version.meta' INFO: Loading config file '/etc/rsv/metrics/org.osg.general.osg-version.conf' INFO: Optional config file '/etc/rsv/metrics/osg-edu.cs.wisc.edu/org.osg.general.osg-version.conf' does not exist INFO: Checking proxy: INFO: Using service certificate proxy INFO: Running command with timeout (1200 seconds): /usr/bin/openssl x509 -in /tmp/rsvproxy -noout -enddate -checkend 21600 INFO: Exit code of job: 0 INFO: Service certificate valid for at least 6 hours. INFO: Pinging host osg-edu.cs.wisc.edu: INFO: Running command with timeout (1200 seconds): /bin/ping -W 3 -c 1 osg-edu.cs.wisc.edu INFO: Exit code of job: 0 INFO: Ping successful Running metric org.osg.general.osg-version: INFO: Executing job remotely using Condor-G INFO: Setting up job environment: INFO: No environment setup declared INFO: Condor-G working directory: /var/tmp/rsv/condor_g-JiQthF INFO: Forming arguments: INFO: Arguments: '' INFO: List of files to transfer: /usr/libexec/rsv/probes/RSVMetric.pm INFO: Condor submission: Submitting job(s). 1 job(s) submitted to cluster 2. INFO: Trimming data to 10000 bytes because details-data-trim-length is set INFO: Creating record for html-consumer consumer at '/var/spool/rsv/html-consumer/org.osg.general.osg-version.7rgLfn' INFO: Creating record for gratia-consumer consumer at '/var/spool/rsv/gratia-consumer/org.osg.general.osg-version.-qelnL' INFO: Result: metricName: org.osg.general.osg-version metricType: status timestamp: 2012-01-25 16:12:40 CST metricStatus: OK serviceType: OSG-CE serviceURI: osg-edu.cs.wisc.edu gatheredAt: fermicloud016.fnal.gov summaryData: OK detailsData: OSG 1.2.26 EOT","title":"Getting more information from rsv-control"},{"location":"monitoring/install-rsv/#getting-help","text":"To get assistance, please use this page and attach the output of rsv-control --profile : root@host # rsv-control --profile Running the rsv-profiler... OSG-RSV Profiler Analyzing... Making tarball (rsv-profiler.tar.gz)","title":"Getting Help"},{"location":"monitoring/install-rsv/#reference","text":"","title":"Reference"},{"location":"monitoring/install-rsv/#users","text":"The RSV installation will create two users unless they are already created. The users are created when the rsv and condor-cron packages are installed. User Comment rsv Runs the RSV tests; the RSV certificate (below) will need to be owned by this user cndrcron Runs the Condor Cron processes to schedule the running of the tests Note if you pre-create the RSV user, it should have a working shell. That is, it shouldn't have a default shell of /sbin/nologin . Warning If you manage your /etc/passwd file with configuration management software such as Puppet, CFEngine or 411, make sure the UID and GID in /etc/condor-cron/config.d/condor_ids matches the UID and GID of the cndrcron user and group in /etc/passwd . If it does not, create a file named /etc/condor-cron/config.d/condor_ids_override with the contents: CONDOR_IDS=UID.GID where UID and GID are the UID and GID of the cndrcron user and group.","title":"Users"},{"location":"monitoring/install-rsv/#certificates","text":"Certificate User that owns certificate Path to certificate RSV service certificate rsv /etc/grid-security/rsv/rsvcert.pem /etc/grid-security/rsv/rsvkey.pem Ensure an RSV service certificate is installed in /etc/grid-security/rsv/ and the certificate files are owned by the rsv user. Adjust the permissions if necessary (cert needs to be readable by all, key needs to be readable by nobody but owner). You may need another certificate owned by apache if you'd like an authenticated web server; see Configuring the RSV web server to use HTTPS instead of HTTP above. See instructions to request a service certificate.","title":"Certificates"},{"location":"monitoring/install-rsv/#networking","text":"Service Name Protocol Port Number Inbound Outbound Comment HTTP tcp 80 YES RSV runs an HTTP server (Apache) that publishes a page with the RSV testing results HTTP tcp 80 YES RSV pushes testing results to the OSG Gratia Collectors at opensciencegrid.org various various various YES Allow outbound network connection to all services that you want to test Or, if you'd rather have your RSV web page appear as https://...:8443/rsv/ like it used to in OSG 1.2, the first column above would be HTTPS / tcp / 8443 . See above for how to configure this.","title":"Networking"},{"location":"monitoring/rsv-control/","text":"Using rsv-control \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. Overview \u00b6 This document is for System Administrators. It details the usage of the rsv-control command for enabling, disabling, testing and running RSV probes. rsv-control provides an interface to many RSV tasks. rsv-control can view RSV jobs, run metrics, enable or disable metrics and consumers, and allow advanced configuration. Warning rsv-control can be used to configure RSV as described here and in the advanced configuration document . Most site admins will be able to configure RSV by editing /etc/osg/config.d/30-rsv.ini and running osg-configure as described in the installation doc . Using rsv-control to configure is for advanced RSV use including enabling non-default metrics. Admins who don't use rsv-control for configuration can still use it to view their RSV jobs, run RSV tests, and help debug RSV problems. Anyone can view the jobs, but you must be root or the RSV user ( rsv by default) to execute other commands, e.g. run, enable and disable probes, or to turn RSV on and off. Viewing RSV jobs \u00b6 rsv-control provides two different views: viewing the desired state and viewing the current actual state. Desired = what metrics and consumers will start the next time RSV is started Actual = what metrics and consumers are currently running Desired state \u00b6 To view the desired state, use the --list ( -l for short) flag. This will create one table for each host showing the metrics that are enabled to run against that host. root@host# rsv-control --list Metrics enabled for host: osgitb1.nhn.ou.edu | Service ----------------------------------------------------------+-------------------- org.osg.batch.jobmanager-default-status | OSG-CE org.osg.batch.jobmanagers-available | OSG-CE org.osg.certificates.cacert-expiry | OSG-CE org.osg.certificates.crl-expiry | OSG-CE org.osg.general.osg-directories-CE-permissions | OSG-CE org.osg.general.osg-version | OSG-CE org.osg.general.ping-host | OSG-CE org.osg.general.vdt-version | OSG-CE org.osg.general.vo-supported | OSG-CE org.osg.globus.gram-authentication | OSG-CE org.osg.globus.gridftp-simple | OSG-GridFTP org.osg.gratia.condor | OSG-CE org.osg.gratia.metric | OSG-CE Metrics enabled for host: osg-edu.cs.wisc.edu:10443 | Service ----------------------------------------------------------+-------------------- org.osg.srm.srmcp-readwrite | OSG-SRM org.osg.srm.srmping | OSG-SRM Other options: To view all installed metrics use the --all ( -a ) flag along with --list . This will print an extra table showing metrics that are disabled on all hosts. If you are having problems with the output being truncated, try the --wide ( -w ) flag. Actual state \u00b6 To view the current, running state of RSV jobs, use the --job-list flag ( -j for short). This will show all metrics and consumers running in RSV. (It queries the underlying Condor Cron system that we use to run the metrics). root@host# rsv-control --job-list Hostname: osg-edu.cs.wisc.edu ID OWNER ST NEXT RUN TIME METRIC 154.0 rsvuser I 11-19 12:15 org.osg.certificates.cacert-expiry 155.0 rsvuser R 11-19 11:23 org.osg.gratia.metric 156.0 rsvuser I 11-19 18:47 org.osg.general.vdt-version 157.0 rsvuser I 11-19 12:30 org.osg.certificates.crl-expiry 158.0 rsvuser I 11-19 11:31 org.osg.globus.gram-authentication 159.0 rsvuser I 11-19 11:41 org.osg.general.osg-version 160.0 rsvuser R 11-19 11:25 org.osg.batch.jobmanager-default-status 161.0 rsvuser I 11-20 04:59 org.osg.batch.jobmanagers-available 162.0 rsvuser I 11-19 11:37 org.osg.general.osg-directories-CE-permissions 163.0 rsvuser I 11-19 12:08 org.osg.globus.gridftp-simple 164.0 rsvuser I 11-19 12:09 org.osg.gratia.condor 165.0 rsvuser R 11-19 11:27 org.osg.general.ping-host 166.0 rsvuser I 11-19 18:47 org.osg.general.vo-supported Hostname: osg-edu.cs.wisc.edu:10443 ID OWNER ST NEXT RUN TIME METRIC 113.0 rsvuser I 11-19 11:33 org.osg.srm.srmping 114.0 rsvuser R 11-19 11:28 org.osg.srm.srmcp-readwrite ID OWNER ST CONSUMER 198.0 rsvuser R html-consumer 199.0 rsvuser R gratia-consumer The ST field indicates the current job status: R = the metric is currently running I = the metric is idle and will be run at the next scheduled interval Any other letter may indicate a problem Consumers will always appear to be running even though they will only run once every five minutes. Running a metric \u00b6 rsv-control can be used to run metrics one time against a host. This can be useful for: updating the status of a metric that had a problem instead of waiting until the next scheduled run time testing a metric against a host before deciding whether to enable it Note that the record for each run will be published to all active consumers . That is, it will be published to Gratia or will show up on your local web page, if you have those enabled. Simplest test \u00b6 Use the --run ( -r ) flag. You must also provide the --host flag. The syntax is: rsv-control --run --host <HOST> <METRIC> [ <METRIC2> ...] where <METRIC> is the full metric name (e.g. org.osg.general.osg-version ). You can get the metric names from the --list output. root@host# rsv-control --run \\ --host osg-edu.cs.wisc.edu org.osg.general.osg-version Running metric org.osg.general.osg-version: metricName: org.osg.general.osg-version metricType: status timestamp: 2010-11-19 11:40:19 CST metricStatus: OK serviceType: OSG-CE serviceURI: osg-edu.cs.wisc.edu gatheredAt: vdt-itb.cs.wisc.edu summaryData: OK detailsData: OSG 1.2.15 EOT Note the metricStatus in the example above: that's where you can see if it was successful or not. In this case, it was successful, because it printed OK. You may run multiple metrics against a single host by specifying multiple metrics to rsv-control . In order to run metrics against multiple hosts you must run rsv-control multiple times, once for each host. Running all enabled metrics \u00b6 When RSV is first installed it can take up to a day for each enabled metric to run once. A new option is provided to force each metric to run immediately, for all hosts. Use the --all-enabled flag along with --run . With this option it is not necessary to specify a host - all enabled metrics for all configured hosts will be run (in fact, if you do specify a host it will be ignored). root@host# rsv-control -r --all-enabled Running metric org.osg.certificates.cacert-expiry (1 of 15) metricName: org.osg.certificates.cacert-expiry metricType: status timestamp: 2010-11-19 13:44:08 CST metricStatus: OK serviceType: OSG-CE serviceURI: osg-edu.cs.wisc.edu gatheredAt: vdt-itb.cs.wisc.edu summaryData: OK detailsData: Security Probe Version: 1.1 OK: CAs are in sync with OSG distribution EOT ... Passing extra configuration \u00b6 If you want to pass extra configuration when running a metric without editing its configuration file you can make an INI-formatted file and pass it on the command line. For example, you can make a file like this for the org.osg.srm.srmclient-ping metric (tmp-srm.ini): [org.osg.srm.srmclient-ping args] srm-destination-dir = /srmcache/~ srm-webservice-path = srm/v2/server Then use the --extra-config-file parameter and pass the path to the INI file: root@host# rsv-control -r --extra-config-file tmp-srm.ini \\ --host osg-edu.cs.wisc.edu:10443 org.osg.srm.srmclient-ping Running metric org.osg.srm.srmclient-ping: metricName: org.osg.srm.srmclient-ping metricType: status timestamp: 2010-11-19 14:12:35 CST metricStatus: OK serviceType: OSG-SRM serviceURI: osg-edu.cs.wisc.edu:10443 gatheredAt: vdt-itb.cs.wisc.edu summaryData: OK detailsData: SRM server running on osg-edu.cs.wisc.edu is alive and responding to the srmping command. . Details: Storage Resource Manager (SRM) Client version 2.1.5-16 Copyright (c) 2002-2009 Fermi National Accelerator Laboratory ... Enabling and disabling metrics and consumers \u00b6 Metrics and consumers can be enabled or disabled by rsv-control using the --enable and --disable flags. Note that \"enable\" and \"disable\" are desired states (this is similar to osg-control ). After enabling a metric you should turn it on if you want it to be running immediately. After disabling a metric that is running, you should still turn it off (a message will print after each of these actions to remind you of this behavior). Enabling \u00b6 The syntax for enabling metrics looks similar to the syntax for running metrics: rsv-control --enable --host <HOST> <METRIC> [ <METRIC2> ...] You must provide a host to enable the metric against (in order to enable a metric on multiple hosts you must run rsv-control once per host). root@host# rsv-control --enable \\ --host osg-edu.cs.wisc.edu org.osg.gip.consistency Enabling metric 'org.osg.gip.consistency' for host 'osg-edu.cs.wisc.edu' One or more metrics have been enabled and will be started the next time RSV is started. To turn them on immediately run 'rsv-control --on'. Consumers do not run against a specific host, they process records for all hosts. When enabling consumers a host is not required (if a host is passed it will be ignored). root@host# rsv-control --enable nagios-consumer Enabling consumer nagios-consumer Disabling \u00b6 The syntax for disabling metrics looks similar to the syntax for running metrics: rsv-control --disable --host <HOST> <METRIC> [ <METRIC2> ...] You must provide a host to disable the metric against (in order to disable a metric on multiple hosts you must run rsv-control once per host). root@host# rsv-control --disable \\ --host vdt-itb.cs.wisc.edu org.osg.local.containercert-expiry Disabling metric 'org.osg.local.containercert-expiry' for host 'vdt-itb.cs.wisc.edu' One or more metrics have been disabled and will not start the next time RSV is started. You may still need to turn them off if they are currently running. Consumers do not run against a specific host, they process records for all hosts. When disabling consumers a host is not required (if a host is passed it will be ignored). root@host# rsv-control --disable html-consumer gratia-consumer Disabling consumer html-consumer Disabling consumer gratia-consumer Consumer already disabled Metrics and consumers can both be listed in the same disable command. Troubleshooting \u00b6 Getting more information from rsv-control \u00b6 The first step to getting more information is to run rsv-control with more verbosity. Use the --verbose ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are: 0 = print nothing 1 = print warnings and errors along with usual output of command being run (1 is the default level) 2 = adds informational messages 3 = full debugging output Using the RSV verify tool \u00b6 The --verify flag will run some basic checks for your RSV installation: root@host# rsv-control --verify Testing if Condor-Cron is running... OK Testing if metrics are running... OK (98 running metrics) Testing if consumers are running... OK (1 running consumers) Checking which consumers are configured... The following consumers are enabled: html-consumer WARNING: The gratia-consumer is not enabled. This indicates that your resource is not reporting to OSG. This tool is still under development and it does only basic checks, but it is a good first step when debugging issues. Running the RSV profiler \u00b6 RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff. To use it: root@host# rsv-control --profile Running the rsv-profiler... OSG-RSV Profiler Analyzing... Making tarball (rsv-profiler.tar.gz) Note If you are getting assistance via the trouble ticket system, you must add a .txt extension to the tarball so it can be uploaded. root@host# mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt","title":"Manage RSV via rsv-control"},{"location":"monitoring/rsv-control/#using-rsv-control","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details.","title":"Using rsv-control"},{"location":"monitoring/rsv-control/#overview","text":"This document is for System Administrators. It details the usage of the rsv-control command for enabling, disabling, testing and running RSV probes. rsv-control provides an interface to many RSV tasks. rsv-control can view RSV jobs, run metrics, enable or disable metrics and consumers, and allow advanced configuration. Warning rsv-control can be used to configure RSV as described here and in the advanced configuration document . Most site admins will be able to configure RSV by editing /etc/osg/config.d/30-rsv.ini and running osg-configure as described in the installation doc . Using rsv-control to configure is for advanced RSV use including enabling non-default metrics. Admins who don't use rsv-control for configuration can still use it to view their RSV jobs, run RSV tests, and help debug RSV problems. Anyone can view the jobs, but you must be root or the RSV user ( rsv by default) to execute other commands, e.g. run, enable and disable probes, or to turn RSV on and off.","title":"Overview"},{"location":"monitoring/rsv-control/#viewing-rsv-jobs","text":"rsv-control provides two different views: viewing the desired state and viewing the current actual state. Desired = what metrics and consumers will start the next time RSV is started Actual = what metrics and consumers are currently running","title":"Viewing RSV jobs"},{"location":"monitoring/rsv-control/#desired-state","text":"To view the desired state, use the --list ( -l for short) flag. This will create one table for each host showing the metrics that are enabled to run against that host. root@host# rsv-control --list Metrics enabled for host: osgitb1.nhn.ou.edu | Service ----------------------------------------------------------+-------------------- org.osg.batch.jobmanager-default-status | OSG-CE org.osg.batch.jobmanagers-available | OSG-CE org.osg.certificates.cacert-expiry | OSG-CE org.osg.certificates.crl-expiry | OSG-CE org.osg.general.osg-directories-CE-permissions | OSG-CE org.osg.general.osg-version | OSG-CE org.osg.general.ping-host | OSG-CE org.osg.general.vdt-version | OSG-CE org.osg.general.vo-supported | OSG-CE org.osg.globus.gram-authentication | OSG-CE org.osg.globus.gridftp-simple | OSG-GridFTP org.osg.gratia.condor | OSG-CE org.osg.gratia.metric | OSG-CE Metrics enabled for host: osg-edu.cs.wisc.edu:10443 | Service ----------------------------------------------------------+-------------------- org.osg.srm.srmcp-readwrite | OSG-SRM org.osg.srm.srmping | OSG-SRM Other options: To view all installed metrics use the --all ( -a ) flag along with --list . This will print an extra table showing metrics that are disabled on all hosts. If you are having problems with the output being truncated, try the --wide ( -w ) flag.","title":"Desired state"},{"location":"monitoring/rsv-control/#actual-state","text":"To view the current, running state of RSV jobs, use the --job-list flag ( -j for short). This will show all metrics and consumers running in RSV. (It queries the underlying Condor Cron system that we use to run the metrics). root@host# rsv-control --job-list Hostname: osg-edu.cs.wisc.edu ID OWNER ST NEXT RUN TIME METRIC 154.0 rsvuser I 11-19 12:15 org.osg.certificates.cacert-expiry 155.0 rsvuser R 11-19 11:23 org.osg.gratia.metric 156.0 rsvuser I 11-19 18:47 org.osg.general.vdt-version 157.0 rsvuser I 11-19 12:30 org.osg.certificates.crl-expiry 158.0 rsvuser I 11-19 11:31 org.osg.globus.gram-authentication 159.0 rsvuser I 11-19 11:41 org.osg.general.osg-version 160.0 rsvuser R 11-19 11:25 org.osg.batch.jobmanager-default-status 161.0 rsvuser I 11-20 04:59 org.osg.batch.jobmanagers-available 162.0 rsvuser I 11-19 11:37 org.osg.general.osg-directories-CE-permissions 163.0 rsvuser I 11-19 12:08 org.osg.globus.gridftp-simple 164.0 rsvuser I 11-19 12:09 org.osg.gratia.condor 165.0 rsvuser R 11-19 11:27 org.osg.general.ping-host 166.0 rsvuser I 11-19 18:47 org.osg.general.vo-supported Hostname: osg-edu.cs.wisc.edu:10443 ID OWNER ST NEXT RUN TIME METRIC 113.0 rsvuser I 11-19 11:33 org.osg.srm.srmping 114.0 rsvuser R 11-19 11:28 org.osg.srm.srmcp-readwrite ID OWNER ST CONSUMER 198.0 rsvuser R html-consumer 199.0 rsvuser R gratia-consumer The ST field indicates the current job status: R = the metric is currently running I = the metric is idle and will be run at the next scheduled interval Any other letter may indicate a problem Consumers will always appear to be running even though they will only run once every five minutes.","title":"Actual state"},{"location":"monitoring/rsv-control/#running-a-metric","text":"rsv-control can be used to run metrics one time against a host. This can be useful for: updating the status of a metric that had a problem instead of waiting until the next scheduled run time testing a metric against a host before deciding whether to enable it Note that the record for each run will be published to all active consumers . That is, it will be published to Gratia or will show up on your local web page, if you have those enabled.","title":"Running a metric"},{"location":"monitoring/rsv-control/#simplest-test","text":"Use the --run ( -r ) flag. You must also provide the --host flag. The syntax is: rsv-control --run --host <HOST> <METRIC> [ <METRIC2> ...] where <METRIC> is the full metric name (e.g. org.osg.general.osg-version ). You can get the metric names from the --list output. root@host# rsv-control --run \\ --host osg-edu.cs.wisc.edu org.osg.general.osg-version Running metric org.osg.general.osg-version: metricName: org.osg.general.osg-version metricType: status timestamp: 2010-11-19 11:40:19 CST metricStatus: OK serviceType: OSG-CE serviceURI: osg-edu.cs.wisc.edu gatheredAt: vdt-itb.cs.wisc.edu summaryData: OK detailsData: OSG 1.2.15 EOT Note the metricStatus in the example above: that's where you can see if it was successful or not. In this case, it was successful, because it printed OK. You may run multiple metrics against a single host by specifying multiple metrics to rsv-control . In order to run metrics against multiple hosts you must run rsv-control multiple times, once for each host.","title":"Simplest test"},{"location":"monitoring/rsv-control/#running-all-enabled-metrics","text":"When RSV is first installed it can take up to a day for each enabled metric to run once. A new option is provided to force each metric to run immediately, for all hosts. Use the --all-enabled flag along with --run . With this option it is not necessary to specify a host - all enabled metrics for all configured hosts will be run (in fact, if you do specify a host it will be ignored). root@host# rsv-control -r --all-enabled Running metric org.osg.certificates.cacert-expiry (1 of 15) metricName: org.osg.certificates.cacert-expiry metricType: status timestamp: 2010-11-19 13:44:08 CST metricStatus: OK serviceType: OSG-CE serviceURI: osg-edu.cs.wisc.edu gatheredAt: vdt-itb.cs.wisc.edu summaryData: OK detailsData: Security Probe Version: 1.1 OK: CAs are in sync with OSG distribution EOT ...","title":"Running all enabled metrics"},{"location":"monitoring/rsv-control/#passing-extra-configuration","text":"If you want to pass extra configuration when running a metric without editing its configuration file you can make an INI-formatted file and pass it on the command line. For example, you can make a file like this for the org.osg.srm.srmclient-ping metric (tmp-srm.ini): [org.osg.srm.srmclient-ping args] srm-destination-dir = /srmcache/~ srm-webservice-path = srm/v2/server Then use the --extra-config-file parameter and pass the path to the INI file: root@host# rsv-control -r --extra-config-file tmp-srm.ini \\ --host osg-edu.cs.wisc.edu:10443 org.osg.srm.srmclient-ping Running metric org.osg.srm.srmclient-ping: metricName: org.osg.srm.srmclient-ping metricType: status timestamp: 2010-11-19 14:12:35 CST metricStatus: OK serviceType: OSG-SRM serviceURI: osg-edu.cs.wisc.edu:10443 gatheredAt: vdt-itb.cs.wisc.edu summaryData: OK detailsData: SRM server running on osg-edu.cs.wisc.edu is alive and responding to the srmping command. . Details: Storage Resource Manager (SRM) Client version 2.1.5-16 Copyright (c) 2002-2009 Fermi National Accelerator Laboratory ...","title":"Passing extra configuration"},{"location":"monitoring/rsv-control/#enabling-and-disabling-metrics-and-consumers","text":"Metrics and consumers can be enabled or disabled by rsv-control using the --enable and --disable flags. Note that \"enable\" and \"disable\" are desired states (this is similar to osg-control ). After enabling a metric you should turn it on if you want it to be running immediately. After disabling a metric that is running, you should still turn it off (a message will print after each of these actions to remind you of this behavior).","title":"Enabling and disabling metrics and consumers"},{"location":"monitoring/rsv-control/#enabling","text":"The syntax for enabling metrics looks similar to the syntax for running metrics: rsv-control --enable --host <HOST> <METRIC> [ <METRIC2> ...] You must provide a host to enable the metric against (in order to enable a metric on multiple hosts you must run rsv-control once per host). root@host# rsv-control --enable \\ --host osg-edu.cs.wisc.edu org.osg.gip.consistency Enabling metric 'org.osg.gip.consistency' for host 'osg-edu.cs.wisc.edu' One or more metrics have been enabled and will be started the next time RSV is started. To turn them on immediately run 'rsv-control --on'. Consumers do not run against a specific host, they process records for all hosts. When enabling consumers a host is not required (if a host is passed it will be ignored). root@host# rsv-control --enable nagios-consumer Enabling consumer nagios-consumer","title":"Enabling"},{"location":"monitoring/rsv-control/#disabling","text":"The syntax for disabling metrics looks similar to the syntax for running metrics: rsv-control --disable --host <HOST> <METRIC> [ <METRIC2> ...] You must provide a host to disable the metric against (in order to disable a metric on multiple hosts you must run rsv-control once per host). root@host# rsv-control --disable \\ --host vdt-itb.cs.wisc.edu org.osg.local.containercert-expiry Disabling metric 'org.osg.local.containercert-expiry' for host 'vdt-itb.cs.wisc.edu' One or more metrics have been disabled and will not start the next time RSV is started. You may still need to turn them off if they are currently running. Consumers do not run against a specific host, they process records for all hosts. When disabling consumers a host is not required (if a host is passed it will be ignored). root@host# rsv-control --disable html-consumer gratia-consumer Disabling consumer html-consumer Disabling consumer gratia-consumer Consumer already disabled Metrics and consumers can both be listed in the same disable command.","title":"Disabling"},{"location":"monitoring/rsv-control/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"monitoring/rsv-control/#getting-more-information-from-rsv-control","text":"The first step to getting more information is to run rsv-control with more verbosity. Use the --verbose ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are: 0 = print nothing 1 = print warnings and errors along with usual output of command being run (1 is the default level) 2 = adds informational messages 3 = full debugging output","title":"Getting more information from rsv-control"},{"location":"monitoring/rsv-control/#using-the-rsv-verify-tool","text":"The --verify flag will run some basic checks for your RSV installation: root@host# rsv-control --verify Testing if Condor-Cron is running... OK Testing if metrics are running... OK (98 running metrics) Testing if consumers are running... OK (1 running consumers) Checking which consumers are configured... The following consumers are enabled: html-consumer WARNING: The gratia-consumer is not enabled. This indicates that your resource is not reporting to OSG. This tool is still under development and it does only basic checks, but it is a good first step when debugging issues.","title":"Using the RSV verify tool"},{"location":"monitoring/rsv-control/#running-the-rsv-profiler","text":"RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff. To use it: root@host# rsv-control --profile Running the rsv-profiler... OSG-RSV Profiler Analyzing... Making tarball (rsv-profiler.tar.gz) Note If you are getting assistance via the trouble ticket system, you must add a .txt extension to the tarball so it can be uploaded. root@host# mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt","title":"Running the RSV profiler"},{"location":"other/configuration-with-osg-configure/","text":"Configuration with OSG-Configure \u00b6 OSG-Configure and the INI files in /etc/osg/config.d allow a high level configuration of OSG services. This document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software. This page gives an overview of the options for each of the sections of the configuration files that osg-configure uses. Invocation and script usage \u00b6 The osg-configure script is used to process the INI files and apply changes to the system. osg-configure must be run as root. The typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes. To verify the config files, run: [root@server] osg-configure -v OSG-Configure will list any errors in your configuration, usually including the section and option where the problem is. Potential problems are: Required option not filled in Invalid value Syntax error Inconsistencies between options To apply changes, run: [root@server] osg-configure -c If your INI files do not change, then re-running osg-configure -c will result in the same configuration as when you ran it the last time. This allows you to experiment with your settings without having to worry about messing up your system. OSG-Configure is split up into modules. Normally, all modules are run when calling osg-configure . However, it is possible to run specific modules separately. To see a list of modules, including whether they can be run separately, run: [root@server] osg-configure -l If the module can be run separately, specify it with the -m <MODULE> option, where <MODULE> is one of the items of the output of the previous command. [root@server] osg-configure -c -m <MODULE> Options may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses. You may query the final value of an option via one of these methods: [root@server] osg-configure -q -o <OPTION> [root@server] osg-configure -q -o <SECTION>.<OPTION> Where <OPTION> is the variable from which we want to know the value and <SECTION> refers to a section in any of the INI files, i.e. any name between brackets e.g. [Squid] . Logs are written to /var/log/osg/osg-configure.log . If something goes wrong, specify the -d flag to add more verbose output to osg-configure.log . The rest of this document will detail what to specify in the INI files. Conventions \u00b6 In the tables below: Mandatory options for a section are given in bold type. Sometime the default value may be OK and no edit required, but the variable has to be in the file. Options that are not found in the default ini file are in italics . Syntax and layout \u00b6 The configuration files used by osg-configure are the one supported by Python's SafeConfigParser , similar in format to the INI configuration file used by MS Windows: Config files are separated into sections, specified by a section name in square brackets (e.g. [Section 1] ) Options should be set using name = value pairs Lines that begin with ; or # are comments Long lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in email RFC 822 ) Variable substitutions are supported -- see below osg-configure reads and uses all of the files in /etc/osg/config.d that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called [PBS] can be given in 20-pbs.ini as well as 99-local-site-settings.ini . Each of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g. 99-local-site-settings.ini ) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated. Variable substitution \u00b6 The osg-configure parser allows variables to be defined and used in the configuration file: any option set in a given section can be used as a variable in that section. Assuming that you have set an option with the name myoption in the section, you can substitute the value of that option elsewhere in the section by referring to it as %(myoption)s . Note The trailing s is required. Also, option names cannot have a variable subsitution in them. Special Settings \u00b6 If a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible. Ignore setting \u00b6 The enabled option, specifying whether a service is enabled or not, is a boolean but also accepts Ignore as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using False (or the %(disabled)s variable), because using False results in the service associated with the section being disabled. osg-configure will not change the configuration of the service if the enabled is set to Ignore . This is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the Ignore setting so that osg-configure does not alter the service's configuration and status. Configuration sections \u00b6 The OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g. [Section 1] ). The configuration is split in multiple files and options form one section can be in more than one files. The following sections give an overview of the options for each of the sections of the configuration files that osg-configure uses. Bosco \u00b6 This section is contained in /etc/osg/config.d/20-bosco.ini which is provided by the osg-configure-bosco RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the Bosco jobmanager is being used or not. users String A comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here. endpoint String The remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of user@example.com , exactly as you would use to ssh into the remote cluster. batch String The type of scheduler installed on the remote cluster. ssh_key String The location of the ssh key, as created above. Condor \u00b6 This section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-condor.ini which is provided by the osg-configure-condor RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the Condor jobmanager is being used or not. condor_location String This should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the osg-configure script will try to get this from the CONDOR_LOCATION environment variable if available otherwise it will use /usr which works for the RPM installation. condor_config String This should be set to be path where the condor_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the osg-configure script will try to get this from the CONDOR_CONFIG environment variable if available otherwise it will use /etc/condor/condor_config , the default for the RPM installation. LSF \u00b6 This section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-lsf.ini which is provided by the osg-configure-lsf RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the LSF jobmanager is being used or not. lsf_location String This should be set to be directory where lsf is installed PBS \u00b6 This section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-pbs.ini which is provided by the osg-configure-pbs RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the PBS jobmanager is being used or not. pbs_location String This should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs_location/bin. accounting_log_directory String This setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting. pbs_server String This setting is optional and should point to your PBS server node if it is different from your OSG CE SGE \u00b6 This section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-sge.ini which is provided by the osg-configure-sge RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the SGE jobmanager is being used or not. sge_root String This should be set to be directory where sge is installed (e.g. same as $SGE_ROOT variable). sge_cell String The sge_cell setting should be set to the value of $SGE_CELL for your SGE install. default_queue String This setting determines queue that jobs should be placed in if the job description does not specify a queue. available_queues String This setting indicates which queues are available on the cluster and should be used for validation when validate_queues is set. validate_queues String This setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note. Note validate_queues : If available_queues is set, that list of queues will be used for validation, otherwise SGE will be queried for available queues. Slurm \u00b6 This section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-slurm.ini which is provided by the osg-configure-slurm RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the Slurm jobmanager is being used or not. slurm_location String This should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm_location/bin. db_host String Hostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe. db_port String Port of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe. db_user String Username used to access the SLURM database. This information is needed to configure the SLURM gratia probe. db_pass String The location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe. db_name String Name of the SLURM database. This information is needed to configure the SLURM gratia probe. slurm_cluster String The name of the Slurm cluster Gratia \u00b6 This section configures Gratia. If probes is set to UNAVAILABLE , then osg-configure will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes, %(osg-jobmanager-gratia)s , %(osg-gridftp-gratia)s , %(osg-metric-gratia)s , %(itb-jobmanager-gratia)s , %(itb-gridftp-gratia)s , %(itb-metric-gratia)s are defined in the default configuration files to make it easier to specify the standard osg reporting. This section is contained in /etc/osg/config.d/30-gratia.ini which is provided by the osg-configure-gratia RPM. Option Values Accepted Explanation enabled True , False , Ignore This should be set to True if gratia should be configured and enabled on the installation being configured. resource String This should be set to the resource name as given in the OIM registration probes String This should be set to the gratia probes that should be enabled. A probe is specified by using as [probe_type]:server:port . See note Note probes : Legal values for probe_type are: metric (for RSV) jobmanager (for the appropriate jobmanager probe) gridftp (for the GridFTP transfer probe) Info Services \u00b6 Reporting to the central CE Collectors is configured in this section. In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations. This section is contained in /etc/osg/config.d/30-infoservices.ini , which is provided by the osg-configure-infoservices RPM. (This is for historical reasons.) Option Values Accepted Explanation enabled True , False , Ignore True if reporting should be configured and enabled ce_collectors String The server(s) HTCondor-CE information should be sent to. See note Note ce_collectors : Set this to DEFAULT to report to the OSG Production or ITB servers (depending on your Site Information configuration). Set this to PRODUCTION to report to the OSG Production servers Set this to ITB to report to the OSG ITB servers Otherwise, set this to the hostname:port of a host running a condor-ce-collector daemon RSV \u00b6 This section handles the configuration and setup of the RSV services. This section is contained in /etc/osg/config.d/30-rsv.ini which is provided by the osg-configure-rsv RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the rsv service is being used or not. rsv_user String This gives username that rsv will run under. If this is blank or set to UNAVAILABLE , it will default to rsv. gratia_probes String This settings indicates which rsv gratia probes should be used. It is a list of probes separated by a comma. Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer ce_hosts String This option lists the serviceURI of the CEs that generic RSV CE probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g. my.host,my.host2,my.host3:2812 ). htcondor_ce_hosts String This option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g. my.host,my.host2,my.host3:2812 ). gridftp_hosts String This option lists the serviceURI of the GridFTP servers that the RSV GridFTP probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g. my.host.iu.edu:2812,my.host2,my.host3 ). gridftp_dir String This should be the directory that the GridFTP probes should use during testing. This defaults to /tmp if left blank or set to UNAVAILABLE . srm_hosts String This option lists the serviceURI of the srm servers that the RSV srm probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g. my.host,my.host2,my.host3:8444 ). srm_dir String This should be the directory that the srm probes should use during testing. srm_webservice_path String This option gives the webservice path that SRM probes need to use along with the host:port. See note. service_cert String This option should point to the public key file (pem) for your service certificate. If this is left blank or set to UNAVAILABLE and the user_proxy setting is set, it will default to /etc/grid-security/rsvcert.pem service_key String This option should point to the private key file (pem) for your service certificate. If this is left blank or set to UNAVAILABLE and the service_cert setting is enabled, it will default to /etc/grid-security/rsvkey.pem . service_proxy String This should point to the location of the rsv proxy file. If this is left blank or set to UNAVAILABLE and the use_service_cert setting is enabled, it will default to /tmp/rsvproxy . user_proxy String If you don't use a service certificate for rsv, you will need to specify a proxy file that RSV should use in the proxy_file setting. If this is set, then service_cert, service_key, and service_proxy should be left blank, or set to UNAVAILABE or DEFAULT . setup_rsv_nagios True , False This option indicates whether rsv should upload results to a local nagios server instance. This should be set to True or False. This plugin is provided as an experimental component, and admins are recommend not to enable it on production resources. rsv_nagios_conf_file String This option indicates the location of the rsv nagios file to use for configuration details. This file needs to be configured locally for RSV-Nagios forwarding to work -- see inline comments in file for more information. condor_location String If you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it) you must specify the path to the install dir here. Note srm_webservice_path : For dcache installations, this should work if left blank. However Bestman-xrootd SEs normally use srm/v2/server as web service path, and so Bestman-xrootd admins will have to pass this option with the appropriate value (for example: srm/v2/server ) for the SRM probes to pass on their SE. Subcluster / Resource Entry \u00b6 Subcluster and Resource Entry configuration is for reporting about the worker resources on your site. A subcluster is a homogeneous set of worker node hardware; a resource is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system. At least one Subcluster or Resource Entry section is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources. This section is contained in /etc/osg/config.d/30-gip.ini which is provided by the osg-configure-gip RPM. (This is for historical reasons.) This configuration uses multiple sections of the OSG configuration files: Subcluster* : options about homogeneous subclusters Resource Entry* : options for specifying ATLAS queues for AGIS Notes for multi-CE sites. \u00b6 If you would like to properly advertise multiple CEs per cluster, make sure that you: Set the value of site_name in the \"Site Information\" section to be the same for each CE. Have the exact same configuration values for the Subcluster* and Resource Entry* sections in each CE. Subcluster Configuration \u00b6 Each homogeneous set of worker node hardware is called a subcluster . For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format: [Subcluster CHANGEME] , where CHANGEME is the globally unique subcluster name (yes, it must be a globally unique name for the whole grid, not just unique to your site. Get creative.) Option Values Accepted Explanation name String The same name that is in the Section label; it should be globally unique ram_mb Positive Integer Megabytes of RAM per node cores_per_node Positive Integer Number of cores per node allowed_vos Comma-separated List or * The VOs that are allowed to run jobs on this subcluster (autodetected if * ). Optional on OSG 3.3 The following attributes are optional: Option Values Accepted Explanation max_wall_time Positive Integer Maximum wall-clock time, in minutes, that a job is allowed to run on this subcluster. The default is 1440, or the equivalent of one day. queue String The queue to which jobs should be submitted in order to run on this subcluster extra_transforms Classad Transformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster OSG 3.4 changes: allowed_vos is mandatory Resource Entry Configuration (ATLAS only) \u00b6 If you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new Resource Entry section with a unique name in the following format: [Resource Entry RESOURCE] where RESOURCE is a globally unique resource name (it must be a globally unique name for the whole grid, not just unique to your site). The following options are required for the Resource Entry section and are used to generate the data required by AGIS: Option Values Accepted Explanation name String The same name that is in the Resource Entry label; it must be globally unique max_wall_time Positive Integer Maximum wall-clock time, in minutes, that a job is allowed to run on this resource queue String The queue to which jobs should be submitted to run on this resource cpucount (alias cores_per_node ) Positive Integer Number of cores that a job using this resource can get maxmemory (alias ram_mb ) Positive Integer Maximum amount of memory (in MB) that a job using this resource can get allowed_vos Comma-separated List or * The VOs that are allowed to run jobs on this resource (autodetected if * ). Optional on OSG 3.3 The following attributes are optional: Option Values Accepted Explanation subclusters Comma-separated List The physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file vo_tag String An arbitrary label that is added to jobs routed through this resource OSG 3.4 changes: allowed_vos is mandatory Gateway \u00b6 This section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses condor-ce as the gateway. This section is contained in /etc/osg/config.d/10-gateway.ini which is provided by the osg-configure-gateway RPM. Option Values Accepted Explanation htcondor_gateway_enabled True , False (default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes. job_envvar_path String The value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination. Local Settings \u00b6 This section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the osg-local-job-environment.conf verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The osg-local-job-environment.conf file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system. Adding a line such as My_Setting = my_Value would result in the an environment variable called My_Setting set to my_Value in the job's environment. my_Value can also be defined in terms of an environment variable (i.e My_Setting = $my_Value ) that will be evaluated on the worker node. For example, to add a variable MY_PATH set to /usr/local/myapp , you'd have the following: [Local Settings] MY_PATH = /usr/local/myapp This section is contained in /etc/osg/config.d/40-localsettings.ini which is provided by the osg-configure-ce RPM. Misc Services \u00b6 This section handles the configuration of services that do not have a dedicated section for their configuration. This section is contained in /etc/osg/config.d/10-misc.ini which is provided by the osg-configure-misc RPM. This section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the authentication section of the HTCondor-CE install documents . Option Values Accepted Explanation authorization_method gridmap , xacml , local-gridmap , vomsmap This indicates which authorization method your site uses. xacml is deprecated in OSG 3.4 edit_lcmaps_db True , False (Optional, default True) If true, osg-configure will overwrite /etc/lcmaps.db to set your authorization method. The previous version will be backed up to /etc/lcmaps.db.pre-configure all_fqans True , False (Optional, default False) If true, vomsmap auth will use all VOMS FQANs of a proxy for mapping -- see documentation OSG 3.4 changes: authorization_method defaults to vomsmap authorization_method will raise a warning if set to xacml Site Information \u00b6 The settings found in the Site Information section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc. This section is contained in /etc/osg/config.d/40-siteinfo.ini which is provided by the osg-configure-ce RPM. Option Values Accepted Description group OSG , OSG-ITB This should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG host_name String This should be set to be hostname of the CE that is being configured resource String The resource name of this CE endpoint as registered in OIM. resource_group String The resource_group of this CE as registered in OIM. sponsor String This should be set to the sponsor of the resource. See note. site_policy Url This should be a url pointing to the resource's usage policy contact String This should be the name of the resource's admin contact email Email address This should be the email address of the admin contact for the resource city String This should be the city that the resource is located in country String This should be two letter country code for the country that the resource is located in. longitude Number This should be the longitude of the resource. It should be a number between -180 and 180. latitude Number This should be the latitude of the resource. It should be a number between -90 and 90. Note sponsor : If your resource has multiple sponsors, you can separate them using commas or specify the percentage using the following format 'osg, atlas, cms' or 'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple sponsors are used. If you have a sponsor that is not an OSG VO, you can indicate this by using 'local' as the VO. Squid \u00b6 This section handles the configuration and setup of the squid web caching and proxy service. This section is contained in /etc/osg/config.d/01-squid.ini which is provided by the osg-configure-squid RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the squid service is being used or not. location String This should be set to the hostname:port of the squid server. Storage \u00b6 This section gives information about the options in the Storage section of the configuration file. Several of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models. Please review the Storage Related Parameters section of the Environment Variables description and Site Planning discussions for explanations of the various storage models and the requirements for them. This section is contained in /etc/osg/config.d/10-storage.ini which is provided by the osg-configure-ce RPM. Option Values Accepted Explanation se_available True , False This indicates whether there is an associated SE available. default_se String If an SE is available at your cluster, set default_se to the hostname of this SE, otherwise set default_se to UNAVAILABLE. grid_dir String This setting should point to the directory which holds the files from the OSG worker node package. See note app_dir String This setting should point to the directory which contains the VO specific applications. See note data_dir String This setting should point to a directory that can be used to store and stage data in and out of the cluster. See note worker_node_temp String This directory should point to a directory that can be used as scratch space on compute nodes. If not set, the default is UNAVAILABLE. See note site_read String This setting should be the location or url to a directory that can be read to stage in data via the variable $OSG_SITE_READ . This is an url if you are using a SE. If not set, the default is UNAVAILABLE site_write String This setting should be the location or url to a directory that can be write to stage out data via the variable $OSG_SITE_WRITE . This is an url if you are using a SE. If not set, the default is UNAVAILABLE Dynamic worker node paths The above variables may be set to an environment variable that is set on your site's worker nodes. For example, if each of your worker nodes has a different location for its scratch directory specified by LOCAL_SCRATCH_DIR , set the following configuration: [Storage] worker_node_temp = $LOCAL_SCRATCH_DIR grid_dir : If you have installed the worker node client via RPM (the normal case) it should be /etc/osg/wn-client . If you have installed the worker node in a special location (perhaps via the worker node client tarball or via OASIS), it should be the location of that directory. This directory will be accessed via the $OSG_GRID environment variable. It should be visible on all of the compute nodes. Read access is required, though worker nodes don't need write access. app_dir : This directory will be accesed via the $OSG_APP environment variable. It should be visible on both the CE and worker nodes. Only the CE needs to have write access to this directory. This directory must also contain a sub-directory etc/ with 1777 permissions. This directory may also be in OASIS, in which case set app_dir to /cvmfs/oasis.opensciencegrid.org . (The CE does not need write access in that case.) data_dir : This directory can be accessed via the $OSG_DATA environment variable. It should be readable and writable on both the CE and worker nodes. worker_node_temp : This directory will be accessed via the $OSG_WN_TMP environment variable. It should allow read and write access on a worker node and can be visible to just that worker node.","title":"Configuration with OSG-Configure"},{"location":"other/configuration-with-osg-configure/#configuration-with-osg-configure","text":"OSG-Configure and the INI files in /etc/osg/config.d allow a high level configuration of OSG services. This document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software. This page gives an overview of the options for each of the sections of the configuration files that osg-configure uses.","title":"Configuration with OSG-Configure"},{"location":"other/configuration-with-osg-configure/#invocation-and-script-usage","text":"The osg-configure script is used to process the INI files and apply changes to the system. osg-configure must be run as root. The typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes. To verify the config files, run: [root@server] osg-configure -v OSG-Configure will list any errors in your configuration, usually including the section and option where the problem is. Potential problems are: Required option not filled in Invalid value Syntax error Inconsistencies between options To apply changes, run: [root@server] osg-configure -c If your INI files do not change, then re-running osg-configure -c will result in the same configuration as when you ran it the last time. This allows you to experiment with your settings without having to worry about messing up your system. OSG-Configure is split up into modules. Normally, all modules are run when calling osg-configure . However, it is possible to run specific modules separately. To see a list of modules, including whether they can be run separately, run: [root@server] osg-configure -l If the module can be run separately, specify it with the -m <MODULE> option, where <MODULE> is one of the items of the output of the previous command. [root@server] osg-configure -c -m <MODULE> Options may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses. You may query the final value of an option via one of these methods: [root@server] osg-configure -q -o <OPTION> [root@server] osg-configure -q -o <SECTION>.<OPTION> Where <OPTION> is the variable from which we want to know the value and <SECTION> refers to a section in any of the INI files, i.e. any name between brackets e.g. [Squid] . Logs are written to /var/log/osg/osg-configure.log . If something goes wrong, specify the -d flag to add more verbose output to osg-configure.log . The rest of this document will detail what to specify in the INI files.","title":"Invocation and script usage"},{"location":"other/configuration-with-osg-configure/#conventions","text":"In the tables below: Mandatory options for a section are given in bold type. Sometime the default value may be OK and no edit required, but the variable has to be in the file. Options that are not found in the default ini file are in italics .","title":"Conventions"},{"location":"other/configuration-with-osg-configure/#syntax-and-layout","text":"The configuration files used by osg-configure are the one supported by Python's SafeConfigParser , similar in format to the INI configuration file used by MS Windows: Config files are separated into sections, specified by a section name in square brackets (e.g. [Section 1] ) Options should be set using name = value pairs Lines that begin with ; or # are comments Long lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in email RFC 822 ) Variable substitutions are supported -- see below osg-configure reads and uses all of the files in /etc/osg/config.d that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called [PBS] can be given in 20-pbs.ini as well as 99-local-site-settings.ini . Each of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g. 99-local-site-settings.ini ) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated.","title":"Syntax and layout"},{"location":"other/configuration-with-osg-configure/#variable-substitution","text":"The osg-configure parser allows variables to be defined and used in the configuration file: any option set in a given section can be used as a variable in that section. Assuming that you have set an option with the name myoption in the section, you can substitute the value of that option elsewhere in the section by referring to it as %(myoption)s . Note The trailing s is required. Also, option names cannot have a variable subsitution in them.","title":"Variable substitution"},{"location":"other/configuration-with-osg-configure/#special-settings","text":"If a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible.","title":"Special Settings"},{"location":"other/configuration-with-osg-configure/#ignore-setting","text":"The enabled option, specifying whether a service is enabled or not, is a boolean but also accepts Ignore as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using False (or the %(disabled)s variable), because using False results in the service associated with the section being disabled. osg-configure will not change the configuration of the service if the enabled is set to Ignore . This is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the Ignore setting so that osg-configure does not alter the service's configuration and status.","title":"Ignore setting"},{"location":"other/configuration-with-osg-configure/#configuration-sections","text":"The OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g. [Section 1] ). The configuration is split in multiple files and options form one section can be in more than one files. The following sections give an overview of the options for each of the sections of the configuration files that osg-configure uses.","title":"Configuration sections"},{"location":"other/configuration-with-osg-configure/#bosco","text":"This section is contained in /etc/osg/config.d/20-bosco.ini which is provided by the osg-configure-bosco RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the Bosco jobmanager is being used or not. users String A comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here. endpoint String The remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of user@example.com , exactly as you would use to ssh into the remote cluster. batch String The type of scheduler installed on the remote cluster. ssh_key String The location of the ssh key, as created above.","title":"Bosco"},{"location":"other/configuration-with-osg-configure/#condor","text":"This section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-condor.ini which is provided by the osg-configure-condor RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the Condor jobmanager is being used or not. condor_location String This should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the osg-configure script will try to get this from the CONDOR_LOCATION environment variable if available otherwise it will use /usr which works for the RPM installation. condor_config String This should be set to be path where the condor_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the osg-configure script will try to get this from the CONDOR_CONFIG environment variable if available otherwise it will use /etc/condor/condor_config , the default for the RPM installation.","title":"Condor"},{"location":"other/configuration-with-osg-configure/#lsf","text":"This section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-lsf.ini which is provided by the osg-configure-lsf RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the LSF jobmanager is being used or not. lsf_location String This should be set to be directory where lsf is installed","title":"LSF"},{"location":"other/configuration-with-osg-configure/#pbs","text":"This section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-pbs.ini which is provided by the osg-configure-pbs RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the PBS jobmanager is being used or not. pbs_location String This should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs_location/bin. accounting_log_directory String This setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting. pbs_server String This setting is optional and should point to your PBS server node if it is different from your OSG CE","title":"PBS"},{"location":"other/configuration-with-osg-configure/#sge","text":"This section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-sge.ini which is provided by the osg-configure-sge RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the SGE jobmanager is being used or not. sge_root String This should be set to be directory where sge is installed (e.g. same as $SGE_ROOT variable). sge_cell String The sge_cell setting should be set to the value of $SGE_CELL for your SGE install. default_queue String This setting determines queue that jobs should be placed in if the job description does not specify a queue. available_queues String This setting indicates which queues are available on the cluster and should be used for validation when validate_queues is set. validate_queues String This setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note. Note validate_queues : If available_queues is set, that list of queues will be used for validation, otherwise SGE will be queried for available queues.","title":"SGE"},{"location":"other/configuration-with-osg-configure/#slurm","text":"This section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the enabled setting should be set to False . This section is contained in /etc/osg/config.d/20-slurm.ini which is provided by the osg-configure-slurm RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the Slurm jobmanager is being used or not. slurm_location String This should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm_location/bin. db_host String Hostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe. db_port String Port of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe. db_user String Username used to access the SLURM database. This information is needed to configure the SLURM gratia probe. db_pass String The location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe. db_name String Name of the SLURM database. This information is needed to configure the SLURM gratia probe. slurm_cluster String The name of the Slurm cluster","title":"Slurm"},{"location":"other/configuration-with-osg-configure/#gratia","text":"This section configures Gratia. If probes is set to UNAVAILABLE , then osg-configure will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes, %(osg-jobmanager-gratia)s , %(osg-gridftp-gratia)s , %(osg-metric-gratia)s , %(itb-jobmanager-gratia)s , %(itb-gridftp-gratia)s , %(itb-metric-gratia)s are defined in the default configuration files to make it easier to specify the standard osg reporting. This section is contained in /etc/osg/config.d/30-gratia.ini which is provided by the osg-configure-gratia RPM. Option Values Accepted Explanation enabled True , False , Ignore This should be set to True if gratia should be configured and enabled on the installation being configured. resource String This should be set to the resource name as given in the OIM registration probes String This should be set to the gratia probes that should be enabled. A probe is specified by using as [probe_type]:server:port . See note Note probes : Legal values for probe_type are: metric (for RSV) jobmanager (for the appropriate jobmanager probe) gridftp (for the GridFTP transfer probe)","title":"Gratia"},{"location":"other/configuration-with-osg-configure/#info-services","text":"Reporting to the central CE Collectors is configured in this section. In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations. This section is contained in /etc/osg/config.d/30-infoservices.ini , which is provided by the osg-configure-infoservices RPM. (This is for historical reasons.) Option Values Accepted Explanation enabled True , False , Ignore True if reporting should be configured and enabled ce_collectors String The server(s) HTCondor-CE information should be sent to. See note Note ce_collectors : Set this to DEFAULT to report to the OSG Production or ITB servers (depending on your Site Information configuration). Set this to PRODUCTION to report to the OSG Production servers Set this to ITB to report to the OSG ITB servers Otherwise, set this to the hostname:port of a host running a condor-ce-collector daemon","title":"Info Services"},{"location":"other/configuration-with-osg-configure/#rsv","text":"This section handles the configuration and setup of the RSV services. This section is contained in /etc/osg/config.d/30-rsv.ini which is provided by the osg-configure-rsv RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the rsv service is being used or not. rsv_user String This gives username that rsv will run under. If this is blank or set to UNAVAILABLE , it will default to rsv. gratia_probes String This settings indicates which rsv gratia probes should be used. It is a list of probes separated by a comma. Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer ce_hosts String This option lists the serviceURI of the CEs that generic RSV CE probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g. my.host,my.host2,my.host3:2812 ). htcondor_ce_hosts String This option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g. my.host,my.host2,my.host3:2812 ). gridftp_hosts String This option lists the serviceURI of the GridFTP servers that the RSV GridFTP probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g. my.host.iu.edu:2812,my.host2,my.host3 ). gridftp_dir String This should be the directory that the GridFTP probes should use during testing. This defaults to /tmp if left blank or set to UNAVAILABLE . srm_hosts String This option lists the serviceURI of the srm servers that the RSV srm probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g. my.host,my.host2,my.host3:8444 ). srm_dir String This should be the directory that the srm probes should use during testing. srm_webservice_path String This option gives the webservice path that SRM probes need to use along with the host:port. See note. service_cert String This option should point to the public key file (pem) for your service certificate. If this is left blank or set to UNAVAILABLE and the user_proxy setting is set, it will default to /etc/grid-security/rsvcert.pem service_key String This option should point to the private key file (pem) for your service certificate. If this is left blank or set to UNAVAILABLE and the service_cert setting is enabled, it will default to /etc/grid-security/rsvkey.pem . service_proxy String This should point to the location of the rsv proxy file. If this is left blank or set to UNAVAILABLE and the use_service_cert setting is enabled, it will default to /tmp/rsvproxy . user_proxy String If you don't use a service certificate for rsv, you will need to specify a proxy file that RSV should use in the proxy_file setting. If this is set, then service_cert, service_key, and service_proxy should be left blank, or set to UNAVAILABE or DEFAULT . setup_rsv_nagios True , False This option indicates whether rsv should upload results to a local nagios server instance. This should be set to True or False. This plugin is provided as an experimental component, and admins are recommend not to enable it on production resources. rsv_nagios_conf_file String This option indicates the location of the rsv nagios file to use for configuration details. This file needs to be configured locally for RSV-Nagios forwarding to work -- see inline comments in file for more information. condor_location String If you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it) you must specify the path to the install dir here. Note srm_webservice_path : For dcache installations, this should work if left blank. However Bestman-xrootd SEs normally use srm/v2/server as web service path, and so Bestman-xrootd admins will have to pass this option with the appropriate value (for example: srm/v2/server ) for the SRM probes to pass on their SE.","title":"RSV"},{"location":"other/configuration-with-osg-configure/#subcluster-resource-entry","text":"Subcluster and Resource Entry configuration is for reporting about the worker resources on your site. A subcluster is a homogeneous set of worker node hardware; a resource is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system. At least one Subcluster or Resource Entry section is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources. This section is contained in /etc/osg/config.d/30-gip.ini which is provided by the osg-configure-gip RPM. (This is for historical reasons.) This configuration uses multiple sections of the OSG configuration files: Subcluster* : options about homogeneous subclusters Resource Entry* : options for specifying ATLAS queues for AGIS","title":"Subcluster / Resource Entry"},{"location":"other/configuration-with-osg-configure/#notes-for-multi-ce-sites","text":"If you would like to properly advertise multiple CEs per cluster, make sure that you: Set the value of site_name in the \"Site Information\" section to be the same for each CE. Have the exact same configuration values for the Subcluster* and Resource Entry* sections in each CE.","title":"Notes for multi-CE sites."},{"location":"other/configuration-with-osg-configure/#subcluster-configuration","text":"Each homogeneous set of worker node hardware is called a subcluster . For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format: [Subcluster CHANGEME] , where CHANGEME is the globally unique subcluster name (yes, it must be a globally unique name for the whole grid, not just unique to your site. Get creative.) Option Values Accepted Explanation name String The same name that is in the Section label; it should be globally unique ram_mb Positive Integer Megabytes of RAM per node cores_per_node Positive Integer Number of cores per node allowed_vos Comma-separated List or * The VOs that are allowed to run jobs on this subcluster (autodetected if * ). Optional on OSG 3.3 The following attributes are optional: Option Values Accepted Explanation max_wall_time Positive Integer Maximum wall-clock time, in minutes, that a job is allowed to run on this subcluster. The default is 1440, or the equivalent of one day. queue String The queue to which jobs should be submitted in order to run on this subcluster extra_transforms Classad Transformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster OSG 3.4 changes: allowed_vos is mandatory","title":"Subcluster Configuration"},{"location":"other/configuration-with-osg-configure/#resource-entry-configuration-atlas-only","text":"If you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new Resource Entry section with a unique name in the following format: [Resource Entry RESOURCE] where RESOURCE is a globally unique resource name (it must be a globally unique name for the whole grid, not just unique to your site). The following options are required for the Resource Entry section and are used to generate the data required by AGIS: Option Values Accepted Explanation name String The same name that is in the Resource Entry label; it must be globally unique max_wall_time Positive Integer Maximum wall-clock time, in minutes, that a job is allowed to run on this resource queue String The queue to which jobs should be submitted to run on this resource cpucount (alias cores_per_node ) Positive Integer Number of cores that a job using this resource can get maxmemory (alias ram_mb ) Positive Integer Maximum amount of memory (in MB) that a job using this resource can get allowed_vos Comma-separated List or * The VOs that are allowed to run jobs on this resource (autodetected if * ). Optional on OSG 3.3 The following attributes are optional: Option Values Accepted Explanation subclusters Comma-separated List The physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file vo_tag String An arbitrary label that is added to jobs routed through this resource OSG 3.4 changes: allowed_vos is mandatory","title":"Resource Entry Configuration (ATLAS only)"},{"location":"other/configuration-with-osg-configure/#gateway","text":"This section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses condor-ce as the gateway. This section is contained in /etc/osg/config.d/10-gateway.ini which is provided by the osg-configure-gateway RPM. Option Values Accepted Explanation htcondor_gateway_enabled True , False (default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes. job_envvar_path String The value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination.","title":"Gateway"},{"location":"other/configuration-with-osg-configure/#local-settings","text":"This section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the osg-local-job-environment.conf verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The osg-local-job-environment.conf file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system. Adding a line such as My_Setting = my_Value would result in the an environment variable called My_Setting set to my_Value in the job's environment. my_Value can also be defined in terms of an environment variable (i.e My_Setting = $my_Value ) that will be evaluated on the worker node. For example, to add a variable MY_PATH set to /usr/local/myapp , you'd have the following: [Local Settings] MY_PATH = /usr/local/myapp This section is contained in /etc/osg/config.d/40-localsettings.ini which is provided by the osg-configure-ce RPM.","title":"Local Settings"},{"location":"other/configuration-with-osg-configure/#misc-services","text":"This section handles the configuration of services that do not have a dedicated section for their configuration. This section is contained in /etc/osg/config.d/10-misc.ini which is provided by the osg-configure-misc RPM. This section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the authentication section of the HTCondor-CE install documents . Option Values Accepted Explanation authorization_method gridmap , xacml , local-gridmap , vomsmap This indicates which authorization method your site uses. xacml is deprecated in OSG 3.4 edit_lcmaps_db True , False (Optional, default True) If true, osg-configure will overwrite /etc/lcmaps.db to set your authorization method. The previous version will be backed up to /etc/lcmaps.db.pre-configure all_fqans True , False (Optional, default False) If true, vomsmap auth will use all VOMS FQANs of a proxy for mapping -- see documentation OSG 3.4 changes: authorization_method defaults to vomsmap authorization_method will raise a warning if set to xacml","title":"Misc Services"},{"location":"other/configuration-with-osg-configure/#site-information","text":"The settings found in the Site Information section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc. This section is contained in /etc/osg/config.d/40-siteinfo.ini which is provided by the osg-configure-ce RPM. Option Values Accepted Description group OSG , OSG-ITB This should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG host_name String This should be set to be hostname of the CE that is being configured resource String The resource name of this CE endpoint as registered in OIM. resource_group String The resource_group of this CE as registered in OIM. sponsor String This should be set to the sponsor of the resource. See note. site_policy Url This should be a url pointing to the resource's usage policy contact String This should be the name of the resource's admin contact email Email address This should be the email address of the admin contact for the resource city String This should be the city that the resource is located in country String This should be two letter country code for the country that the resource is located in. longitude Number This should be the longitude of the resource. It should be a number between -180 and 180. latitude Number This should be the latitude of the resource. It should be a number between -90 and 90. Note sponsor : If your resource has multiple sponsors, you can separate them using commas or specify the percentage using the following format 'osg, atlas, cms' or 'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple sponsors are used. If you have a sponsor that is not an OSG VO, you can indicate this by using 'local' as the VO.","title":"Site Information"},{"location":"other/configuration-with-osg-configure/#squid","text":"This section handles the configuration and setup of the squid web caching and proxy service. This section is contained in /etc/osg/config.d/01-squid.ini which is provided by the osg-configure-squid RPM. Option Values Accepted Explanation enabled True , False , Ignore This indicates whether the squid service is being used or not. location String This should be set to the hostname:port of the squid server.","title":"Squid"},{"location":"other/configuration-with-osg-configure/#storage","text":"This section gives information about the options in the Storage section of the configuration file. Several of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models. Please review the Storage Related Parameters section of the Environment Variables description and Site Planning discussions for explanations of the various storage models and the requirements for them. This section is contained in /etc/osg/config.d/10-storage.ini which is provided by the osg-configure-ce RPM. Option Values Accepted Explanation se_available True , False This indicates whether there is an associated SE available. default_se String If an SE is available at your cluster, set default_se to the hostname of this SE, otherwise set default_se to UNAVAILABLE. grid_dir String This setting should point to the directory which holds the files from the OSG worker node package. See note app_dir String This setting should point to the directory which contains the VO specific applications. See note data_dir String This setting should point to a directory that can be used to store and stage data in and out of the cluster. See note worker_node_temp String This directory should point to a directory that can be used as scratch space on compute nodes. If not set, the default is UNAVAILABLE. See note site_read String This setting should be the location or url to a directory that can be read to stage in data via the variable $OSG_SITE_READ . This is an url if you are using a SE. If not set, the default is UNAVAILABLE site_write String This setting should be the location or url to a directory that can be write to stage out data via the variable $OSG_SITE_WRITE . This is an url if you are using a SE. If not set, the default is UNAVAILABLE Dynamic worker node paths The above variables may be set to an environment variable that is set on your site's worker nodes. For example, if each of your worker nodes has a different location for its scratch directory specified by LOCAL_SCRATCH_DIR , set the following configuration: [Storage] worker_node_temp = $LOCAL_SCRATCH_DIR grid_dir : If you have installed the worker node client via RPM (the normal case) it should be /etc/osg/wn-client . If you have installed the worker node in a special location (perhaps via the worker node client tarball or via OASIS), it should be the location of that directory. This directory will be accessed via the $OSG_GRID environment variable. It should be visible on all of the compute nodes. Read access is required, though worker nodes don't need write access. app_dir : This directory will be accesed via the $OSG_APP environment variable. It should be visible on both the CE and worker nodes. Only the CE needs to have write access to this directory. This directory must also contain a sub-directory etc/ with 1777 permissions. This directory may also be in OASIS, in which case set app_dir to /cvmfs/oasis.opensciencegrid.org . (The CE does not need write access in that case.) data_dir : This directory can be accessed via the $OSG_DATA environment variable. It should be readable and writable on both the CE and worker nodes. worker_node_temp : This directory will be accessed via the $OSG_WN_TMP environment variable. It should allow read and write access on a worker node and can be visible to just that worker node.","title":"Storage"},{"location":"other/gsissh/","text":"Installing and Maintaining GSI OpenSSH \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. This document contains instructions to install and configure the GSI OpenSSH server available in the OSG repository for use on your cluster. Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If they do not exist already, the installation will create the Linux users gsisshd and gsisshd As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates Installing GSI OpenSSH \u00b6 Install the GSI OpenSSH rpms: root@server # yum install gsi-openssh-server gsi-openssh-clients Configuring GSI OpenSSH \u00b6 In order to get a running instance of the GSI OpenSSH server, you'll need to change the default configuration. However, before you go any further, you'll need to decide whether you want GSI OpenSSH to be your primary ssh service or not (e.g. whether the GSI OpenSSH service will replace your existing SSH service). Regardless of your choice, you should probably have both services use the same host keys. This can be done by running the following commands : root@host # cd /etc/gsissh root@host # ln -s /etc/ssh/ssh_host_rsa_key ssh_host_rsa_key root@host # ln -s /etc/ssh/ssh_host_rsa_key.pub ssh_host_rsa_key.pub root@host # ln -s /etc/ssh/ssh_host_dsa_key ssh_host_dsa_key root@host # ln -s /etc/ssh/ssh_host_dsa_key.pub ssh_host_dsa_key.pub root@host # ln -s /etc/ssh/ssh_host_ecdsa_key ssh_host_ecdsa_key root@host # ln -s /etc/ssh/ssh_host_ecdsa_key.pub ssh_host_ecdsa_key.pub root@host # ln -s /etc/ssh/ssh_host_ed25519_key ssh_host_ed25519_key root@host # ln -s /etc/ssh/ssh_host_ed25519_key.pub ssh_host_ed25519_key.pub Note Your system may not have all of these host keys If you choose not to replace your existing SSH service, you'll need to change the port setting in the GSI OpenSSH configuration to another port (e.g. 2222) so that you can run both SSH services at the same time. This can be done by editing /etc/gsissh/sshd and setting Port 2222 . Note Regardless of the authorization method used for the user, any account that will be used with GSI OpenSSH must have a shell assigned to it and not be locked (e.g., have ! in the password field of /etc/shadow ). Configuring authentication \u00b6 To configure authentication for GSI OpenSSH, follow the instructions in the LCMAPS VOMS plugin document to prepare the LCMAPS VOMS plugin. Using GSI OpenSSH \u00b6 The following table gives the commands needed to start, stop, enable, and disable GSI OpenSSH. To... Run the command... Start the service systemctl start gsisshd Stop the service systemctl stop gsisshd Enable the service to start on boot systemctl enable gsisshd Disable the service from starting on boot systemctl disable gsisshd Validating GSI OpenSSH \u00b6 After starting the gsisshd service you can check if it is running correctly user@client $ grid-proxy-init Your identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name Enter GRID pass phrase for this identity: Creating proxy ............................................................................................... Done Your proxy is valid until: Sat Apr 23 08:18:27 2016 $ gsissh localhost -p 2222 Last login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu $ Troubleshooting \u00b6 You can get information on troubleshooting errors on the NCSA page . To troubleshoot LCMAPS authorization, you can add the following to /etc/sysconfig/gsisshd and choose a higher debug level: # level 0: no messages, 1: errors, 2: also warnings, 3: also notices, # 4: also info, 5: maximum debug LCMAPS_DEBUG_LEVEL = 2 Output goes to /var/log/messages or journalctl by default. Help \u00b6 To get assistance please use this Help Procedure . Reference \u00b6 Useful configuration and log files \u00b6 Configuration Files Service or Process Configuration File Description gsisshd /etc/gsissh/sshd_config Configuration file gsisshd /etc/sysconfig/gsisshd Environment variables for gsisshd gsisshd /etc/lcmaps.db LCMAPS configuration Log Files Service or Process Log File Description gsisshd /var/log/messages All log messages Other Files Service or Process File Description gsisshd /etc/grid-security/hostcert.pem Host certificate gsisshd /etc/grid-security/hostkey.pem X.509 host key gsisshd /etc/gsissh/ssh_host_rsa_key RSA Host key","title":"Install GSI-enabled SSH"},{"location":"other/gsissh/#installing-and-maintaining-gsi-openssh","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. This document contains instructions to install and configure the GSI OpenSSH server available in the OSG repository for use on your cluster.","title":"Installing and Maintaining GSI OpenSSH"},{"location":"other/gsissh/#before-starting","text":"Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If they do not exist already, the installation will create the Linux users gsisshd and gsisshd As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"other/gsissh/#installing-gsi-openssh","text":"Install the GSI OpenSSH rpms: root@server # yum install gsi-openssh-server gsi-openssh-clients","title":"Installing GSI OpenSSH"},{"location":"other/gsissh/#configuring-gsi-openssh","text":"In order to get a running instance of the GSI OpenSSH server, you'll need to change the default configuration. However, before you go any further, you'll need to decide whether you want GSI OpenSSH to be your primary ssh service or not (e.g. whether the GSI OpenSSH service will replace your existing SSH service). Regardless of your choice, you should probably have both services use the same host keys. This can be done by running the following commands : root@host # cd /etc/gsissh root@host # ln -s /etc/ssh/ssh_host_rsa_key ssh_host_rsa_key root@host # ln -s /etc/ssh/ssh_host_rsa_key.pub ssh_host_rsa_key.pub root@host # ln -s /etc/ssh/ssh_host_dsa_key ssh_host_dsa_key root@host # ln -s /etc/ssh/ssh_host_dsa_key.pub ssh_host_dsa_key.pub root@host # ln -s /etc/ssh/ssh_host_ecdsa_key ssh_host_ecdsa_key root@host # ln -s /etc/ssh/ssh_host_ecdsa_key.pub ssh_host_ecdsa_key.pub root@host # ln -s /etc/ssh/ssh_host_ed25519_key ssh_host_ed25519_key root@host # ln -s /etc/ssh/ssh_host_ed25519_key.pub ssh_host_ed25519_key.pub Note Your system may not have all of these host keys If you choose not to replace your existing SSH service, you'll need to change the port setting in the GSI OpenSSH configuration to another port (e.g. 2222) so that you can run both SSH services at the same time. This can be done by editing /etc/gsissh/sshd and setting Port 2222 . Note Regardless of the authorization method used for the user, any account that will be used with GSI OpenSSH must have a shell assigned to it and not be locked (e.g., have ! in the password field of /etc/shadow ).","title":"Configuring GSI OpenSSH"},{"location":"other/gsissh/#configuring-authentication","text":"To configure authentication for GSI OpenSSH, follow the instructions in the LCMAPS VOMS plugin document to prepare the LCMAPS VOMS plugin.","title":"Configuring authentication"},{"location":"other/gsissh/#using-gsi-openssh","text":"The following table gives the commands needed to start, stop, enable, and disable GSI OpenSSH. To... Run the command... Start the service systemctl start gsisshd Stop the service systemctl stop gsisshd Enable the service to start on boot systemctl enable gsisshd Disable the service from starting on boot systemctl disable gsisshd","title":"Using GSI OpenSSH"},{"location":"other/gsissh/#validating-gsi-openssh","text":"After starting the gsisshd service you can check if it is running correctly user@client $ grid-proxy-init Your identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name Enter GRID pass phrase for this identity: Creating proxy ............................................................................................... Done Your proxy is valid until: Sat Apr 23 08:18:27 2016 $ gsissh localhost -p 2222 Last login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu $","title":"Validating GSI OpenSSH"},{"location":"other/gsissh/#troubleshooting","text":"You can get information on troubleshooting errors on the NCSA page . To troubleshoot LCMAPS authorization, you can add the following to /etc/sysconfig/gsisshd and choose a higher debug level: # level 0: no messages, 1: errors, 2: also warnings, 3: also notices, # 4: also info, 5: maximum debug LCMAPS_DEBUG_LEVEL = 2 Output goes to /var/log/messages or journalctl by default.","title":"Troubleshooting"},{"location":"other/gsissh/#help","text":"To get assistance please use this Help Procedure .","title":"Help"},{"location":"other/gsissh/#reference","text":"","title":"Reference"},{"location":"other/gsissh/#useful-configuration-and-log-files","text":"Configuration Files Service or Process Configuration File Description gsisshd /etc/gsissh/sshd_config Configuration file gsisshd /etc/sysconfig/gsisshd Environment variables for gsisshd gsisshd /etc/lcmaps.db LCMAPS configuration Log Files Service or Process Log File Description gsisshd /var/log/messages All log messages Other Files Service or Process File Description gsisshd /etc/grid-security/hostcert.pem Host certificate gsisshd /etc/grid-security/hostkey.pem X.509 host key gsisshd /etc/gsissh/ssh_host_rsa_key RSA Host key","title":"Useful configuration and log files"},{"location":"other/install-cvmfs-stratum1/","text":"Install a CVMFS Stratum 1 \u00b6 This document describes how to install a CVMFS Stratum 1. There are many different variations on how to do that, but this document focuses on the configuration of the OSG Operations Stratum 1 oasis-replica.opensciencegrid.org. It is applicable to other Stratum 1s as well, very likely with modifications (some of which are suggested in the document below). Applicable versions The applicable software versions for this document are cvmfs and cvmfs-server >= 2.4.2. Before Starting \u00b6 Before starting the installation process, consider the following points: User IDs and Group IDs: If your machine is also going to be a repository server like OSG Operations, the installation will create the same user and group IDs as the cvmfs client . If you are installing frontier-squid, the installation will also create the same user id as frontier-squid . Network ports: This installation will host the stratum 1 on ports 80, 8000 and 8080, and if squid is installed it will host the uncached apache on port 8081. Port 80 is default but sometimes runs into operational problems, port 8000 is the alternate for most production use, and port 8080 is for Cloudflare (https://openhtc.io). Host choice: - Make sure there is adequate disk space for the repositories that will be served, at /srv/cvmfs . Do not use xfs as the filesystem type on operating systems older than EL7, because it has been demonstrated to perform poorly for CVMFS repositories; instead use ext3 or ext4. About 10GB should be reserved for apache and squid logs under /var/log on a production server, although they normally will not get that large. A Stratum 1 that is also a repository server should have at least 5GB available at /var/cache . SELinux - Ensure SELinux is disabled As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Installing \u00b6 All CVMFS Stratum 1s require cvmfs-server software and apache (httpd). It is highly recommended to also install frontier-squid and frontier-awstats on the same machine to be able to easily join the WLCG MRTG and awstats monitoring systems. The recommended configuration for frontier-squid below only caches geo api lookups. Other than that, it is primarily for monitoring. Installing cvmfs-server and httpd \u00b6 The OSG Operations Stratum 1 has to function as a repository server in addition to serving repository replications; most Stratum 1s serve only replications. Instructions are also provided for how to install cvmfs-server on Stratum 1s that do not have to be repository servers. Choose the appropriate subsection. Installing a CVMFS stratum 1 that is also a repository server \u00b6 EL7.2 cannot be reliably used as a repository server, because of bugs in the union filesystem OverlayFS. The bugs are fixed in EL7.3, so use EL7.3 or later. root@host # yum -y install cvmfs-server cvmfs mod_wsgi Installing CVMFS stratum 1 that is not a repository server \u00b6 If you're not installing for OSG Operations or otherwise want to support serving repositories on the same machine as a Stratum 1, use this command: root@host # yum -y install cvmfs-server cvmfs-config mod_wsgi Installing frontier-squid and frontier-awstats \u00b6 frontier-awstats is not distributed by OSG so these instructions get it from its original source. Do these commands to install frontier-squid and frontier-awstats: root@host # rpm -i http://frontier.cern.ch/dist/rpms/RPMS/noarch/frontier-release-1.1-1.noarch.rpm root@host # yum -y install frontier-squid frontier-awstats Configuring \u00b6 Configuring the system \u00b6 Increase the default number of open file descriptors: root@host # echo -e \"*\\t\\t-\\tnofile\\t\\t16384\" >>/etc/security/limits.conf root@host # ulimit -n 16384 In order for this to apply also interactively when logging in over ssh, the option UsePAM has to be set to yes in /etc/ssh/sshd_config . Configuring cron \u00b6 First, create the log directory: root@host # mkdir -p /var/log/cvmfs Put the following in /etc/cron.d/cvmfs : */5 * * * * root test -d /srv/cvmfs || exit;cvmfs_server snapshot -ai 6 1 * * * root cvmfs_server gc -af 2>/dev/null || true 0 9 * * * root find /srv/cvmfs/*.*/data/txn -name \"*.*\" -mtime +2 2>/dev/null|xargs rm -f Also, put the following in /etc/logrotate.d/cvmfs : /var/log/cvmfs/*.log { weekly missingok notifempty } Configuring apache \u00b6 If you are installing frontier-squid, create /etc/httpd/conf.d/cvmfs.conf and put the following lines into it: Listen 8081 KeepAlive On If you are not installing frontier-squid, instead put the following lines into that file: Listen 8000 KeepAlive On Listen 8080 KeepAlive On If you will be serving opensciencegrid.org repositories, you have to allow for old client configurations that access repositories without the domain name added. For that reason, you will need to remove each /etc/httpd/conf.d/cvmfs.<repositoryname>.conf that adding a replica creates (this is included in the add_osg_repository script ), and instead add the following to /etc/httpd/conf.d/cvmfs.conf : RewriteEngine On RewriteRule ^/cvmfs/([^./]*)/(.*)$ /cvmfs/$1.opensciencegrid.org/$2 RewriteRule ^/cvmfs/([^/]+)/api/(.*)$ /var/www/wsgi-scripts/cvmfs-server/cvmfs-api.wsgi/$1/$2 RewriteRule ^/cvmfs/(.*)$ /srv/cvmfs/$1 <Directory \"/srv/cvmfs\"> Options -MultiViews +FollowSymLinks -Indexes AllowOverride All Require all granted EnableMMAP Off EnableSendFile Off <FilesMatch \"^\\.cvmfs\"> ForceType application/x-cvmfs </FilesMatch> Header unset Last-Modified RequestHeader unset If-Modified-Since FileETag None ExpiresActive On ExpiresDefault \"access plus 3 days\" ExpiresByType text/html \"access plus 15 minutes\" ExpiresByType application/x-cvmfs \"access plus 61 seconds\" ExpiresByType application/json \"access plus 61 seconds\" </Directory> WSGIDaemonProcess cvmfs-api threads=64 display-name=%{GROUP} \\ python-path=/usr/share/cvmfs-server/webapi <Directory /var/www/wsgi-scripts/cvmfs-server> WSGIProcessGroup cvmfs-api WSGIApplicationGroup cvmfs-api Options ExecCGI SetHandler wsgi-script Require all granted </Directory> WSGISocketPrefix /var/run/wsgi If you will be serving cern.ch repositories, it has the same problem; replace opensciencegrid.org above with cern.ch. If you need to serve both opensciencegrid.org and cern.ch contact Dave Dykstra to discuss the options. Then enable apache: root@host # systemctl enable httpd root@host # systemctl start httpd Configuring frontier-squid \u00b6 Put the following in /etc/squid/customize.sh after the existing comment header: awk -- file ` dirname $ 0 ` / customhelps . awk -- source '{ # cache only api calls insertline(\"^http_access deny all\", \"acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/\") insertline(\"^http_access deny all\", \"cache deny !CVMFSAPI\") # port 80 is also supported, through an iptables redirect setoption(\"http_port\", \"8000 accel defaultsite=localhost:8081 no-vhost\") insertline(\"TAG: http_port\",\"http_port 8080 accel defaultsite=localhost:8081 no-vhost\") setoption(\"cache_peer\", \"localhost parent 8081 0 no-query originserver\") # allow incoming http accesses from anywhere # all requests will be forwarded to the originserver commentout(\"http_access allow NET_LOCAL\") insertline(\"^http_access deny all\", \"http_access allow all\") # do not let squid cache DNS entries more than 5 minutes setoption(\"positive_dns_ttl\", \"5 minutes\") # set shutdown_lifetime to 0 to avoid giving new connections error # codes, which get cached upstream setoption(\"shutdown_lifetime\", \"0 seconds\") # turn off collapsed_forwarding to prevent slow clients from slowing down # faster ones setoption(\"collapsed_forwarding\", \"off\") print }' On an EL7 system, make sure that iptables-services is installed and enabled: root@host # yum -y install iptables-services root@host # systemctl enable iptables Forward port 80 to port 8000: root@host # iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8000 root@host # service iptables save On EL7 also set up the the same port forwarding for IPv6: root@host # ip6tables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8000 root@host # service ip6tables save Enable frontier-squid: root@host # systemctl enable frontier-squid root@host # systemctl start frontier-squid Note The above configuration is for a single squid thread, which is fine for 1Gbit/s and possibly 2Gbit/s, but if higher bandwidth is needed, see the instructions for running multiple squid workers . Verifying \u00b6 In order to verify that everything is installed correctly, create a repository replica. The repository chosen for the instructions below is one from egi.eu because it is very small, but you can use another one if you prefer. Adding an example repository \u00b6 The OSG Operations Stratum 1 should add a repository replica using the add_osg_repository script from the oasis-goc rpm. Instructions for installing that are elsewhere. That script assumes that the oasis.opensciencegrid.org replica repository was first created, so this instruction creates it but does not download the first snapshot because that would take a lot of space and time. Use these commands to create the oasis replica and to create and download the example replica: root@host # cvmfs_server add-replica -o root http://oasis.opensciencegrid.org:8000/cvmfs/oasis.opensciencegrid.org /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub root@host # add_osg_repository http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu It's a good idea for other Stratum 1s to make their own scripts for adding repository replicas, because there's always two or three commands to run, and it's easy to forget the commands after the first one. The first command is this: root@host # cvmfs_server add-replica -o root http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub However, non-OSG Operations Stratum 1s (that is, at BNL and FNAL), for the sake of fulfilling an OSG security requirement, need to instead read from the OSG Operations machine with this as their first command: root@host # cvmfs_server add-replica -o root http://oasis-replica.opensciencegrid.org:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub:/etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub The second command for Stratum 1s that have the httpd configuration as described above in the Configuring apache section is this: root@host # rm -f /etc/httpd/conf.d/cvmfs.config-egi.egi.eu.conf Then the next command is this: root@host # cvmfs_server snapshot config-egi.egi.eu With large repositories that can take a very long time, but with small repositories it should be very quick and not show any errors. Verifying that the replica is being served \u00b6 Now to verify that the replication is working, do the following commands: root@host # wget -qdO- http://localhost:8000/cvmfs/config-egi.egi.eu/.cvmfspublished | cat -v root@host # wget -qdO- http://localhost:80/cvmfs/config-egi.egi.eu/.cvmfspublished | cat -v Both commands should show a short file including gibberish at the end which is the signature. It is a good idea to familiarize yourself with the log entries at /var/log/httpd/access_log and also, if you have installed frontier-squid, at /var/log/squid/access.log . Also, at least 15 minutes after the snapshot is finished, check the log /var/log/cvmfs/snapshots.log to see that it tried to get an update and got no errors. Setting up monitoring \u00b6 If you installed frontier-squid and frontier-awstats, there is a little more to do to configure monitoring. First, make sure that your firewall accepts UDP queries from the monitoring server at CERN. Details are in the frontier-squid instructions . Next, choose any random password and put it in /etc/awstats/password-file . Then tell Dave Dykstra the fully qualified domain name of your machine and the password you chose, and he'll set up the monitoring servers.","title":"Install a CVMFS Stratum 1"},{"location":"other/install-cvmfs-stratum1/#install-a-cvmfs-stratum-1","text":"This document describes how to install a CVMFS Stratum 1. There are many different variations on how to do that, but this document focuses on the configuration of the OSG Operations Stratum 1 oasis-replica.opensciencegrid.org. It is applicable to other Stratum 1s as well, very likely with modifications (some of which are suggested in the document below). Applicable versions The applicable software versions for this document are cvmfs and cvmfs-server >= 2.4.2.","title":"Install a CVMFS Stratum 1"},{"location":"other/install-cvmfs-stratum1/#before-starting","text":"Before starting the installation process, consider the following points: User IDs and Group IDs: If your machine is also going to be a repository server like OSG Operations, the installation will create the same user and group IDs as the cvmfs client . If you are installing frontier-squid, the installation will also create the same user id as frontier-squid . Network ports: This installation will host the stratum 1 on ports 80, 8000 and 8080, and if squid is installed it will host the uncached apache on port 8081. Port 80 is default but sometimes runs into operational problems, port 8000 is the alternate for most production use, and port 8080 is for Cloudflare (https://openhtc.io). Host choice: - Make sure there is adequate disk space for the repositories that will be served, at /srv/cvmfs . Do not use xfs as the filesystem type on operating systems older than EL7, because it has been demonstrated to perform poorly for CVMFS repositories; instead use ext3 or ext4. About 10GB should be reserved for apache and squid logs under /var/log on a production server, although they normally will not get that large. A Stratum 1 that is also a repository server should have at least 5GB available at /var/cache . SELinux - Ensure SELinux is disabled As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories","title":"Before Starting"},{"location":"other/install-cvmfs-stratum1/#installing","text":"All CVMFS Stratum 1s require cvmfs-server software and apache (httpd). It is highly recommended to also install frontier-squid and frontier-awstats on the same machine to be able to easily join the WLCG MRTG and awstats monitoring systems. The recommended configuration for frontier-squid below only caches geo api lookups. Other than that, it is primarily for monitoring.","title":"Installing"},{"location":"other/install-cvmfs-stratum1/#installing-cvmfs-server-and-httpd","text":"The OSG Operations Stratum 1 has to function as a repository server in addition to serving repository replications; most Stratum 1s serve only replications. Instructions are also provided for how to install cvmfs-server on Stratum 1s that do not have to be repository servers. Choose the appropriate subsection.","title":"Installing cvmfs-server and httpd"},{"location":"other/install-cvmfs-stratum1/#installing-a-cvmfs-stratum-1-that-is-also-a-repository-server","text":"EL7.2 cannot be reliably used as a repository server, because of bugs in the union filesystem OverlayFS. The bugs are fixed in EL7.3, so use EL7.3 or later. root@host # yum -y install cvmfs-server cvmfs mod_wsgi","title":"Installing a CVMFS stratum 1 that is also a repository server"},{"location":"other/install-cvmfs-stratum1/#installing-cvmfs-stratum-1-that-is-not-a-repository-server","text":"If you're not installing for OSG Operations or otherwise want to support serving repositories on the same machine as a Stratum 1, use this command: root@host # yum -y install cvmfs-server cvmfs-config mod_wsgi","title":"Installing CVMFS stratum 1 that is not a repository server"},{"location":"other/install-cvmfs-stratum1/#installing-frontier-squid-and-frontier-awstats","text":"frontier-awstats is not distributed by OSG so these instructions get it from its original source. Do these commands to install frontier-squid and frontier-awstats: root@host # rpm -i http://frontier.cern.ch/dist/rpms/RPMS/noarch/frontier-release-1.1-1.noarch.rpm root@host # yum -y install frontier-squid frontier-awstats","title":"Installing frontier-squid and frontier-awstats"},{"location":"other/install-cvmfs-stratum1/#configuring","text":"","title":"Configuring"},{"location":"other/install-cvmfs-stratum1/#configuring-the-system","text":"Increase the default number of open file descriptors: root@host # echo -e \"*\\t\\t-\\tnofile\\t\\t16384\" >>/etc/security/limits.conf root@host # ulimit -n 16384 In order for this to apply also interactively when logging in over ssh, the option UsePAM has to be set to yes in /etc/ssh/sshd_config .","title":"Configuring the system"},{"location":"other/install-cvmfs-stratum1/#configuring-cron","text":"First, create the log directory: root@host # mkdir -p /var/log/cvmfs Put the following in /etc/cron.d/cvmfs : */5 * * * * root test -d /srv/cvmfs || exit;cvmfs_server snapshot -ai 6 1 * * * root cvmfs_server gc -af 2>/dev/null || true 0 9 * * * root find /srv/cvmfs/*.*/data/txn -name \"*.*\" -mtime +2 2>/dev/null|xargs rm -f Also, put the following in /etc/logrotate.d/cvmfs : /var/log/cvmfs/*.log { weekly missingok notifempty }","title":"Configuring cron"},{"location":"other/install-cvmfs-stratum1/#configuring-apache","text":"If you are installing frontier-squid, create /etc/httpd/conf.d/cvmfs.conf and put the following lines into it: Listen 8081 KeepAlive On If you are not installing frontier-squid, instead put the following lines into that file: Listen 8000 KeepAlive On Listen 8080 KeepAlive On If you will be serving opensciencegrid.org repositories, you have to allow for old client configurations that access repositories without the domain name added. For that reason, you will need to remove each /etc/httpd/conf.d/cvmfs.<repositoryname>.conf that adding a replica creates (this is included in the add_osg_repository script ), and instead add the following to /etc/httpd/conf.d/cvmfs.conf : RewriteEngine On RewriteRule ^/cvmfs/([^./]*)/(.*)$ /cvmfs/$1.opensciencegrid.org/$2 RewriteRule ^/cvmfs/([^/]+)/api/(.*)$ /var/www/wsgi-scripts/cvmfs-server/cvmfs-api.wsgi/$1/$2 RewriteRule ^/cvmfs/(.*)$ /srv/cvmfs/$1 <Directory \"/srv/cvmfs\"> Options -MultiViews +FollowSymLinks -Indexes AllowOverride All Require all granted EnableMMAP Off EnableSendFile Off <FilesMatch \"^\\.cvmfs\"> ForceType application/x-cvmfs </FilesMatch> Header unset Last-Modified RequestHeader unset If-Modified-Since FileETag None ExpiresActive On ExpiresDefault \"access plus 3 days\" ExpiresByType text/html \"access plus 15 minutes\" ExpiresByType application/x-cvmfs \"access plus 61 seconds\" ExpiresByType application/json \"access plus 61 seconds\" </Directory> WSGIDaemonProcess cvmfs-api threads=64 display-name=%{GROUP} \\ python-path=/usr/share/cvmfs-server/webapi <Directory /var/www/wsgi-scripts/cvmfs-server> WSGIProcessGroup cvmfs-api WSGIApplicationGroup cvmfs-api Options ExecCGI SetHandler wsgi-script Require all granted </Directory> WSGISocketPrefix /var/run/wsgi If you will be serving cern.ch repositories, it has the same problem; replace opensciencegrid.org above with cern.ch. If you need to serve both opensciencegrid.org and cern.ch contact Dave Dykstra to discuss the options. Then enable apache: root@host # systemctl enable httpd root@host # systemctl start httpd","title":"Configuring apache"},{"location":"other/install-cvmfs-stratum1/#configuring-frontier-squid","text":"Put the following in /etc/squid/customize.sh after the existing comment header: awk -- file ` dirname $ 0 ` / customhelps . awk -- source '{ # cache only api calls insertline(\"^http_access deny all\", \"acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/\") insertline(\"^http_access deny all\", \"cache deny !CVMFSAPI\") # port 80 is also supported, through an iptables redirect setoption(\"http_port\", \"8000 accel defaultsite=localhost:8081 no-vhost\") insertline(\"TAG: http_port\",\"http_port 8080 accel defaultsite=localhost:8081 no-vhost\") setoption(\"cache_peer\", \"localhost parent 8081 0 no-query originserver\") # allow incoming http accesses from anywhere # all requests will be forwarded to the originserver commentout(\"http_access allow NET_LOCAL\") insertline(\"^http_access deny all\", \"http_access allow all\") # do not let squid cache DNS entries more than 5 minutes setoption(\"positive_dns_ttl\", \"5 minutes\") # set shutdown_lifetime to 0 to avoid giving new connections error # codes, which get cached upstream setoption(\"shutdown_lifetime\", \"0 seconds\") # turn off collapsed_forwarding to prevent slow clients from slowing down # faster ones setoption(\"collapsed_forwarding\", \"off\") print }' On an EL7 system, make sure that iptables-services is installed and enabled: root@host # yum -y install iptables-services root@host # systemctl enable iptables Forward port 80 to port 8000: root@host # iptables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8000 root@host # service iptables save On EL7 also set up the the same port forwarding for IPv6: root@host # ip6tables -t nat -A PREROUTING -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8000 root@host # service ip6tables save Enable frontier-squid: root@host # systemctl enable frontier-squid root@host # systemctl start frontier-squid Note The above configuration is for a single squid thread, which is fine for 1Gbit/s and possibly 2Gbit/s, but if higher bandwidth is needed, see the instructions for running multiple squid workers .","title":"Configuring frontier-squid"},{"location":"other/install-cvmfs-stratum1/#verifying","text":"In order to verify that everything is installed correctly, create a repository replica. The repository chosen for the instructions below is one from egi.eu because it is very small, but you can use another one if you prefer.","title":"Verifying"},{"location":"other/install-cvmfs-stratum1/#adding-an-example-repository","text":"The OSG Operations Stratum 1 should add a repository replica using the add_osg_repository script from the oasis-goc rpm. Instructions for installing that are elsewhere. That script assumes that the oasis.opensciencegrid.org replica repository was first created, so this instruction creates it but does not download the first snapshot because that would take a lot of space and time. Use these commands to create the oasis replica and to create and download the example replica: root@host # cvmfs_server add-replica -o root http://oasis.opensciencegrid.org:8000/cvmfs/oasis.opensciencegrid.org /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub root@host # add_osg_repository http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu It's a good idea for other Stratum 1s to make their own scripts for adding repository replicas, because there's always two or three commands to run, and it's easy to forget the commands after the first one. The first command is this: root@host # cvmfs_server add-replica -o root http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub However, non-OSG Operations Stratum 1s (that is, at BNL and FNAL), for the sake of fulfilling an OSG security requirement, need to instead read from the OSG Operations machine with this as their first command: root@host # cvmfs_server add-replica -o root http://oasis-replica.opensciencegrid.org:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub:/etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub The second command for Stratum 1s that have the httpd configuration as described above in the Configuring apache section is this: root@host # rm -f /etc/httpd/conf.d/cvmfs.config-egi.egi.eu.conf Then the next command is this: root@host # cvmfs_server snapshot config-egi.egi.eu With large repositories that can take a very long time, but with small repositories it should be very quick and not show any errors.","title":"Adding an example repository"},{"location":"other/install-cvmfs-stratum1/#verifying-that-the-replica-is-being-served","text":"Now to verify that the replication is working, do the following commands: root@host # wget -qdO- http://localhost:8000/cvmfs/config-egi.egi.eu/.cvmfspublished | cat -v root@host # wget -qdO- http://localhost:80/cvmfs/config-egi.egi.eu/.cvmfspublished | cat -v Both commands should show a short file including gibberish at the end which is the signature. It is a good idea to familiarize yourself with the log entries at /var/log/httpd/access_log and also, if you have installed frontier-squid, at /var/log/squid/access.log . Also, at least 15 minutes after the snapshot is finished, check the log /var/log/cvmfs/snapshots.log to see that it tried to get an update and got no errors.","title":"Verifying that the replica is being served"},{"location":"other/install-cvmfs-stratum1/#setting-up-monitoring","text":"If you installed frontier-squid and frontier-awstats, there is a little more to do to configure monitoring. First, make sure that your firewall accepts UDP queries from the monitoring server at CERN. Details are in the frontier-squid instructions . Next, choose any random password and put it in /etc/awstats/password-file . Then tell Dave Dykstra the fully qualified domain name of your machine and the password you chose, and he'll set up the monitoring servers.","title":"Setting up monitoring"},{"location":"other/install-gwms-frontend/","text":"GlideinWMS VO Frontend Installation \u00b6 This document describes how to install the Glidein Workflow Managment System (GlideinWMS) VO Frontend for use with the OSG Glidein factory. This software is the minimum requirement for a VO to use GlideinWMS. This document assumes expertise with HTCondor and familiarity with the GlideinWMS software. It does not cover anything but the simplest possible install. Please consult the GlideinWMS reference documentation for advanced topics, including non- root , non-RPM-based installation. This document covers three components of the GlideinWMS a VO needs to install: User Pool Collectors : A set of condor_collector processes. Pilots submitted by the factory will join to one of these collectors to form a HTCondor pool. User Pool Schedd : A condor_schedd . Users may submit HTCondor vanilla universe jobs to this schedd; it will run jobs in the HTCondor pool formed by the User Pool Collectors . Glidein Frontend : The frontend will periodically query the User Pool Schedd to determine the desired number of running job slots. If necessary, it will request the Factory to launch additional pilots. This guide covers installation of all three components on the same host: it is designed for small to medium VOs (see the Hardware Requirements below). Given a significant, large host, we have been able to scale the single-host install to 20,000 running jobs. Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If they do not exist already, the installation will create the Linux users apache (UID 48), condor , frontend , and gratia Network: The VO frontend must have reliable network connectivity and be on the public internet (i.e. no NAT). The latest version requires the following TCP ports to be open: 80 (HTTP) for monitoring and serving configuration to workers 9618 (HTCondor shared port) for HTCondor daemons including the Schedd and User Collector 9620 to 9660 for secondary collectors (depending on configuration, see below) Host choice : The GlideinWMS VO Frontend has the following hardware requirements for a production host: CPU : Four cores, preferably no more than 2 years old. RAM : 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed. Disk : 30GB will be sufficient for all the binaries, config and log files related to GlideinWMS. As this will be an interactive access point, have enough disk space for your users' jobs. Note The default configuration uses a port range (9620 to 9660) for the secondary collectors. You can configure the secondary collectors to use the shared port 9618 instead; this will become the default in the future. Note GlideinWMS versions prior to 3.4.1 also required port 9615 for the Schedd, and did not support using shared port for the secondary collectors. If you are upgrading a standalone access point from version 3.4 or earlier, the default open port has changed from 9615 to 9618, and you need to update your firewall rules to reflect this change. You can figure out which port will be used by running the following command: console condor_config_val SHARED_PORT_ARGS For more detailed information, see Configuring GlideinWMS Frontend . As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates Credentials and Proxies \u00b6 The VO Frontend will use two credentials in its interactions with the other GlideinWMS services. At this time, these will be proxy files. the VO Frontend proxy (used to authenticate with the other GlideinWMS services). one or more GlideinWMS pilot proxies (used/delegated to the Factory services and submitted on the GlideinWMS pilot jobs). The VO Frontend proxy and the pilot proxy can be the same. By default, the VO Frontend will run as user frontend (UID is machine dependent) so these proxies must be owned by the user frontend . Note Both proxies need to be passwordless to allow automatic proxy renewal . VO Frontend proxy \u00b6 The use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the proxy configuration section . You must give the Factory operations team the DN of this proxy when you initially setup the Frontend and each time the DN changes . Pilot proxies \u00b6 These proxies are used by the Factory to submit the GlideinWMS pilot jobs. Therefore, they must be authorized to access to the CEs (Factory entry points) where jobs are submitted. There is no need to notify the Factory operation about the DN of these proxies (neither at the initial registration nor for subsequent changes). These additional proxies have no special requirements or controls added by the Factory but will probably require VO attributes because of the CEs: if you are able to use one of these proxies to submit jobs to the corresponding CEs where the Factory runs GlideinWMS pilots for you, then the proxies are OK. You can test each of your proxies using globusrun or HTCondor-G. To check the important information about a PEM certificate you can use: openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout . You will need that to find out information for the configuration files and the request to the GlideinWMS Factory. OSG Factory access \u00b6 Before installing the GlideinWMS VO Frontend you need the information about a Glidein Factory that you can access: (recommended) OSG is managing a factory at UCSD You have another Glidein Factory that you can access To request access to the OSG Glidein Factory at UCSD you have to send an email to osg-gfactory-support@physics.ucsd.edu providing: Your Name The VO that is utilizing the VO Frontend The DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the proxy configuration section ) You can propose a security name that will have to be confirmed/changed by the Factory managers (see below) A list of sites where you want to run: Your VO must be supported on those sites You can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers You can start with one single site In the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend The exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\". The host of the Factory Collector: gfactory-1.t2.ucsd.edu The DN os the factory, e.g. /DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu The factory identity, e.g.: gfactory@gfactory-1.t2.ucsd.edu The identity on the Factory you will be mapped to. Something like: username@gfactory-1.t2.ucsd.edu Your security name. A unique name, usually containing your VO name: My_SecName A string to add in the main Factory query_expr in the Frontend configuration, e.g. stringListMember(\"<VO>\",GLIDEIN_Supported_VOs) . This is used to select the entries you can use. From there you get the correct name of the VO (above in this list). Installing GlideinWMS Frontend \u00b6 Installing HTCondor \u00b6 If you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository: root@host # yum install condor.x86_64 If you already have installed HTCondor using a tarball or a source other than the OSG ROM, you will need to install the empty-condor RPM: root@host # yum install empty-condor --enablerepo = osg-empty Installing the VO Frontend RPM \u00b6 Install the RPM and dependencies (be prepared for a lot of dependencies). root@host # yum install glideinwms-vofrontend This will install the current production release verified and tested by OSG with default HTCondor configuration. This command will install the GlideinWMS vofrontend, HTCondor, the OSG client, and all the required dependencies all on one node. If you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows: yum install --enablerepo=osg-testing glideinwms-vofrontend : The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well. yum install --enablerepo=osg-upcoming glideinwms-vofrontend : The most recent development series release, i.e. version 3.3 release. This has newer features such as cloud submission support, but is less tested. Note that these commands will install default HTCondor configurations with all GlideinWMS services on one node. Installing GlideinWMS Frontend on Multiple Nodes (Advanced) \u00b6 For advanced users expecting heavy usage on their access point, you may want to consider splitting the user collector, user submit, and vo frontend services. This can be doing using the following three commands (on different machines): root@host # yum install glideinwms-vofrontend-standalone root@host # yum install glideinwms-usercollector root@host # yum install glideinwms-userschedd In addition, you will need to perform the following steps: On the vofrontend and userschedd, modify CONDOR_HOST to point to your usercollector. This is in /etc/condor/config.d/00_gwms_general.config . You can also override this value by placing it in a new config file. (For instance, /etc/condor/config.d/99_local_custom.config to avoid rpmsave/rpmnew conflicts on upgrades). In /etc/condor/certs/condor_mapfile , you will need to add the DNs of each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the glidecondor_addDN to add these values. In the /etc/gwms-frontend/frontend.xml file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend.xml are in the following sections. Configuring GlideinWMS Frontend \u00b6 After installing the RPM, you need to configure the components of the GlideinWMS VO Frontend: Edit Frontend configuration options Edit HTCondor configuration options Create a HTCondor grid map file Reconfigure and Start the Frontend Configuring the Frontend \u00b6 The VO Frontend configuration file is /etc/gwms-frontend/frontend.xml . The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference. The VO you are affiliated with. This will identify those CEs that the GlideinWMS pilot will be authorized to run on using the pilot proxy described previously in this section . Sometimes the whole query_expr is provided to you by the Factory operators (see Factory access above): <factory query_expr= '((stringListMember(\"VO\", GLIDEIN_Supported_VOs)))' > Factory collector information. The username that you are assigned by the Factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production Factory, you will have to change also DN , factory_identity and node attributes. (refer to the information provided to you by the Factory operator): <collector DN= \"/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\" comment= \"Define factory collector globally for simplicity\" factory_identity= \"gfactory@gfactory-1.t2.ucsd.edu\" my_identity= \"username@gfactory-1.t2.ucsd.edu\" node= \"gfactory-1.t2.ucsd.edu\" /> Frontend security information. The classad_proxy in the security entry is the location of the VO Frontend proxy described previously here . The proxy_DN is the DN of the classad_proxy above. The security_name identifies this VO Frontend to the the Factory, It is provided by the Factory operator. The absfname in the credential entry is the location of the GlideinWMS pilot proxy described in the requirements section here . There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources). The type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the Factory. If there is no match, between these two attributes in one of the credentials and the corresponding ones in some entry in one of the Factories, then this Frontend cannot trigger glideins. Both the classad_proxy and absfname files should be owned by frontend user. <security classad_proxy= \"/tmp/vo_proxy\" proxy_DN= \"DN of vo_proxy\" proxy_selection_plugin= \"ProxyAll\" security_name= \"The security name, this is used by factory\" sym_key= \"aes_256_cbc\" > <credentials> <credential absfname= \"/tmp/pilot_proxy\" security_class= \"frontend\" trust_domain= \"OSG\" type= \"grid_proxy\" /> </credentials> </security> The schedd information. The DN of the VO Frontend Proxy described previously here . The fullname attribute is the fully qualified domain name of the host where you installed the VO Frontend ( hostname --fqdn ). A secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the Frontend to service requests from multiple access points. <schedds> <schedd DN= \"Cert DN used by the schedd at fullname:\" fullname= \"Hostname of the schedd\" /> <schedd DN= \"Cert DN used by the second Schedd at fullname:\" fullname= \"schedd name@Hostname of second schedd\" /> </schedds> The User Collector information. The DN of the VO Frontend Proxy described previously here . The node attribute is the full hostname of the collectors ( hostname --fqdn ) and port The secondary attribute indicates whether the element is for the primary or secondary collectors (True/False). The default HTCondor configuration of the VO Frontend starts multiple Collector processes on the host ( /etc/condor/config.d/11_gwms_secondary_collectors.config ). The DN and hostname on the first line are the hostname and the host certificate of the VO Frontend. The DN and hostname on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector connection can be defined as sinful string for the sock case , e.g., hostname.domain.tld:9618?sock=collector16. [Example 1] :::xml <collector DN=\"DN of main collector\" node=\"hostname.domain.tld:9618\" secondary=\"False\"/> <collector DN=\"DN of secondary collectors (usually same as DN in line above)\" node=\"hostname.domain.tld:9620-9660\" secondary=\"True\"/> Note In GlideinWMS v3.4.1, shared port only configuration is incompatible if talking to older Factories (v3.4 or older). We strongly recommend any user of GlideinWMS Frontend v3.4.1 or newer, to transition to the use of shared port for secondary collectors and CCBs. The shared port configuration is incompatible if your Frontend is talking to Factories v3.4 or older and you'll get an error telling you to wait. To transition to the use of shared port for secondary collectors, you have to change the collectors section in the Frontend configuration. If you are using the default port range for the secondary collectors as shown in [Example 2] below, then you should replace it with port 9618 and the sock-range as shown in [Example 1] above. If you have a more complex configuration, please read the detailed GlideinWMS configuration [Example 2] :::xml <collector DN=\"DN of main collector\" node=\"hostname.domain.tld:9618\" secondary=\"False\"/> <collector DN=\"DN of secondary collectors (usually same as DN in line above)\" node=\u201chostname.domain.tld:9618?sock=collector0-40\" secondary=\"True\"/> The CCBs information. If you have a different configuration of the HTCondor Connection Brokering (CCB servers) from the default (usually the section is empty as the User Collectors acts as CCB if needed), you can set the connection in the CCB section the same way that User Collector information previously mentioned. Also, the same rules for transition to shared_port of the connections, apply to the CCBs. :::xml <ccb DN=\"DN of the CCB server\" node=\"hostname.domain.tld:9618\"/> <ccb DN=\"DN of the CCB server\" node=\u201chostname.domain.tld:9618?sock=collector0-40\" secondary=\"True\"/> Warning The Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths. Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation): frontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions) for RPM installs, work base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value) Using a Different Factory \u00b6 The configuration above points to the OSG production Factory. If you are using a different Factory, then you have to: replace gfactory@gfactory-1.t2.ucsd.edu and gfactory-1.t2.ucsd.edu with the correct values for your Factory. And control also that the name used for the Frontend () matches. make sure that the Factory is advertising the attributes used in the Factory query expression ( query_expr ). Configuring HTCondor \u00b6 The HTCondor configuration for the Frontend is placed in /etc/condor/config.d . 00_gwms_general.config 01_gwms_collectors.config 02_gwms_schedds.config 03_gwms_local.config 11_gwms_secondary_collectors.config 90_gwms_dns.config For most installations create a new file named /etc/condor/config.d/92_local_condor.config Using other HTCondor RPMs, e.g. UW Madison HTCondor RPM \u00b6 The above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using yum list condor . The version name should include \"osg\", e.g. 8.6.4-3.osg.el7 . If you are using the UW Madison HTCondor RPMS, be aware of the following changes: This HTCondor RPM uses a file /etc/condor/condor_config.local to add your local machine slot to the user pool. If you want to disable this behavior (recommended), you should blank out that file or comment out the line in /etc/condor/condor_config for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to /etc/condor/config.d ) Note that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the GlideinWMS RPMs, but please take note if you use this variable in your job submissions or other customizations. In general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files: root@host # condor_config_val -config Configuration source: /etc/condor/condor_config Local configuration sources: /etc/condor/config.d/00_gwms_general.config /etc/condor/config.d/01_gwms_collectors.config /etc/condor/config.d/02_gwms_schedds.config /etc/condor/config.d/03_gwms_local.config /etc/condor/config.d/11_gwms_secondary_collectors.config /etc/condor/config.d/90_gwms_dns.config /etc/condor/condor_config.local If, like in the example above, the GlideinWMS configuration files are not the last ones in the list please verify that important configuration options have not been overridden by the other configuration files. Verifying your HTCondor configuration \u00b6 The GlideinWMS configuration files in /etc/condor/config.d should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files. Verify the alll the expected HTCondor daemons are running: root@host # condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, COLLECTOR0 COLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12, COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23, COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40 Defined in '/etc/condor/config.d/11_gwms_secondary_collectors.config', line 193. If you don't see all the collectors. shared port and the two schedd, then the configuration must be corrected. There should be no startd daemons listed. Creating a HTCondor grid mapfile. \u00b6 The HTCondor mapfile ( /etc/condor/certs/condor_mapfile ) is used for authentication between the GlideinWMS pilot running on a remote worker node, and the local collector. HTCondor uses the mapfile to map certificates to pseudo-users on the local machine. It is important that you map the DN's of: Each schedd proxy : The DN of each schedd that the frontend talks to. Specified in the frontend.xml schedd element DN attribute: <schedds> <schedd DN= \"/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\" fullname= \"YOUR_HOST\" /> <schedd DN= \"/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\" fullname= \"schedd_jobs2@YOUR_HOST\" /> </schedds> Frontend proxy : The DN of the proxy that the Frontend uses to communicate with the other GlideinWMS services. Specified in the frontend.xml security element proxy_DN attribute: <security classad_proxy= \"/tmp/vo_proxy\" proxy_DN= \"DN of vo_proxy\" .... Each pilot proxy The DN of each proxy that the frontend forwards to the factory to use with the GlideinWMS pilots. This allows the GlideinWMS pilot jobs to communicate with the User Collector. Specified in the frontend.xml proxy absfname attribute (you need to specify the DN of each of those proxies: <security .... <proxies > < proxy absfname= \"/tmp/vo_proxy\" .... : </proxies > Below is an example mapfile, by default found in /etc/condor/certs/condor_mapfile . In this example there are lines for each of services mentioned above. GSI \"<DN OF SCHEDD PROXY>\" schedd GSI \"<DN OF FRONTEND PROXY>\" frontend GSI \"<DN OF PILOT PROXY>\" pilot_proxy GSI \"^/DC=org/DC=doegrids/OU=Services/CN=personal-submit-host2.mydomain.edu$\" <example_of_format> GSI (.*) anonymous FS (.*) \\1 Change <DN OF SCHEDD PROXY> , <DN OF FRONTEND PROXY> , and <DN OF PILOT PROXY> to the distinguished names of the respective proxies. Restarting HTCondor \u00b6 After configuring HTCondor, be sure to restart HTCondor: root@host # service condor restart Proxy Configuration \u00b6 GlideinWMS comes with the gwms-renew-proxies service that can automatically generate and renew the pilot proxies and VO Frontend proxy . To configure this service, modify /etc/gwms-frontend/proxies.ini using the following instructions: For each of your pilot proxies , create a [PILOT <NAME>] section, where <NAME> is a descriptive name for the proxy that is unique to your local configuration. In each section, set the proxy_cert , proxy_key , output , and vo corresponding to each pilot proxy: [PILOT <NAME>] proxy_cert = <PATH TO THE PILOT CERTIFICATE> proxy_key = <PATH TO THE PILOT KEY> output = <PATH TO CREATE THE PILOT PROXY> vo = <NAME OF VIRTUAL ORGANIZATION> Change <PATH TO THE PILOT CERTIFICATE> , <PATH TO THE PILOT KEY> and <PATH TO CREATE THE PILOT PROXY> appropriately to point to the locations of the pilot certificate, pilot key, and pilot proxy, respectively. Additionally, in each [PILOT <NAME>] section, you must specify how the proxy's VOMS attributes will be signed by setting use_voms_server . Choose one of the following options: To directly sign the VOMS attributes (recommended), you must have access to the vo 's certificate and key. Specify the paths to the vo certificate and key, and optionally, the VOMS attribute (e.g. /osg/Role=NULL/Capability=NULL for the OSG VO): use_voms_server = false vo_cert = <PATH TO THE PILOT CERTIFICATE> vo_key = <PATH TO THE PILOT KEY> fqan = <VOMS ATTRIBUTE> Note If you do not have access to the vo 's voms_cert and voms_key , contact the VO manager. To have your proxy's VOMS attributes signed by the vo 's VOMS server, set use_voms_server = true and the VOMS attribute (e.g. /osg/Role=NULL/Capability=NULL for the OSG VO): use_voms_server = true fqan = <VOMS ATTRIBUTE> Warning Due to the retirement of VOMS Admin server in the OSG, use_voms_server = false is the preferred method for signing VOMS attributes. Optionally, the proxy renewal frequency and lifetime (in hours) can be specified in each [PILOT <NAME>] section: # Default: 1 frequency = <RENEWAL FREQUENCY> # Default: 24 lifetime = <PROXY LIFETIME> Configure the location and output of the VO Frontend proxy under the [FRONTEND] section and set the proxy_cert , proxy_key , and output to paths corresponding to your VO Frontend: [FRONTEND] proxy_cert = <PATH TO THE FRONTEND CERTIFICATE> proxy_key = <PATH TO THE FRONTEND KEY> output = <PATH TO CREATE THE FRONTEND PROXY> Note output must be the same path as the classad_proxy specified in this section (OPTIONAL) If you are running the gwms-frontend service under a <NON-DEFAULT USER> (default: frontend ), specify the user as the owner of your proxies under the [COMMON] section: [COMMON] owner = <NON-DEFAULT USER> Note The [COMMON] section is required but its contents are optional Adding Gratia Accounting and a Local Monitoring Page on a Production Server \u00b6 You must report accounting information if you are running more than a few test jobs on the OSG . Install the GlideinWMS Gratia Probe on each of your access points in your GlideinWMS installation: root@host # yum install gratia-probe-glideinwms Edit the ProbeConfig located in /etc/gratia/condor/ProbeConfig . First, edit the SiteName and ProbeName to be a unique identifier for your GlideinWMS access point. There can be multiple probes (with different names) per site. If you haven't already, you should register your GlideinWMS access point in OIM . Then you can use the name you used to register the resource. ProbeName=\"condor:<hostname>\" SiteName=\"HCC-GlideinWMW-Frontend\" Next, turn the probe on by setting EnableProbe : EnableProbe=\"1\" Reconfigure HTCondor: root@host # condor_reconfig Optional Accounting Configuration \u00b6 The following sections contain additional configuration that may be required depending on the customizations you've made to your GlideinWMS frontend installation. Users without Certificates \u00b6 If you have users that submit jobs without a certificate explicitly declared in the submit file, you will need to add MapUnknownToGroup to the ProbeConfig. In the file /etc/gratia/condor/ProbeConfig , add the value after the EnableProbe . ... SuppressGridLocalRecords=\"0\" EnableProbe=\"1\" MapUnknownToGroup=\"1\" Title3=\"Tuning parameter\" ... Further, if you want to record all usage as coming from a single VO, you can configure the probe to override the 'guessed' VO. In the below example, replace <ENGAGE> with a registered VO that you would like to report as. If you don't have a VO that you are affiliated with, you may use \"Engage\". ... MapUnknownToGroup=\"1\" MapGroupToRole=\"1\" VOOverride=\"<ENGAGE>\" ... Non-Standard HTCondor Install \u00b6 If HTCondor is installed in a non-standard location (i.e., not RPMs, or relocated RPM outside /usr/bin ), then you need to tell the probe where to find the HTCondor binaries. This can be done with a script with a special attribute in /etc/gratia/condor/ProbeConfig , CondorLocation . Point it to the location of the HTCondor install, such that CondorLocation/bin/condor_version exists. New Data Directory \u00b6 If your PER_JOB_HISTORY_DIR HTCondor configuration variable is different from the default value, you must update the value of DataFolder in /etc/gratia/condor/ProbeConfig . To check the value of PER_JOB_HISTORY_DIR run the following command: user@host $ condor_config_val PER_JOB_HISTORY_DIR Different collector and other customizations \u00b6 By default the probe reports to the OSG GRACC. To change that you must edit the configuration file, /etc/gratia/condor/ProbeConfig , and replace the OSG production host with your desired one: ... CollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\" SSLHost=\"gratia-osg-prod.opensciencegrid.org:443\" SSLRegistrationHost=\"gratia-osg-prod.opensciencegrid.org:80\" ... Optional Configuration \u00b6 The following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to the section on using GlideinWMS . Allow users to specify where their jobs run Creating a group to test configuration changes Allowing users to specify where their jobs run \u00b6 In order to allow users to specify the sites at which their jobs want to run (or to test a specific site), a Frontend can be configured to match on DESIRED_Sites or ignore it if not specified. Modify /etc/gwms-frontend/frontend.xml using the following instructions: In the Frontend's global <match> stanza, set the match_expr : '((job.get(\"DESIRED_Sites\",\"nosite\")==\"nosite\") or (glidein[\"attrs\"][\"GLIDEIN_Site\"] in job.get(\"DESIRED_Sites\",\"nosite\").split(\",\")))' In the same <match> stanza, set the start_expr : '(DESIRED_Sites=?=undefined || stringListMember(GLIDEIN_Site,DESIRED_Sites,\",\")) Add the DESIRED_Sites attribute to the match attributes list: <match_attrs> <match_attr name= \"DESIRED_Sites\" type= \"string\" /> </match_attrs> Reconfigure the Frontend: root@host # /etc/init.d/gwms-frontend reconfig Creating a group for testing configuration changes \u00b6 To perform configuration changes without impacting production the recommended way is to create an ITB group in /etc/gwms-frontend/frontend.xml . This groupwould only match jobs that have the +is_itb=True ClassAd. Create a group named itb. Set the group's start_expr so that the group's glideins will only match user jobs with +is_itb=True : <match match_expr= \"True\" start_expr= \"(is_itb)\" > Set the factory_query_expr so that this group only communicates with ITB factories: <factory query_expr= 'FactoryType=?=\"itb\"' > Set the group's collector stanza to reference the ITB factory, replacing username@gfactory-1.t2.ucsd.edu with your factory identity: <collector DN= \"/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=glidein-itb.grid.iu.edu\" \\ factory_identity= \"gfactory@glidein-itb.grid.iu.edu\" \\ my_identity= \"username@gfactory-1.t2.ucsd.edu\" \\ node= \"glidein-itb.grid.iu.edu\" /> Set the job query_expr so that only ITB jobs appear in condor_q : <job query_expr= \"(!isUndefined(is_itb) && is_itb)\" > Reconfigure the Frontend (see the section below ): # on EL7 systems systemctl reload gwms-frontend Using GlideinWMS \u00b6 Managing GlideinWMS Services \u00b6 In addition to the GlideinWMS service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation for more info Gratia gratia-probes-cron Accounting software HTCondor condor HTTPD httpd GlideinWMS monitoring and staging GlideinWMS gwms-renew-proxies.timer Automatic proxy renewal gwms-frontend The main GlideinWMS service Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> Reconfiguring GlideinWMS \u00b6 After changing the configuration of GlideinWMS, run the following command as root : root@host # systemctl reload gwms-frontend Note Note that systemctl reload gwms-frontend will work only if: - gwms-frontend service is running - gwms-frontend service was started with systemctl Otherwise, you will get the following error in any of the cases: # systemctl reload gwms-frontend Job for gwms-frontend.service invalid. Upgrading GlideinWMS FrontEnd \u00b6 After upgrading the GlideinWMS RPM, you must issue an upgrade command to GlideinWMS: Stop the condor and gwms-frontend services as specified in this section Issue the upgrade command: root@host # /usr/sbin/gwms-frontend upgrade Start the condor and gwms-frontend services as specified in this section Validating GlideinWMS Frontend \u00b6 The complete validation of the Frontend is the submission of actual jobs. However, there are a few things that can be checked prior to submitting user jobs to HTCondor. Verifying Services Are Running \u00b6 There are a few things that can be checked prior to submitting user jobs to HTCondor. Verify all HTCondor daemons are started. user@host $ condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, SCHEDDJOBS2 COLLECTOR0 COLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12, COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23, COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40 Defined in '/etc/condor/config.d/11_gwms_secondary_collectors.config', line 193. If you don't see all the collectors and the two schedds , then the configuration must be corrected. There should be no startd daemons listed Verify all VO Frontend HTCondor services are communicating. user@host $ condor_status -any MyType TargetType Name glideresource None MM_fermicloud026@gfactory_inst Scheduler None fermicloud020.fnal.gov DaemonMaster None fermicloud020.fnal.gov Negotiator None fermicloud020.fnal.gov Collector None frontend_service@fermicloud020.fnal.gov Scheduler None schedd_jobs2@fermicloud020.fnal.gov To see the details of the glidein resource use condor_status -subsystem glideresource -l , including the GlideFactoryName. Verify that the Factory is seeing correctly the Frontend using condor_status -pool <FACTORY_HOST> -any -constraint 'FrontendName==\"<FRONTEND_NAME_FROM_CONFIG>\"' -l , including the GlideFactoryName. Where <FACTORY_HOST> is the hostname of the factory being used, for example: gfactory-1.t2.ucsd.edu and is the value set for \"frontend_name\" in the frontend.xml file GlideinWMS Job submission \u00b6 HTCondor submit file glidein-job.sub . This is a simple job printing the hostname of the host where the job is running: #file glidein-job.sub universe = vanilla executable = /bin/hostname output = glidein/test.out error = glidein/test.err requirements = IS_GLIDEIN == True log = glidein/test.log ShouldTransferFiles = YES when_to_transfer_output = ON_EXIT queue To submit the job: root@host # condor_submit glidein-job.sub Then you can control the job like a normal HTCondor job, e.g. to check the status of the job use condor_q . Monitoring Web pages \u00b6 You should be able to see the jobs also in the GlideinWMS monitoring pages that are made available on the Web: http://gwms-frontend-host.domain/vofrontend/monitor/ Troubleshooting GlideinWMS \u00b6 File Locations \u00b6 File Description File Location Configuration file /etc/gwms-frontend/frontend.xml Logs /var/log/gwms-frontend/ Startup script /usr/bin/gwms-frontend Web Directory /var/lib/gwms-frontend/web-area Web Base /var/lib/gwms-frontend/web-base Web configuration /etc/httpd/conf.d/gwms-frontend.conf Working Directory /var/lib/gwms-frontend/vofrontend/ Lock files /var/lib/gwms-frontend/vofrontend/lock/frontend.lock /var/lib/gwms-frontend/vofrontend/group_*/lock/frontend.lock Status files /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml Note /var/lib/gwms-frontend is also the home directory of the frontend user Certificates brief \u00b6 Here a short list of files to check when you change the certificates. Note that if you renew a proxy or certificate and the DN remains the same no configuration file needs to change, just put the renewed certificate/proxy in place. File Description File Location Configuration file /etc/gwms-frontend/frontend.xml HTCondor certificates map /etc/condor/certs/condor_mapfile (1) Host certificate and key (2) /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem VO Frontend proxy (from host certificate) /tmp/vofe_proxy (3) Pilot proxy /tmp/pilot_proxy (3) If using HTCondor RPM installation, e.g. the one coming from OSG. If you have separate/multiple HTCondor hosts (schedds, collectors, negotiators, ..) you may have to check this file on all of them to make sure that the HTCondor authentication works correctly. Used to create the VO Frontend proxy if following the instructions above If using the Frontend configuration and scripts described above in this document . These paths are the ones specified in the configuration file. Remember also that when you change DN: The VO Frontend certificate DN must be communicated to the GlideinWMS Factory ( see above ) The pilot proxy must be able to run jobs at the sites you are using, e.g. by being added to the correct VO in OSG (the Factory forwards the proxy and does not care about the DN) Increase the log level and change rotation policies \u00b6 You can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the process_log section of /etc/gwms-frontend/frontend.xml : <log_retention> <process_logs> <process_log extension= \"all\" max_days= \"7.0\" max_mbytes= \"100.0\" min_days= \"3.0\" msg_types= \"DEBUG,EXCEPTION,INFO,ERROR,ERR\" /> You can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files: max_bytes is the max size of the log files max_days it will be rotated. compression specifies if rotated files are compressed backup_count is the number of rotated log files kept Further details are in the reference documentation . Frontend reconfig failing \u00b6 If service gwms-frontend reconfig fails at the end with an error like \"Writing back config file failed, Reconfiguring the frontend [FAILED]\", make sure that /etc/gwms-frontend/ belongs to the frontend user. It must be able to write to update the configuration file. Frontend failing to start \u00b6 If the startup script of the frontend is failing, check the log file for errors (probably /var/log/gwms-frontend/frontend/frontend.<TODAY's DATE>.err.log and .debug.log ). If you find errors like \"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\" and \"IOError: [Errno 9] Bad file descriptor\" you may have an empty status file ( /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml ) that causes GlideinWMS Frontend not to start. The glideinFrontend crashes after a XML parsing exception visible in the log file (\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\"). Remove the status file. Then start the frontend. The Frontend will be fixed in future versions to handle this automatically. Certificates not there \u00b6 The scripts should send an email warning if there are problems and they fail to generate the proxies. Anyway something could go wrong and you want to check manually. If you are using the scripts to generate automatically the proxies but the proxies are not there (in /tmp or wherever you expect them): make sure that the scripts are there and configured with the correct values make sure that the scripts are executable make sure that the scripts are in frontend 's crontab make sure that the certificates (or master proxy) used to generate the proxies is not expired Failed authentication \u00b6 If you get a failed authentication error (e.g. \"Failed to talk to factory_pool gfactory-1.t2.ucsd.edu...) then: check that you have the right x509 certificates mentioned in the security section of /etc/gwms-frontend/frontend.xml the owner must be frontend (user running the frontend) the permission must be 600 they must be valid for more than one hour (2/300 hours), at least the non VO part check that the clock is synchronized (see HostTimeSetup) Frontend doesn't trust Factory \u00b6 If your frontend complains in the debug log: code 256:['Error: communication error\\n', 'AUTHENTICATE:1003:Failed to authenticate with any method\\n', 'AUTHENTICATE:1004:Failed to authenticate using GSI\\n', \"GSI:5006:Failed to authenticate because the subject '/DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu' is not currently trusted by you. If it should be, add it to GSI_DAEMON_NAME in the condor_config, or use the environment variable override (check the manual).\\n\", 'GSI:5004:Failed to gss_assist_gridmap /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu to a local user. A possible solution is to comment/remove the LOCAL_CONFIG_DIR in the file /var/lib/gwms-frontend/vofrontend/frontend.condor_config . No security credentials match for factory pool ..., not advertising request \u00b6 You may see a warning like \"No security credentials match for factory pool ..., not advertising request\", if the trust_domain and auth_method of an entry in the Factory configuration is not matching any of the trust_domain , type couples in the credentials in the Frontend configuration. This causes the Frontend not to use some Factory entries (the ones not matching) and may end up without entries to send glideins to. To fix the problem make sure that those attributes match as desired. Jobs not running \u00b6 If your jobs remain Idle Check the frontend log files (see above) Check the HTCondor log files ( condor_config_val LOG will give you the correct log directory): Specifically look the CollectorXXXLog files Common causes of problems could be: x509 certificates missing or expired or too short-lived proxy incorrect ownership or permission on the certificate/proxy file missing certificates If the Frontend http server is down in the glidein logs in the Factory there will be errors like \"Failed to load file 'description.dbceCN.cfg' from http://FRONTEND_HOST/vofrontend/stage .\" check that the http server is running and you can reach the URL ( http://FRONTEND_HOST/vofrontend/stage/description.dbceCN.cfg ) Getting Help \u00b6 To get assistance about the OSG software please use this page . For specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support glideinwms-support@fnal.gov To request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to osg-gfactory-support@physics.ucsd.edu (see below). References \u00b6 Definitions: What is a Virtual Organization Documents about the Glidein-WMS system and the VO frontend: http://glideinwms.fnal.gov/ Users \u00b6 The Glidein WMS Frontend installation will create the following users unless they are already created. User Default uid Comment apache 48 Runs httpd to provide the monitoring page (installed via dependencies). condor none HTCondor user (installed via dependencies). frontend none This user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins. gratia none Runs the Gratia probes to collect accounting data (optional see the Gratia section below ) Warning UID 48 is reserved by RedHat for user apache . If it is already taken by a different username, you will experience errors. Certificates \u00b6 This document has a proxy configuration section that uses the host certificate/key and a user certificate to generate the required proxies. Certificate User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem Host key root /etc/grid-security/hostkey.pem Here are instructions to request a host certificate. Networking \u00b6 Service Name Protocol Port Number Inbound Outbound Comment HTCondor port range tcp LOWPORT, HIGHPORT YES contiguous range of ports GlideinWMS Frontend tcp 9618, 9620 to 9660 YES HTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs) The VO frontend must have reliable network connectivity, be on the public internet (no NAT), and preferably with no firewalls. Incoming TCP ports 9618 to 9660 must be open.","title":"Install GlideinWMS Frontend"},{"location":"other/install-gwms-frontend/#glideinwms-vo-frontend-installation","text":"This document describes how to install the Glidein Workflow Managment System (GlideinWMS) VO Frontend for use with the OSG Glidein factory. This software is the minimum requirement for a VO to use GlideinWMS. This document assumes expertise with HTCondor and familiarity with the GlideinWMS software. It does not cover anything but the simplest possible install. Please consult the GlideinWMS reference documentation for advanced topics, including non- root , non-RPM-based installation. This document covers three components of the GlideinWMS a VO needs to install: User Pool Collectors : A set of condor_collector processes. Pilots submitted by the factory will join to one of these collectors to form a HTCondor pool. User Pool Schedd : A condor_schedd . Users may submit HTCondor vanilla universe jobs to this schedd; it will run jobs in the HTCondor pool formed by the User Pool Collectors . Glidein Frontend : The frontend will periodically query the User Pool Schedd to determine the desired number of running job slots. If necessary, it will request the Factory to launch additional pilots. This guide covers installation of all three components on the same host: it is designed for small to medium VOs (see the Hardware Requirements below). Given a significant, large host, we have been able to scale the single-host install to 20,000 running jobs.","title":"GlideinWMS VO Frontend Installation"},{"location":"other/install-gwms-frontend/#before-starting","text":"Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If they do not exist already, the installation will create the Linux users apache (UID 48), condor , frontend , and gratia Network: The VO frontend must have reliable network connectivity and be on the public internet (i.e. no NAT). The latest version requires the following TCP ports to be open: 80 (HTTP) for monitoring and serving configuration to workers 9618 (HTCondor shared port) for HTCondor daemons including the Schedd and User Collector 9620 to 9660 for secondary collectors (depending on configuration, see below) Host choice : The GlideinWMS VO Frontend has the following hardware requirements for a production host: CPU : Four cores, preferably no more than 2 years old. RAM : 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed. Disk : 30GB will be sufficient for all the binaries, config and log files related to GlideinWMS. As this will be an interactive access point, have enough disk space for your users' jobs. Note The default configuration uses a port range (9620 to 9660) for the secondary collectors. You can configure the secondary collectors to use the shared port 9618 instead; this will become the default in the future. Note GlideinWMS versions prior to 3.4.1 also required port 9615 for the Schedd, and did not support using shared port for the secondary collectors. If you are upgrading a standalone access point from version 3.4 or earlier, the default open port has changed from 9615 to 9618, and you need to update your firewall rules to reflect this change. You can figure out which port will be used by running the following command: console condor_config_val SHARED_PORT_ARGS For more detailed information, see Configuring GlideinWMS Frontend . As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"other/install-gwms-frontend/#credentials-and-proxies","text":"The VO Frontend will use two credentials in its interactions with the other GlideinWMS services. At this time, these will be proxy files. the VO Frontend proxy (used to authenticate with the other GlideinWMS services). one or more GlideinWMS pilot proxies (used/delegated to the Factory services and submitted on the GlideinWMS pilot jobs). The VO Frontend proxy and the pilot proxy can be the same. By default, the VO Frontend will run as user frontend (UID is machine dependent) so these proxies must be owned by the user frontend . Note Both proxies need to be passwordless to allow automatic proxy renewal .","title":"Credentials and Proxies"},{"location":"other/install-gwms-frontend/#vo-frontend-proxy","text":"The use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the proxy configuration section . You must give the Factory operations team the DN of this proxy when you initially setup the Frontend and each time the DN changes .","title":"VO Frontend proxy"},{"location":"other/install-gwms-frontend/#pilot-proxies","text":"These proxies are used by the Factory to submit the GlideinWMS pilot jobs. Therefore, they must be authorized to access to the CEs (Factory entry points) where jobs are submitted. There is no need to notify the Factory operation about the DN of these proxies (neither at the initial registration nor for subsequent changes). These additional proxies have no special requirements or controls added by the Factory but will probably require VO attributes because of the CEs: if you are able to use one of these proxies to submit jobs to the corresponding CEs where the Factory runs GlideinWMS pilots for you, then the proxies are OK. You can test each of your proxies using globusrun or HTCondor-G. To check the important information about a PEM certificate you can use: openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout . You will need that to find out information for the configuration files and the request to the GlideinWMS Factory.","title":"Pilot proxies"},{"location":"other/install-gwms-frontend/#osg-factory-access","text":"Before installing the GlideinWMS VO Frontend you need the information about a Glidein Factory that you can access: (recommended) OSG is managing a factory at UCSD You have another Glidein Factory that you can access To request access to the OSG Glidein Factory at UCSD you have to send an email to osg-gfactory-support@physics.ucsd.edu providing: Your Name The VO that is utilizing the VO Frontend The DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the proxy configuration section ) You can propose a security name that will have to be confirmed/changed by the Factory managers (see below) A list of sites where you want to run: Your VO must be supported on those sites You can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers You can start with one single site In the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend The exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\". The host of the Factory Collector: gfactory-1.t2.ucsd.edu The DN os the factory, e.g. /DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu The factory identity, e.g.: gfactory@gfactory-1.t2.ucsd.edu The identity on the Factory you will be mapped to. Something like: username@gfactory-1.t2.ucsd.edu Your security name. A unique name, usually containing your VO name: My_SecName A string to add in the main Factory query_expr in the Frontend configuration, e.g. stringListMember(\"<VO>\",GLIDEIN_Supported_VOs) . This is used to select the entries you can use. From there you get the correct name of the VO (above in this list).","title":"OSG Factory access"},{"location":"other/install-gwms-frontend/#installing-glideinwms-frontend","text":"","title":"Installing GlideinWMS Frontend"},{"location":"other/install-gwms-frontend/#installing-htcondor","text":"If you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository: root@host # yum install condor.x86_64 If you already have installed HTCondor using a tarball or a source other than the OSG ROM, you will need to install the empty-condor RPM: root@host # yum install empty-condor --enablerepo = osg-empty","title":"Installing HTCondor"},{"location":"other/install-gwms-frontend/#installing-the-vo-frontend-rpm","text":"Install the RPM and dependencies (be prepared for a lot of dependencies). root@host # yum install glideinwms-vofrontend This will install the current production release verified and tested by OSG with default HTCondor configuration. This command will install the GlideinWMS vofrontend, HTCondor, the OSG client, and all the required dependencies all on one node. If you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows: yum install --enablerepo=osg-testing glideinwms-vofrontend : The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well. yum install --enablerepo=osg-upcoming glideinwms-vofrontend : The most recent development series release, i.e. version 3.3 release. This has newer features such as cloud submission support, but is less tested. Note that these commands will install default HTCondor configurations with all GlideinWMS services on one node.","title":"Installing the VO Frontend RPM"},{"location":"other/install-gwms-frontend/#installing-glideinwms-frontend-on-multiple-nodes-advanced","text":"For advanced users expecting heavy usage on their access point, you may want to consider splitting the user collector, user submit, and vo frontend services. This can be doing using the following three commands (on different machines): root@host # yum install glideinwms-vofrontend-standalone root@host # yum install glideinwms-usercollector root@host # yum install glideinwms-userschedd In addition, you will need to perform the following steps: On the vofrontend and userschedd, modify CONDOR_HOST to point to your usercollector. This is in /etc/condor/config.d/00_gwms_general.config . You can also override this value by placing it in a new config file. (For instance, /etc/condor/config.d/99_local_custom.config to avoid rpmsave/rpmnew conflicts on upgrades). In /etc/condor/certs/condor_mapfile , you will need to add the DNs of each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the glidecondor_addDN to add these values. In the /etc/gwms-frontend/frontend.xml file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend.xml are in the following sections.","title":"Installing GlideinWMS Frontend on Multiple Nodes (Advanced)"},{"location":"other/install-gwms-frontend/#configuring-glideinwms-frontend","text":"After installing the RPM, you need to configure the components of the GlideinWMS VO Frontend: Edit Frontend configuration options Edit HTCondor configuration options Create a HTCondor grid map file Reconfigure and Start the Frontend","title":"Configuring GlideinWMS Frontend"},{"location":"other/install-gwms-frontend/#configuring-the-frontend","text":"The VO Frontend configuration file is /etc/gwms-frontend/frontend.xml . The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference. The VO you are affiliated with. This will identify those CEs that the GlideinWMS pilot will be authorized to run on using the pilot proxy described previously in this section . Sometimes the whole query_expr is provided to you by the Factory operators (see Factory access above): <factory query_expr= '((stringListMember(\"VO\", GLIDEIN_Supported_VOs)))' > Factory collector information. The username that you are assigned by the Factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production Factory, you will have to change also DN , factory_identity and node attributes. (refer to the information provided to you by the Factory operator): <collector DN= \"/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\" comment= \"Define factory collector globally for simplicity\" factory_identity= \"gfactory@gfactory-1.t2.ucsd.edu\" my_identity= \"username@gfactory-1.t2.ucsd.edu\" node= \"gfactory-1.t2.ucsd.edu\" /> Frontend security information. The classad_proxy in the security entry is the location of the VO Frontend proxy described previously here . The proxy_DN is the DN of the classad_proxy above. The security_name identifies this VO Frontend to the the Factory, It is provided by the Factory operator. The absfname in the credential entry is the location of the GlideinWMS pilot proxy described in the requirements section here . There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources). The type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the Factory. If there is no match, between these two attributes in one of the credentials and the corresponding ones in some entry in one of the Factories, then this Frontend cannot trigger glideins. Both the classad_proxy and absfname files should be owned by frontend user. <security classad_proxy= \"/tmp/vo_proxy\" proxy_DN= \"DN of vo_proxy\" proxy_selection_plugin= \"ProxyAll\" security_name= \"The security name, this is used by factory\" sym_key= \"aes_256_cbc\" > <credentials> <credential absfname= \"/tmp/pilot_proxy\" security_class= \"frontend\" trust_domain= \"OSG\" type= \"grid_proxy\" /> </credentials> </security> The schedd information. The DN of the VO Frontend Proxy described previously here . The fullname attribute is the fully qualified domain name of the host where you installed the VO Frontend ( hostname --fqdn ). A secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the Frontend to service requests from multiple access points. <schedds> <schedd DN= \"Cert DN used by the schedd at fullname:\" fullname= \"Hostname of the schedd\" /> <schedd DN= \"Cert DN used by the second Schedd at fullname:\" fullname= \"schedd name@Hostname of second schedd\" /> </schedds> The User Collector information. The DN of the VO Frontend Proxy described previously here . The node attribute is the full hostname of the collectors ( hostname --fqdn ) and port The secondary attribute indicates whether the element is for the primary or secondary collectors (True/False). The default HTCondor configuration of the VO Frontend starts multiple Collector processes on the host ( /etc/condor/config.d/11_gwms_secondary_collectors.config ). The DN and hostname on the first line are the hostname and the host certificate of the VO Frontend. The DN and hostname on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector connection can be defined as sinful string for the sock case , e.g., hostname.domain.tld:9618?sock=collector16. [Example 1] :::xml <collector DN=\"DN of main collector\" node=\"hostname.domain.tld:9618\" secondary=\"False\"/> <collector DN=\"DN of secondary collectors (usually same as DN in line above)\" node=\"hostname.domain.tld:9620-9660\" secondary=\"True\"/> Note In GlideinWMS v3.4.1, shared port only configuration is incompatible if talking to older Factories (v3.4 or older). We strongly recommend any user of GlideinWMS Frontend v3.4.1 or newer, to transition to the use of shared port for secondary collectors and CCBs. The shared port configuration is incompatible if your Frontend is talking to Factories v3.4 or older and you'll get an error telling you to wait. To transition to the use of shared port for secondary collectors, you have to change the collectors section in the Frontend configuration. If you are using the default port range for the secondary collectors as shown in [Example 2] below, then you should replace it with port 9618 and the sock-range as shown in [Example 1] above. If you have a more complex configuration, please read the detailed GlideinWMS configuration [Example 2] :::xml <collector DN=\"DN of main collector\" node=\"hostname.domain.tld:9618\" secondary=\"False\"/> <collector DN=\"DN of secondary collectors (usually same as DN in line above)\" node=\u201chostname.domain.tld:9618?sock=collector0-40\" secondary=\"True\"/> The CCBs information. If you have a different configuration of the HTCondor Connection Brokering (CCB servers) from the default (usually the section is empty as the User Collectors acts as CCB if needed), you can set the connection in the CCB section the same way that User Collector information previously mentioned. Also, the same rules for transition to shared_port of the connections, apply to the CCBs. :::xml <ccb DN=\"DN of the CCB server\" node=\"hostname.domain.tld:9618\"/> <ccb DN=\"DN of the CCB server\" node=\u201chostname.domain.tld:9618?sock=collector0-40\" secondary=\"True\"/> Warning The Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths. Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation): frontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions) for RPM installs, work base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value)","title":"Configuring the Frontend"},{"location":"other/install-gwms-frontend/#using-a-different-factory","text":"The configuration above points to the OSG production Factory. If you are using a different Factory, then you have to: replace gfactory@gfactory-1.t2.ucsd.edu and gfactory-1.t2.ucsd.edu with the correct values for your Factory. And control also that the name used for the Frontend () matches. make sure that the Factory is advertising the attributes used in the Factory query expression ( query_expr ).","title":"Using a Different Factory"},{"location":"other/install-gwms-frontend/#configuring-htcondor","text":"The HTCondor configuration for the Frontend is placed in /etc/condor/config.d . 00_gwms_general.config 01_gwms_collectors.config 02_gwms_schedds.config 03_gwms_local.config 11_gwms_secondary_collectors.config 90_gwms_dns.config For most installations create a new file named /etc/condor/config.d/92_local_condor.config","title":"Configuring HTCondor"},{"location":"other/install-gwms-frontend/#using-other-htcondor-rpms-eg-uw-madison-htcondor-rpm","text":"The above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using yum list condor . The version name should include \"osg\", e.g. 8.6.4-3.osg.el7 . If you are using the UW Madison HTCondor RPMS, be aware of the following changes: This HTCondor RPM uses a file /etc/condor/condor_config.local to add your local machine slot to the user pool. If you want to disable this behavior (recommended), you should blank out that file or comment out the line in /etc/condor/condor_config for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to /etc/condor/config.d ) Note that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the GlideinWMS RPMs, but please take note if you use this variable in your job submissions or other customizations. In general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files: root@host # condor_config_val -config Configuration source: /etc/condor/condor_config Local configuration sources: /etc/condor/config.d/00_gwms_general.config /etc/condor/config.d/01_gwms_collectors.config /etc/condor/config.d/02_gwms_schedds.config /etc/condor/config.d/03_gwms_local.config /etc/condor/config.d/11_gwms_secondary_collectors.config /etc/condor/config.d/90_gwms_dns.config /etc/condor/condor_config.local If, like in the example above, the GlideinWMS configuration files are not the last ones in the list please verify that important configuration options have not been overridden by the other configuration files.","title":"Using other HTCondor RPMs, e.g. UW Madison HTCondor RPM"},{"location":"other/install-gwms-frontend/#verifying-your-htcondor-configuration","text":"The GlideinWMS configuration files in /etc/condor/config.d should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files. Verify the alll the expected HTCondor daemons are running: root@host # condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, COLLECTOR0 COLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12, COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23, COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40 Defined in '/etc/condor/config.d/11_gwms_secondary_collectors.config', line 193. If you don't see all the collectors. shared port and the two schedd, then the configuration must be corrected. There should be no startd daemons listed.","title":"Verifying your HTCondor configuration"},{"location":"other/install-gwms-frontend/#creating-a-htcondor-grid-mapfile","text":"The HTCondor mapfile ( /etc/condor/certs/condor_mapfile ) is used for authentication between the GlideinWMS pilot running on a remote worker node, and the local collector. HTCondor uses the mapfile to map certificates to pseudo-users on the local machine. It is important that you map the DN's of: Each schedd proxy : The DN of each schedd that the frontend talks to. Specified in the frontend.xml schedd element DN attribute: <schedds> <schedd DN= \"/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\" fullname= \"YOUR_HOST\" /> <schedd DN= \"/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\" fullname= \"schedd_jobs2@YOUR_HOST\" /> </schedds> Frontend proxy : The DN of the proxy that the Frontend uses to communicate with the other GlideinWMS services. Specified in the frontend.xml security element proxy_DN attribute: <security classad_proxy= \"/tmp/vo_proxy\" proxy_DN= \"DN of vo_proxy\" .... Each pilot proxy The DN of each proxy that the frontend forwards to the factory to use with the GlideinWMS pilots. This allows the GlideinWMS pilot jobs to communicate with the User Collector. Specified in the frontend.xml proxy absfname attribute (you need to specify the DN of each of those proxies: <security .... <proxies > < proxy absfname= \"/tmp/vo_proxy\" .... : </proxies > Below is an example mapfile, by default found in /etc/condor/certs/condor_mapfile . In this example there are lines for each of services mentioned above. GSI \"<DN OF SCHEDD PROXY>\" schedd GSI \"<DN OF FRONTEND PROXY>\" frontend GSI \"<DN OF PILOT PROXY>\" pilot_proxy GSI \"^/DC=org/DC=doegrids/OU=Services/CN=personal-submit-host2.mydomain.edu$\" <example_of_format> GSI (.*) anonymous FS (.*) \\1 Change <DN OF SCHEDD PROXY> , <DN OF FRONTEND PROXY> , and <DN OF PILOT PROXY> to the distinguished names of the respective proxies.","title":"Creating a HTCondor grid mapfile."},{"location":"other/install-gwms-frontend/#restarting-htcondor","text":"After configuring HTCondor, be sure to restart HTCondor: root@host # service condor restart","title":"Restarting HTCondor"},{"location":"other/install-gwms-frontend/#proxy-configuration","text":"GlideinWMS comes with the gwms-renew-proxies service that can automatically generate and renew the pilot proxies and VO Frontend proxy . To configure this service, modify /etc/gwms-frontend/proxies.ini using the following instructions: For each of your pilot proxies , create a [PILOT <NAME>] section, where <NAME> is a descriptive name for the proxy that is unique to your local configuration. In each section, set the proxy_cert , proxy_key , output , and vo corresponding to each pilot proxy: [PILOT <NAME>] proxy_cert = <PATH TO THE PILOT CERTIFICATE> proxy_key = <PATH TO THE PILOT KEY> output = <PATH TO CREATE THE PILOT PROXY> vo = <NAME OF VIRTUAL ORGANIZATION> Change <PATH TO THE PILOT CERTIFICATE> , <PATH TO THE PILOT KEY> and <PATH TO CREATE THE PILOT PROXY> appropriately to point to the locations of the pilot certificate, pilot key, and pilot proxy, respectively. Additionally, in each [PILOT <NAME>] section, you must specify how the proxy's VOMS attributes will be signed by setting use_voms_server . Choose one of the following options: To directly sign the VOMS attributes (recommended), you must have access to the vo 's certificate and key. Specify the paths to the vo certificate and key, and optionally, the VOMS attribute (e.g. /osg/Role=NULL/Capability=NULL for the OSG VO): use_voms_server = false vo_cert = <PATH TO THE PILOT CERTIFICATE> vo_key = <PATH TO THE PILOT KEY> fqan = <VOMS ATTRIBUTE> Note If you do not have access to the vo 's voms_cert and voms_key , contact the VO manager. To have your proxy's VOMS attributes signed by the vo 's VOMS server, set use_voms_server = true and the VOMS attribute (e.g. /osg/Role=NULL/Capability=NULL for the OSG VO): use_voms_server = true fqan = <VOMS ATTRIBUTE> Warning Due to the retirement of VOMS Admin server in the OSG, use_voms_server = false is the preferred method for signing VOMS attributes. Optionally, the proxy renewal frequency and lifetime (in hours) can be specified in each [PILOT <NAME>] section: # Default: 1 frequency = <RENEWAL FREQUENCY> # Default: 24 lifetime = <PROXY LIFETIME> Configure the location and output of the VO Frontend proxy under the [FRONTEND] section and set the proxy_cert , proxy_key , and output to paths corresponding to your VO Frontend: [FRONTEND] proxy_cert = <PATH TO THE FRONTEND CERTIFICATE> proxy_key = <PATH TO THE FRONTEND KEY> output = <PATH TO CREATE THE FRONTEND PROXY> Note output must be the same path as the classad_proxy specified in this section (OPTIONAL) If you are running the gwms-frontend service under a <NON-DEFAULT USER> (default: frontend ), specify the user as the owner of your proxies under the [COMMON] section: [COMMON] owner = <NON-DEFAULT USER> Note The [COMMON] section is required but its contents are optional","title":"Proxy Configuration"},{"location":"other/install-gwms-frontend/#adding-gratia-accounting-and-a-local-monitoring-page-on-a-production-server","text":"You must report accounting information if you are running more than a few test jobs on the OSG . Install the GlideinWMS Gratia Probe on each of your access points in your GlideinWMS installation: root@host # yum install gratia-probe-glideinwms Edit the ProbeConfig located in /etc/gratia/condor/ProbeConfig . First, edit the SiteName and ProbeName to be a unique identifier for your GlideinWMS access point. There can be multiple probes (with different names) per site. If you haven't already, you should register your GlideinWMS access point in OIM . Then you can use the name you used to register the resource. ProbeName=\"condor:<hostname>\" SiteName=\"HCC-GlideinWMW-Frontend\" Next, turn the probe on by setting EnableProbe : EnableProbe=\"1\" Reconfigure HTCondor: root@host # condor_reconfig","title":"Adding Gratia Accounting and a Local Monitoring Page on a Production Server"},{"location":"other/install-gwms-frontend/#optional-accounting-configuration","text":"The following sections contain additional configuration that may be required depending on the customizations you've made to your GlideinWMS frontend installation.","title":"Optional Accounting Configuration"},{"location":"other/install-gwms-frontend/#users-without-certificates","text":"If you have users that submit jobs without a certificate explicitly declared in the submit file, you will need to add MapUnknownToGroup to the ProbeConfig. In the file /etc/gratia/condor/ProbeConfig , add the value after the EnableProbe . ... SuppressGridLocalRecords=\"0\" EnableProbe=\"1\" MapUnknownToGroup=\"1\" Title3=\"Tuning parameter\" ... Further, if you want to record all usage as coming from a single VO, you can configure the probe to override the 'guessed' VO. In the below example, replace <ENGAGE> with a registered VO that you would like to report as. If you don't have a VO that you are affiliated with, you may use \"Engage\". ... MapUnknownToGroup=\"1\" MapGroupToRole=\"1\" VOOverride=\"<ENGAGE>\" ...","title":"Users without Certificates"},{"location":"other/install-gwms-frontend/#non-standard-htcondor-install","text":"If HTCondor is installed in a non-standard location (i.e., not RPMs, or relocated RPM outside /usr/bin ), then you need to tell the probe where to find the HTCondor binaries. This can be done with a script with a special attribute in /etc/gratia/condor/ProbeConfig , CondorLocation . Point it to the location of the HTCondor install, such that CondorLocation/bin/condor_version exists.","title":"Non-Standard HTCondor Install"},{"location":"other/install-gwms-frontend/#new-data-directory","text":"If your PER_JOB_HISTORY_DIR HTCondor configuration variable is different from the default value, you must update the value of DataFolder in /etc/gratia/condor/ProbeConfig . To check the value of PER_JOB_HISTORY_DIR run the following command: user@host $ condor_config_val PER_JOB_HISTORY_DIR","title":"New Data Directory"},{"location":"other/install-gwms-frontend/#different-collector-and-other-customizations","text":"By default the probe reports to the OSG GRACC. To change that you must edit the configuration file, /etc/gratia/condor/ProbeConfig , and replace the OSG production host with your desired one: ... CollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\" SSLHost=\"gratia-osg-prod.opensciencegrid.org:443\" SSLRegistrationHost=\"gratia-osg-prod.opensciencegrid.org:80\" ...","title":"Different collector and other customizations"},{"location":"other/install-gwms-frontend/#optional-configuration","text":"The following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to the section on using GlideinWMS . Allow users to specify where their jobs run Creating a group to test configuration changes","title":"Optional Configuration"},{"location":"other/install-gwms-frontend/#allowing-users-to-specify-where-their-jobs-run","text":"In order to allow users to specify the sites at which their jobs want to run (or to test a specific site), a Frontend can be configured to match on DESIRED_Sites or ignore it if not specified. Modify /etc/gwms-frontend/frontend.xml using the following instructions: In the Frontend's global <match> stanza, set the match_expr : '((job.get(\"DESIRED_Sites\",\"nosite\")==\"nosite\") or (glidein[\"attrs\"][\"GLIDEIN_Site\"] in job.get(\"DESIRED_Sites\",\"nosite\").split(\",\")))' In the same <match> stanza, set the start_expr : '(DESIRED_Sites=?=undefined || stringListMember(GLIDEIN_Site,DESIRED_Sites,\",\")) Add the DESIRED_Sites attribute to the match attributes list: <match_attrs> <match_attr name= \"DESIRED_Sites\" type= \"string\" /> </match_attrs> Reconfigure the Frontend: root@host # /etc/init.d/gwms-frontend reconfig","title":"Allowing users to specify where their jobs run"},{"location":"other/install-gwms-frontend/#creating-a-group-for-testing-configuration-changes","text":"To perform configuration changes without impacting production the recommended way is to create an ITB group in /etc/gwms-frontend/frontend.xml . This groupwould only match jobs that have the +is_itb=True ClassAd. Create a group named itb. Set the group's start_expr so that the group's glideins will only match user jobs with +is_itb=True : <match match_expr= \"True\" start_expr= \"(is_itb)\" > Set the factory_query_expr so that this group only communicates with ITB factories: <factory query_expr= 'FactoryType=?=\"itb\"' > Set the group's collector stanza to reference the ITB factory, replacing username@gfactory-1.t2.ucsd.edu with your factory identity: <collector DN= \"/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=glidein-itb.grid.iu.edu\" \\ factory_identity= \"gfactory@glidein-itb.grid.iu.edu\" \\ my_identity= \"username@gfactory-1.t2.ucsd.edu\" \\ node= \"glidein-itb.grid.iu.edu\" /> Set the job query_expr so that only ITB jobs appear in condor_q : <job query_expr= \"(!isUndefined(is_itb) && is_itb)\" > Reconfigure the Frontend (see the section below ): # on EL7 systems systemctl reload gwms-frontend","title":"Creating a group for testing configuration changes"},{"location":"other/install-gwms-frontend/#using-glideinwms","text":"","title":"Using GlideinWMS"},{"location":"other/install-gwms-frontend/#managing-glideinwms-services","text":"In addition to the GlideinWMS service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation for more info Gratia gratia-probes-cron Accounting software HTCondor condor HTTPD httpd GlideinWMS monitoring and staging GlideinWMS gwms-renew-proxies.timer Automatic proxy renewal gwms-frontend The main GlideinWMS service Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME>","title":"Managing GlideinWMS Services"},{"location":"other/install-gwms-frontend/#reconfiguring-glideinwms","text":"After changing the configuration of GlideinWMS, run the following command as root : root@host # systemctl reload gwms-frontend Note Note that systemctl reload gwms-frontend will work only if: - gwms-frontend service is running - gwms-frontend service was started with systemctl Otherwise, you will get the following error in any of the cases: # systemctl reload gwms-frontend Job for gwms-frontend.service invalid.","title":"Reconfiguring GlideinWMS"},{"location":"other/install-gwms-frontend/#upgrading-glideinwms-frontend","text":"After upgrading the GlideinWMS RPM, you must issue an upgrade command to GlideinWMS: Stop the condor and gwms-frontend services as specified in this section Issue the upgrade command: root@host # /usr/sbin/gwms-frontend upgrade Start the condor and gwms-frontend services as specified in this section","title":"Upgrading GlideinWMS FrontEnd"},{"location":"other/install-gwms-frontend/#validating-glideinwms-frontend","text":"The complete validation of the Frontend is the submission of actual jobs. However, there are a few things that can be checked prior to submitting user jobs to HTCondor.","title":"Validating GlideinWMS Frontend"},{"location":"other/install-gwms-frontend/#verifying-services-are-running","text":"There are a few things that can be checked prior to submitting user jobs to HTCondor. Verify all HTCondor daemons are started. user@host $ condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, SCHEDDJOBS2 COLLECTOR0 COLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12, COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23, COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40 Defined in '/etc/condor/config.d/11_gwms_secondary_collectors.config', line 193. If you don't see all the collectors and the two schedds , then the configuration must be corrected. There should be no startd daemons listed Verify all VO Frontend HTCondor services are communicating. user@host $ condor_status -any MyType TargetType Name glideresource None MM_fermicloud026@gfactory_inst Scheduler None fermicloud020.fnal.gov DaemonMaster None fermicloud020.fnal.gov Negotiator None fermicloud020.fnal.gov Collector None frontend_service@fermicloud020.fnal.gov Scheduler None schedd_jobs2@fermicloud020.fnal.gov To see the details of the glidein resource use condor_status -subsystem glideresource -l , including the GlideFactoryName. Verify that the Factory is seeing correctly the Frontend using condor_status -pool <FACTORY_HOST> -any -constraint 'FrontendName==\"<FRONTEND_NAME_FROM_CONFIG>\"' -l , including the GlideFactoryName. Where <FACTORY_HOST> is the hostname of the factory being used, for example: gfactory-1.t2.ucsd.edu and is the value set for \"frontend_name\" in the frontend.xml file","title":"Verifying Services Are Running"},{"location":"other/install-gwms-frontend/#glideinwms-job-submission","text":"HTCondor submit file glidein-job.sub . This is a simple job printing the hostname of the host where the job is running: #file glidein-job.sub universe = vanilla executable = /bin/hostname output = glidein/test.out error = glidein/test.err requirements = IS_GLIDEIN == True log = glidein/test.log ShouldTransferFiles = YES when_to_transfer_output = ON_EXIT queue To submit the job: root@host # condor_submit glidein-job.sub Then you can control the job like a normal HTCondor job, e.g. to check the status of the job use condor_q .","title":"GlideinWMS Job submission"},{"location":"other/install-gwms-frontend/#monitoring-web-pages","text":"You should be able to see the jobs also in the GlideinWMS monitoring pages that are made available on the Web: http://gwms-frontend-host.domain/vofrontend/monitor/","title":"Monitoring Web pages"},{"location":"other/install-gwms-frontend/#troubleshooting-glideinwms","text":"","title":"Troubleshooting GlideinWMS"},{"location":"other/install-gwms-frontend/#file-locations","text":"File Description File Location Configuration file /etc/gwms-frontend/frontend.xml Logs /var/log/gwms-frontend/ Startup script /usr/bin/gwms-frontend Web Directory /var/lib/gwms-frontend/web-area Web Base /var/lib/gwms-frontend/web-base Web configuration /etc/httpd/conf.d/gwms-frontend.conf Working Directory /var/lib/gwms-frontend/vofrontend/ Lock files /var/lib/gwms-frontend/vofrontend/lock/frontend.lock /var/lib/gwms-frontend/vofrontend/group_*/lock/frontend.lock Status files /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml Note /var/lib/gwms-frontend is also the home directory of the frontend user","title":"File Locations"},{"location":"other/install-gwms-frontend/#certificates-brief","text":"Here a short list of files to check when you change the certificates. Note that if you renew a proxy or certificate and the DN remains the same no configuration file needs to change, just put the renewed certificate/proxy in place. File Description File Location Configuration file /etc/gwms-frontend/frontend.xml HTCondor certificates map /etc/condor/certs/condor_mapfile (1) Host certificate and key (2) /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem VO Frontend proxy (from host certificate) /tmp/vofe_proxy (3) Pilot proxy /tmp/pilot_proxy (3) If using HTCondor RPM installation, e.g. the one coming from OSG. If you have separate/multiple HTCondor hosts (schedds, collectors, negotiators, ..) you may have to check this file on all of them to make sure that the HTCondor authentication works correctly. Used to create the VO Frontend proxy if following the instructions above If using the Frontend configuration and scripts described above in this document . These paths are the ones specified in the configuration file. Remember also that when you change DN: The VO Frontend certificate DN must be communicated to the GlideinWMS Factory ( see above ) The pilot proxy must be able to run jobs at the sites you are using, e.g. by being added to the correct VO in OSG (the Factory forwards the proxy and does not care about the DN)","title":"Certificates brief"},{"location":"other/install-gwms-frontend/#increase-the-log-level-and-change-rotation-policies","text":"You can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the process_log section of /etc/gwms-frontend/frontend.xml : <log_retention> <process_logs> <process_log extension= \"all\" max_days= \"7.0\" max_mbytes= \"100.0\" min_days= \"3.0\" msg_types= \"DEBUG,EXCEPTION,INFO,ERROR,ERR\" /> You can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files: max_bytes is the max size of the log files max_days it will be rotated. compression specifies if rotated files are compressed backup_count is the number of rotated log files kept Further details are in the reference documentation .","title":"Increase the log level and change rotation policies"},{"location":"other/install-gwms-frontend/#frontend-reconfig-failing","text":"If service gwms-frontend reconfig fails at the end with an error like \"Writing back config file failed, Reconfiguring the frontend [FAILED]\", make sure that /etc/gwms-frontend/ belongs to the frontend user. It must be able to write to update the configuration file.","title":"Frontend reconfig failing"},{"location":"other/install-gwms-frontend/#frontend-failing-to-start","text":"If the startup script of the frontend is failing, check the log file for errors (probably /var/log/gwms-frontend/frontend/frontend.<TODAY's DATE>.err.log and .debug.log ). If you find errors like \"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\" and \"IOError: [Errno 9] Bad file descriptor\" you may have an empty status file ( /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml ) that causes GlideinWMS Frontend not to start. The glideinFrontend crashes after a XML parsing exception visible in the log file (\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\"). Remove the status file. Then start the frontend. The Frontend will be fixed in future versions to handle this automatically.","title":"Frontend failing to start"},{"location":"other/install-gwms-frontend/#certificates-not-there","text":"The scripts should send an email warning if there are problems and they fail to generate the proxies. Anyway something could go wrong and you want to check manually. If you are using the scripts to generate automatically the proxies but the proxies are not there (in /tmp or wherever you expect them): make sure that the scripts are there and configured with the correct values make sure that the scripts are executable make sure that the scripts are in frontend 's crontab make sure that the certificates (or master proxy) used to generate the proxies is not expired","title":"Certificates not there"},{"location":"other/install-gwms-frontend/#failed-authentication","text":"If you get a failed authentication error (e.g. \"Failed to talk to factory_pool gfactory-1.t2.ucsd.edu...) then: check that you have the right x509 certificates mentioned in the security section of /etc/gwms-frontend/frontend.xml the owner must be frontend (user running the frontend) the permission must be 600 they must be valid for more than one hour (2/300 hours), at least the non VO part check that the clock is synchronized (see HostTimeSetup)","title":"Failed authentication"},{"location":"other/install-gwms-frontend/#frontend-doesnt-trust-factory","text":"If your frontend complains in the debug log: code 256:['Error: communication error\\n', 'AUTHENTICATE:1003:Failed to authenticate with any method\\n', 'AUTHENTICATE:1004:Failed to authenticate using GSI\\n', \"GSI:5006:Failed to authenticate because the subject '/DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu' is not currently trusted by you. If it should be, add it to GSI_DAEMON_NAME in the condor_config, or use the environment variable override (check the manual).\\n\", 'GSI:5004:Failed to gss_assist_gridmap /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu to a local user. A possible solution is to comment/remove the LOCAL_CONFIG_DIR in the file /var/lib/gwms-frontend/vofrontend/frontend.condor_config .","title":"Frontend doesn't trust Factory"},{"location":"other/install-gwms-frontend/#no-security-credentials-match-for-factory-pool-not-advertising-request","text":"You may see a warning like \"No security credentials match for factory pool ..., not advertising request\", if the trust_domain and auth_method of an entry in the Factory configuration is not matching any of the trust_domain , type couples in the credentials in the Frontend configuration. This causes the Frontend not to use some Factory entries (the ones not matching) and may end up without entries to send glideins to. To fix the problem make sure that those attributes match as desired.","title":"No security credentials match for factory pool ..., not advertising request"},{"location":"other/install-gwms-frontend/#jobs-not-running","text":"If your jobs remain Idle Check the frontend log files (see above) Check the HTCondor log files ( condor_config_val LOG will give you the correct log directory): Specifically look the CollectorXXXLog files Common causes of problems could be: x509 certificates missing or expired or too short-lived proxy incorrect ownership or permission on the certificate/proxy file missing certificates If the Frontend http server is down in the glidein logs in the Factory there will be errors like \"Failed to load file 'description.dbceCN.cfg' from http://FRONTEND_HOST/vofrontend/stage .\" check that the http server is running and you can reach the URL ( http://FRONTEND_HOST/vofrontend/stage/description.dbceCN.cfg )","title":"Jobs not running"},{"location":"other/install-gwms-frontend/#getting-help","text":"To get assistance about the OSG software please use this page . For specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support glideinwms-support@fnal.gov To request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to osg-gfactory-support@physics.ucsd.edu (see below).","title":"Getting Help"},{"location":"other/install-gwms-frontend/#references","text":"Definitions: What is a Virtual Organization Documents about the Glidein-WMS system and the VO frontend: http://glideinwms.fnal.gov/","title":"References"},{"location":"other/install-gwms-frontend/#users","text":"The Glidein WMS Frontend installation will create the following users unless they are already created. User Default uid Comment apache 48 Runs httpd to provide the monitoring page (installed via dependencies). condor none HTCondor user (installed via dependencies). frontend none This user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins. gratia none Runs the Gratia probes to collect accounting data (optional see the Gratia section below ) Warning UID 48 is reserved by RedHat for user apache . If it is already taken by a different username, you will experience errors.","title":"Users"},{"location":"other/install-gwms-frontend/#certificates","text":"This document has a proxy configuration section that uses the host certificate/key and a user certificate to generate the required proxies. Certificate User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem Host key root /etc/grid-security/hostkey.pem Here are instructions to request a host certificate.","title":"Certificates"},{"location":"other/install-gwms-frontend/#networking","text":"Service Name Protocol Port Number Inbound Outbound Comment HTCondor port range tcp LOWPORT, HIGHPORT YES contiguous range of ports GlideinWMS Frontend tcp 9618, 9620 to 9660 YES HTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs) The VO frontend must have reliable network connectivity, be on the public internet (no NAT), and preferably with no firewalls. Incoming TCP ports 9618 to 9660 must be open.","title":"Networking"},{"location":"other/schedd-filebeats/","text":"Warning This is a technology preview document and will probably change content and location withouth notice. Installation of FileBeats for Access Points \u00b6 This document is for frontend administrators. It describes the installation of Filebeats to continuously upload the HTCondor access point transfer log to Elastic Search. Introduction \u00b6 An access point (HTCondor schedd) is a login node where users submit jobs to the Grid. One interesting log that it produces is the TransferLog. The TransferLogs report all the transfers of files between compute node and access points. In this guide we describe the installation of Filebeats to upload this log to Elastic Search. Installation \u00b6 FileBeat Installation \u00b6 For the installation of filebeats follow the official instruction to set up the repositories and install filebeats as described here . Configuration \u00b6 Configuration of Filebeats \u00b6 The configuration of filebeats revolves around this file /etc/filebeat/filebeat.yml . Bellow are the steps to modify the different sections of this file The Filebeat Inputs section, the input should look like this: filebeat.inputs: - type: log enabled: true paths: - /var/log/condor/XferStatsLog The output logstash section should look like: #----------------------------- Logstash output -------------------------------- output.logstash: # The Logstash hosts hosts: [\"gracc.opensciencegrid.org:6938\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications ssl.certificate_authorities: [\"/etc/grid-security/certificates/InCommon-IGTF-Server-CA.pem\"] # Certificate for SSL client authentication ssl.certificate: \"/etc/grid-security/hostcert.pem\" # Client Certificate Key ssl.key: \"/etc/grid-security/hostkey.pem\" Comment out all of the Elasticsearch output since we are using LogStash #-------------------------- Elasticsearch output ------------------------------ #output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] # Optional protocol and basic auth credentials. #protocol: \"https\" #username: \"elastic\" #password: \"changeme\" The general section should look like this, where <HOSTNAME> should be replaced by the hostname of the machine you are installing filebeats on. #================================ General ===================================== name: <HOSTNAME> tags: [\"xfer-log\"] Test that the configuration is correct by running: root@host # filebeat test config Start the filebeats services: root@host # service filebeat start Configuration of HTCondor \u00b6 For the configuration of the HTCondor access point to use the TransferLog follow the next instructions: Note The transfer metrics was introduced in HTCondor 8.6 series. You need to be running a version equal or greater than 8.6.1 to enable it. Create a file named /etc/condor/config.d/50-transferLog.config with the following contents: SHADOW_DEBUG = D_STATS SHADOW_STATS_LOG = $(LOG)/XferStatsLog STARTER_STATS_LOG = $(LOG)/XferStatsLog Reconfigure condor: root@host # condor_reconfig Make sure that after a couple of minutes the new log /var/log/condor/XferStatsLog is present.","title":"Install Transfer Log Filebeats"},{"location":"other/schedd-filebeats/#installation-of-filebeats-for-access-points","text":"This document is for frontend administrators. It describes the installation of Filebeats to continuously upload the HTCondor access point transfer log to Elastic Search.","title":"Installation of FileBeats for Access Points"},{"location":"other/schedd-filebeats/#introduction","text":"An access point (HTCondor schedd) is a login node where users submit jobs to the Grid. One interesting log that it produces is the TransferLog. The TransferLogs report all the transfers of files between compute node and access points. In this guide we describe the installation of Filebeats to upload this log to Elastic Search.","title":"Introduction"},{"location":"other/schedd-filebeats/#installation","text":"","title":"Installation"},{"location":"other/schedd-filebeats/#filebeat-installation","text":"For the installation of filebeats follow the official instruction to set up the repositories and install filebeats as described here .","title":"FileBeat Installation"},{"location":"other/schedd-filebeats/#configuration","text":"","title":"Configuration"},{"location":"other/schedd-filebeats/#configuration-of-filebeats","text":"The configuration of filebeats revolves around this file /etc/filebeat/filebeat.yml . Bellow are the steps to modify the different sections of this file The Filebeat Inputs section, the input should look like this: filebeat.inputs: - type: log enabled: true paths: - /var/log/condor/XferStatsLog The output logstash section should look like: #----------------------------- Logstash output -------------------------------- output.logstash: # The Logstash hosts hosts: [\"gracc.opensciencegrid.org:6938\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications ssl.certificate_authorities: [\"/etc/grid-security/certificates/InCommon-IGTF-Server-CA.pem\"] # Certificate for SSL client authentication ssl.certificate: \"/etc/grid-security/hostcert.pem\" # Client Certificate Key ssl.key: \"/etc/grid-security/hostkey.pem\" Comment out all of the Elasticsearch output since we are using LogStash #-------------------------- Elasticsearch output ------------------------------ #output.elasticsearch: # Array of hosts to connect to. #hosts: [\"localhost:9200\"] # Optional protocol and basic auth credentials. #protocol: \"https\" #username: \"elastic\" #password: \"changeme\" The general section should look like this, where <HOSTNAME> should be replaced by the hostname of the machine you are installing filebeats on. #================================ General ===================================== name: <HOSTNAME> tags: [\"xfer-log\"] Test that the configuration is correct by running: root@host # filebeat test config Start the filebeats services: root@host # service filebeat start","title":"Configuration of Filebeats"},{"location":"other/schedd-filebeats/#configuration-of-htcondor","text":"For the configuration of the HTCondor access point to use the TransferLog follow the next instructions: Note The transfer metrics was introduced in HTCondor 8.6 series. You need to be running a version equal or greater than 8.6.1 to enable it. Create a file named /etc/condor/config.d/50-transferLog.config with the following contents: SHADOW_DEBUG = D_STATS SHADOW_STATS_LOG = $(LOG)/XferStatsLog STARTER_STATS_LOG = $(LOG)/XferStatsLog Reconfigure condor: root@host # condor_reconfig Make sure that after a couple of minutes the new log /var/log/condor/XferStatsLog is present.","title":"Configuration of HTCondor"},{"location":"other/troubleshooting-gratia/","text":"Troubleshooting Gratia Accounting \u00b6 This document will help you troubleshoot problems with the Gratia Accounting, particularly with problems in collecting and reporting accounting information to the central OSG accounting service. Gratia/GRACC: The Big Picture \u00b6 Gratia is software used in OSG to gather accounting information for usage of computational resources. The information is collected from individual resources at a site, such as a Compute Entrypoint or a submission host. The program that collects the data is called a \"Gratia probe\". The information is transferred to the central OSG GRACC server. Here is a diagram: Difference between Gratia and GRACC Gratia is the legacy name of the OSG Accounting system. GRACC is the new name of the server and hosted components of the accounting system. When we refer to Gratia, we mean either the data or the probes on the resources. If we mention GRACC, we are referring to the hosted components that the OSG maintains. These are the definitions of the major elements in the above figure. Gratia probe : A piece of software that collects accounting data from the computer on which it's running, and transmits it to a Gratia server. GRACC server : A server that collects Gratia accounting data from one or more sites and can share it with users via a web page. The GRACC server is hosted by the OSG. Reporter : A web service running on the GRACC server. Users can connect to the reporter via a web browser to explore the Gratia data. Collector : A web service running on the GRACC server that collects data from one or more Gratia probes. Users do not directly interact with the collector. You can see the OSG's GRACC website at https://gracc.opensciencegrid.org . You can see a fancier version of the Gratia data at https://display.opensciencegrid.org/ . This is not running a Gratia collector, but is a separate service. Gratia Probes \u00b6 Gratia Probes are periodically run as cron jobs, but different probes will run at different intervals. The cron jobs will always run and you should not remove them. You can find them in /etc/cron.d . However, the cron jobs will only do anything if you have enabled them. You enable them via an init script. For example, to enable them: root@host # service gratia-probes-cron start Enabling gratia probes cron: [ OK ] To disable them: root@host # service gratia-probes-cron stop Disabling gratia probes cron: [ OK ] You also need to enable individual probes, usually via osg-configure . Documentation on using osg-configure with Gratia documented elsewhere . Running Gratia Probes \u00b6 When the cron jobs are enabled and run, they go through the following process, with minor changes between different Gratia probes: The probe is invoked. It reads its configuration from /etc/gratia/PROBE-NAME/ProbeConfig . It collects the accounting information from the underlying system. For example, the HTCondor probe will read it from the PER_JOB_HISTORY_DIR , which is usually /var/lib/gratia/data . It transforms the data into Gratia records and saves them into /var/lib/gratia/tmp/gratiafiles/ When there are sufficient Gratia records, or when sufficient time has passed, it uploads sets of records in batches to the GRACC server, then removes them from the gratiafiles directory. All progress is logged to /var/log/gratia . If there are failures in uploading the files to the GRACC server Files are not removed from gratiafiles until they are successfully uploaded. Errors are logged to log files in /var/log/gratia . The uploads will be tried again later. Gratia Probe Configuration \u00b6 In normal cases, osg-configure does the editing of the probe configuration files, at least on the CE. The configuration is found in /etc/osg/config.d/30-gratia.ini and documented elsewhere . If there are problems or special configuration, you might need to edit the Gratia configuration files yourself. Each probe has a separate configuration file found in /etc/gratia/PROBE-NAME/ProbeConfig . The ProbeConfig files have many details. A few options that you might need to edit are shown before. This is not a complete file, but only shows a subset of the options. <ProbeConfiguration CollectorHost=\"gratia-osg-itb.opensciencegrid.org:80\" SSLHost=\"gratia-osg-itb.opensciencegrid.org:80\" SSLRegistrationHost=\"gratia-osg-itb.opensciencegrid.org:80\" ProbeName=\"condor:fermicloud084.fnal.gov\" SiteName=\"WISC_OSG_EDU\" EnableProbe=\"1\" /> The options you see here are: Option Comments CollectorHost The GRACC server this probe reports to SSLHost The GRACC server this probe reports to SSLRegistrationHost The GRACC server this probe reports to ProbeName The unique name for this probe. Note that it includes the probe type and the host name SiteName The name of your Resource, as registered in OSG Topology . EnableProbe The probe will only run if this is \"1\" Again, there are many more options in this file. Most of the time you won't need to touch them. Are the Gratia cron jobs running? \u00b6 You should make sure the Gratia cron jobs are running. The simplest way is with the service command: root@host # /sbin/service gratia-probes-cron status gratia probes cron is enabled. If it is not enabled, enable it as described above. This only ensures that the basic gratia-probe-cron \"service\" is running. To check if the individual Gratia probes are enabled, look at the EnableProbe option in the ProbeConfig file, as described above. A quick command to do this is shown here. Note that the HTCondor and GridFTP Transfer probes are enabled while the glexec probe is disabled: root@host # cd /etc/gratia root@host # grep -r EnableProbe * condor/ProbeConfig: EnableProbe=\"1\" glexec/ProbeConfig: EnableProbe=\"0\" gridftp-transfer/ProbeConfig: EnableProbe=\"1\" If you see no log files in /var/log/gratia you may have an error in the probe configuration file. Manually run the test for your probe (check /etc/cron.d/gratia-probe-condor.cron ), e.g. /usr/share/gratia/common/cron_check /etc/gratia/condor/ProbeConfig . If there is an error you may get a suggestion on where it is, e.g.: root@host # /usr/share/gratia/common/cron_check /etc/gratia/condor/ProbeConfig Parse error in /etc/gratia/condor/ProbeConfig: not well-formed (invalid token): line 21, column 4 Correct the error and restart gratia. Have you configured the resource names correctly? \u00b6 Do the names of your resources match the names in OSG Topology ? Gratia retrieves the resource name from the Site Information section of the /etc/osg/config.d/40-siteinfo.ini ;=================================================================== ; Site Information ;=================================================================== [Site Information] ; Set \"group\" to \"OSG\" for a production site, or \"OSG-ITB\" for an ITB site. ; ; YOU WILL NEED TO CHANGE THIS group = OSG ; Set \"host_name\" to the host name of the CE being configured. ; This should resolve in DNS; if DNS is not set up yet, enter an IPv4/v6 address instead. ; ; YOU WILL NEED TO CHANGE THIS host_name = tusker-gw1.unl.edu ; Set \"resource\" to the name of the resource that you have registered ; in the OSG topology repository at https://github.com/opensciencegrid/topology ; ; YOU WILL NEED TO CHANGE THIS resource = Tusker-CE1 Do those names match the names that you registered with OSG Topology? If not, edit the names, and rerun \"osg-configure -c\". Did the site name change? \u00b6 Was the site previously reporting data, but the site name (not host name, but site name) changed? When the site name changes, you need to ask the GRACC operations team to update the name of your site at the GRACC collector. To do this: Open a support ticket Select \"Software or Service\" Select \"GRACC Operations\" Type a friendly email that asks the GRACC team to change your site name at the collector. Make sure to tell them the old name and the new name. Below is an example email: Hello GRACC Team, Please change the site name of my site from <Insert Old Name> to <Insert New Name>. Thanks, ... Is a site reporting data? \u00b6 You can see if the OSG GRACC Server is getting data from a site by going to GRACC : Specify the site name in Facility HTCondor's Gratia Configuration \u00b6 Note Only applicable to HTCondor batch sites, not SLURM, PBS, SGE or LSF sites HTCondor must be configured to put information about each job into a special directory. Gratia will read and remove the files in order to collect the accounting information. The configuration variable is called PER_JOB_HISTORY_DIR . If you install the OSG RPM for HTCondor, the Gratia probe will extend its configuration by adding a file to /etc/condor/config.d , and will set this variable to /var/lib/gratia/data . If you are using a different installation method, you may need to set the variable yourself. You can check if it's set by using condor_config_val , like this: user@host $ condor_config_val -v PER_JOB_HISTORY_DIR PER_JOB_HISTORY_DIR: /var/lib/gratia/data Defined in '/etc/condor/config.d/99_gratia.conf', line 5. If you set this value, you need to restart condor: root@host # condor_restart Sent \"Restart\" command to local master Unlike many HTCondor settings, a condor_reconfig is not sufficient - you must restart! Bad Gratia hostname \u00b6 This is an example problem where the configuration was bad: there was an incorrect hostname for the Gratia server. The problem is clearly visible in the Gratia log file, which is located in /var/log/gratia/ . There is one log file per day, labeled by the date: root@host # cd /var/log/gratia/ root@host # cat 2012 -04-03.log You can see that Gratia is using the correct configuration file: 15:06:55 CDT Gratia: Using config file: /etc/gratia/condor/ProbeConfig Here Gratia is removing a file from the HTCondor PER_JOB_HISTORY_DIR and creating a Gratia accounting record for it 15:06:55 CDT Gratia: Creating a UsageRecord 2012-04-03T20:06:55Z 15:06:55 CDT Gratia: Registering transient input file: /var/lib/gratia/data/history.37.0 15:06:55 CDT Gratia: *********************************************************** 15:06:55 CDT Gratia: Saved record to /var/lib/gratia/tmp/gratiafiles/ subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/ outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606 15:06:55 CDT Gratia: Deleting transient input file: /var/lib/gratia/data/history.37.0 Later, Gratia failed to connect to the server due to a bad hostname 15:06:55 CDT Gratia: Failed to send xml to web service due to an error of type \"socket.gaierror\": (-2, 'Name or service not known') ... 15:06:55 CDT Gratia: Response indicates failure, the following files will not be deleted: 15:06:55 CDT Gratia: /var/lib/gratia/tmp/gratiafiles/ subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/ outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606 If you accidentally had a bad Gratia hostname, you probably want to recover your Gratia data. This can be done, though it's not simple. There are a few things you need to do. But first, you need to understand exactly where Gratia stores files. When a Gratia extracts accounting information, it creates one file per record and stores it in a directory. The directory is a long name that contains the type of the probe (such as condor ), the name of the host you're running on, and the name of the GRACC host you're sending the information to. For simplicity, lets call that name <PROBE-RECORDS> , but you'll see what it really looks like below. Within this directory, you'll see some subdirectories: Directory Purpose /var/lib/gratia/tmp/grataifiles/ <PROBE-RECORDS> /outbox The usual location for the accounting records /var/lib/gratia/tmp/grataifiles/ <PROBE-RECORDS> /staged/store An overflow location when there are problems When you recover old records, you need to: Move files from the outbox of the incorrect probe-records directory into the outbox of the correctly named probe-records directory. Move tarred and compressed files from the staged/store of the incorrect probe-records directory into the staged/store of the correctly named probe-records directory. Then you uncompress them and remove the compressed version. In the examples below, the hostname for gratia was \"accidentally\" spelled backwards. Instead of gratia-osg-itb.opensciencegrid.org , it was aitarg-osg-itb.opensciencegrid.org . First you need to fix the hostname. For a CE, you can edit /etc/osg/config.d/30-gratia.ini and rerun osg-configure -c . In other installations, you have to edit the appropriate ProbeConfig file. Next, submit a job via to your batch system, then run the appropriate Gratia probe (or wait for it to run via cron). This will create the properly named directories on your disk. For example: As a user: user@host $ globus-job-run fermicloud084.fnal.gov/jobmanager-condor /bin/hostname As root (adjust for your batch system): root@host # /usr/share/gratia/condor/condor_meter Find the Gratia records that can be easily uploaded. They are located in a a directory with an unwieldy name that includes your hostname and the incorrect name of the Gratia host. You can see the directory name in the Gratia log: the misspelled name is between angled brackets and capital letters below, but it will be different on your computer . user@host $ less /var/log/gratia/2012-04-06 ... 16:04:29 CDT Gratia: Response indicates failure, the following files will not be deleted: 16:04:29 CDT Gratia: /var/lib/gratia/tmp/gratiafiles/ subdir.condor_fermicloud084.fnal.gov_<AITARG>-osg-itb.opensciencegrid.org_80/ outbox/r.916.condor_fermicloud084.fnal.gov_aitarg-osg-itb.opensciencegrid.org_80.gratia.xml__JDlHbNb918 (The filename was wrapped for legibility.) You can simply copy these to the correct directory. Wait for the Gratia cron job to run, or force it to run. root@host # cd /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_<AITARG>-osg-itb.opensciencegrid.org_80/outbox/. root@host # mv * /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_gratia-osg-itb.opensciencegrid.org_80/outbox/. If this has been a persistent problem, you might have many records. After a while, they are put into a compressed files in another directory. You can move those files, then uncompress them. This is a long name: note that the path ends in \"staged/store\" instead of \"outbox\" as above: # Find the old files root@host # cd /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_<AITARG>-osg-itb.opensciencegrid.org_80/staged/store # Move them to the correct directory root@host # mv tz* /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_gratia-osg-itb.opensciencegrid.org_80/outbox/. root@host # cd !$ # For each tz file: root@host # tar xf tz.1223.... [ name shortened for legibility ] root@host # rm tz.1223.... When you've done this, you can re-run the Gratia probe by hand, or wait for it to run via cron. Reference: Important Gratia files \u00b6 If you need to look for more data, you can look at log files for the various services on your CE. File Purpose /var/log/gratia/<DATE>.log Log file that records information about processing and uploading of Gratia accounting data /var/log/gratia/gridftpTransfer.log Log file specific to the Gratia GridFTP probe /var/lib/gratia/data Location for HTCondor and PBS job data before being processed by Gratia HTCondor's PER_JOB_HISTORY_DIR should be set to this location /var/lib/gratia/tmp/gratiafiles Location for temporary Gratia data as it is being processed, usually empty. If you have files that are more than 30 minutes old in this directory, there may be a problem /etc/gratia/<PROBE-NAME>/ProbeConfig Configuration for Gratia probes, one per probe type Normally you don't need to edit this Not all RPMs will be on all hosts. Instead, only the gratia-probe-common and the one RPM specific to that host will be installed. The most common RPMs you will see are: RPM Purpose gratia-probe-common Code shared between all Graita probes gratia-probe-condor The probe that tracks HTCondor usage gratia-probe-slurm The probe that tracks SLURM usage gratia-probe-pbs-lsf The probe that tracks PBS and/or LSF usage gratia-probe-gridftp-transfer The probe that tracks transfers done with GridFTP","title":"Troubleshooting Gratia"},{"location":"other/troubleshooting-gratia/#troubleshooting-gratia-accounting","text":"This document will help you troubleshoot problems with the Gratia Accounting, particularly with problems in collecting and reporting accounting information to the central OSG accounting service.","title":"Troubleshooting Gratia Accounting"},{"location":"other/troubleshooting-gratia/#gratiagracc-the-big-picture","text":"Gratia is software used in OSG to gather accounting information for usage of computational resources. The information is collected from individual resources at a site, such as a Compute Entrypoint or a submission host. The program that collects the data is called a \"Gratia probe\". The information is transferred to the central OSG GRACC server. Here is a diagram: Difference between Gratia and GRACC Gratia is the legacy name of the OSG Accounting system. GRACC is the new name of the server and hosted components of the accounting system. When we refer to Gratia, we mean either the data or the probes on the resources. If we mention GRACC, we are referring to the hosted components that the OSG maintains. These are the definitions of the major elements in the above figure. Gratia probe : A piece of software that collects accounting data from the computer on which it's running, and transmits it to a Gratia server. GRACC server : A server that collects Gratia accounting data from one or more sites and can share it with users via a web page. The GRACC server is hosted by the OSG. Reporter : A web service running on the GRACC server. Users can connect to the reporter via a web browser to explore the Gratia data. Collector : A web service running on the GRACC server that collects data from one or more Gratia probes. Users do not directly interact with the collector. You can see the OSG's GRACC website at https://gracc.opensciencegrid.org . You can see a fancier version of the Gratia data at https://display.opensciencegrid.org/ . This is not running a Gratia collector, but is a separate service.","title":"Gratia/GRACC: The Big Picture"},{"location":"other/troubleshooting-gratia/#gratia-probes","text":"Gratia Probes are periodically run as cron jobs, but different probes will run at different intervals. The cron jobs will always run and you should not remove them. You can find them in /etc/cron.d . However, the cron jobs will only do anything if you have enabled them. You enable them via an init script. For example, to enable them: root@host # service gratia-probes-cron start Enabling gratia probes cron: [ OK ] To disable them: root@host # service gratia-probes-cron stop Disabling gratia probes cron: [ OK ] You also need to enable individual probes, usually via osg-configure . Documentation on using osg-configure with Gratia documented elsewhere .","title":"Gratia Probes"},{"location":"other/troubleshooting-gratia/#running-gratia-probes","text":"When the cron jobs are enabled and run, they go through the following process, with minor changes between different Gratia probes: The probe is invoked. It reads its configuration from /etc/gratia/PROBE-NAME/ProbeConfig . It collects the accounting information from the underlying system. For example, the HTCondor probe will read it from the PER_JOB_HISTORY_DIR , which is usually /var/lib/gratia/data . It transforms the data into Gratia records and saves them into /var/lib/gratia/tmp/gratiafiles/ When there are sufficient Gratia records, or when sufficient time has passed, it uploads sets of records in batches to the GRACC server, then removes them from the gratiafiles directory. All progress is logged to /var/log/gratia . If there are failures in uploading the files to the GRACC server Files are not removed from gratiafiles until they are successfully uploaded. Errors are logged to log files in /var/log/gratia . The uploads will be tried again later.","title":"Running Gratia Probes"},{"location":"other/troubleshooting-gratia/#gratia-probe-configuration","text":"In normal cases, osg-configure does the editing of the probe configuration files, at least on the CE. The configuration is found in /etc/osg/config.d/30-gratia.ini and documented elsewhere . If there are problems or special configuration, you might need to edit the Gratia configuration files yourself. Each probe has a separate configuration file found in /etc/gratia/PROBE-NAME/ProbeConfig . The ProbeConfig files have many details. A few options that you might need to edit are shown before. This is not a complete file, but only shows a subset of the options. <ProbeConfiguration CollectorHost=\"gratia-osg-itb.opensciencegrid.org:80\" SSLHost=\"gratia-osg-itb.opensciencegrid.org:80\" SSLRegistrationHost=\"gratia-osg-itb.opensciencegrid.org:80\" ProbeName=\"condor:fermicloud084.fnal.gov\" SiteName=\"WISC_OSG_EDU\" EnableProbe=\"1\" /> The options you see here are: Option Comments CollectorHost The GRACC server this probe reports to SSLHost The GRACC server this probe reports to SSLRegistrationHost The GRACC server this probe reports to ProbeName The unique name for this probe. Note that it includes the probe type and the host name SiteName The name of your Resource, as registered in OSG Topology . EnableProbe The probe will only run if this is \"1\" Again, there are many more options in this file. Most of the time you won't need to touch them.","title":"Gratia Probe Configuration"},{"location":"other/troubleshooting-gratia/#are-the-gratia-cron-jobs-running","text":"You should make sure the Gratia cron jobs are running. The simplest way is with the service command: root@host # /sbin/service gratia-probes-cron status gratia probes cron is enabled. If it is not enabled, enable it as described above. This only ensures that the basic gratia-probe-cron \"service\" is running. To check if the individual Gratia probes are enabled, look at the EnableProbe option in the ProbeConfig file, as described above. A quick command to do this is shown here. Note that the HTCondor and GridFTP Transfer probes are enabled while the glexec probe is disabled: root@host # cd /etc/gratia root@host # grep -r EnableProbe * condor/ProbeConfig: EnableProbe=\"1\" glexec/ProbeConfig: EnableProbe=\"0\" gridftp-transfer/ProbeConfig: EnableProbe=\"1\" If you see no log files in /var/log/gratia you may have an error in the probe configuration file. Manually run the test for your probe (check /etc/cron.d/gratia-probe-condor.cron ), e.g. /usr/share/gratia/common/cron_check /etc/gratia/condor/ProbeConfig . If there is an error you may get a suggestion on where it is, e.g.: root@host # /usr/share/gratia/common/cron_check /etc/gratia/condor/ProbeConfig Parse error in /etc/gratia/condor/ProbeConfig: not well-formed (invalid token): line 21, column 4 Correct the error and restart gratia.","title":"Are the Gratia cron jobs running?"},{"location":"other/troubleshooting-gratia/#have-you-configured-the-resource-names-correctly","text":"Do the names of your resources match the names in OSG Topology ? Gratia retrieves the resource name from the Site Information section of the /etc/osg/config.d/40-siteinfo.ini ;=================================================================== ; Site Information ;=================================================================== [Site Information] ; Set \"group\" to \"OSG\" for a production site, or \"OSG-ITB\" for an ITB site. ; ; YOU WILL NEED TO CHANGE THIS group = OSG ; Set \"host_name\" to the host name of the CE being configured. ; This should resolve in DNS; if DNS is not set up yet, enter an IPv4/v6 address instead. ; ; YOU WILL NEED TO CHANGE THIS host_name = tusker-gw1.unl.edu ; Set \"resource\" to the name of the resource that you have registered ; in the OSG topology repository at https://github.com/opensciencegrid/topology ; ; YOU WILL NEED TO CHANGE THIS resource = Tusker-CE1 Do those names match the names that you registered with OSG Topology? If not, edit the names, and rerun \"osg-configure -c\".","title":"Have you configured the resource names correctly?"},{"location":"other/troubleshooting-gratia/#did-the-site-name-change","text":"Was the site previously reporting data, but the site name (not host name, but site name) changed? When the site name changes, you need to ask the GRACC operations team to update the name of your site at the GRACC collector. To do this: Open a support ticket Select \"Software or Service\" Select \"GRACC Operations\" Type a friendly email that asks the GRACC team to change your site name at the collector. Make sure to tell them the old name and the new name. Below is an example email: Hello GRACC Team, Please change the site name of my site from <Insert Old Name> to <Insert New Name>. Thanks, ...","title":"Did the site name change?"},{"location":"other/troubleshooting-gratia/#is-a-site-reporting-data","text":"You can see if the OSG GRACC Server is getting data from a site by going to GRACC : Specify the site name in Facility","title":"Is a site reporting data?"},{"location":"other/troubleshooting-gratia/#htcondors-gratia-configuration","text":"Note Only applicable to HTCondor batch sites, not SLURM, PBS, SGE or LSF sites HTCondor must be configured to put information about each job into a special directory. Gratia will read and remove the files in order to collect the accounting information. The configuration variable is called PER_JOB_HISTORY_DIR . If you install the OSG RPM for HTCondor, the Gratia probe will extend its configuration by adding a file to /etc/condor/config.d , and will set this variable to /var/lib/gratia/data . If you are using a different installation method, you may need to set the variable yourself. You can check if it's set by using condor_config_val , like this: user@host $ condor_config_val -v PER_JOB_HISTORY_DIR PER_JOB_HISTORY_DIR: /var/lib/gratia/data Defined in '/etc/condor/config.d/99_gratia.conf', line 5. If you set this value, you need to restart condor: root@host # condor_restart Sent \"Restart\" command to local master Unlike many HTCondor settings, a condor_reconfig is not sufficient - you must restart!","title":"HTCondor's Gratia Configuration"},{"location":"other/troubleshooting-gratia/#bad-gratia-hostname","text":"This is an example problem where the configuration was bad: there was an incorrect hostname for the Gratia server. The problem is clearly visible in the Gratia log file, which is located in /var/log/gratia/ . There is one log file per day, labeled by the date: root@host # cd /var/log/gratia/ root@host # cat 2012 -04-03.log You can see that Gratia is using the correct configuration file: 15:06:55 CDT Gratia: Using config file: /etc/gratia/condor/ProbeConfig Here Gratia is removing a file from the HTCondor PER_JOB_HISTORY_DIR and creating a Gratia accounting record for it 15:06:55 CDT Gratia: Creating a UsageRecord 2012-04-03T20:06:55Z 15:06:55 CDT Gratia: Registering transient input file: /var/lib/gratia/data/history.37.0 15:06:55 CDT Gratia: *********************************************************** 15:06:55 CDT Gratia: Saved record to /var/lib/gratia/tmp/gratiafiles/ subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/ outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606 15:06:55 CDT Gratia: Deleting transient input file: /var/lib/gratia/data/history.37.0 Later, Gratia failed to connect to the server due to a bad hostname 15:06:55 CDT Gratia: Failed to send xml to web service due to an error of type \"socket.gaierror\": (-2, 'Name or service not known') ... 15:06:55 CDT Gratia: Response indicates failure, the following files will not be deleted: 15:06:55 CDT Gratia: /var/lib/gratia/tmp/gratiafiles/ subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/ outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606 If you accidentally had a bad Gratia hostname, you probably want to recover your Gratia data. This can be done, though it's not simple. There are a few things you need to do. But first, you need to understand exactly where Gratia stores files. When a Gratia extracts accounting information, it creates one file per record and stores it in a directory. The directory is a long name that contains the type of the probe (such as condor ), the name of the host you're running on, and the name of the GRACC host you're sending the information to. For simplicity, lets call that name <PROBE-RECORDS> , but you'll see what it really looks like below. Within this directory, you'll see some subdirectories: Directory Purpose /var/lib/gratia/tmp/grataifiles/ <PROBE-RECORDS> /outbox The usual location for the accounting records /var/lib/gratia/tmp/grataifiles/ <PROBE-RECORDS> /staged/store An overflow location when there are problems When you recover old records, you need to: Move files from the outbox of the incorrect probe-records directory into the outbox of the correctly named probe-records directory. Move tarred and compressed files from the staged/store of the incorrect probe-records directory into the staged/store of the correctly named probe-records directory. Then you uncompress them and remove the compressed version. In the examples below, the hostname for gratia was \"accidentally\" spelled backwards. Instead of gratia-osg-itb.opensciencegrid.org , it was aitarg-osg-itb.opensciencegrid.org . First you need to fix the hostname. For a CE, you can edit /etc/osg/config.d/30-gratia.ini and rerun osg-configure -c . In other installations, you have to edit the appropriate ProbeConfig file. Next, submit a job via to your batch system, then run the appropriate Gratia probe (or wait for it to run via cron). This will create the properly named directories on your disk. For example: As a user: user@host $ globus-job-run fermicloud084.fnal.gov/jobmanager-condor /bin/hostname As root (adjust for your batch system): root@host # /usr/share/gratia/condor/condor_meter Find the Gratia records that can be easily uploaded. They are located in a a directory with an unwieldy name that includes your hostname and the incorrect name of the Gratia host. You can see the directory name in the Gratia log: the misspelled name is between angled brackets and capital letters below, but it will be different on your computer . user@host $ less /var/log/gratia/2012-04-06 ... 16:04:29 CDT Gratia: Response indicates failure, the following files will not be deleted: 16:04:29 CDT Gratia: /var/lib/gratia/tmp/gratiafiles/ subdir.condor_fermicloud084.fnal.gov_<AITARG>-osg-itb.opensciencegrid.org_80/ outbox/r.916.condor_fermicloud084.fnal.gov_aitarg-osg-itb.opensciencegrid.org_80.gratia.xml__JDlHbNb918 (The filename was wrapped for legibility.) You can simply copy these to the correct directory. Wait for the Gratia cron job to run, or force it to run. root@host # cd /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_<AITARG>-osg-itb.opensciencegrid.org_80/outbox/. root@host # mv * /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_gratia-osg-itb.opensciencegrid.org_80/outbox/. If this has been a persistent problem, you might have many records. After a while, they are put into a compressed files in another directory. You can move those files, then uncompress them. This is a long name: note that the path ends in \"staged/store\" instead of \"outbox\" as above: # Find the old files root@host # cd /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_<AITARG>-osg-itb.opensciencegrid.org_80/staged/store # Move them to the correct directory root@host # mv tz* /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_gratia-osg-itb.opensciencegrid.org_80/outbox/. root@host # cd !$ # For each tz file: root@host # tar xf tz.1223.... [ name shortened for legibility ] root@host # rm tz.1223.... When you've done this, you can re-run the Gratia probe by hand, or wait for it to run via cron.","title":"Bad Gratia hostname"},{"location":"other/troubleshooting-gratia/#reference-important-gratia-files","text":"If you need to look for more data, you can look at log files for the various services on your CE. File Purpose /var/log/gratia/<DATE>.log Log file that records information about processing and uploading of Gratia accounting data /var/log/gratia/gridftpTransfer.log Log file specific to the Gratia GridFTP probe /var/lib/gratia/data Location for HTCondor and PBS job data before being processed by Gratia HTCondor's PER_JOB_HISTORY_DIR should be set to this location /var/lib/gratia/tmp/gratiafiles Location for temporary Gratia data as it is being processed, usually empty. If you have files that are more than 30 minutes old in this directory, there may be a problem /etc/gratia/<PROBE-NAME>/ProbeConfig Configuration for Gratia probes, one per probe type Normally you don't need to edit this Not all RPMs will be on all hosts. Instead, only the gratia-probe-common and the one RPM specific to that host will be installed. The most common RPMs you will see are: RPM Purpose gratia-probe-common Code shared between all Graita probes gratia-probe-condor The probe that tracks HTCondor usage gratia-probe-slurm The probe that tracks SLURM usage gratia-probe-pbs-lsf The probe that tracks PBS and/or LSF usage gratia-probe-gridftp-transfer The probe that tracks transfers done with GridFTP","title":"Reference: Important Gratia files"},{"location":"release/notes/","text":"OSG 3.5 Release Notes \u00b6 Supported OS Versions: EL7 The OSG 3.5 release series introduces HTCondor 8.8 and 8.9, available in osg-release and osg-upcoming , respectively. It also drops support for the RSV monitoring probes, CREAM CEs, and Enterprise Linux 6. See the initial OSG 3.5.0 release notes for additional details. To update to the OSG 3.5 series, please consult the page on updating between release series . Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Version Date Summary 3.5.45-2 2021-08-16 IGTF 1.112 3.5.45 2021-08-12 gratia-probe 1.24.0; Upcoming: XRootD 5.3.1 3.5.44 2021-08-05 VOMS 2.0.16 (EL7), VOMS 2.1.0-rc2 (EL8), htvault-config 1.4, htgettoken 1.3; Upcoming: XCache 2.0.1 3.5.43 2021-07-30 High Priority Release: HTCondor 8.8.15; Upcoming: HTCondor 9.0.4 3.5.42 2021-07-27 High Priority Release: HTCondor 8.8.14; Upcoming: HTCondor 9.0.3 3.5.41 2021-07-22 Upcoming: HTCondor 9.0.2, blahp 2.1.0, XRootD 5.3.0 3.5.40-2 2021-07-15 VO Package v114 3.5.40 2021-07-01 Frontier Squid 4.15-2.1, vault 1.7.3, htvault-config 1.2, EL8: XRootD 4.12.6 and plugins, osg-flock 1.3, Upcoming: xrootd-multiuser 1.1.0 3.5.39 2021-06-24 scitokens-cpp 0.6.2, Upcoming: HTCondor 9.0.1-1.1, HTCondor-CE 5.1.1-1.1 3.5.38-2 2021-06-16 VO Package v113 3.5.38 2021-06-10 HTCondor 8.8.13-1.1, Upcoming: XRootD 5.2.0, xrootd-hdfs 2.2.0-1.1 3.5.37 2021-06-03 HTCondor-CE 4.5.2, gratia-probe 1.23.3, vault 1.7.2, osg-gridftp on EL8, Upcoming: GlideinWMS 3.7.4 3.5.36-2 2021-05-25 IGTF 1.111 3.5.36 2021-05-17 HTCondor 8.8.13; Upcoming: HTCondor 9.0.0-1.5, HTCondor-CE 5.1.0, GlideinWMS 3.7.3 3.5.35 2021-05-13 High Priority Release: Frontier Squid 4.15-1.2, IGTF 1.110 3.5.34 2021-04-22 CVMFS 2.8.1, gratia-probe 1.23.2 3.5.33 2021-04-01 Upcoming: XRootD 5.1.1 and plugins, XCache 2.0.0 3.5.32 2021-03-25 Vault 1.6.2-1, SciTokens mapfile, VO Package v110; Upcoming: HTcondor 8.9.11-1 3.5.31 2021-02-04 CVMFS 2.8.0, XRootD 4.12.6, osg-ca-certs 1.94, osg-release 3.5-5, osg-flock 1.3, python-scitokens 1.3.1 3.5.30 2021-01-27 High Priority Release: Upcoming: HTCondor 8.9.11 3.5.29 2021-01-21 IGTF 1.109, osg-configure 3.11.0, htgettoken 1.1, Upcoming: GlideinWMS 3.7.2 3.5.28-2 2020-12-15 IGTF 1.108 3.5.28 2020-12-10 osg-ca-certs 1.90, htgettoken 1.0, XRootD 4.12.5, HTCondor 8.8.12; Upcoming: HTCondor 8.9.10 3.5.27 2020-11-12 gfal2 2.18.1-1.1; Upcoming: HTCondor 8.9.9, GlideinWMS 3.7.1 3.5.26 2020-11-05 gfal2 2.18.1, HTCondor 8.8.11, CVMFS 2.7.5, osg-flock 1.2, python-scitokens 1.2.4-3, scitokens-credmon 0.8.1; Upcoming: XRootD 5.0.2 3.5.25-2 2020-10-29 VO Package v109 3.5.25 2020-10-08 GlideinWMS 3.6.5, BLAHP 1.18.48; Upcoming: BLAHP 1.18.48 3.5.24 2020-09-17 CVMFS 2.7.4, stashcache-client 6.1.0, hosted-ce-tools 0.8.2, CCTools 7.1.7, VO Package v108 3.5.23 2020-09-03 xrootd-hdfs 2.1.8, osg-release 3.5-4; Upcoming: HTCondor 8.9.8, XRootD 5.0.1 and associated plugins 3.5.22 2020-08-27 HTCondor 8.8.10, Frontier Squid 4.13-1.1, XCache 1.5.2, xrootd-scitokens 1.2.2, gratia-probe 1.20.14, BLAHP 1.18.47, oidc-agent 3.3.3, XRootD plugins 3.5.21-2 2020-08-10 IGTF 1.107, VO Package v107 3.5.21 2020-07-30 WN client and OASIS on EL8, HTCondor-CE 4.4.1, osg-flock 1.1-2, osg-pki-tools 3.4.0, osg-system-profiler 1.6.0, osg-xrootd 3.5-13, stashcache-client 6.0.0 3.5.20 2020-07-23 HTCondor-CE 4.4.0, CVMFS 2.7.3, Frontier Squid 4.12-2.1, scitokens-cpp 0.5.1 3.5.19 2020-07-01 XRoodD 4.12.3, xrootd-lcmaps 1.7.7, scitokens-credmon 0.7; Upcoming: HTCondor 8.9.7 3.5.18 2020-06-11 Frontier Squid 4.11-3.1, VOMS 2.0.14-6, XCache 1.4, stashcache-client 5.6.1 3.5.17 2020-06-04 BLAHP 1.18.46, HTCondor 8.8.9, gratia-probe 1.20.13, VO Package v106; Upcoming: GlideinWMS 3.7, HTCondor 8.9.7 3.5.16 2020-05-14 CVMFS 2.7.2, Frontier Squid 4.11-2.1, osg-ce 3.5-5, hosted-ce-tools 0.7, CCTools 7.1.5, VO Package v105 3.5.15-3 2020-05-06 IGTF 1.106 3.5.15-2 2020-04-15 VO Package v104 3.5.15 2020-04-08 Frontier Squid 4.10.3, VO Package v103, XRootD 4.11.3, osg-xrootd 3.5-12 3.5.14 2020-04-07 High Priority Release: HTCondor 8.8.8; Upcoming: HTCondor 8.9.6 3.5.13 2020-04-02 GlideinWMS 3.6.2, IGTF 1.105, HTCondor-CE 4.2.1, Pegasus 4.9.3, LCMAPS 1.6.6-1.12, globus-gridftp-server 13.20-1.1, scitokens-cpp 0.5.0 3.5.12 2020-03-26 High Priority Release: XRootD-SciTokens 1.2.0 3.5.11 2020-03-11 CVMFS 2.7.1, oidc-agent 3.3.1, CCTools 7.0.22, GSI-OpenSSH 7.4p1-5, VO Package v101 3.5.10-2 2020-03-04 VO Package v100 3.5.10 2020-02-20 XRootD 4.11.2, XCache 1.2.1, VO Package v99, UberFTP 2.8-3, osg-configure 3.1.1, osg-system-profiler 1.5.0 3.5.9 2020-02-06 High Priority Release: Frontier Squid 4.10 3.5.8-4 2020-01-30 High Priority Release: IGTF 1.104 3.5.8-3 2020-01-29 High Priority Release: OSG CA certs based on IGTF 1.104 pre-release 3.5.8-2 2020-01-28 IGTF 1.103 3.5.8 2020-01-16 XRootD 4.11.1, VOMS 2.0.14-15, HTCondor 8.8.7, gratia-probe 1.20.12, osg-xrootd, host-ce-tools 0.5-2, scitokens-cpp 0.4.0, osg-ce, gsi-openssh, globus-gridftp-server, Upcoming: HTCondor 8.9.5 3.5.7 2019-12-19 HTCondor-CE 4.1.0, CVMFS 2.7.0, GlideinWMS 3.6.1, HTCondor 8.8.6; Upcoming: HTCondor 8.9.4 3.5.6 2019-11-26 XCache 1.2, CCTools 7.0.21, osg-release 3.5-3 3.5.5 2019-11-14 High Priority Release: Frontier Squid 4.9, XRootD 4.11.0, BLAHP 1.81.45, scitokens-credmon 0.4.2, scitokens-cpp 0.3.5 3.5.4 2019-10-23 HTCondor 8.8.5-1.7, StashCache-Client 5.5.0, IGTF 1.102, VO Package v97 3.5.3 2019-10-17 GlideinWMS 3.6, oidc-agent 3.2.6, scitokens-cpp 0.3.4, XRootD 4.10.1, HTCondor 8.8.5, osg-configure 3.1.0, gratia-probe 1.20.11; Upcoming: HTCondor 8.9.3 3.5.2 2019-10-10 HTCondor-CE 4.0.1, OSG CE 3.5-2, CVMFS 2.6.3, Frontier-Squid 4.8-2, CCTools 7.0.18, VO Package v96 3.5.1 2019-09-19 High Priority Release: MyProxy 6.2.6, GSI-OpenSSH, Globus GridFTP Server 3.5.0 2019-08-30 CVMFS 2.6.2, HTCondor 8.8.4, XCache 1.1.1, OSG XRootD 3.5, OSG Configure 3.0.0, XRootD LCMAPS 1.7.4, XRootD HDFS 2.1.6; Upcoming: HTCondor 8.9.2 Announcements \u00b6 OSG releases are also announced by mail and are sent to the following recipients and lists: Registered administrative contacts osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org site-announce@opensciencegrid.org software-discuss@opensciencegrid.org","title":"Release Notes"},{"location":"release/notes/#osg-35-release-notes","text":"Supported OS Versions: EL7 The OSG 3.5 release series introduces HTCondor 8.8 and 8.9, available in osg-release and osg-upcoming , respectively. It also drops support for the RSV monitoring probes, CREAM CEs, and Enterprise Linux 6. See the initial OSG 3.5.0 release notes for additional details. To update to the OSG 3.5 series, please consult the page on updating between release series . Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Version Date Summary 3.5.45-2 2021-08-16 IGTF 1.112 3.5.45 2021-08-12 gratia-probe 1.24.0; Upcoming: XRootD 5.3.1 3.5.44 2021-08-05 VOMS 2.0.16 (EL7), VOMS 2.1.0-rc2 (EL8), htvault-config 1.4, htgettoken 1.3; Upcoming: XCache 2.0.1 3.5.43 2021-07-30 High Priority Release: HTCondor 8.8.15; Upcoming: HTCondor 9.0.4 3.5.42 2021-07-27 High Priority Release: HTCondor 8.8.14; Upcoming: HTCondor 9.0.3 3.5.41 2021-07-22 Upcoming: HTCondor 9.0.2, blahp 2.1.0, XRootD 5.3.0 3.5.40-2 2021-07-15 VO Package v114 3.5.40 2021-07-01 Frontier Squid 4.15-2.1, vault 1.7.3, htvault-config 1.2, EL8: XRootD 4.12.6 and plugins, osg-flock 1.3, Upcoming: xrootd-multiuser 1.1.0 3.5.39 2021-06-24 scitokens-cpp 0.6.2, Upcoming: HTCondor 9.0.1-1.1, HTCondor-CE 5.1.1-1.1 3.5.38-2 2021-06-16 VO Package v113 3.5.38 2021-06-10 HTCondor 8.8.13-1.1, Upcoming: XRootD 5.2.0, xrootd-hdfs 2.2.0-1.1 3.5.37 2021-06-03 HTCondor-CE 4.5.2, gratia-probe 1.23.3, vault 1.7.2, osg-gridftp on EL8, Upcoming: GlideinWMS 3.7.4 3.5.36-2 2021-05-25 IGTF 1.111 3.5.36 2021-05-17 HTCondor 8.8.13; Upcoming: HTCondor 9.0.0-1.5, HTCondor-CE 5.1.0, GlideinWMS 3.7.3 3.5.35 2021-05-13 High Priority Release: Frontier Squid 4.15-1.2, IGTF 1.110 3.5.34 2021-04-22 CVMFS 2.8.1, gratia-probe 1.23.2 3.5.33 2021-04-01 Upcoming: XRootD 5.1.1 and plugins, XCache 2.0.0 3.5.32 2021-03-25 Vault 1.6.2-1, SciTokens mapfile, VO Package v110; Upcoming: HTcondor 8.9.11-1 3.5.31 2021-02-04 CVMFS 2.8.0, XRootD 4.12.6, osg-ca-certs 1.94, osg-release 3.5-5, osg-flock 1.3, python-scitokens 1.3.1 3.5.30 2021-01-27 High Priority Release: Upcoming: HTCondor 8.9.11 3.5.29 2021-01-21 IGTF 1.109, osg-configure 3.11.0, htgettoken 1.1, Upcoming: GlideinWMS 3.7.2 3.5.28-2 2020-12-15 IGTF 1.108 3.5.28 2020-12-10 osg-ca-certs 1.90, htgettoken 1.0, XRootD 4.12.5, HTCondor 8.8.12; Upcoming: HTCondor 8.9.10 3.5.27 2020-11-12 gfal2 2.18.1-1.1; Upcoming: HTCondor 8.9.9, GlideinWMS 3.7.1 3.5.26 2020-11-05 gfal2 2.18.1, HTCondor 8.8.11, CVMFS 2.7.5, osg-flock 1.2, python-scitokens 1.2.4-3, scitokens-credmon 0.8.1; Upcoming: XRootD 5.0.2 3.5.25-2 2020-10-29 VO Package v109 3.5.25 2020-10-08 GlideinWMS 3.6.5, BLAHP 1.18.48; Upcoming: BLAHP 1.18.48 3.5.24 2020-09-17 CVMFS 2.7.4, stashcache-client 6.1.0, hosted-ce-tools 0.8.2, CCTools 7.1.7, VO Package v108 3.5.23 2020-09-03 xrootd-hdfs 2.1.8, osg-release 3.5-4; Upcoming: HTCondor 8.9.8, XRootD 5.0.1 and associated plugins 3.5.22 2020-08-27 HTCondor 8.8.10, Frontier Squid 4.13-1.1, XCache 1.5.2, xrootd-scitokens 1.2.2, gratia-probe 1.20.14, BLAHP 1.18.47, oidc-agent 3.3.3, XRootD plugins 3.5.21-2 2020-08-10 IGTF 1.107, VO Package v107 3.5.21 2020-07-30 WN client and OASIS on EL8, HTCondor-CE 4.4.1, osg-flock 1.1-2, osg-pki-tools 3.4.0, osg-system-profiler 1.6.0, osg-xrootd 3.5-13, stashcache-client 6.0.0 3.5.20 2020-07-23 HTCondor-CE 4.4.0, CVMFS 2.7.3, Frontier Squid 4.12-2.1, scitokens-cpp 0.5.1 3.5.19 2020-07-01 XRoodD 4.12.3, xrootd-lcmaps 1.7.7, scitokens-credmon 0.7; Upcoming: HTCondor 8.9.7 3.5.18 2020-06-11 Frontier Squid 4.11-3.1, VOMS 2.0.14-6, XCache 1.4, stashcache-client 5.6.1 3.5.17 2020-06-04 BLAHP 1.18.46, HTCondor 8.8.9, gratia-probe 1.20.13, VO Package v106; Upcoming: GlideinWMS 3.7, HTCondor 8.9.7 3.5.16 2020-05-14 CVMFS 2.7.2, Frontier Squid 4.11-2.1, osg-ce 3.5-5, hosted-ce-tools 0.7, CCTools 7.1.5, VO Package v105 3.5.15-3 2020-05-06 IGTF 1.106 3.5.15-2 2020-04-15 VO Package v104 3.5.15 2020-04-08 Frontier Squid 4.10.3, VO Package v103, XRootD 4.11.3, osg-xrootd 3.5-12 3.5.14 2020-04-07 High Priority Release: HTCondor 8.8.8; Upcoming: HTCondor 8.9.6 3.5.13 2020-04-02 GlideinWMS 3.6.2, IGTF 1.105, HTCondor-CE 4.2.1, Pegasus 4.9.3, LCMAPS 1.6.6-1.12, globus-gridftp-server 13.20-1.1, scitokens-cpp 0.5.0 3.5.12 2020-03-26 High Priority Release: XRootD-SciTokens 1.2.0 3.5.11 2020-03-11 CVMFS 2.7.1, oidc-agent 3.3.1, CCTools 7.0.22, GSI-OpenSSH 7.4p1-5, VO Package v101 3.5.10-2 2020-03-04 VO Package v100 3.5.10 2020-02-20 XRootD 4.11.2, XCache 1.2.1, VO Package v99, UberFTP 2.8-3, osg-configure 3.1.1, osg-system-profiler 1.5.0 3.5.9 2020-02-06 High Priority Release: Frontier Squid 4.10 3.5.8-4 2020-01-30 High Priority Release: IGTF 1.104 3.5.8-3 2020-01-29 High Priority Release: OSG CA certs based on IGTF 1.104 pre-release 3.5.8-2 2020-01-28 IGTF 1.103 3.5.8 2020-01-16 XRootD 4.11.1, VOMS 2.0.14-15, HTCondor 8.8.7, gratia-probe 1.20.12, osg-xrootd, host-ce-tools 0.5-2, scitokens-cpp 0.4.0, osg-ce, gsi-openssh, globus-gridftp-server, Upcoming: HTCondor 8.9.5 3.5.7 2019-12-19 HTCondor-CE 4.1.0, CVMFS 2.7.0, GlideinWMS 3.6.1, HTCondor 8.8.6; Upcoming: HTCondor 8.9.4 3.5.6 2019-11-26 XCache 1.2, CCTools 7.0.21, osg-release 3.5-3 3.5.5 2019-11-14 High Priority Release: Frontier Squid 4.9, XRootD 4.11.0, BLAHP 1.81.45, scitokens-credmon 0.4.2, scitokens-cpp 0.3.5 3.5.4 2019-10-23 HTCondor 8.8.5-1.7, StashCache-Client 5.5.0, IGTF 1.102, VO Package v97 3.5.3 2019-10-17 GlideinWMS 3.6, oidc-agent 3.2.6, scitokens-cpp 0.3.4, XRootD 4.10.1, HTCondor 8.8.5, osg-configure 3.1.0, gratia-probe 1.20.11; Upcoming: HTCondor 8.9.3 3.5.2 2019-10-10 HTCondor-CE 4.0.1, OSG CE 3.5-2, CVMFS 2.6.3, Frontier-Squid 4.8-2, CCTools 7.0.18, VO Package v96 3.5.1 2019-09-19 High Priority Release: MyProxy 6.2.6, GSI-OpenSSH, Globus GridFTP Server 3.5.0 2019-08-30 CVMFS 2.6.2, HTCondor 8.8.4, XCache 1.1.1, OSG XRootD 3.5, OSG Configure 3.0.0, XRootD LCMAPS 1.7.4, XRootD HDFS 2.1.6; Upcoming: HTCondor 8.9.2","title":"OSG 3.5 Release Notes"},{"location":"release/notes/#announcements","text":"OSG releases are also announced by mail and are sent to the following recipients and lists: Registered administrative contacts osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org site-announce@opensciencegrid.org software-discuss@opensciencegrid.org","title":"Announcements"},{"location":"release/osg-36/","text":"Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series OSG 3.6 News \u00b6 Supported OS Versions: EL7, EL8 The OSG 3.6 release series is a major overhaul of the OSG software stack compared to previous release series with changes to core protocols used for authentication and data transfer: bearer tokens, such as SciTokens or WLCG tokens, are used for authentication instead of GSI proxies and HTTP is used for data transfer instead of GridFTP. To support these new protocols, OSG 3.6 includes HTCondor 8.9, HTCondor-CE 5, and will shortly include HTCondor 9.0, GlideinWMS 3.9, and XRootD 5.1. We also dropped support for the GridFTP, GSI authentication, and Hadoop. Latest News \u00b6 August 16, 2021: IGTF 1.112 \u00b6 This release contains updated CA Certificates based on IGTF 1.112: Updated ANSPGrid CA with extended validity date (BR) August 12, 2021: Gratia probes 2.1.0 \u00b6 Gratia probes 2.1.0 Fix a problem that caused a traceback message in the condor_meter Fix a traceback caused by missing LogLevel in ProbeConfig Ensure that Gratia accounts for SciTokens-based pilots August 5, 2021: VOMS Update, htvault-config 1.4, htgettoken 1.3 \u00b6 VOMS 2.0.16-1.2 (EL7) and VOMS 2.1.0-0.14.rc2.2 (EL8) Add IAM and TLS SNI support htvault-config 1.4 and htgettoken 1.3 Improved security through more fine-grained vault tokens and detailed logging Miscellaneous improvements July 30, 2021: High Priority Release \u00b6 HTCondor 9.0.4 and 9.1.2 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 HTCONDOR-2021-0004 July 27, 2021: High Priority Release \u00b6 HTCondor 9.0.3 and 9.1.1 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: Unfortunately, these releases did not fully mitigate the vulnerability described in HTCONDOR-2021-0003 HTCONDOR-2021-0003 HTCONDOR-2021-0004 July 22, 2021: HTCondor 9.0.2 and blahp 2.1.0 \u00b6 This release of OSG 3.6 contains the following packages: HTCondor 9.0.2-1.1 : Bug fix release HTCondor can be setup to use only FIPS 140-2 approved security functions If the Singularity test fails, the job returns to the idle state Can divide GPU memory, when making multiple GPU entries for a single GPU Startd and Schedd cron job maximum line length increased to 64k bytes Added first class submit keywords for SciTokens Fixed MUNGE authentication blahp 2.1.0 : Bug fix release Fix bug where GPU request was not passed onto the batch script Fix issue where proxy symlinks were not cleaned up by not creating them Fix bug where output files are overwritten if no transfer output remap Added support for passing in extra submit arguments from the job ad July 15, 2021: VO Package v114 \u00b6 This release contains an updated VO Package with the following changes: Fix typo in CLAS12 and EIC VOMS certificate issuers Add LSC files for CERN VO IAM endpoints July 1, 2021: Frontier Squid 4.15-2.1, Vault 1.7.3, Upcoming: HTCondor 9.1.0 \u00b6 This release of OSG 3.6 contains the following packages: Frontier Squid 4.15-2.1 : Fix log rotation when not compressing Vault 1.7.3 : Bug fix release htvault-config 1.2: Updated to match vault 1.7.3 Upcoming HTCondor 9.1.0 : Start of next feature series June 24, 2021: HTCondor 9.0.1, HTCondor-CE 5.1.1 \u00b6 This release of OSG 3.6 contains the following packages: HTCondor 9.0.1-1.2 : Bug fix release Fix problem where X.509 proxy refresh kills job when using AES encryption Fix problem when jobs require a different machine after a failure Fix problem where a job matched a machine it can't use, delaying job start Fix exit code and retry checking when a job exits because of a signal Fix a memory leak in the job router when a job is removed via job policy Fixed the back-end support for the 'bosco_cluster --add' command HTCondor-CE 5.1.1 Improve restart time of HTCondor-CE View Fix bug that caused HTCondor-CE to ignore incoming BatchRuntime requests Fixed error that occurred during RPM installation of non-HTCondor batch systems regarding missing file batch_gahp June 16, 2021: VO Package v113 \u00b6 This release contains an updated VO Package with the following changes: Added new CLAS12 and EIC VO certificates Retired old CLAS12 and EIC VO certificates June 3, 2021: Vault security update and gratia probes \u00b6 This release of OSG 3.6 contains the following packages: gratia-probe 1.23.3: Fix problem that could cause pilot hours to be zero for non-HTCondor batch systems vault 1.7.2 : Security update; fixes CVE-2021-32923. (OSG configuration not vulnerable) May 25, 2021: IGTF 1.111 \u00b6 This release contains updated CA Certificates based on IGTF 1.111: Removed discontinued NERSC-SLCS CA (US) Removed discontinued MYIFAM CA (MY) May 17, 2021: HTCondor-CE 5.1.0 and HTCondor 9.0.0 \u00b6 This release of OSG 3.6 contains the following packages: HTCondor 9.0.0-1.5 : Major new release with enhanced security Blahp 2.0.2 : GPU Support, Converted to Python 3 HTCondor-CE 5.1.0 Support for Job Router Transform configuration syntax Credential mapping changes Converted to Python 3 osg-scitokens-mapfile 3: Updated to support HTCondor-CE 5.1.0 osg-ce: now requires osg-scitokens-mapfile vault 1.7.1: Update to latest upstream release htvault-config 1.1: Uses yaml configuration files htgettoken 1.2: improved error message handling and bug fixes May 13, 2021: High Priority Release \u00b6 This release of OSG 3.6 contains the following packages: Frontier Squid 4.15-1.2 : Closes multiple security vulnerabilities Updated CA certificates based on IGTF 1.110 osg-ca-certs 1.96 : Fixed Let's Encrypt signing policy to accept cross-signing chain April 22, 2021: CVMFS 2.8.1 \u00b6 This release of OSG 3.6 contains the following packages: CVMFS 2.8.1 : Bug fix release gratia-probe 1.23.2 : Converted to use Python 3 March 25, 2021: HTCondor 8.9.11 patches \u00b6 This release of OSG 3.6 contains the following packages: HTCondor 8.9.11-1.4 (EL7 only) Fixes a potential SchedD crash when using malformed tokens condor_watch_q now works on DAGs vo-client-110-1 with updated WeNMR VOMS information Additionally, the following packages that were already available in OSG 3.6 for EL7 were released for EL8: osg-scitokens-mapfile-1-1 containing a new HTCondor-CE mapfile for VO token issuers vault-1.6.2-1 and htvault-config-0.5-1 for managing tokens cvmfs-gateway-1.2.0-1 : note the upstream documentation for updating from version 0.2.5 February 26, 2021: 3.6 Released \u00b6 Where are GlideinWMS and XRootD? XRootD and GlideinWMS are both absent in the initial OSG 3.6 release: we expect major version updates that may require manual intervention for both of these packages so we are holding their initial releases in this series until they are ready. OSG 3.5 end-of-life As a result of this initial OSG 3.6 release, the end-of-life dates have been set for OSG 3.5 per our policy : regular support will end in August 2021 and critical bug/security support will end in February 2022 . This initial release of the OSG 3.6 release series is based on the packages available in OSG 3.5.31. One of the major changes in this release series is the shift to token-based authentication from GSI proxy-based authentication. Here is a list of the differences in this initial release: GridFTP, GSI, and Hadoop are no longer available Added packages to support token-based authentication HTCondor 8.9.11 : initial token support (8.9.12, which will contain default configuration using tokens, was delayed) HTCondor-CE 5.0.0 : support for Python 3 Gratia Probe 2.0.0 : replace all batch system probes with the non-root HTCondor-CE probe OSG-Configure 4.0.0 : Deprecated RSV Dropped unused configuration modules and attributes Reorganized some configuration (see update instructions for more details) In addition, we have updated our Software Release Policy to follow a rolling release model. Finally, our Docker image releases will more closely track our OSG 3.6 repositories. Announcements \u00b6 Updates to critical packages also announced by email and are sent to the following recipients and lists: Registered administrative contacts osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org site-announce@opensciencegrid.org software-discuss@opensciencegrid.org","title":"News"},{"location":"release/osg-36/#osg-36-news","text":"Supported OS Versions: EL7, EL8 The OSG 3.6 release series is a major overhaul of the OSG software stack compared to previous release series with changes to core protocols used for authentication and data transfer: bearer tokens, such as SciTokens or WLCG tokens, are used for authentication instead of GSI proxies and HTTP is used for data transfer instead of GridFTP. To support these new protocols, OSG 3.6 includes HTCondor 8.9, HTCondor-CE 5, and will shortly include HTCondor 9.0, GlideinWMS 3.9, and XRootD 5.1. We also dropped support for the GridFTP, GSI authentication, and Hadoop.","title":"OSG 3.6 News"},{"location":"release/osg-36/#latest-news","text":"","title":"Latest News"},{"location":"release/osg-36/#august-16-2021-igtf-1112","text":"This release contains updated CA Certificates based on IGTF 1.112: Updated ANSPGrid CA with extended validity date (BR)","title":"August 16, 2021: IGTF 1.112"},{"location":"release/osg-36/#august-12-2021-gratia-probes-210","text":"Gratia probes 2.1.0 Fix a problem that caused a traceback message in the condor_meter Fix a traceback caused by missing LogLevel in ProbeConfig Ensure that Gratia accounts for SciTokens-based pilots","title":"August 12, 2021: Gratia probes 2.1.0"},{"location":"release/osg-36/#august-5-2021-voms-update-htvault-config-14-htgettoken-13","text":"VOMS 2.0.16-1.2 (EL7) and VOMS 2.1.0-0.14.rc2.2 (EL8) Add IAM and TLS SNI support htvault-config 1.4 and htgettoken 1.3 Improved security through more fine-grained vault tokens and detailed logging Miscellaneous improvements","title":"August 5, 2021: VOMS Update, htvault-config 1.4, htgettoken 1.3"},{"location":"release/osg-36/#july-30-2021-high-priority-release","text":"HTCondor 9.0.4 and 9.1.2 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 HTCONDOR-2021-0004","title":"July 30, 2021: High Priority Release"},{"location":"release/osg-36/#july-27-2021-high-priority-release","text":"HTCondor 9.0.3 and 9.1.1 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: Unfortunately, these releases did not fully mitigate the vulnerability described in HTCONDOR-2021-0003 HTCONDOR-2021-0003 HTCONDOR-2021-0004","title":"July 27, 2021: High Priority Release"},{"location":"release/osg-36/#july-22-2021-htcondor-902-and-blahp-210","text":"This release of OSG 3.6 contains the following packages: HTCondor 9.0.2-1.1 : Bug fix release HTCondor can be setup to use only FIPS 140-2 approved security functions If the Singularity test fails, the job returns to the idle state Can divide GPU memory, when making multiple GPU entries for a single GPU Startd and Schedd cron job maximum line length increased to 64k bytes Added first class submit keywords for SciTokens Fixed MUNGE authentication blahp 2.1.0 : Bug fix release Fix bug where GPU request was not passed onto the batch script Fix issue where proxy symlinks were not cleaned up by not creating them Fix bug where output files are overwritten if no transfer output remap Added support for passing in extra submit arguments from the job ad","title":"July 22, 2021: HTCondor 9.0.2 and blahp 2.1.0"},{"location":"release/osg-36/#july-15-2021-vo-package-v114","text":"This release contains an updated VO Package with the following changes: Fix typo in CLAS12 and EIC VOMS certificate issuers Add LSC files for CERN VO IAM endpoints","title":"July 15, 2021: VO Package v114"},{"location":"release/osg-36/#july-1-2021-frontier-squid-415-21-vault-173-upcoming-htcondor-910","text":"This release of OSG 3.6 contains the following packages: Frontier Squid 4.15-2.1 : Fix log rotation when not compressing Vault 1.7.3 : Bug fix release htvault-config 1.2: Updated to match vault 1.7.3 Upcoming HTCondor 9.1.0 : Start of next feature series","title":"July 1, 2021: Frontier Squid 4.15-2.1, Vault 1.7.3, Upcoming: HTCondor 9.1.0"},{"location":"release/osg-36/#june-24-2021-htcondor-901-htcondor-ce-511","text":"This release of OSG 3.6 contains the following packages: HTCondor 9.0.1-1.2 : Bug fix release Fix problem where X.509 proxy refresh kills job when using AES encryption Fix problem when jobs require a different machine after a failure Fix problem where a job matched a machine it can't use, delaying job start Fix exit code and retry checking when a job exits because of a signal Fix a memory leak in the job router when a job is removed via job policy Fixed the back-end support for the 'bosco_cluster --add' command HTCondor-CE 5.1.1 Improve restart time of HTCondor-CE View Fix bug that caused HTCondor-CE to ignore incoming BatchRuntime requests Fixed error that occurred during RPM installation of non-HTCondor batch systems regarding missing file batch_gahp","title":"June 24, 2021: HTCondor 9.0.1, HTCondor-CE 5.1.1"},{"location":"release/osg-36/#june-16-2021-vo-package-v113","text":"This release contains an updated VO Package with the following changes: Added new CLAS12 and EIC VO certificates Retired old CLAS12 and EIC VO certificates","title":"June 16, 2021: VO Package v113"},{"location":"release/osg-36/#june-3-2021-vault-security-update-and-gratia-probes","text":"This release of OSG 3.6 contains the following packages: gratia-probe 1.23.3: Fix problem that could cause pilot hours to be zero for non-HTCondor batch systems vault 1.7.2 : Security update; fixes CVE-2021-32923. (OSG configuration not vulnerable)","title":"June 3, 2021: Vault security update and gratia probes"},{"location":"release/osg-36/#may-25-2021-igtf-1111","text":"This release contains updated CA Certificates based on IGTF 1.111: Removed discontinued NERSC-SLCS CA (US) Removed discontinued MYIFAM CA (MY)","title":"May 25, 2021: IGTF 1.111"},{"location":"release/osg-36/#may-17-2021-htcondor-ce-510-and-htcondor-900","text":"This release of OSG 3.6 contains the following packages: HTCondor 9.0.0-1.5 : Major new release with enhanced security Blahp 2.0.2 : GPU Support, Converted to Python 3 HTCondor-CE 5.1.0 Support for Job Router Transform configuration syntax Credential mapping changes Converted to Python 3 osg-scitokens-mapfile 3: Updated to support HTCondor-CE 5.1.0 osg-ce: now requires osg-scitokens-mapfile vault 1.7.1: Update to latest upstream release htvault-config 1.1: Uses yaml configuration files htgettoken 1.2: improved error message handling and bug fixes","title":"May 17, 2021: HTCondor-CE 5.1.0 and HTCondor 9.0.0"},{"location":"release/osg-36/#may-13-2021-high-priority-release","text":"This release of OSG 3.6 contains the following packages: Frontier Squid 4.15-1.2 : Closes multiple security vulnerabilities Updated CA certificates based on IGTF 1.110 osg-ca-certs 1.96 : Fixed Let's Encrypt signing policy to accept cross-signing chain","title":"May 13, 2021: High Priority Release"},{"location":"release/osg-36/#april-22-2021-cvmfs-281","text":"This release of OSG 3.6 contains the following packages: CVMFS 2.8.1 : Bug fix release gratia-probe 1.23.2 : Converted to use Python 3","title":"April 22, 2021: CVMFS 2.8.1"},{"location":"release/osg-36/#march-25-2021-htcondor-8911-patches","text":"This release of OSG 3.6 contains the following packages: HTCondor 8.9.11-1.4 (EL7 only) Fixes a potential SchedD crash when using malformed tokens condor_watch_q now works on DAGs vo-client-110-1 with updated WeNMR VOMS information Additionally, the following packages that were already available in OSG 3.6 for EL7 were released for EL8: osg-scitokens-mapfile-1-1 containing a new HTCondor-CE mapfile for VO token issuers vault-1.6.2-1 and htvault-config-0.5-1 for managing tokens cvmfs-gateway-1.2.0-1 : note the upstream documentation for updating from version 0.2.5","title":"March 25, 2021: HTCondor 8.9.11 patches"},{"location":"release/osg-36/#february-26-2021-36-released","text":"Where are GlideinWMS and XRootD? XRootD and GlideinWMS are both absent in the initial OSG 3.6 release: we expect major version updates that may require manual intervention for both of these packages so we are holding their initial releases in this series until they are ready. OSG 3.5 end-of-life As a result of this initial OSG 3.6 release, the end-of-life dates have been set for OSG 3.5 per our policy : regular support will end in August 2021 and critical bug/security support will end in February 2022 . This initial release of the OSG 3.6 release series is based on the packages available in OSG 3.5.31. One of the major changes in this release series is the shift to token-based authentication from GSI proxy-based authentication. Here is a list of the differences in this initial release: GridFTP, GSI, and Hadoop are no longer available Added packages to support token-based authentication HTCondor 8.9.11 : initial token support (8.9.12, which will contain default configuration using tokens, was delayed) HTCondor-CE 5.0.0 : support for Python 3 Gratia Probe 2.0.0 : replace all batch system probes with the non-root HTCondor-CE probe OSG-Configure 4.0.0 : Deprecated RSV Dropped unused configuration modules and attributes Reorganized some configuration (see update instructions for more details) In addition, we have updated our Software Release Policy to follow a rolling release model. Finally, our Docker image releases will more closely track our OSG 3.6 repositories.","title":"February 26, 2021: 3.6 Released"},{"location":"release/osg-36/#announcements","text":"Updates to critical packages also announced by email and are sent to the following recipients and lists: Registered administrative contacts osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org site-announce@opensciencegrid.org software-discuss@opensciencegrid.org","title":"Announcements"},{"location":"release/release_series/","text":"Release Series \u00b6 An OSG release series is a sequence of OSG software releases that are intended to provide a painless upgrade path. For example, the 3.2 release series contains OSG software 3.2.0, 3.2.1, 3.2.2, and so forth. A release series corresponds to a set of Yum software repositories, including ones for development, testing, and production use. The Yum repositories for one release series are completely distinct from the repositories for a different release series, even though they share many common packages. A particular release within a series is a snapshot of packages and their exact versions at one point in time. When you install software from a release series, say 3.2, you get the most current versions of software packages within that series, regardless of the current release version. When a new series is released, it is an opportunity for the OSG Technology area to add major new software packages, make substantial updates to existing packages, and remove obsolete packages. When a new series is initially released, most packages are identical to the previous release, but two adjacent series will diverge over time. Our goal is, within a series, that one may upgrade their OSG services via yum update cleanly and without any necessary config file changes or excessive downtime. Series Overviews \u00b6 Since the start of the RPM-based OSG software stack, we have offered the following release series: Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series OSG 3.5 was started in August 2019 and will reach its end-of-life in February 2022. The main differences between it and 3.4 were the introduction of the HTCondor 8.8 and 8.9 series; also the RSV monitoring probes, EL6 support, and CREAM support were all dropped. OSG 3.6 (started February 2021) overhauls the authentication and data transfer protocols used in the OSG software stack: bearer tokens, such as SciTokens or WLCG tokens, are used for authentication instead of GSI proxies and HTTP is used for data transfer instead of GridFTP. See the OSG GridFTP and GSI migration plan for more details. To support these new protocols, OSG 3.6 includes HTCondor 8.9, HTCondor-CE 5, and will include XRootD 5.1. OSG 3.4 started June 2017 and was end-of-lifed in November 2020. The main differences between it and 3.3 are the removal of edg-mkgridmap, GUMS, BeStMan, and VOMS Admin Server packages. OSG 3.3 started in August 2015 and was end-of-lifed in May 2018. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between 3.3 and 3.2 are the dropping of EL5 support, the addition of EL7 support, and the dropping of Globus GRAM support. OSG 3.2 started in November 2013, and was end-of-lifed in August 2016. The main differences between it and 3.1 were the introduction of glideinWMS 3.2, HTCondor 8.0, and Hadoop/HDFS 2.0; also the gLite CE Monitor system was dropped in favor of osg-info-services. OSG 3.1 started in April 2012, and was end-of-lifed in April 2015. Historically, there were 3.0.x releases as well, but there was no separate release series for 3.0 and 3.1; we simply went from 3.0.10 to 3.1.0 in the same repositories. Installing an OSG Release Series \u00b6 See the yum repositories document for instructions on installing the OSG repositories. References \u00b6 Yum repositories Basic use of Yum","title":"Release Series"},{"location":"release/release_series/#release-series","text":"An OSG release series is a sequence of OSG software releases that are intended to provide a painless upgrade path. For example, the 3.2 release series contains OSG software 3.2.0, 3.2.1, 3.2.2, and so forth. A release series corresponds to a set of Yum software repositories, including ones for development, testing, and production use. The Yum repositories for one release series are completely distinct from the repositories for a different release series, even though they share many common packages. A particular release within a series is a snapshot of packages and their exact versions at one point in time. When you install software from a release series, say 3.2, you get the most current versions of software packages within that series, regardless of the current release version. When a new series is released, it is an opportunity for the OSG Technology area to add major new software packages, make substantial updates to existing packages, and remove obsolete packages. When a new series is initially released, most packages are identical to the previous release, but two adjacent series will diverge over time. Our goal is, within a series, that one may upgrade their OSG services via yum update cleanly and without any necessary config file changes or excessive downtime.","title":"Release Series"},{"location":"release/release_series/#series-overviews","text":"Since the start of the RPM-based OSG software stack, we have offered the following release series: Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series OSG 3.5 was started in August 2019 and will reach its end-of-life in February 2022. The main differences between it and 3.4 were the introduction of the HTCondor 8.8 and 8.9 series; also the RSV monitoring probes, EL6 support, and CREAM support were all dropped. OSG 3.6 (started February 2021) overhauls the authentication and data transfer protocols used in the OSG software stack: bearer tokens, such as SciTokens or WLCG tokens, are used for authentication instead of GSI proxies and HTTP is used for data transfer instead of GridFTP. See the OSG GridFTP and GSI migration plan for more details. To support these new protocols, OSG 3.6 includes HTCondor 8.9, HTCondor-CE 5, and will include XRootD 5.1. OSG 3.4 started June 2017 and was end-of-lifed in November 2020. The main differences between it and 3.3 are the removal of edg-mkgridmap, GUMS, BeStMan, and VOMS Admin Server packages. OSG 3.3 started in August 2015 and was end-of-lifed in May 2018. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between 3.3 and 3.2 are the dropping of EL5 support, the addition of EL7 support, and the dropping of Globus GRAM support. OSG 3.2 started in November 2013, and was end-of-lifed in August 2016. The main differences between it and 3.1 were the introduction of glideinWMS 3.2, HTCondor 8.0, and Hadoop/HDFS 2.0; also the gLite CE Monitor system was dropped in favor of osg-info-services. OSG 3.1 started in April 2012, and was end-of-lifed in April 2015. Historically, there were 3.0.x releases as well, but there was no separate release series for 3.0 and 3.1; we simply went from 3.0.10 to 3.1.0 in the same repositories.","title":"Series Overviews"},{"location":"release/release_series/#installing-an-osg-release-series","text":"See the yum repositories document for instructions on installing the OSG repositories.","title":"Installing an OSG Release Series"},{"location":"release/release_series/#references","text":"Yum repositories Basic use of Yum","title":"References"},{"location":"release/signing/","text":"OSG Release Signing Information \u00b6 Verifying OSG's RPMs \u00b6 We use a GPG key to sign our software packages. Normally yum and rpm transparently use the GPG signatures to verify the packages have not been corrupted and were created by us. You get our GPG public key when you install the osg-release RPM. If you wish to verify one of our RPMs manually, you can run: $ rpm --checksig -v <NAME.RPM> For example: $ rpm --checksig -v globus-core-8.0-2.osg.x86_64.rpm globus-core-8.0-2.osg.x86_64.rpm: Header V3 DSA signature: OK, key ID 824b8603 Header SHA1 digest: OK (2b5af4348c548c27f10e2e47e1ec80500c4f85d7) MD5 digest: OK (d11503a229a1a0e02262034efe0f7e46) V3 DSA signature: OK, key ID 824b8603 The OSG Packaging Signing Keys \u00b6 The OSG Software Team has two GPG keys for signing RPMs; the first key is used for packages before the 3.6 release series, and the second key is used for packages in the 3.6 release series and afterward. Key 1 (3.0 to 3.5) Location /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG Download UW-Madison , GitHub Fingerprint 6459 !D9D2 AAA9 AB67 A251 FB44 2110 !B1C8 824B 8603 Key ID 824b8603 Key 2 (3.6 and on) Location /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG-2 Download UW-Madison , GitHub Fingerprint 1216 FF68 897A 77EA 222F C961 27DC 6864 96D2 B90F Key ID 96d2b90f Note Some packages in the 3.6 repos may still be signed with the old key; the osg-release RPM contains both keys so you can verify old packages. You can see the fingerprint for yourself. On EL 7 and older (GnuPG < 2.1.13): $ gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG pub 1024D/824B8603 2011-09-15 OSG Software Team (RPM Signing Key for Koji Packages) <vdt-support@opensciencegrid.org> Key fingerprint = 6459 D9D2 AAA9 AB67 A251 FB44 2110 B1C8 824B 8603 sub 2048g/28E5857C 2011-09-15 $ gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG-2 pub 4096R/96D2B90F 2021-02-24 Open Science Grid Software <help@opensciencegrid.org> Key fingerprint = 1216 FF68 897A 77EA 222F C961 27DC 6864 96D2 B90F sub 4096R/49E9ACC2 2021-02-24 On EL 8 and newer (GnuPG >= 2.1.13): $ gpg --import-options show-only --import < /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG pub dsa1024 2011-09-15 [SC] 6459D9D2AAA9AB67A251FB442110B1C8824B8603 uid OSG Software Team (RPM Signing Key for Koji Packages) <vdt-support@opensciencegrid.org> sub elg2048 2011-09-15 [E] $ gpg --import-options show-only --import < /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG-2 pub rsa4096 2021-02-24 [SC] 1216FF68897A77EA222FC96127DC686496D2B90F uid Open Science Grid Software <help@opensciencegrid.org> sub rsa4096 2021-02-24 [E]","title":"RPM Signing"},{"location":"release/signing/#osg-release-signing-information","text":"","title":"OSG Release Signing Information"},{"location":"release/signing/#verifying-osgs-rpms","text":"We use a GPG key to sign our software packages. Normally yum and rpm transparently use the GPG signatures to verify the packages have not been corrupted and were created by us. You get our GPG public key when you install the osg-release RPM. If you wish to verify one of our RPMs manually, you can run: $ rpm --checksig -v <NAME.RPM> For example: $ rpm --checksig -v globus-core-8.0-2.osg.x86_64.rpm globus-core-8.0-2.osg.x86_64.rpm: Header V3 DSA signature: OK, key ID 824b8603 Header SHA1 digest: OK (2b5af4348c548c27f10e2e47e1ec80500c4f85d7) MD5 digest: OK (d11503a229a1a0e02262034efe0f7e46) V3 DSA signature: OK, key ID 824b8603","title":"Verifying OSG's RPMs"},{"location":"release/signing/#the-osg-packaging-signing-keys","text":"The OSG Software Team has two GPG keys for signing RPMs; the first key is used for packages before the 3.6 release series, and the second key is used for packages in the 3.6 release series and afterward. Key 1 (3.0 to 3.5) Location /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG Download UW-Madison , GitHub Fingerprint 6459 !D9D2 AAA9 AB67 A251 FB44 2110 !B1C8 824B 8603 Key ID 824b8603 Key 2 (3.6 and on) Location /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG-2 Download UW-Madison , GitHub Fingerprint 1216 FF68 897A 77EA 222F C961 27DC 6864 96D2 B90F Key ID 96d2b90f Note Some packages in the 3.6 repos may still be signed with the old key; the osg-release RPM contains both keys so you can verify old packages. You can see the fingerprint for yourself. On EL 7 and older (GnuPG < 2.1.13): $ gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG pub 1024D/824B8603 2011-09-15 OSG Software Team (RPM Signing Key for Koji Packages) <vdt-support@opensciencegrid.org> Key fingerprint = 6459 D9D2 AAA9 AB67 A251 FB44 2110 B1C8 824B 8603 sub 2048g/28E5857C 2011-09-15 $ gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG-2 pub 4096R/96D2B90F 2021-02-24 Open Science Grid Software <help@opensciencegrid.org> Key fingerprint = 1216 FF68 897A 77EA 222F C961 27DC 6864 96D2 B90F sub 4096R/49E9ACC2 2021-02-24 On EL 8 and newer (GnuPG >= 2.1.13): $ gpg --import-options show-only --import < /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG pub dsa1024 2011-09-15 [SC] 6459D9D2AAA9AB67A251FB442110B1C8824B8603 uid OSG Software Team (RPM Signing Key for Koji Packages) <vdt-support@opensciencegrid.org> sub elg2048 2011-09-15 [E] $ gpg --import-options show-only --import < /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG-2 pub rsa4096 2021-02-24 [SC] 1216FF68897A77EA222FC96127DC686496D2B90F uid Open Science Grid Software <help@opensciencegrid.org> sub rsa4096 2021-02-24 [E]","title":"The OSG Packaging Signing Keys"},{"location":"release/supported_platforms/","text":"Supported Platforms \u00b6 The OSG release series are supported on Red Hat Enterprise Linux (RHEL) compatible platforms for 64-bit Intel architectures according to the following table: Note As of July 2020, only of a subset of the software stack is supported on RHEL 8-compatible platforms. You can track packages released for EL8 through our release notes . Platform OSG 3.5 OSG 3.6 CentOS 7 \u2705 \u2705 CentOS 8 \u2705 \u2705 Red Hat Enterprise Linux 7 \u2705 \u2705 Red Hat Enterprise Linux 8 \u2705 \u2705 Scientifix Linux 7 \u2705 \u2705 OSG builds and tests its RPMs on the latest releases of the relevant platforms (e.g., in 2018, the RHEL 7 builds were based on RHEL 7.5). Older platform release versions may not receive thorough testing and may have subtle bugs. In particular, versions of RHEL/CentOS/SL 7 less than 7.5 have known issues with several pieces of software, including osg-oasis and xrootd-lcmaps . If sites run into problems with one of those versions, we will ask them to update to the latest operating system packages as part of the support process.","title":"Supported Platforms"},{"location":"release/supported_platforms/#supported-platforms","text":"The OSG release series are supported on Red Hat Enterprise Linux (RHEL) compatible platforms for 64-bit Intel architectures according to the following table: Note As of July 2020, only of a subset of the software stack is supported on RHEL 8-compatible platforms. You can track packages released for EL8 through our release notes . Platform OSG 3.5 OSG 3.6 CentOS 7 \u2705 \u2705 CentOS 8 \u2705 \u2705 Red Hat Enterprise Linux 7 \u2705 \u2705 Red Hat Enterprise Linux 8 \u2705 \u2705 Scientifix Linux 7 \u2705 \u2705 OSG builds and tests its RPMs on the latest releases of the relevant platforms (e.g., in 2018, the RHEL 7 builds were based on RHEL 7.5). Older platform release versions may not receive thorough testing and may have subtle bugs. In particular, versions of RHEL/CentOS/SL 7 less than 7.5 have known issues with several pieces of software, including osg-oasis and xrootd-lcmaps . If sites run into problems with one of those versions, we will ask them to update to the latest operating system packages as part of the support process.","title":"Supported Platforms"},{"location":"release/updating-to-osg-35/","text":"Updating to OSG 3.5 \u00b6 OS Version Support OSG 3.5 only supports EL7 If you have an existing installation based on OSG release version <= 3.4 (which will be referred to as the old series ), and want to upgrade to 3.5 (the new series ), we recommend the following procedure: First, remove the old series yum repositories: root@host # rpm -e osg-release This step ensures that any local modifications to *.repo files will not prevent installing the new series repos. Any modified *.repo files should appear under /etc/yum.repos.d/ with the *.rpmsave extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the *.rpmsave files to the new *.repo files. Install the OSG repositories: root@host # yum install https://repo.opensciencegrid.org/osg/3.5/osg-3.5-el7-release-latest.rpm Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update Info Please be aware that running yum update may also update other RPMs. You can exclude packages from being updated using the --exclude=[package-name or glob] option for the yum command. Watch the yum update carefully for any messages about a .rpmnew file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a .rpmnew extension. You will need to merge any edits that have made into the .rpmnew file and then move the merged version into place (that is, without the .rpmnew extension). Watch especially for /etc/lcmaps.db , which every site is expected to edit. Remove any deprecated packages that were previously installed: root@host # yum remove osg-version \\ osg-control \\ 'rsv*' \\ glite-ce-cream-client-api-c \\ glite-lbjp-common-gsoap-plugin \\ xacml If you did not have any of the above packages installed, Yum will not remove any packages: No Match for argument: osg-version No Match for argument: osg-control No Match for argument: rsv* No Match for argument: glite-ce-cream-client-api-c No Match for argument: glite-lbjp-common-gsoap-plugin No Match for argument: xacml No Packages marked for removal If you are updating an HTCondor-CE host, please consult the manual HTCondor and OSG Configure instructions below. Running into issues? If you are not having the expected result or having problems with Yum please see the Yum troubleshooting guide Updating to HTCondor-CE 4.x \u00b6 The OSG 3.5 release series contains HTCondor-CE 4, a major version upgrade from the previously released versions in the OSG. See the HTCondor-CE 4.0.0 release notes for an overview of the changes. In particular, this version includes a major reorganization of the default configuration so updates will require manual intervention. To update your HTCondor-CE host(s), perform the following steps: Update all CE packages: root@host # yum update htcondor-ce 'osg-ce*' The new default condor_mapfile is sufficient since HTCondor-CE no longer relies on GSI authentication between its daemons. If /etc/condor-ce/condor_mapfile.rpmnew exists, replace your old condor_mapfile with the .rpmnew version: root@host # mv /etc/condor-ce/condor_mapfile.rpmnew /etc/condor-ce/condor_mapfile Merge any *.rpmnew files in /etc/condor-ce/config.d/ Additionally, you may wish to make one or more of the following optional changes: HTCondor-CE now disables batch system job retries by default. To re-enable job retries, set the following configuration in /etc/condor-ce/config.d/99-local.conf : ENABLE_JOB_RETRIES = True For non-HTCondor sites that use remote CE requirements , the new version of HTCondor-CE accepts a simplified format. For example, a snippet from an example job route in the old format: set_default_remote_cerequirements = strcat(\"Walltime == 3600 && AccountingGroup ==\"\", x509UserProxyFirstFQAN, \"\\\"\"); May be rewritten as the following: set_WallTime = 3600; set_AccountingGroup = x509UserProxyFirstFQAN; set_default_CERequirements = \"Walltime,AccountingGroup\"; Reload and restart the HTCondor-CE daemons: root@host # systemctl daemon-reload root@host # systemctl restart condor-ce Updating to HTCondor 8.8.x \u00b6 The OSG 3.5 release series contains HTCondor 8.8, a major version upgrade from the previously released versions in the OSG. See the detailed update instructions below to update to HTCondor 8.8. Updating to OSG Configure 3 \u00b6 The OSG 3.5 release series contains OSG-Configure 3, a major version upgrade from the previously released versions in the OSG. See the OSG Configure release notes for an overview of the changes . To update OSG Configure on your HTCondor-CE, perform the following steps: If you haven't already, update to OSG 3.5 . If you have site_name set in /etc/osg/config.d/40-siteinfo.ini , delete it and specify resource instead. resource should match the resource name that's registered in OSG Topology . Set resource_group in /etc/osg/config.d/40-siteinfo.ini to the resource group registered in OSG Topology , i.e. the name of the .yaml file in OSG Topology that contains the registered resource above. Set host_name to the host name that is registered in OSG Topology . This may be different from the FQDN of the host if you're using a DNS alias, for example. OSG Configure will warn about config options that it does not recognize; delete these options from the config to get rid of the warnings. Updating to HTCondor-CE 5 \u00b6 HTCondor-CE 5 is a major version upgrade from HTCondor-CE 4, available through the OSG upcoming repository. HTCondor-CE 5 provides improved support for SciTokens and WLCG Tokens as well as support for a new Job Router configuration syntax. To update HTCondor-CE 5, perform the following steps: Update your HTCondor-CE packages: root@host # yum update htcondor-ce --enablerepo = osg-upcoming Updating to HTCondor 9.0.x This update may also pull in updates to HTCondor 9.0. To update, consult the upgrade section below , especially if you use HTCondor for your local batch system. If you support the OSG or GLOW VOs and map their jobs to non-standard local Unix accounts (e.g., not osg and glow , respectively) add SciTokens mappings to a file in /etc/condor-ce/mapfiles.d/ : # OSG SCITOKENS /^https\\:\\/\\/scitokens\\.org\\/osg\\-connect,/ osg # GLOW SCITOKENS /^https\\:\\/\\/chtc\\.cs\\.wisc\\.edu,/ glow Replacing osg and glow with the local Unix account for the OSG and GLOW VOs, respectively. Also consult the upgrade documentation for other required configuration updates. Updating to HTCondor 8.8.x \u00b6 The OSG 3.5 release series contains HTCondor 8.8, a major version upgrade from the previously released versions in the OSG. See the HTCondor 8.8 manual for an overview of the changes . To update HTCondor on your HTCondor-CE and/or HTCondor pool hosts, perform the following steps: Update all HTCondor packages: root@host # yum update 'condor*' HTCondor pools only: The DAEMON_LIST , and CONDOR_HOST configuration changed in HTCondor 8.8. Additionally in OSG 3.5, the default security was changed to use FS and pool password. If you are experiencing issues with communication between hosts in your pool after the upgrade, the default OSG configuration is listed in /etc/condor/config.d/00-osg_default_*.config : ensure that any default configuration is overridden with your own DAEMON_LIST , CONDOR_HOST , and/or security configuration in subsequent files. As of HTCondor 8.8, MOUNT_UNDER_SCRATCH has default values of /tmp and /var/tmp , which may cause issues if your OSG_WN_TMP is a subdirectory of either of these directories. If the partition containing your execute directories is large enough , we recommend setting your OSG_WN_TMP to /tmp or /var/tmp . If that partition is not large enough, we recommend setting your OSG_WN_TMP variable to a directory outside of /tmp or /var/tmp . HTCondor-CE hosts only: The HTCondor 8.8 series changed the default job route matching order from round-robin to first matching route . To use the old round-robin matching order, add the following configuration to /etc/condor-ce/config.d/99-local.conf : JOB_ROUTER_ROUND_ROBIN_SELECTION = True Clean-up deprecated packages: root@host # yum remove 'rsv*' glite-ce-cream-client-api-c Updating to HTCondor 9.0.0+ \u00b6 Where to find HTCondor 9.0 The HTCondor 9.0 series is available through the OSG upcoming repository. Manual intervention may be required to upgrade from the HTCondor 8.8 series to HTCondor 9.0.x. Please consult the HTCondor 9.0 upgrade instructions . If you are upgrading from the HTCondor 8.9 series (8.9.11 and earlier), please consult the Upgrading to 9.0 instructions For HTCondor hosts < 8.9.7 using the SciTokens CredMon, updates to HTCondor 8.9.7+ require manual intervention and a corresponding update to python2-scitokens-credmon , available in the OSG 3.5 release repository. If you do not have the python2-scitokens-credmon package installed, you may skip these instructions. Otherwise, follow these steps for a seamless update to HTCondor 8.9.7+: Determine if your HTCondor installation is configured to use the SciTokens CredMon: # condor_config_val -v DAEMON_LIST If CREDD and SEC_CREDENTIAL_MONITOR are in the output of the above command, continue onto the next step. Otherwise, your installation is not configured to use SciTokens CredMon and you should skip the rest of these instructions. Add the following to a file in /etc/condor/config.d/ : SEC_CREDENTIAL_DIRECTORY_OAUTH = /var/lib/condor/oauth_credentials CREDMON_OAUTH = /usr/bin/condor_credmon_oauth SEC_CREDENTIAL_MONITOR_OAUTH_LOG = $(LOG)/CredMonOAuthLog if version < 8.9.7 CREDMON_OAUTH = /usr/bin/scitokens_credmon endif Update your DAEMON_LIST configuration from: DAEMON_LIST = $(DAEMON_LIST), CREDD, SEC_CREDENTIAL_MONITOR to DAEMON_LIST = $(DAEMON_LIST), CREDD, CREDMON_OAUTH Turn off the SchedD and CredMon daemons: # condor_off -daemon SCHEDD # condor_off -daemon SEC_CREDENTIAL_MONITOR # condor_off -daemon CREDMON_OAUTH Move the existing credential directory and set up a temporary symlink: # mv $(condor_config_val SEC_CREDENTIAL_DIRECTORY) $(condor_config_val SEC_CREDENTIAL_DIRECTORY_OAUTH) # ln -s $(condor_config_val SEC_CREDENTIAL_DIRECTORY_OAUTH) $(condor_config_val SEC_CREDENTIAL_DIRECTORY) Update HTCondor and SciTokens CredMon packages: # yum -y upgrade python2-scitokens-credmon condor If you are running Apache on this host, reload the Apache configuration: # systemctl reload httpd.service Reconfigure HTCondor: # condor_reconfig Turn the SchedD and CredMon daemons back on: # condor_on -daemon CREDMON_OAUTH # condor_on -daemon SCHEDD Clean up old CredMon configuration. Remove the following entries from your HTCondor configuration: SEC_CREDENTIAL_DIRECTORY = /var/lib/condor/credentials SEC_CREDENTIAL_MONITOR = /usr/bin/scitokens_credmon SEC_CREDENTIAL_MONITOR_LOG = $(LOG)/CredMonLog To allow for seamless HTCondor downgrades, update the if version < 8.9.7 block that you added in step 2. if version < 8.9.7 CREDMON_OAUTH = /usr/bin/scitokens_credmon SEC_CREDENTIAL_DIRECTORY = $(SEC_CREDENTIAL_DIRECTORY_OAUTH) endif Remove the symlink to the old credential directory that you created in step 5. This is whatever you had set SEC_CREDENTIAL_DIRECTORY to before. For example: # rm /var/lib/condor/credentials Updating to XRootD 5 \u00b6 Known issues with XRootD 5.1.1 The XRootD team is evaluating solutions for a memory leak in the HTTP Third-Party Copy (HTTP-TPC) use case related to libcurl and NSS . These leaks appear to exist in libcurl for all versions of XRootD and their impact depends on the transfer load at each site. Incompatibility with the multi-user plugin : users of the XRootD multi-user plugin will be unable to update to XRootD 5.1.x until a fixed version of XRootD multi-user is released into the OSG repositories In some cases, XCaches using the Rucio plug-in may crash due to malformed URLs generated by the plug-in. Use the OSG XRootD meta-package! The osg-xrootd and osg-xrootd-standalone meta-packages provide default XRootD configurations in /etc/xrootd/config.d/ with the ability to easily enable or disable features (such as HTTP or LCMAPS) through simple configuration flags. Additionally, the configurations provided by osg-xrootd and osg-xrootd-standalone are designed to work with the OSG-released versions of XRootD, reducing the number of necessary manual configuration updates. The OSG 3.5 upcoming repositories contain XRootD 5, a major version upgrade from the previously released versions in the OSG. See the upstream release notes for an overview of the changes. To update XRootD on your StashCache, XCache, XRootD clustered, and XRootD standalone hosts, perform the following steps: Update all XRootD packages: root@host # yum update --enablerepo = osg-upcoming 'xrootd' * xcache Determine whether or not you are using OSG XRootD meta-packages root@host # rpm -q --verify xrootd-server | \\ egrep \"/etc/xrootd/xrootd-(standalone|clustered).cfg\" If the above command does not return any output, skip to step 6. If you are not using OSG XRootD meta-packaging and are using xrootd-lcmaps , update the -authzfunparams to the new key=value syntax. For example, the following configuration: -authzfunparms:--lcmapscfg,/etc/xrootd/lcmaps.cfg,--loglevel,0,--policy,authorize_only Should be turned into: -authzfunparms:lcmapscfg=/etc/lcmaps.db,loglevel=0,policy=authorize_only If you are not using OSG XRootD meta-packaging and are loading multiple ofs.authlib libraries, separate out each library into its own ofs.authlib directive. For example, the following configuration: ofs.authlib libXrdMacaroons.so libXrdAccSciTokens.so Should be re-ordered and turned into: # Enable SciTokens-based mappings ; if no token is present, then the GSI certificate will be used. ofs.authlib ++ libXrdAccSciTokens.so ofs.authlib ++ libXrdMacaroons.so Restart the relevant XRootD services: If you are running a(n)... Restart the service with... ATLAS XCache systemctl restart xrootd@atlas-xcache CMS XCache systemctl restart xrootd@cms-xcache Stash Cache systemctl restart xrootd@stash-cache Stash Origin systemctl restart xrootd@stash-origin XRootD Standalone systemctl restart xrootd@standalone XRootD Clustered systemctl restart xrootd@clustered Getting Help \u00b6 To get assistance, please use the this page .","title":"Updating to OSG 3.5"},{"location":"release/updating-to-osg-35/#updating-to-osg-35","text":"OS Version Support OSG 3.5 only supports EL7 If you have an existing installation based on OSG release version <= 3.4 (which will be referred to as the old series ), and want to upgrade to 3.5 (the new series ), we recommend the following procedure: First, remove the old series yum repositories: root@host # rpm -e osg-release This step ensures that any local modifications to *.repo files will not prevent installing the new series repos. Any modified *.repo files should appear under /etc/yum.repos.d/ with the *.rpmsave extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the *.rpmsave files to the new *.repo files. Install the OSG repositories: root@host # yum install https://repo.opensciencegrid.org/osg/3.5/osg-3.5-el7-release-latest.rpm Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update Info Please be aware that running yum update may also update other RPMs. You can exclude packages from being updated using the --exclude=[package-name or glob] option for the yum command. Watch the yum update carefully for any messages about a .rpmnew file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a .rpmnew extension. You will need to merge any edits that have made into the .rpmnew file and then move the merged version into place (that is, without the .rpmnew extension). Watch especially for /etc/lcmaps.db , which every site is expected to edit. Remove any deprecated packages that were previously installed: root@host # yum remove osg-version \\ osg-control \\ 'rsv*' \\ glite-ce-cream-client-api-c \\ glite-lbjp-common-gsoap-plugin \\ xacml If you did not have any of the above packages installed, Yum will not remove any packages: No Match for argument: osg-version No Match for argument: osg-control No Match for argument: rsv* No Match for argument: glite-ce-cream-client-api-c No Match for argument: glite-lbjp-common-gsoap-plugin No Match for argument: xacml No Packages marked for removal If you are updating an HTCondor-CE host, please consult the manual HTCondor and OSG Configure instructions below. Running into issues? If you are not having the expected result or having problems with Yum please see the Yum troubleshooting guide","title":"Updating to OSG 3.5"},{"location":"release/updating-to-osg-35/#updating-to-htcondor-ce-4x","text":"The OSG 3.5 release series contains HTCondor-CE 4, a major version upgrade from the previously released versions in the OSG. See the HTCondor-CE 4.0.0 release notes for an overview of the changes. In particular, this version includes a major reorganization of the default configuration so updates will require manual intervention. To update your HTCondor-CE host(s), perform the following steps: Update all CE packages: root@host # yum update htcondor-ce 'osg-ce*' The new default condor_mapfile is sufficient since HTCondor-CE no longer relies on GSI authentication between its daemons. If /etc/condor-ce/condor_mapfile.rpmnew exists, replace your old condor_mapfile with the .rpmnew version: root@host # mv /etc/condor-ce/condor_mapfile.rpmnew /etc/condor-ce/condor_mapfile Merge any *.rpmnew files in /etc/condor-ce/config.d/ Additionally, you may wish to make one or more of the following optional changes: HTCondor-CE now disables batch system job retries by default. To re-enable job retries, set the following configuration in /etc/condor-ce/config.d/99-local.conf : ENABLE_JOB_RETRIES = True For non-HTCondor sites that use remote CE requirements , the new version of HTCondor-CE accepts a simplified format. For example, a snippet from an example job route in the old format: set_default_remote_cerequirements = strcat(\"Walltime == 3600 && AccountingGroup ==\"\", x509UserProxyFirstFQAN, \"\\\"\"); May be rewritten as the following: set_WallTime = 3600; set_AccountingGroup = x509UserProxyFirstFQAN; set_default_CERequirements = \"Walltime,AccountingGroup\"; Reload and restart the HTCondor-CE daemons: root@host # systemctl daemon-reload root@host # systemctl restart condor-ce","title":"Updating to HTCondor-CE 4.x"},{"location":"release/updating-to-osg-35/#updating-to-htcondor-88x","text":"The OSG 3.5 release series contains HTCondor 8.8, a major version upgrade from the previously released versions in the OSG. See the detailed update instructions below to update to HTCondor 8.8.","title":"Updating to HTCondor 8.8.x"},{"location":"release/updating-to-osg-35/#updating-to-osg-configure-3","text":"The OSG 3.5 release series contains OSG-Configure 3, a major version upgrade from the previously released versions in the OSG. See the OSG Configure release notes for an overview of the changes . To update OSG Configure on your HTCondor-CE, perform the following steps: If you haven't already, update to OSG 3.5 . If you have site_name set in /etc/osg/config.d/40-siteinfo.ini , delete it and specify resource instead. resource should match the resource name that's registered in OSG Topology . Set resource_group in /etc/osg/config.d/40-siteinfo.ini to the resource group registered in OSG Topology , i.e. the name of the .yaml file in OSG Topology that contains the registered resource above. Set host_name to the host name that is registered in OSG Topology . This may be different from the FQDN of the host if you're using a DNS alias, for example. OSG Configure will warn about config options that it does not recognize; delete these options from the config to get rid of the warnings.","title":"Updating to OSG Configure 3"},{"location":"release/updating-to-osg-35/#updating-to-htcondor-ce-5","text":"HTCondor-CE 5 is a major version upgrade from HTCondor-CE 4, available through the OSG upcoming repository. HTCondor-CE 5 provides improved support for SciTokens and WLCG Tokens as well as support for a new Job Router configuration syntax. To update HTCondor-CE 5, perform the following steps: Update your HTCondor-CE packages: root@host # yum update htcondor-ce --enablerepo = osg-upcoming Updating to HTCondor 9.0.x This update may also pull in updates to HTCondor 9.0. To update, consult the upgrade section below , especially if you use HTCondor for your local batch system. If you support the OSG or GLOW VOs and map their jobs to non-standard local Unix accounts (e.g., not osg and glow , respectively) add SciTokens mappings to a file in /etc/condor-ce/mapfiles.d/ : # OSG SCITOKENS /^https\\:\\/\\/scitokens\\.org\\/osg\\-connect,/ osg # GLOW SCITOKENS /^https\\:\\/\\/chtc\\.cs\\.wisc\\.edu,/ glow Replacing osg and glow with the local Unix account for the OSG and GLOW VOs, respectively. Also consult the upgrade documentation for other required configuration updates.","title":"Updating to HTCondor-CE 5"},{"location":"release/updating-to-osg-35/#updating-to-htcondor-88x_1","text":"The OSG 3.5 release series contains HTCondor 8.8, a major version upgrade from the previously released versions in the OSG. See the HTCondor 8.8 manual for an overview of the changes . To update HTCondor on your HTCondor-CE and/or HTCondor pool hosts, perform the following steps: Update all HTCondor packages: root@host # yum update 'condor*' HTCondor pools only: The DAEMON_LIST , and CONDOR_HOST configuration changed in HTCondor 8.8. Additionally in OSG 3.5, the default security was changed to use FS and pool password. If you are experiencing issues with communication between hosts in your pool after the upgrade, the default OSG configuration is listed in /etc/condor/config.d/00-osg_default_*.config : ensure that any default configuration is overridden with your own DAEMON_LIST , CONDOR_HOST , and/or security configuration in subsequent files. As of HTCondor 8.8, MOUNT_UNDER_SCRATCH has default values of /tmp and /var/tmp , which may cause issues if your OSG_WN_TMP is a subdirectory of either of these directories. If the partition containing your execute directories is large enough , we recommend setting your OSG_WN_TMP to /tmp or /var/tmp . If that partition is not large enough, we recommend setting your OSG_WN_TMP variable to a directory outside of /tmp or /var/tmp . HTCondor-CE hosts only: The HTCondor 8.8 series changed the default job route matching order from round-robin to first matching route . To use the old round-robin matching order, add the following configuration to /etc/condor-ce/config.d/99-local.conf : JOB_ROUTER_ROUND_ROBIN_SELECTION = True Clean-up deprecated packages: root@host # yum remove 'rsv*' glite-ce-cream-client-api-c","title":"Updating to HTCondor 8.8.x"},{"location":"release/updating-to-osg-35/#updating-to-htcondor-900","text":"Where to find HTCondor 9.0 The HTCondor 9.0 series is available through the OSG upcoming repository. Manual intervention may be required to upgrade from the HTCondor 8.8 series to HTCondor 9.0.x. Please consult the HTCondor 9.0 upgrade instructions . If you are upgrading from the HTCondor 8.9 series (8.9.11 and earlier), please consult the Upgrading to 9.0 instructions For HTCondor hosts < 8.9.7 using the SciTokens CredMon, updates to HTCondor 8.9.7+ require manual intervention and a corresponding update to python2-scitokens-credmon , available in the OSG 3.5 release repository. If you do not have the python2-scitokens-credmon package installed, you may skip these instructions. Otherwise, follow these steps for a seamless update to HTCondor 8.9.7+: Determine if your HTCondor installation is configured to use the SciTokens CredMon: # condor_config_val -v DAEMON_LIST If CREDD and SEC_CREDENTIAL_MONITOR are in the output of the above command, continue onto the next step. Otherwise, your installation is not configured to use SciTokens CredMon and you should skip the rest of these instructions. Add the following to a file in /etc/condor/config.d/ : SEC_CREDENTIAL_DIRECTORY_OAUTH = /var/lib/condor/oauth_credentials CREDMON_OAUTH = /usr/bin/condor_credmon_oauth SEC_CREDENTIAL_MONITOR_OAUTH_LOG = $(LOG)/CredMonOAuthLog if version < 8.9.7 CREDMON_OAUTH = /usr/bin/scitokens_credmon endif Update your DAEMON_LIST configuration from: DAEMON_LIST = $(DAEMON_LIST), CREDD, SEC_CREDENTIAL_MONITOR to DAEMON_LIST = $(DAEMON_LIST), CREDD, CREDMON_OAUTH Turn off the SchedD and CredMon daemons: # condor_off -daemon SCHEDD # condor_off -daemon SEC_CREDENTIAL_MONITOR # condor_off -daemon CREDMON_OAUTH Move the existing credential directory and set up a temporary symlink: # mv $(condor_config_val SEC_CREDENTIAL_DIRECTORY) $(condor_config_val SEC_CREDENTIAL_DIRECTORY_OAUTH) # ln -s $(condor_config_val SEC_CREDENTIAL_DIRECTORY_OAUTH) $(condor_config_val SEC_CREDENTIAL_DIRECTORY) Update HTCondor and SciTokens CredMon packages: # yum -y upgrade python2-scitokens-credmon condor If you are running Apache on this host, reload the Apache configuration: # systemctl reload httpd.service Reconfigure HTCondor: # condor_reconfig Turn the SchedD and CredMon daemons back on: # condor_on -daemon CREDMON_OAUTH # condor_on -daemon SCHEDD Clean up old CredMon configuration. Remove the following entries from your HTCondor configuration: SEC_CREDENTIAL_DIRECTORY = /var/lib/condor/credentials SEC_CREDENTIAL_MONITOR = /usr/bin/scitokens_credmon SEC_CREDENTIAL_MONITOR_LOG = $(LOG)/CredMonLog To allow for seamless HTCondor downgrades, update the if version < 8.9.7 block that you added in step 2. if version < 8.9.7 CREDMON_OAUTH = /usr/bin/scitokens_credmon SEC_CREDENTIAL_DIRECTORY = $(SEC_CREDENTIAL_DIRECTORY_OAUTH) endif Remove the symlink to the old credential directory that you created in step 5. This is whatever you had set SEC_CREDENTIAL_DIRECTORY to before. For example: # rm /var/lib/condor/credentials","title":"Updating to HTCondor 9.0.0+"},{"location":"release/updating-to-osg-35/#updating-to-xrootd-5","text":"Known issues with XRootD 5.1.1 The XRootD team is evaluating solutions for a memory leak in the HTTP Third-Party Copy (HTTP-TPC) use case related to libcurl and NSS . These leaks appear to exist in libcurl for all versions of XRootD and their impact depends on the transfer load at each site. Incompatibility with the multi-user plugin : users of the XRootD multi-user plugin will be unable to update to XRootD 5.1.x until a fixed version of XRootD multi-user is released into the OSG repositories In some cases, XCaches using the Rucio plug-in may crash due to malformed URLs generated by the plug-in. Use the OSG XRootD meta-package! The osg-xrootd and osg-xrootd-standalone meta-packages provide default XRootD configurations in /etc/xrootd/config.d/ with the ability to easily enable or disable features (such as HTTP or LCMAPS) through simple configuration flags. Additionally, the configurations provided by osg-xrootd and osg-xrootd-standalone are designed to work with the OSG-released versions of XRootD, reducing the number of necessary manual configuration updates. The OSG 3.5 upcoming repositories contain XRootD 5, a major version upgrade from the previously released versions in the OSG. See the upstream release notes for an overview of the changes. To update XRootD on your StashCache, XCache, XRootD clustered, and XRootD standalone hosts, perform the following steps: Update all XRootD packages: root@host # yum update --enablerepo = osg-upcoming 'xrootd' * xcache Determine whether or not you are using OSG XRootD meta-packages root@host # rpm -q --verify xrootd-server | \\ egrep \"/etc/xrootd/xrootd-(standalone|clustered).cfg\" If the above command does not return any output, skip to step 6. If you are not using OSG XRootD meta-packaging and are using xrootd-lcmaps , update the -authzfunparams to the new key=value syntax. For example, the following configuration: -authzfunparms:--lcmapscfg,/etc/xrootd/lcmaps.cfg,--loglevel,0,--policy,authorize_only Should be turned into: -authzfunparms:lcmapscfg=/etc/lcmaps.db,loglevel=0,policy=authorize_only If you are not using OSG XRootD meta-packaging and are loading multiple ofs.authlib libraries, separate out each library into its own ofs.authlib directive. For example, the following configuration: ofs.authlib libXrdMacaroons.so libXrdAccSciTokens.so Should be re-ordered and turned into: # Enable SciTokens-based mappings ; if no token is present, then the GSI certificate will be used. ofs.authlib ++ libXrdAccSciTokens.so ofs.authlib ++ libXrdMacaroons.so Restart the relevant XRootD services: If you are running a(n)... Restart the service with... ATLAS XCache systemctl restart xrootd@atlas-xcache CMS XCache systemctl restart xrootd@cms-xcache Stash Cache systemctl restart xrootd@stash-cache Stash Origin systemctl restart xrootd@stash-origin XRootD Standalone systemctl restart xrootd@standalone XRootD Clustered systemctl restart xrootd@clustered","title":"Updating to XRootD 5"},{"location":"release/updating-to-osg-35/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"release/updating-to-osg-36/","text":"Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series Updating to OSG 3.6 \u00b6 OSG 3.6 (the new series ) is a major overhaul of the OSG software stack compared to OSG 3.5 (the old series ) with changes to core protocols used for authentication and data transfer. Depending on the VO(s) that you support, updating to the new series could result in issues with your site receiving pilot jobs and/or issues with data transfer. If you have verified that your VO(s) support token-based pilot submission and HTTP-based data transfers, use this document to update your OSG software to OSG 3.6. Updating Your OSG Compute Entrypoint \u00b6 Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series In OSG 3.6, OSG Compute Entrypoints (CEs) only accept token-based pilot job submissions. If you need to support token-based and GSI proxy-based pilot job submission, you must install or remain on OSG 3.5 . If the VOs that you support have the capability to submit token-based pilots, you may update your CE to OSG 3.6. In addition to the change in authentication protocol, OSG 3.6 CEs include new major versions of software that require manual updates. To upgrade your CE to OSG 3.6, follow the sections below to make your configuration OSG 3.6-compatible. Turning off CE services \u00b6 Register a downtime During the update, turn off the following services on your HTCondor-CE host: root@host # systemctl stop condor-ce root@host # systemctl stop gratia-probes-cron Run the command corresponding to your batch system to upload any remaining accounting records to the GRACC: If your batch system is... Then run the following command... HTCondor /usr/share/gratia/condor/condor_meter LSF /usr/share/gratia/lsf/lsf PBS /usr/share/gratia/pbs-lsf/pbs-lsf_meter.cron.sh SGE /usr/share/gratia/sge/sge_meter.cron.sh Slurm /usr/share/gratia/slurm/slurm_meter -c Disable the gratia-probes-cron service: root@host # systemctl disable gratia-probes-cron Updating CE packages \u00b6 After turning off your CE's services, you may proceed with the RPM update process. First, remove the old series Yum repositories: root@host # rpm -e osg-release This step ensures that any local modifications to *.repo files will not prevent installing the new series repos. Any modified *.repo files should appear under /etc/yum.repos.d/ with the *.rpmsave extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the *.rpmsave files to the new *.repo files. Update your Yum repositories to OSG 3.6 Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update Warning Please be aware that running yum update may also update other RPMs. You can exclude packages from being updated using the --exclude=[package-name or glob] option for the yum command. Watch the yum update carefully for any messages about a .rpmnew file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a .rpmnew extension. You will need to merge any edits that have made into the .rpmnew file and then move the merged version into place (that is, without the .rpmnew extension). Watch especially for /etc/lcmaps.db , which every site is expected to edit. Updating CE configuration \u00b6 Gratia Probe \u00b6 The OSG 3.6 release series contains Gratia Probe 2 , which uses the non-root HTCondor-CE probe to account for your site's resource contributions. To ensure that your contributions continue to be properly accounted for, verify that your HTCondor-CE's PER_JOB_HISTORY_DIR is set to /var/lib/condor-ce/gratia/data : root@host # condor_ce_config_val -v PER_JOB_HISTORY_DIR PER_JOB_HISTORY_DIR = /var/lib/condor-ce/gratia/data # at: /etc/condor-ce/config.d/99_gratia-ce.conf, line 5 # raw: PER_JOB_HISTORY_DIR = /var/lib/condor-ce/gratia/data If you see the above output , your Gratia Probe configuration is correct and you may continue onto the next section . If you do not see the above output , visit the file listed in the output of condor_ce_config_val , remove the offending value, and repeat until the proper value is returned. OSG-Configure \u00b6 The OSG 3.6 release series contains OSG-Configure 4, a major version upgrade from the previously released versions in the OSG. See the OSG-Configure 4.0.0 release notes for an overview of the changes. Several configuration modules and options were removed or deprecated and CE configuration has been simplified; the update from version 3 to version 4 will require some manual changes to your configuration. To update OSG-Configure, perform the following steps: Merge any *.rpmnew files in /etc/osg/config.d/ Uninstall osg-configure-gip and osg-configure-misc if they are installed: root@host# yum erase osg-configure-gip osg-configure-misc If /etc/osg/config.d/30-gip.ini.rpmsave exists, merge its contents into 31-cluster.ini Edit the Site Information configuration section (in 40-siteinfo.ini ): If resource_group is not set, add: resource_group = <TOPOLOGY RESOURCE GROUP FOR THIS HOST> Delete the following attributes: sponsor `site_policy contact email city country latitude longitude Run osg-configure to apply your changes: osg-configure -dc HTCondor-CE \u00b6 The OSG 3.6 release series contains HTCondor-CE 5 , a major version upgrade from HTCondor-CE 4, which was available in the OSG 3.5 release repositories. To update HTCondor-CE, perform the following steps: If you support the OSG or GLOW VOs and map their jobs to non-standard local Unix accounts (e.g., not osg and glow , respectively) add SciTokens mappings to a file in /etc/condor-ce/mapfiles.d/ : # OSG SCITOKENS /^https\\:\\/\\/scitokens\\.org\\/osg\\-connect,/ osg # GLOW SCITOKENS /^https\\:\\/\\/chtc\\.cs\\.wisc\\.edu,/ glow Replacing osg and glow with the local Unix account for the OSG and GLOW VOs, respectively. Also consult the upgrade documentation for other required configuration updates. For OSG CEs serving an HTCondor pool If your OSG CE routes pilot jobs to a local HTCondor pool, also see the section for updating your HTCondor hosts Starting CE services \u00b6 After updating your RPMs and updating your configuration, turn on the HTCondor-CE service: root@host # systemctl start condor-ce !!! question \"What about gratia-probes-cron ? In OSG 3.6, the OSG CE no longer needs a separate service for Gratia Probe. Instead, the default CE configuration runs its Gratia Probe as a periodic process under the HTCondor-CE process tree. Updating Your HTCondor Hosts \u00b6 Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series The following OSG specific configuration was dropped in anticipation of HTCondor's new secure by default configuration coming in HTCondor version 9.0. HTCondor's 9.0 recommended security configuration requires authentication for all access (including read access). CONDOR_HOST = $(FULL_HOSTNAME) DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, SCHEDD, STARTD # require authentication and integrity for everything... SEC_DEFAULT_AUTHENTICATION=REQUIRED SEC_DEFAULT_INTEGRITY=REQUIRED # ...except read access... SEC_READ_AUTHENTICATION=OPTIONAL SEC_READ_INTEGRITY=OPTIONAL # ...and the outgoing (client side) connection since the server side will enforce its policy SEC_CLIENT_AUTHENTICATION=OPTIONAL SEC_CLIENT_INTEGRITY=OPTIONAL # this will required PASSWORD authentications for daemon-to-daemon, and # allow FS authentication for submitting jobs and running administrator commands SEC_DEFAULT_AUTHENTICATION_METHODS = FS, PASSWORD SEC_DAEMON_AUTHENTICATION_METHODS = PASSWORD SEC_NEGOTIATOR_AUTHENTICATION_METHODS = PASSWORD SEC_PASSWORD_FILE = /etc/condor/passwords.d/POOL # admin commands (e.g. condor_off) can be run by: # 1. root on the local host or the central manager # 2. condor user on the local host or the central manager ALLOW_ADMINISTRATOR = condor@*/$(FULL_HOSTNAME) condor@*/$(CONDOR_HOST) condor_pool@*/$(FULL_HOSTNAME) condor_pool@*/$(CONDOR_HOST) root@$(UID_DOMAIN)/$(FULL_HOSTNAME) # only the condor daemons on the central manager can negotiate ALLOW_NEGOTIATOR = condor@*/$(CONDOR_HOST) condor_pool@*/$(CONDOR_HOST) # any authenticated daemons in the pool can read/write/advertise ALLOW_DAEMON = condor@* condor_pool@* Manual intervention may be required to upgrade from the HTCondor 8.8 series to HTCondor 9.0.x. Please consult the HTCondor 9.0 upgrade instructions . If you are upgrading from the HTCondor 8.9 series (8.9.11 and earlier), please consult the Upgrading to 9.0 instructions Getting Help \u00b6 To get assistance, please use the this page .","title":"Updating to OSG 3.6"},{"location":"release/updating-to-osg-36/#updating-to-osg-36","text":"OSG 3.6 (the new series ) is a major overhaul of the OSG software stack compared to OSG 3.5 (the old series ) with changes to core protocols used for authentication and data transfer. Depending on the VO(s) that you support, updating to the new series could result in issues with your site receiving pilot jobs and/or issues with data transfer. If you have verified that your VO(s) support token-based pilot submission and HTTP-based data transfers, use this document to update your OSG software to OSG 3.6.","title":"Updating to OSG 3.6"},{"location":"release/updating-to-osg-36/#updating-your-osg-compute-entrypoint","text":"Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series In OSG 3.6, OSG Compute Entrypoints (CEs) only accept token-based pilot job submissions. If you need to support token-based and GSI proxy-based pilot job submission, you must install or remain on OSG 3.5 . If the VOs that you support have the capability to submit token-based pilots, you may update your CE to OSG 3.6. In addition to the change in authentication protocol, OSG 3.6 CEs include new major versions of software that require manual updates. To upgrade your CE to OSG 3.6, follow the sections below to make your configuration OSG 3.6-compatible.","title":"Updating Your OSG Compute Entrypoint"},{"location":"release/updating-to-osg-36/#turning-off-ce-services","text":"Register a downtime During the update, turn off the following services on your HTCondor-CE host: root@host # systemctl stop condor-ce root@host # systemctl stop gratia-probes-cron Run the command corresponding to your batch system to upload any remaining accounting records to the GRACC: If your batch system is... Then run the following command... HTCondor /usr/share/gratia/condor/condor_meter LSF /usr/share/gratia/lsf/lsf PBS /usr/share/gratia/pbs-lsf/pbs-lsf_meter.cron.sh SGE /usr/share/gratia/sge/sge_meter.cron.sh Slurm /usr/share/gratia/slurm/slurm_meter -c Disable the gratia-probes-cron service: root@host # systemctl disable gratia-probes-cron","title":"Turning off CE services"},{"location":"release/updating-to-osg-36/#updating-ce-packages","text":"After turning off your CE's services, you may proceed with the RPM update process. First, remove the old series Yum repositories: root@host # rpm -e osg-release This step ensures that any local modifications to *.repo files will not prevent installing the new series repos. Any modified *.repo files should appear under /etc/yum.repos.d/ with the *.rpmsave extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the *.rpmsave files to the new *.repo files. Update your Yum repositories to OSG 3.6 Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update Warning Please be aware that running yum update may also update other RPMs. You can exclude packages from being updated using the --exclude=[package-name or glob] option for the yum command. Watch the yum update carefully for any messages about a .rpmnew file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a .rpmnew extension. You will need to merge any edits that have made into the .rpmnew file and then move the merged version into place (that is, without the .rpmnew extension). Watch especially for /etc/lcmaps.db , which every site is expected to edit.","title":"Updating CE packages"},{"location":"release/updating-to-osg-36/#updating-ce-configuration","text":"","title":"Updating CE configuration"},{"location":"release/updating-to-osg-36/#gratia-probe","text":"The OSG 3.6 release series contains Gratia Probe 2 , which uses the non-root HTCondor-CE probe to account for your site's resource contributions. To ensure that your contributions continue to be properly accounted for, verify that your HTCondor-CE's PER_JOB_HISTORY_DIR is set to /var/lib/condor-ce/gratia/data : root@host # condor_ce_config_val -v PER_JOB_HISTORY_DIR PER_JOB_HISTORY_DIR = /var/lib/condor-ce/gratia/data # at: /etc/condor-ce/config.d/99_gratia-ce.conf, line 5 # raw: PER_JOB_HISTORY_DIR = /var/lib/condor-ce/gratia/data If you see the above output , your Gratia Probe configuration is correct and you may continue onto the next section . If you do not see the above output , visit the file listed in the output of condor_ce_config_val , remove the offending value, and repeat until the proper value is returned.","title":"Gratia Probe"},{"location":"release/updating-to-osg-36/#osg-configure","text":"The OSG 3.6 release series contains OSG-Configure 4, a major version upgrade from the previously released versions in the OSG. See the OSG-Configure 4.0.0 release notes for an overview of the changes. Several configuration modules and options were removed or deprecated and CE configuration has been simplified; the update from version 3 to version 4 will require some manual changes to your configuration. To update OSG-Configure, perform the following steps: Merge any *.rpmnew files in /etc/osg/config.d/ Uninstall osg-configure-gip and osg-configure-misc if they are installed: root@host# yum erase osg-configure-gip osg-configure-misc If /etc/osg/config.d/30-gip.ini.rpmsave exists, merge its contents into 31-cluster.ini Edit the Site Information configuration section (in 40-siteinfo.ini ): If resource_group is not set, add: resource_group = <TOPOLOGY RESOURCE GROUP FOR THIS HOST> Delete the following attributes: sponsor `site_policy contact email city country latitude longitude Run osg-configure to apply your changes: osg-configure -dc","title":"OSG-Configure"},{"location":"release/updating-to-osg-36/#htcondor-ce","text":"The OSG 3.6 release series contains HTCondor-CE 5 , a major version upgrade from HTCondor-CE 4, which was available in the OSG 3.5 release repositories. To update HTCondor-CE, perform the following steps: If you support the OSG or GLOW VOs and map their jobs to non-standard local Unix accounts (e.g., not osg and glow , respectively) add SciTokens mappings to a file in /etc/condor-ce/mapfiles.d/ : # OSG SCITOKENS /^https\\:\\/\\/scitokens\\.org\\/osg\\-connect,/ osg # GLOW SCITOKENS /^https\\:\\/\\/chtc\\.cs\\.wisc\\.edu,/ glow Replacing osg and glow with the local Unix account for the OSG and GLOW VOs, respectively. Also consult the upgrade documentation for other required configuration updates. For OSG CEs serving an HTCondor pool If your OSG CE routes pilot jobs to a local HTCondor pool, also see the section for updating your HTCondor hosts","title":"HTCondor-CE"},{"location":"release/updating-to-osg-36/#starting-ce-services","text":"After updating your RPMs and updating your configuration, turn on the HTCondor-CE service: root@host # systemctl start condor-ce !!! question \"What about gratia-probes-cron ? In OSG 3.6, the OSG CE no longer needs a separate service for Gratia Probe. Instead, the default CE configuration runs its Gratia Probe as a periodic process under the HTCondor-CE process tree.","title":"Starting CE services"},{"location":"release/updating-to-osg-36/#updating-your-htcondor-hosts","text":"Before considering an upgrade to OSG 3.6\u2026 OSG 3.6 is under active development and is not currently supported for production use. Due to potentially disruptive changes in protocols, contact your VO(s) to verify that they support token-based authentication and/or HTTP-based data transfer before considering an upgrade to OSG 3.6. If your VO(s) don't support these new protocols or you don't know which protocols your VO(s) support, install or remain on the OSG 3.5 release series The following OSG specific configuration was dropped in anticipation of HTCondor's new secure by default configuration coming in HTCondor version 9.0. HTCondor's 9.0 recommended security configuration requires authentication for all access (including read access). CONDOR_HOST = $(FULL_HOSTNAME) DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, SCHEDD, STARTD # require authentication and integrity for everything... SEC_DEFAULT_AUTHENTICATION=REQUIRED SEC_DEFAULT_INTEGRITY=REQUIRED # ...except read access... SEC_READ_AUTHENTICATION=OPTIONAL SEC_READ_INTEGRITY=OPTIONAL # ...and the outgoing (client side) connection since the server side will enforce its policy SEC_CLIENT_AUTHENTICATION=OPTIONAL SEC_CLIENT_INTEGRITY=OPTIONAL # this will required PASSWORD authentications for daemon-to-daemon, and # allow FS authentication for submitting jobs and running administrator commands SEC_DEFAULT_AUTHENTICATION_METHODS = FS, PASSWORD SEC_DAEMON_AUTHENTICATION_METHODS = PASSWORD SEC_NEGOTIATOR_AUTHENTICATION_METHODS = PASSWORD SEC_PASSWORD_FILE = /etc/condor/passwords.d/POOL # admin commands (e.g. condor_off) can be run by: # 1. root on the local host or the central manager # 2. condor user on the local host or the central manager ALLOW_ADMINISTRATOR = condor@*/$(FULL_HOSTNAME) condor@*/$(CONDOR_HOST) condor_pool@*/$(FULL_HOSTNAME) condor_pool@*/$(CONDOR_HOST) root@$(UID_DOMAIN)/$(FULL_HOSTNAME) # only the condor daemons on the central manager can negotiate ALLOW_NEGOTIATOR = condor@*/$(CONDOR_HOST) condor_pool@*/$(CONDOR_HOST) # any authenticated daemons in the pool can read/write/advertise ALLOW_DAEMON = condor@* condor_pool@* Manual intervention may be required to upgrade from the HTCondor 8.8 series to HTCondor 9.0.x. Please consult the HTCondor 9.0 upgrade instructions . If you are upgrading from the HTCondor 8.9 series (8.9.11 and earlier), please consult the Upgrading to 9.0 instructions","title":"Updating Your HTCondor Hosts"},{"location":"release/updating-to-osg-36/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"release/yum-basics/","text":"Basics of using yum and RPM \u00b6 About This Document \u00b6 This document introduces package management tools that help you install, update, and remove packages. OSG uses RPMs (the Red Hat Packaging Manager) to package its software. While RPM is the packaging format, yum is the command you will use to do the installation. For example, yum will resolve and download the dependencies for the package you want to install; rpm will simply complain if you want to install a package that does not have all its dependencies installed. Installation \u00b6 Installation is done with the yum install command. Each of the individual installation guide shows you the correct command to use to do an installation. Here is an example installation with all of the output from yum. root@host # sudo yum install osg-ca-certs Loaded plugins: kernel-module, priorities epel | 3.7 kB 00:00 epel/primary_db | 3.8 MB 00:00 fermi-base | 2.1 kB 00:00 fermi-base/primary_db | 48 kB 00:00 fermi-security | 1.9 kB 00:00 fermi-security/primary_db | 1.7 MB 00:00 osg | 1.9 kB 00:00 osg/primary_db | 65 kB 00:00 sl-base | 2.1 kB 00:00 sl-base/primary_db | 2.0 MB 00:00 957 packages excluded due to repository priority protections Setting up Install Process Resolving Dependencies --> Running transaction check ---> Package osg-ca-certs.noarch 0:1.23-1 set to be updated --> Finished Dependency Resolution Beginning Kernel Module Plugin Finished Kernel Module Plugin Dependencies Resolved ==================================================================================================================== Package Arch Version Repository Size ==================================================================================================================== Installing: osg-ca-certs noarch 1.23-1 osg 450 k Transaction Summary ==================================================================================================================== Install 1 Package(s) Upgrade 0 Package(s) Total download size: 450 k Is this ok [y/N]: y Downloading Packages: osg-ca-certs-1.23-1.noarch.rpm | 450 kB 00:00 warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603 osg/gpgkey | 1.7 kB 00:00 Importing GPG key 0x824B8603 \"OSG Software Team (RPM Signing Key for Koji Packages) <vdt-support@opensciencegrid.org>\" from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG Is this ok [y/N]: y Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Installing : osg-ca-certs 1/1 Installed: osg-ca-certs.noarch 0:1.23-1 Complete! Please Note : When you first install a package from the OSG repository, you will be prompted to import the GPG key. We use this key to sign our RPMs as a security measure. You should double-check the key id (above it is 824B8603) with the information on our signed RPMs . If it doesn't match, there is a problem somewhere and you should report it to the OSG via help@opensciencegrid.org. Verifying Packages and Installations \u00b6 You can check if an RPM has been modified. For instance, to check to see if any files have been modified in the osg-ca-certs RPM you just installed: user@host $ rpm --verify osg-ca-certs The lack of any output means there were no problems. If you would like to see all the files for which there are no problems, you can do: user@host $ rpm --verify -v osg-ca-certs ........ /etc/grid-security/certificates ........ /etc/grid-security/certificates/0119347c.0 ........ /etc/grid-security/certificates/0119347c.namespaces ........ /etc/grid-security/certificates/0119347c.signing_policy ... etc ... Each dot indicates a specific check that was made and passed. If someone had modified a file, you might see this: user@host $ rpm --verify osg-ca-certs ..5....T /etc/grid-security/certificates/ffc3d59b.0 This means the files MD5 checksum has changed (so the contents have changed) and the timestamp is different. The complete set of changes you might see (copied from the rpm man page) are: Letter Meaning S file Size differs M Mode differs (includes permissions and file type) 5 MD5 sum differs D Device major/minor number mismatch L readLink(2) path mismatch U User ownership differs G Group ownership differs T mTime differs If you don't care about some of those changes, you can tell rpm to ignore them. For instance, to ignore changes in the modification time: user@host $ rpm --verify --nomtime osg-ca-certs ..5..... /etc/grid-security/certificates/ffc3d59b.0 Understanding a package \u00b6 If you want to know what package a file belongs to, you can ask rpm. For instance, if you're curious what package contains the srm-ls command, you can do: # 1 . Find the exact path user@host $ which srm-ls /usr/bin/srm-ls # 2 . Ask rpm what package it is part of: user@host $ rpm -q --file /usr/bin/srm-ls bestman2-client-2.2.0-14.osg.el5.noarch If you want to know what other things are in a package--perhaps the other available tools or configuration files--you can do that as well: user@host $ rpm -ql bestman2-client /etc/bestman2/conf/bestman2.rc /etc/bestman2/conf/bestman2.rc.samples /etc/bestman2/conf/srmclient.conf /etc/sysconfig/bestman2 /usr/bin/srm-copy /usr/bin/srm-copy-status /usr/bin/srm-extendfilelifetime /usr/bin/srm-ls /usr/bin/srm-ls-status ... output trimmed ... What else does a package install? \u00b6 Sometimes you need to understand what other software is installed by a package. This can be particularly useful for understanding meta-packages , which are packages such as the osg-wn-client (worker node client) that contain nothing by themselves but only depend on other RPMs. To do this, use the --requires option to rpm. For example, you can see that the worker node client (as of OSG 3.1.8 in early September, 2012) will install curl , uberftp , lcg-utils , and a dozen or so other packages. user@host $ rpm -q --requires osg-wn-client /usr/bin/curl /usr/bin/dccp /usr/bin/ldapsearch /usr/bin/uberftp /usr/bin/wget bestman2-client config(osg-wn-client) = 3.0.0-16.osg.el5 dcache-srmclient dcap-tunnel-gsi edg-gridftp-client fetch-crl glite-fts-client globus-gass-copy-progs grid-certificates java-1.6.0-sun-compat lcg-utils lfc-client lfc-python myproxy osg-system-profiler osg-version rpmlib(CompressedFileNames) <= 3.0.4-1 rpmlib(PayloadFilesHavePrefix) <= 4.0-1 vo-client Finding RPM Packages \u00b6 It is normally best to read the OSG documentation to decide which packages to install because it may not be obvious what the correct packages to install are. That said, you can use yum to find out all sort of things. For instance, you can list packages that begin with \"voms\": user@host $ yum list \"voms*\" Loaded plugins: kernel-module, priorities 957 packages excluded due to repository priority protections Available Packages voms.i386 2.0.6-3.osg osg voms.x86_64 2.0.6-3.osg osg voms-admin-client.x86_64 2.0.16-1 osg voms-admin-server.noarch 2.6.1-9 osg voms-clients.x86_64 2.0.6-3.osg osg voms-compat.i386 1.9.19.2-6.osg osg voms-compat.x86_64 1.9.19.2-6.osg osg voms-devel.i386 2.0.6-3.osg osg voms-devel.x86_64 2.0.6-3.osg osg voms-doc.x86_64 2.0.6-3.osg osg voms-mysql-plugin.x86_64 3.1.5.1-1.el5 epel voms-server.x86_64 2.0.6-3.osg osg vomsjapi.x86_64 2.0.6-3.osg osg vomsjapi-javadoc.x86_64 2.0.6-3.osg osg If you want to search for packages that contain VOMS anywhere in the name or description, you can use yum search : user@host $ yum search voms Loaded plugins: kernel-module, priorities 957 packages excluded due to repository priority protections ================================================== Matched: voms =================================================== osg-voms.noarch : OSG VOMS perl-VOMS-Lite.noarch : Perl extension for VOMS Attribute certificate creation perl-voms-server.noarch : Perl extension for VOMS Attribute certificate creation php-voms-admin.noarch : Web based interface to control VOMS parameters written in PHP voms.i386 : Virtual Organization Membership Service voms.x86_64 : Virtual Organization Membership Service ... etc ... One last example, if you want to know what RPM would give you the voms-proxy-init command, you can ask yum . The * indicates that you don't know the full pathname of voms-proxy-init . user@host $ yum whatprovides \"*voms-proxy-init\" Loaded plugins: kernel-module, priorities 957 packages excluded due to repository priority protections voms-clients-2.0.6-3.osg.x86_64 : Virtual Organization Membership Service Clients Repo : osg Matched from: Filename : /usr/bin/voms-proxy-init Removing Packages \u00b6 To remove a single RPM, you can use yum remove . Not only will it uninstall the RPM you requested, but it will uninstall anything that depends on it. For example, if I previously installed the voms-clients package, I also installed another package it depends on called voms . If I remove voms , yum will also remove voms-clients : user@host $ sudo yum remove voms Loaded plugins: kernel-module, priorities Setting up Remove Process Resolving Dependencies --> Running transaction check ---> Package voms.x86_64 0:2.0.6-3.osg set to be erased --> Processing Dependency: libvomsapi.so.1()(64bit) for package: voms-clients --> Processing Dependency: voms = 2.0.6-3.osg for package: voms-clients --> Running transaction check ---> Package voms-clients.x86_64 0:2.0.6-3.osg set to be erased --> Finished Dependency Resolution Beginning Kernel Module Plugin Finished Kernel Module Plugin Dependencies Resolved ==================================================================================================================== Package Arch Version Repository Size ==================================================================================================================== Removing: voms x86_64 2.0.6-3.osg installed 407 k Removing for dependencies: voms-clients x86_64 2.0.6-3.osg installed 373 k Transaction Summary ==================================================================================================================== Remove 2 Package(s) Reinstall 0 Package(s) Downgrade 0 Package(s) Is this ok [y/N]: y Downloading Packages: Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Erasing : voms 1/2 Erasing : voms-clients 2/2 Removed: voms.x86_64 0:2.0.6-3.osg Dependency Removed: voms-clients.x86_64 0:2.0.6-3.osg Complete! Upgrading Packages \u00b6 You can check for updates with yum check-update . For example: root@host # yum check-update Loaded plugins: kernel-module, priorities 957 packages excluded due to repository priority protections kernel.x86_64 2.6.18-274.3.1.el5 fermi-security Obsoleting Packages ocsinventory-agent.noarch 1.1.2.1-1.el5 epel ocsinventory-client.noarch 0.9.9-10 installed You can do the update with yum update . Note that in this case we got more than was listed due to dependencies that needed to be resolved: root@host # yum update 957 packages excluded due to repository priority protections Setting up Update Process Resolving Dependencies --> Running transaction check ---> Package kernel.x86_64 0:2.6.18-274.3.1.el5 set to be installed ---> Package ocsinventory-agent.noarch 0:1.1.2.1-1.el5 set to be updated --> Processing Dependency: perl(Crypt::SSLeay) for package: ocsinventory-agent --> Processing Dependency: perl(Proc::Daemon) for package: ocsinventory-agent --> Processing Dependency: monitor-edid for package: ocsinventory-agent --> Processing Dependency: perl(Net::IP) for package: ocsinventory-agent --> Processing Dependency: nmap for package: ocsinventory-agent --> Processing Dependency: perl(Net::SSLeay) for package: ocsinventory-agent --> Running transaction check ---> Package monitor-edid.x86_64 0:2.5-1.el5.1 set to be updated ---> Package nmap.x86_64 2:4.11-1.1 set to be updated ---> Package perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 set to be updated ---> Package perl-Net-IP.noarch 0:1.25-2.fc6 set to be updated ---> Package perl-Net-SSLeay.x86_64 0:1.30-4.fc6 set to be updated ---> Package perl-Proc-Daemon.noarch 0:0.03-1.el5 set to be updated --> Finished Dependency Resolution Beginning Kernel Module Plugin Finished Kernel Module Plugin --> Running transaction check ---> Package kernel.x86_64 0:2.6.18-238.1.1.el5 set to be erased --> Finished Dependency Resolution Dependencies Resolved ==================================================================================================================== Package Arch Version Repository Size ==================================================================================================================== Installing: kernel x86_64 2.6.18-274.3.1.el5 fermi-security 21 M ocsinventory-agent noarch 1.1.2.1-1.el5 epel 156 k replacing ocsinventory-client.noarch 0.9.9-10 Removing: kernel x86_64 2.6.18-238.1.1.el5 installed 93 M Installing for dependencies: monitor-edid x86_64 2.5-1.el5.1 epel 82 k nmap x86_64 2:4.11-1.1 sl-base 680 k perl-Crypt-SSLeay x86_64 0.51-11.el5 sl-base 45 k perl-Net-IP noarch 1.25-2.fc6 sl-base 31 k perl-Net-SSLeay x86_64 1.30-4.fc6 sl-base 192 k perl-Proc-Daemon noarch 0.03-1.el5 epel 9.4 k Transaction Summary ==================================================================================================================== Install 8 Package(s) Upgrade 0 Package(s) Remove 1 Package(s) Reinstall 0 Package(s) Downgrade 0 Package(s) Total download size: 22 M Is this ok [y/N]: y Downloading Packages: (1/8) : perl-Proc-Daemon-0.03-1.el5.noarch.rpm | 9.4 kB 00:00 (2/8) : perl-Net-IP-1.25-2.fc6.noarch.rpm | 31 kB 00:00 (3/8) : perl-Crypt-SSLeay-0.51-11.el5.x86_64.rpm | 45 kB 00:00 (4/8) : monitor-edid-2.5-1.el5.1.x86_64.rpm | 82 kB 00:00 (5/8) : ocsinventory-agent-1.1.2.1-1.el5.noarch.rpm | 156 kB 00:00 (6/8) : perl-Net-SSLeay-1.30-4.fc6.x86_64.rpm | 192 kB 00:00 (7/8) : nmap-4.11-1.1.x86_64.rpm | 680 kB 00:00 (8/8) : kernel-2.6.18-274.3.1.el5.x86_64.rpm | 21 MB 00:00 -------------------------------------------------------------------------------------------------------------------- Total 3.5 MB/s | 22 MB 00:06 warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 217521f6 epel/gpgkey | 1.7 kB 00:00 Importing GPG key 0x217521F6 \"Fedora EPEL <epel@fedoraproject.org>\" from /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL Is this ok [y/N]: y Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Installing : perl-Net-SSLeay 1/10 Installing : nmap 2/10 Installing : monitor-edid 3/10 Installing : perl-Crypt-SSLeay 4/10 Installing : perl-Net-IP 5/10 Installing : perl-Proc-Daemon 6/10 Installing : kernel 7/10 Installing : ocsinventory-agent 8/10 ule, priorities 957 packages excluded due to repository priority protections kernel.x86_64 2.6.18-274.3.1.el5 fermi-security Obsoleting Packages ocsinventory-agent.noarch 1.1.2.1-1.el5 epel ocsinventory-client.noarch 0.9.9-10 installed Erasing : ocsinventory-client 9/10 warning: /etc/ocsinventory-client/ocsinv.conf saved as /etc/ocsinventory-client/ocsinv.conf.rpmsave Cleanup : kernel 10/10 Removed: kernel.x86_64 0:2.6.18-238.1.1.el5 Installed: kernel.x86_64 0:2.6.18-274.3.1.el5 ocsinventory-agent.noarch 0:1.1.2.1-1.el5 Dependency Installed: monitor-edid.x86_64 0:2.5-1.el5.1 nmap.x86_64 2:4.11-1.1 perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 perl-Net-IP.noarch 0:1.25-2.fc6 perl-Net-SSLeay.x86_64 0:1.30-4.fc6 perl-Proc-Daemon.noarch 0:0.03-1.el5 Replaced: ocsinventory-client.noarch 0:0.9.9-10 Complete! Advanced topic: Only geting OSG updates \u00b6 If you only want to get updates from the OSG repository and no other repositories, you can tell yum to do that with the following command: root@host # yum --disablerepo = * --enablerepo = osg update Advanced topic: Getting debugging information for installed software \u00b6 If you run into a problem with our software and have a hankering to debug it directly (or perhaps we need to ask you for some help), you can install so-called \"debuginfo\" packages. These packages will provide debugging symbols and source code so that you can do things like run gdb or pstack to get information about a program. Installing the debuginfo package requires three steps. Enable the installation of debuginfo packages. This only needs to be done once. Edit the yum repo file, usually /etc/yum.repos.d/osg.repo to enable the separate debuginfo repository. Near the bottom of the file, you'll see the osg-debug repo: [osg-debug] name=OSG Software for Enterprise Linux 5 - $basearch - Debug baseurl=http://repo.opensciencegrid.org/osg-release/$basearch/debu failovermethod=priority priority=98 enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG Make sure that \"enabled\" is set to 1. Figure out which package installed the program you want to debug. One way to figure it out is to ask RPM. For example, if you want to debug grid-proxy-init: user@host $ rpm -qf ` which grid-proxy-init ` globus-proxy-utils-5.0-5.osg.x86_64 Install the debugging information for that package. Continuing this example: root@host # debuginfo-install globus-proxy-utils ... ================================================================================================================================= Package Arch Version Repository Size ================================================================================================================================= Installing: globus-proxy-utils-debuginfo x86_64 5.0-5.osg osg-debug 61 k Transaction Summary ================================================================================================================================= Install 1 Package(s) Upgrade 0 Package(s) Total download size: 61 k Is this ok [y/N]: y ... Installed: globus-proxy-utils-debuginfo.x86_64 0:5.0-5.osg This last step will select the right package name, then use yum to install it. Troubleshooting \u00b6 Yum not finding packages \u00b6 If you is not finding some packages, e.g.: Error Downloading Packages: packageXYZ: failure: packageXYZ.rpm from osg: [Errno 256] No more mirrors to try. then you can try cleaning up Yum's cache: root@host # yum clean all --enablerepo = * Yum complaining about missing keys \u00b6 If yum is complaining you can re-import the keys in your distribution: root@host # rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY* References \u00b6 The main Yum web site A good description of the commands for RPM and Yum can be found at Learn Linux 101: RPM and Yum Package Management .","title":"Yum Basics"},{"location":"release/yum-basics/#basics-of-using-yum-and-rpm","text":"","title":"Basics of using yum and RPM"},{"location":"release/yum-basics/#about-this-document","text":"This document introduces package management tools that help you install, update, and remove packages. OSG uses RPMs (the Red Hat Packaging Manager) to package its software. While RPM is the packaging format, yum is the command you will use to do the installation. For example, yum will resolve and download the dependencies for the package you want to install; rpm will simply complain if you want to install a package that does not have all its dependencies installed.","title":"About This Document"},{"location":"release/yum-basics/#installation","text":"Installation is done with the yum install command. Each of the individual installation guide shows you the correct command to use to do an installation. Here is an example installation with all of the output from yum. root@host # sudo yum install osg-ca-certs Loaded plugins: kernel-module, priorities epel | 3.7 kB 00:00 epel/primary_db | 3.8 MB 00:00 fermi-base | 2.1 kB 00:00 fermi-base/primary_db | 48 kB 00:00 fermi-security | 1.9 kB 00:00 fermi-security/primary_db | 1.7 MB 00:00 osg | 1.9 kB 00:00 osg/primary_db | 65 kB 00:00 sl-base | 2.1 kB 00:00 sl-base/primary_db | 2.0 MB 00:00 957 packages excluded due to repository priority protections Setting up Install Process Resolving Dependencies --> Running transaction check ---> Package osg-ca-certs.noarch 0:1.23-1 set to be updated --> Finished Dependency Resolution Beginning Kernel Module Plugin Finished Kernel Module Plugin Dependencies Resolved ==================================================================================================================== Package Arch Version Repository Size ==================================================================================================================== Installing: osg-ca-certs noarch 1.23-1 osg 450 k Transaction Summary ==================================================================================================================== Install 1 Package(s) Upgrade 0 Package(s) Total download size: 450 k Is this ok [y/N]: y Downloading Packages: osg-ca-certs-1.23-1.noarch.rpm | 450 kB 00:00 warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603 osg/gpgkey | 1.7 kB 00:00 Importing GPG key 0x824B8603 \"OSG Software Team (RPM Signing Key for Koji Packages) <vdt-support@opensciencegrid.org>\" from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG Is this ok [y/N]: y Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Installing : osg-ca-certs 1/1 Installed: osg-ca-certs.noarch 0:1.23-1 Complete! Please Note : When you first install a package from the OSG repository, you will be prompted to import the GPG key. We use this key to sign our RPMs as a security measure. You should double-check the key id (above it is 824B8603) with the information on our signed RPMs . If it doesn't match, there is a problem somewhere and you should report it to the OSG via help@opensciencegrid.org.","title":"Installation"},{"location":"release/yum-basics/#verifying-packages-and-installations","text":"You can check if an RPM has been modified. For instance, to check to see if any files have been modified in the osg-ca-certs RPM you just installed: user@host $ rpm --verify osg-ca-certs The lack of any output means there were no problems. If you would like to see all the files for which there are no problems, you can do: user@host $ rpm --verify -v osg-ca-certs ........ /etc/grid-security/certificates ........ /etc/grid-security/certificates/0119347c.0 ........ /etc/grid-security/certificates/0119347c.namespaces ........ /etc/grid-security/certificates/0119347c.signing_policy ... etc ... Each dot indicates a specific check that was made and passed. If someone had modified a file, you might see this: user@host $ rpm --verify osg-ca-certs ..5....T /etc/grid-security/certificates/ffc3d59b.0 This means the files MD5 checksum has changed (so the contents have changed) and the timestamp is different. The complete set of changes you might see (copied from the rpm man page) are: Letter Meaning S file Size differs M Mode differs (includes permissions and file type) 5 MD5 sum differs D Device major/minor number mismatch L readLink(2) path mismatch U User ownership differs G Group ownership differs T mTime differs If you don't care about some of those changes, you can tell rpm to ignore them. For instance, to ignore changes in the modification time: user@host $ rpm --verify --nomtime osg-ca-certs ..5..... /etc/grid-security/certificates/ffc3d59b.0","title":"Verifying Packages and Installations"},{"location":"release/yum-basics/#understanding-a-package","text":"If you want to know what package a file belongs to, you can ask rpm. For instance, if you're curious what package contains the srm-ls command, you can do: # 1 . Find the exact path user@host $ which srm-ls /usr/bin/srm-ls # 2 . Ask rpm what package it is part of: user@host $ rpm -q --file /usr/bin/srm-ls bestman2-client-2.2.0-14.osg.el5.noarch If you want to know what other things are in a package--perhaps the other available tools or configuration files--you can do that as well: user@host $ rpm -ql bestman2-client /etc/bestman2/conf/bestman2.rc /etc/bestman2/conf/bestman2.rc.samples /etc/bestman2/conf/srmclient.conf /etc/sysconfig/bestman2 /usr/bin/srm-copy /usr/bin/srm-copy-status /usr/bin/srm-extendfilelifetime /usr/bin/srm-ls /usr/bin/srm-ls-status ... output trimmed ...","title":"Understanding a package"},{"location":"release/yum-basics/#what-else-does-a-package-install","text":"Sometimes you need to understand what other software is installed by a package. This can be particularly useful for understanding meta-packages , which are packages such as the osg-wn-client (worker node client) that contain nothing by themselves but only depend on other RPMs. To do this, use the --requires option to rpm. For example, you can see that the worker node client (as of OSG 3.1.8 in early September, 2012) will install curl , uberftp , lcg-utils , and a dozen or so other packages. user@host $ rpm -q --requires osg-wn-client /usr/bin/curl /usr/bin/dccp /usr/bin/ldapsearch /usr/bin/uberftp /usr/bin/wget bestman2-client config(osg-wn-client) = 3.0.0-16.osg.el5 dcache-srmclient dcap-tunnel-gsi edg-gridftp-client fetch-crl glite-fts-client globus-gass-copy-progs grid-certificates java-1.6.0-sun-compat lcg-utils lfc-client lfc-python myproxy osg-system-profiler osg-version rpmlib(CompressedFileNames) <= 3.0.4-1 rpmlib(PayloadFilesHavePrefix) <= 4.0-1 vo-client","title":"What else does a package install?"},{"location":"release/yum-basics/#finding-rpm-packages","text":"It is normally best to read the OSG documentation to decide which packages to install because it may not be obvious what the correct packages to install are. That said, you can use yum to find out all sort of things. For instance, you can list packages that begin with \"voms\": user@host $ yum list \"voms*\" Loaded plugins: kernel-module, priorities 957 packages excluded due to repository priority protections Available Packages voms.i386 2.0.6-3.osg osg voms.x86_64 2.0.6-3.osg osg voms-admin-client.x86_64 2.0.16-1 osg voms-admin-server.noarch 2.6.1-9 osg voms-clients.x86_64 2.0.6-3.osg osg voms-compat.i386 1.9.19.2-6.osg osg voms-compat.x86_64 1.9.19.2-6.osg osg voms-devel.i386 2.0.6-3.osg osg voms-devel.x86_64 2.0.6-3.osg osg voms-doc.x86_64 2.0.6-3.osg osg voms-mysql-plugin.x86_64 3.1.5.1-1.el5 epel voms-server.x86_64 2.0.6-3.osg osg vomsjapi.x86_64 2.0.6-3.osg osg vomsjapi-javadoc.x86_64 2.0.6-3.osg osg If you want to search for packages that contain VOMS anywhere in the name or description, you can use yum search : user@host $ yum search voms Loaded plugins: kernel-module, priorities 957 packages excluded due to repository priority protections ================================================== Matched: voms =================================================== osg-voms.noarch : OSG VOMS perl-VOMS-Lite.noarch : Perl extension for VOMS Attribute certificate creation perl-voms-server.noarch : Perl extension for VOMS Attribute certificate creation php-voms-admin.noarch : Web based interface to control VOMS parameters written in PHP voms.i386 : Virtual Organization Membership Service voms.x86_64 : Virtual Organization Membership Service ... etc ... One last example, if you want to know what RPM would give you the voms-proxy-init command, you can ask yum . The * indicates that you don't know the full pathname of voms-proxy-init . user@host $ yum whatprovides \"*voms-proxy-init\" Loaded plugins: kernel-module, priorities 957 packages excluded due to repository priority protections voms-clients-2.0.6-3.osg.x86_64 : Virtual Organization Membership Service Clients Repo : osg Matched from: Filename : /usr/bin/voms-proxy-init","title":"Finding RPM Packages"},{"location":"release/yum-basics/#removing-packages","text":"To remove a single RPM, you can use yum remove . Not only will it uninstall the RPM you requested, but it will uninstall anything that depends on it. For example, if I previously installed the voms-clients package, I also installed another package it depends on called voms . If I remove voms , yum will also remove voms-clients : user@host $ sudo yum remove voms Loaded plugins: kernel-module, priorities Setting up Remove Process Resolving Dependencies --> Running transaction check ---> Package voms.x86_64 0:2.0.6-3.osg set to be erased --> Processing Dependency: libvomsapi.so.1()(64bit) for package: voms-clients --> Processing Dependency: voms = 2.0.6-3.osg for package: voms-clients --> Running transaction check ---> Package voms-clients.x86_64 0:2.0.6-3.osg set to be erased --> Finished Dependency Resolution Beginning Kernel Module Plugin Finished Kernel Module Plugin Dependencies Resolved ==================================================================================================================== Package Arch Version Repository Size ==================================================================================================================== Removing: voms x86_64 2.0.6-3.osg installed 407 k Removing for dependencies: voms-clients x86_64 2.0.6-3.osg installed 373 k Transaction Summary ==================================================================================================================== Remove 2 Package(s) Reinstall 0 Package(s) Downgrade 0 Package(s) Is this ok [y/N]: y Downloading Packages: Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Erasing : voms 1/2 Erasing : voms-clients 2/2 Removed: voms.x86_64 0:2.0.6-3.osg Dependency Removed: voms-clients.x86_64 0:2.0.6-3.osg Complete!","title":"Removing Packages"},{"location":"release/yum-basics/#upgrading-packages","text":"You can check for updates with yum check-update . For example: root@host # yum check-update Loaded plugins: kernel-module, priorities 957 packages excluded due to repository priority protections kernel.x86_64 2.6.18-274.3.1.el5 fermi-security Obsoleting Packages ocsinventory-agent.noarch 1.1.2.1-1.el5 epel ocsinventory-client.noarch 0.9.9-10 installed You can do the update with yum update . Note that in this case we got more than was listed due to dependencies that needed to be resolved: root@host # yum update 957 packages excluded due to repository priority protections Setting up Update Process Resolving Dependencies --> Running transaction check ---> Package kernel.x86_64 0:2.6.18-274.3.1.el5 set to be installed ---> Package ocsinventory-agent.noarch 0:1.1.2.1-1.el5 set to be updated --> Processing Dependency: perl(Crypt::SSLeay) for package: ocsinventory-agent --> Processing Dependency: perl(Proc::Daemon) for package: ocsinventory-agent --> Processing Dependency: monitor-edid for package: ocsinventory-agent --> Processing Dependency: perl(Net::IP) for package: ocsinventory-agent --> Processing Dependency: nmap for package: ocsinventory-agent --> Processing Dependency: perl(Net::SSLeay) for package: ocsinventory-agent --> Running transaction check ---> Package monitor-edid.x86_64 0:2.5-1.el5.1 set to be updated ---> Package nmap.x86_64 2:4.11-1.1 set to be updated ---> Package perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 set to be updated ---> Package perl-Net-IP.noarch 0:1.25-2.fc6 set to be updated ---> Package perl-Net-SSLeay.x86_64 0:1.30-4.fc6 set to be updated ---> Package perl-Proc-Daemon.noarch 0:0.03-1.el5 set to be updated --> Finished Dependency Resolution Beginning Kernel Module Plugin Finished Kernel Module Plugin --> Running transaction check ---> Package kernel.x86_64 0:2.6.18-238.1.1.el5 set to be erased --> Finished Dependency Resolution Dependencies Resolved ==================================================================================================================== Package Arch Version Repository Size ==================================================================================================================== Installing: kernel x86_64 2.6.18-274.3.1.el5 fermi-security 21 M ocsinventory-agent noarch 1.1.2.1-1.el5 epel 156 k replacing ocsinventory-client.noarch 0.9.9-10 Removing: kernel x86_64 2.6.18-238.1.1.el5 installed 93 M Installing for dependencies: monitor-edid x86_64 2.5-1.el5.1 epel 82 k nmap x86_64 2:4.11-1.1 sl-base 680 k perl-Crypt-SSLeay x86_64 0.51-11.el5 sl-base 45 k perl-Net-IP noarch 1.25-2.fc6 sl-base 31 k perl-Net-SSLeay x86_64 1.30-4.fc6 sl-base 192 k perl-Proc-Daemon noarch 0.03-1.el5 epel 9.4 k Transaction Summary ==================================================================================================================== Install 8 Package(s) Upgrade 0 Package(s) Remove 1 Package(s) Reinstall 0 Package(s) Downgrade 0 Package(s) Total download size: 22 M Is this ok [y/N]: y Downloading Packages: (1/8) : perl-Proc-Daemon-0.03-1.el5.noarch.rpm | 9.4 kB 00:00 (2/8) : perl-Net-IP-1.25-2.fc6.noarch.rpm | 31 kB 00:00 (3/8) : perl-Crypt-SSLeay-0.51-11.el5.x86_64.rpm | 45 kB 00:00 (4/8) : monitor-edid-2.5-1.el5.1.x86_64.rpm | 82 kB 00:00 (5/8) : ocsinventory-agent-1.1.2.1-1.el5.noarch.rpm | 156 kB 00:00 (6/8) : perl-Net-SSLeay-1.30-4.fc6.x86_64.rpm | 192 kB 00:00 (7/8) : nmap-4.11-1.1.x86_64.rpm | 680 kB 00:00 (8/8) : kernel-2.6.18-274.3.1.el5.x86_64.rpm | 21 MB 00:00 -------------------------------------------------------------------------------------------------------------------- Total 3.5 MB/s | 22 MB 00:06 warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 217521f6 epel/gpgkey | 1.7 kB 00:00 Importing GPG key 0x217521F6 \"Fedora EPEL <epel@fedoraproject.org>\" from /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL Is this ok [y/N]: y Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Installing : perl-Net-SSLeay 1/10 Installing : nmap 2/10 Installing : monitor-edid 3/10 Installing : perl-Crypt-SSLeay 4/10 Installing : perl-Net-IP 5/10 Installing : perl-Proc-Daemon 6/10 Installing : kernel 7/10 Installing : ocsinventory-agent 8/10 ule, priorities 957 packages excluded due to repository priority protections kernel.x86_64 2.6.18-274.3.1.el5 fermi-security Obsoleting Packages ocsinventory-agent.noarch 1.1.2.1-1.el5 epel ocsinventory-client.noarch 0.9.9-10 installed Erasing : ocsinventory-client 9/10 warning: /etc/ocsinventory-client/ocsinv.conf saved as /etc/ocsinventory-client/ocsinv.conf.rpmsave Cleanup : kernel 10/10 Removed: kernel.x86_64 0:2.6.18-238.1.1.el5 Installed: kernel.x86_64 0:2.6.18-274.3.1.el5 ocsinventory-agent.noarch 0:1.1.2.1-1.el5 Dependency Installed: monitor-edid.x86_64 0:2.5-1.el5.1 nmap.x86_64 2:4.11-1.1 perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 perl-Net-IP.noarch 0:1.25-2.fc6 perl-Net-SSLeay.x86_64 0:1.30-4.fc6 perl-Proc-Daemon.noarch 0:0.03-1.el5 Replaced: ocsinventory-client.noarch 0:0.9.9-10 Complete!","title":"Upgrading Packages"},{"location":"release/yum-basics/#advanced-topic-only-geting-osg-updates","text":"If you only want to get updates from the OSG repository and no other repositories, you can tell yum to do that with the following command: root@host # yum --disablerepo = * --enablerepo = osg update","title":"Advanced topic: Only geting OSG updates"},{"location":"release/yum-basics/#advanced-topic-getting-debugging-information-for-installed-software","text":"If you run into a problem with our software and have a hankering to debug it directly (or perhaps we need to ask you for some help), you can install so-called \"debuginfo\" packages. These packages will provide debugging symbols and source code so that you can do things like run gdb or pstack to get information about a program. Installing the debuginfo package requires three steps. Enable the installation of debuginfo packages. This only needs to be done once. Edit the yum repo file, usually /etc/yum.repos.d/osg.repo to enable the separate debuginfo repository. Near the bottom of the file, you'll see the osg-debug repo: [osg-debug] name=OSG Software for Enterprise Linux 5 - $basearch - Debug baseurl=http://repo.opensciencegrid.org/osg-release/$basearch/debu failovermethod=priority priority=98 enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG Make sure that \"enabled\" is set to 1. Figure out which package installed the program you want to debug. One way to figure it out is to ask RPM. For example, if you want to debug grid-proxy-init: user@host $ rpm -qf ` which grid-proxy-init ` globus-proxy-utils-5.0-5.osg.x86_64 Install the debugging information for that package. Continuing this example: root@host # debuginfo-install globus-proxy-utils ... ================================================================================================================================= Package Arch Version Repository Size ================================================================================================================================= Installing: globus-proxy-utils-debuginfo x86_64 5.0-5.osg osg-debug 61 k Transaction Summary ================================================================================================================================= Install 1 Package(s) Upgrade 0 Package(s) Total download size: 61 k Is this ok [y/N]: y ... Installed: globus-proxy-utils-debuginfo.x86_64 0:5.0-5.osg This last step will select the right package name, then use yum to install it.","title":"Advanced topic: Getting debugging information for installed software"},{"location":"release/yum-basics/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"release/yum-basics/#yum-not-finding-packages","text":"If you is not finding some packages, e.g.: Error Downloading Packages: packageXYZ: failure: packageXYZ.rpm from osg: [Errno 256] No more mirrors to try. then you can try cleaning up Yum's cache: root@host # yum clean all --enablerepo = *","title":"Yum not finding packages"},{"location":"release/yum-basics/#yum-complaining-about-missing-keys","text":"If yum is complaining you can re-import the keys in your distribution: root@host # rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY*","title":"Yum complaining about missing keys"},{"location":"release/yum-basics/#references","text":"The main Yum web site A good description of the commands for RPM and Yum can be found at Learn Linux 101: RPM and Yum Package Management .","title":"References"},{"location":"release/3.5/release-3-5-0/","text":"OSG Software Release 3.5.0 \u00b6 Release Date: 2019-08-30 Supported OS Versions: EL7 Summary of changes \u00b6 This initial release of the OSG 3.5 release series is based on the packages available in OSG 3.4.33 with some additions and subtractions . Additionally, the contents of the upcoming repository have been emptied of packages related to OSG 3.4 and replaced with packages for OSG 3.5. Other notable changes in this release series include dropping support for Enterprise Linux 6 and CREAM CEs . To update to the OSG 3.5 release series, please consult the page on updating between release series . Where are GlideinWMS and HTCondor-CE? HTCondor-CE (including osg-ce metapackages) and GlideinWMS are both absent in OSG 3.5.0: we expect major version updates that may require manual intervention for both of these packages so we are holding their initial releases in this series until they are ready. OSG 3.4 end-of-life As a result of this initial OSG 3.5 release, the end-of-life dates have been set for OSG 3.4 per our policy : regular support will end in February 2020 and critical bug/security support will end in November 2020 . Package updates \u00b6 In addition to the packages that were carried over from OSG 3.4.33, this release contains the following package updates: HTCondor 8.8.4: The current HTCondor stable release . See the manual update instructions before updating to this version. Some highlights from the 8.8 release series include: Automatically add AWS resources to your pool using HTCondor Annex The Python bindings now include submit functionality Added the ability to run a job immediately by replacing a running job HTCondor now tracks and reports GPU utilization Several performance enhancements in the collector The grid universe can create and manage VM instances in Microsoft Azure The MUNGE security method is now supported on all Linux platforms CVMFS 2.6.2: A bug fix release . Note the update recommendations from the developers: As with previous releases, upgrading clients should be seamless just by installing the new package from the repository. As usual, we recommend to update only a few worker nodes first and gradually ramp up once the new version proves to work correctly. Please take special care when upgrading a cvmfs client in NFS mode. For Stratum 1 servers, there should be no running snapshots during the upgrade. For publisher and gateway nodes, all transactions must be closed and no active leases must be present before upgrading. See the known issue with this version below . XCache 1.1.1: This release includes packages for ATLAS and CMS XCaches as well as Stash Origin HTTP/S support. OSG Configure 3.0.0: A major version release , including changes from the OSG Configure 2.4 series and dropping some deprecated features. See the manual update instructions before updating to this version. OSG XRootD 3.5: A meta-package including common configuration across standalone , storage element , and caching installations of XRootD. XRootD LCMAPS 1.7.4: includes default authorization configuration in /etc/xrootd/config.d/40-xrootd-lcmaps.cfg . To use the default configuration, uncomment the # set EnableLcmaps = 1 line in /etc/xrootd/config.d/10-xrootd-lcmaps.cfg . XRootD HDFS 2.1.6: includes default configuration in /etc/xrootd/40-xrootd-hdfs.cfg . MyProxy 6.2.4: Remove usage statistics collection support CCTools 7.0.14 : Bug fix release OSG System Profiler 1.4.3: Remove collection of obsolete information See the known issue with this version below . HTCondor 8.9.2 (upcoming): The current HTCondor development release . Some highlights from the 8.9 release series include: New TOKEN authentication method enables fine-grained authorization control All HTCondor daemons run under a condor_master share a security session An efficient HTTP/S plugin that supports uploads and authentication tokens The HTTP/HTTPS file transfer plugin will timeout and retry transfers HTCondor automatically supports GPU jobs in Docker and Singularity File transfer times are now recorded in the user job log and the job ad A new multi-file box.com file transfer plugin to download files Configuration options for job-log time-stamps (UTC, ISO 8601, sub-second) Several improvements to SSL authentication These JIRA tickets were addressed in this release. Package removals \u00b6 A new OSG release series , gives us the opportunity to clean up our Yum repositories : either removing packages that are the same version of those available in EPEL; or removing pacakges that are now obsolete in the OSG Software stack. Below, you will find a list of packages that were in OSG 3.4 but have been removed in OSG 3.5. If you believe any of the following packages have been removed in error, please contact us . The following packages were removed from the OSG 3.5 Yum repositories but are available via EPEL repositories : singularity globus-ftp-client globus-gridftp-server-control The following packages are obsolete and have been removed from the OSG 3.5 Yum repositories: autopyfactory CREAM packages: glite-ce-cream-client-api-c (replaced by an empty package to ease updates to 3.5) glite-ce-wsdl glite-build-common-cpp PerfSONAR client tools: bwctl l2util nuttcp owamp perfsonar-tools lcmaps-plugins-scas-client osg-control osg-version osg-vo-map rsv (replaced by an empty package to ease updates to 3.5) xacml Known Issues \u00b6 CVMFS 2.6.2 has a known memory leak when using an /etc/hosts file with lines only containing whitespace ( CVM-1796 ) OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . avro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osg35.el7 bigtop-jsvc-0.3.0-1.2.osg35.el7 bigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osg35.el7 blahp-1.18.41.bosco-1.osg35.el7 cctools-7.0.14-1.osg35.el7 cigetcert-1.16-2.osg35.el7 cilogon-openid-ca-cert-1.1-4.osg35.el7 condor-8.8.4-1.8.osg35.el7 cvmfs-2.6.2-1.osg35.el7 cvmfs-config-osg-2.4-1.osg35.el7 cvmfs-gateway-0.3.1-1.1.osg35.el7 cvmfs-x509-helper-2.0-3.osg35.el7 frontier-squid-4.8-1.1.osg35.el7 glite-ce-cream-client-api-c-1.15.4-2.5.osg35.el7 globus-gridftp-osg-extensions-0.4-1.osg35.el7 globus-gridftp-server-13.9-1.1.osg35.el7 gratia-probe-1.20.8-1.osg35.el7 gridftp-dsi-posix-1.4-2.osg35.el7 gridftp-hdfs-1.1.1-1.2.osg35.el7 gsi-openssh-7.4p1-2.3.osg35.el7 hadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 igtf-ca-certs-1.101-1.osg35.el7 javascriptrrd-1.1.1-1.osg35.el7 lcas-lcmaps-gt4-interface-0.3.1-1.3.osg35.el7 lcmaps-1.6.6-1.10.osg35.el7 lcmaps-plugins-basic-1.7.0-2.osg35.el7 lcmaps-plugins-verify-proxy-1.5.11-1.1.osg35.el7 lcmaps-plugins-voms-1.7.1-1.6.osg35.el7 llrun-0.1.3-1.3.osg35.el7 myproxy-6.2.4-1.1.osg35.el7 osg-ca-certs-1.83-1.osg35.el7 osg-ca-certs-updater-1.8-1.osg35.el7 osg-ca-scripts-1.2.4-1.osg35.el7 osg-configure-3.0.0-1.osg35.el7 osg-flock-1.1-1.osg35.el7 osg-gridftp-3.5-3.osg35.el7 osg-oasis-15-2.osg35.el7 osg-pki-tools-3.3.0-1.osg35.el7 osg-release-itb-3.5-2.osg35.el7 osg-se-hadoop-3.5-1.osg35.el7 osg-system-profiler-1.4.3-1.osg35.el7 osg-update-vos-1.4.0-1.osg35.el7 osg-wn-client-3.5-1.osg35.el7 osg-xrootd-3.5-3.osg35.el7 pegasus-4.9.1-1.osg35.el7 python-jwt-1.6.1-1.osg35.el7 python-scitokens-1.2.1-2.osg35.el7 rsv-3.19.8-2.osg35.el7 scitokens-cpp-0.3.3-1.osg35.el7 stashcache-client-5.2.0-1.osg35.el7 uberftp-2.8-2.1.osg35.el7 vo-client-94-1.osg35.el7 voms-2.0.14-1.4.osg35.el7 vomsxrd-0.6.0-3.osg35.el7 xcache-1.1.1-1.osg35.el7 xrootd-4.10.0-1.osg35.el7 xrootd-hdfs-2.1.6-1.osg35.el7 xrootd-lcmaps-1.7.4-2.osg35.el7 xrootd-multiuser-0.4.2-4.osg35.el7 xrootd-scitokens-1.0.0-1.1.osg35.el7 zookeeper-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache avro-doc avro-libs avro-tools bigtop-jsvc bigtop-jsvc-debuginfo bigtop-utils blahp blahp-debuginfo cctools cctools-debuginfo cctools-devel cigetcert cilogon-openid-ca-cert cms-xcache condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-ducc cvmfs-gateway cvmfs-server cvmfs-shrinkwrap cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo glite-ce-cream-client-api-c glite-ce-cream-client-devel globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gridftp-dsi-posix gridftp-dsi-posix-debuginfo gridftp-hdfs gridftp-hdfs-debuginfo gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-nfs3 hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-kms hadoop-kms-server hadoop-libhdfs hadoop-libhdfs-devel hadoop-mapreduce hadoop-yarn igtf-ca-certs javascriptrrd lcas-lcmaps-gt4-interface lcas-lcmaps-gt4-interface-debuginfo lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-plugins-basic lcmaps-plugins-basic-debuginfo lcmaps-plugins-basic-ldap lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo lcmaps-plugins-voms lcmaps-plugins-voms-debuginfo lcmaps-without-gsi lcmaps-without-gsi-devel llrun llrun-debuginfo minicondor myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs osg-ca-certs-updater osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-misc osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-siteinfo osg-configure-slurm osg-configure-squid osg-configure-tests osg-flock osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd osg-oasis osg-pki-tools osg-release osg-release-itb osg-se-hadoop osg-se-hadoop-client osg-se-hadoop-datanode osg-se-hadoop-gridftp osg-se-hadoop-namenode osg-se-hadoop-secondarynamenode osg-system-profiler osg-system-profiler-viewer osg-update-data osg-update-vos osg-wn-client osg-xrootd osg-xrootd-standalone pegasus pegasus-debuginfo python2-condor python2-jwt python2-scitokens python2-xrootd python36-jwt python3-condor rsv rsv-consumers rsv-core rsv-metrics scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel stash-cache stashcache-client stash-origin uberftp uberftp-debuginfo vo-client vo-client-dcache vo-client-lcmaps-voms voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server vomsxrd vomsxrd-debuginfo vomsxrd-devel xcache xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs zookeeper zookeeper-debuginfo zookeeper-native zookeeper-server If you wish to only update the RPMs that changed, the set of RPMs is: atlas-xcache-1.1.1-1.osg35.el7 avro-doc-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osg35.el7 avro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osg35.el7 avro-tools-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osg35.el7 bigtop-jsvc-0.3.0-1.2.osg35.el7 bigtop-jsvc-debuginfo-0.3.0-1.2.osg35.el7 bigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osg35.el7 blahp-1.18.41.bosco-1.osg35.el7 blahp-debuginfo-1.18.41.bosco-1.osg35.el7 cctools-7.0.14-1.osg35.el7 cctools-debuginfo-7.0.14-1.osg35.el7 cctools-devel-7.0.14-1.osg35.el7 cigetcert-1.16-2.osg35.el7 cilogon-openid-ca-cert-1.1-4.osg35.el7 cms-xcache-1.1.1-1.osg35.el7 condor-8.8.4-1.8.osg35.el7 condor-all-8.8.4-1.8.osg35.el7 condor-annex-ec2-8.8.4-1.8.osg35.el7 condor-bosco-8.8.4-1.8.osg35.el7 condor-classads-8.8.4-1.8.osg35.el7 condor-classads-devel-8.8.4-1.8.osg35.el7 condor-debuginfo-8.8.4-1.8.osg35.el7 condor-kbdd-8.8.4-1.8.osg35.el7 condor-procd-8.8.4-1.8.osg35.el7 condor-test-8.8.4-1.8.osg35.el7 condor-vm-gahp-8.8.4-1.8.osg35.el7 cvmfs-2.6.2-1.osg35.el7 cvmfs-config-osg-2.4-1.osg35.el7 cvmfs-devel-2.6.2-1.osg35.el7 cvmfs-ducc-2.6.2-1.osg35.el7 cvmfs-gateway-0.3.1-1.1.osg35.el7 cvmfs-server-2.6.2-1.osg35.el7 cvmfs-shrinkwrap-2.6.2-1.osg35.el7 cvmfs-unittests-2.6.2-1.osg35.el7 cvmfs-x509-helper-2.0-3.osg35.el7 cvmfs-x509-helper-debuginfo-2.0-3.osg35.el7 frontier-squid-4.8-1.1.osg35.el7 frontier-squid-debuginfo-4.8-1.1.osg35.el7 glite-ce-cream-client-api-c-1.15.4-2.5.osg35.el7 glite-ce-cream-client-devel-1.15.4-2.5.osg35.el7 globus-gridftp-osg-extensions-0.4-1.osg35.el7 globus-gridftp-osg-extensions-debuginfo-0.4-1.osg35.el7 globus-gridftp-server-13.9-1.1.osg35.el7 globus-gridftp-server-debuginfo-13.9-1.1.osg35.el7 globus-gridftp-server-devel-13.9-1.1.osg35.el7 globus-gridftp-server-progs-13.9-1.1.osg35.el7 gratia-probe-1.20.8-1.osg35.el7 gratia-probe-common-1.20.8-1.osg35.el7 gratia-probe-condor-1.20.8-1.osg35.el7 gratia-probe-condor-events-1.20.8-1.osg35.el7 gratia-probe-dcache-storage-1.20.8-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.8-1.osg35.el7 gratia-probe-dcache-transfer-1.20.8-1.osg35.el7 gratia-probe-debuginfo-1.20.8-1.osg35.el7 gratia-probe-enstore-storage-1.20.8-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.8-1.osg35.el7 gratia-probe-enstore-transfer-1.20.8-1.osg35.el7 gratia-probe-glideinwms-1.20.8-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.8-1.osg35.el7 gratia-probe-hadoop-storage-1.20.8-1.osg35.el7 gratia-probe-htcondor-ce-1.20.8-1.osg35.el7 gratia-probe-lsf-1.20.8-1.osg35.el7 gratia-probe-metric-1.20.8-1.osg35.el7 gratia-probe-onevm-1.20.8-1.osg35.el7 gratia-probe-pbs-lsf-1.20.8-1.osg35.el7 gratia-probe-services-1.20.8-1.osg35.el7 gratia-probe-sge-1.20.8-1.osg35.el7 gratia-probe-slurm-1.20.8-1.osg35.el7 gratia-probe-xrootd-storage-1.20.8-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.8-1.osg35.el7 gridftp-dsi-posix-1.4-2.osg35.el7 gridftp-dsi-posix-debuginfo-1.4-2.osg35.el7 gridftp-hdfs-1.1.1-1.2.osg35.el7 gridftp-hdfs-debuginfo-1.1.1-1.2.osg35.el7 gsi-openssh-7.4p1-2.3.osg35.el7 gsi-openssh-clients-7.4p1-2.3.osg35.el7 gsi-openssh-debuginfo-7.4p1-2.3.osg35.el7 gsi-openssh-server-7.4p1-2.3.osg35.el7 hadoop-0.20-conf-pseudo-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-0.20-mapreduce-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-client-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-conf-pseudo-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-debuginfo-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-doc-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-datanode-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-fuse-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-journalnode-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-namenode-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-nfs3-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-secondarynamenode-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-zkfc-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-httpfs-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-kms-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-kms-server-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-libhdfs-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-libhdfs-devel-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-mapreduce-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-yarn-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 igtf-ca-certs-1.101-1.osg35.el7 javascriptrrd-1.1.1-1.osg35.el7 lcas-lcmaps-gt4-interface-0.3.1-1.3.osg35.el7 lcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.3.osg35.el7 lcmaps-1.6.6-1.10.osg35.el7 lcmaps-common-devel-1.6.6-1.10.osg35.el7 lcmaps-db-templates-1.6.6-1.10.osg35.el7 lcmaps-debuginfo-1.6.6-1.10.osg35.el7 lcmaps-devel-1.6.6-1.10.osg35.el7 lcmaps-plugins-basic-1.7.0-2.osg35.el7 lcmaps-plugins-basic-debuginfo-1.7.0-2.osg35.el7 lcmaps-plugins-basic-ldap-1.7.0-2.osg35.el7 lcmaps-plugins-verify-proxy-1.5.11-1.1.osg35.el7 lcmaps-plugins-verify-proxy-debuginfo-1.5.11-1.1.osg35.el7 lcmaps-plugins-voms-1.7.1-1.6.osg35.el7 lcmaps-plugins-voms-debuginfo-1.7.1-1.6.osg35.el7 lcmaps-without-gsi-1.6.6-1.10.osg35.el7 lcmaps-without-gsi-devel-1.6.6-1.10.osg35.el7 llrun-0.1.3-1.3.osg35.el7 llrun-debuginfo-0.1.3-1.3.osg35.el7 minicondor-8.8.4-1.8.osg35.el7 myproxy-6.2.4-1.1.osg35.el7 myproxy-admin-6.2.4-1.1.osg35.el7 myproxy-debuginfo-6.2.4-1.1.osg35.el7 myproxy-devel-6.2.4-1.1.osg35.el7 myproxy-doc-6.2.4-1.1.osg35.el7 myproxy-libs-6.2.4-1.1.osg35.el7 myproxy-server-6.2.4-1.1.osg35.el7 myproxy-voms-6.2.4-1.1.osg35.el7 osg-ca-certs-1.83-1.osg35.el7 osg-ca-certs-updater-1.8-1.osg35.el7 osg-ca-scripts-1.2.4-1.osg35.el7 osg-configure-3.0.0-1.osg35.el7 osg-configure-bosco-3.0.0-1.osg35.el7 osg-configure-ce-3.0.0-1.osg35.el7 osg-configure-condor-3.0.0-1.osg35.el7 osg-configure-gateway-3.0.0-1.osg35.el7 osg-configure-gip-3.0.0-1.osg35.el7 osg-configure-gratia-3.0.0-1.osg35.el7 osg-configure-infoservices-3.0.0-1.osg35.el7 osg-configure-lsf-3.0.0-1.osg35.el7 osg-configure-misc-3.0.0-1.osg35.el7 osg-configure-pbs-3.0.0-1.osg35.el7 osg-configure-rsv-3.0.0-1.osg35.el7 osg-configure-sge-3.0.0-1.osg35.el7 osg-configure-siteinfo-3.0.0-1.osg35.el7 osg-configure-slurm-3.0.0-1.osg35.el7 osg-configure-squid-3.0.0-1.osg35.el7 osg-configure-tests-3.0.0-1.osg35.el7 osg-flock-1.1-1.osg35.el7 osg-gridftp-3.5-3.osg35.el7 osg-gridftp-hdfs-3.5-3.osg35.el7 osg-gridftp-xrootd-3.5-3.osg35.el7 osg-oasis-15-2.osg35.el7 osg-pki-tools-3.3.0-1.osg35.el7 osg-release-itb-3.5-2.osg35.el7 osg-se-hadoop-3.5-1.osg35.el7 osg-se-hadoop-client-3.5-1.osg35.el7 osg-se-hadoop-datanode-3.5-1.osg35.el7 osg-se-hadoop-gridftp-3.5-1.osg35.el7 osg-se-hadoop-namenode-3.5-1.osg35.el7 osg-se-hadoop-secondarynamenode-3.5-1.osg35.el7 osg-system-profiler-1.4.3-1.osg35.el7 osg-system-profiler-viewer-1.4.3-1.osg35.el7 osg-update-data-1.4.0-1.osg35.el7 osg-update-vos-1.4.0-1.osg35.el7 osg-wn-client-3.5-1.osg35.el7 osg-xrootd-3.5-3.osg35.el7 osg-xrootd-standalone-3.5-3.osg35.el7 pegasus-4.9.1-1.osg35.el7 pegasus-debuginfo-4.9.1-1.osg35.el7 python2-condor-8.8.4-1.8.osg35.el7 python2-jwt-1.6.1-1.osg35.el7 python2-scitokens-1.2.1-2.osg35.el7 python2-xrootd-4.10.0-1.osg35.el7 python36-jwt-1.6.1-1.osg35.el7 python3-condor-8.8.4-1.8.osg35.el7 python-jwt-1.6.1-1.osg35.el7 python-scitokens-1.2.1-2.osg35.el7 rsv-3.19.8-2.osg35.el7 rsv-consumers-3.19.8-2.osg35.el7 rsv-core-3.19.8-2.osg35.el7 rsv-metrics-3.19.8-2.osg35.el7 scitokens-cpp-0.3.3-1.osg35.el7 scitokens-cpp-debuginfo-0.3.3-1.osg35.el7 scitokens-cpp-devel-0.3.3-1.osg35.el7 stash-cache-1.1.1-1.osg35.el7 stashcache-client-5.2.0-1.osg35.el7 stash-origin-1.1.1-1.osg35.el7 uberftp-2.8-2.1.osg35.el7 uberftp-debuginfo-2.8-2.1.osg35.el7 vo-client-94-1.osg35.el7 vo-client-dcache-94-1.osg35.el7 vo-client-lcmaps-voms-94-1.osg35.el7 voms-2.0.14-1.4.osg35.el7 voms-clients-cpp-2.0.14-1.4.osg35.el7 voms-debuginfo-2.0.14-1.4.osg35.el7 voms-devel-2.0.14-1.4.osg35.el7 voms-doc-2.0.14-1.4.osg35.el7 voms-server-2.0.14-1.4.osg35.el7 vomsxrd-0.6.0-3.osg35.el7 vomsxrd-debuginfo-0.6.0-3.osg35.el7 vomsxrd-devel-0.6.0-3.osg35.el7 xcache-1.1.1-1.osg35.el7 xrootd-4.10.0-1.osg35.el7 xrootd-client-4.10.0-1.osg35.el7 xrootd-client-devel-4.10.0-1.osg35.el7 xrootd-client-libs-4.10.0-1.osg35.el7 xrootd-debuginfo-4.10.0-1.osg35.el7 xrootd-devel-4.10.0-1.osg35.el7 xrootd-doc-4.10.0-1.osg35.el7 xrootd-fuse-4.10.0-1.osg35.el7 xrootd-hdfs-2.1.6-1.osg35.el7 xrootd-hdfs-debuginfo-2.1.6-1.osg35.el7 xrootd-hdfs-devel-2.1.6-1.osg35.el7 xrootd-lcmaps-1.7.4-2.osg35.el7 xrootd-lcmaps-debuginfo-1.7.4-2.osg35.el7 xrootd-libs-4.10.0-1.osg35.el7 xrootd-multiuser-0.4.2-4.osg35.el7 xrootd-multiuser-debuginfo-0.4.2-4.osg35.el7 xrootd-private-devel-4.10.0-1.osg35.el7 xrootd-scitokens-1.0.0-1.1.osg35.el7 xrootd-scitokens-debuginfo-1.0.0-1.1.osg35.el7 xrootd-selinux-4.10.0-1.osg35.el7 xrootd-server-4.10.0-1.osg35.el7 xrootd-server-devel-4.10.0-1.osg35.el7 xrootd-server-libs-4.10.0-1.osg35.el7 zookeeper-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7 zookeeper-debuginfo-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7 zookeeper-native-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7 zookeeper-server-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.41.bosco-2.osgup.el7 condor-8.9.2-1.4.osgup.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glite-ce-cream-client-api-c glite-ce-cream-client-devel minicondor osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd python2-condor If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.41.bosco-2.osgup.el7 blahp-debuginfo-1.18.41.bosco-2.osgup.el7 condor-8.9.2-1.4.osgup.el7 condor-all-8.9.2-1.4.osgup.el7 condor-annex-ec2-8.9.2-1.4.osgup.el7 condor-bosco-8.9.2-1.4.osgup.el7 condor-classads-8.9.2-1.4.osgup.el7 condor-classads-devel-8.9.2-1.4.osgup.el7 condor-debuginfo-8.9.2-1.4.osgup.el7 condor-kbdd-8.9.2-1.4.osgup.el7 condor-procd-8.9.2-1.4.osgup.el7 condor-test-8.9.2-1.4.osgup.el7 condor-vm-gahp-8.9.2-1.4.osgup.el7 minicondor-8.9.2-1.4.osgup.el7 python2-condor-8.9.2-1.4.osgup.el7","title":"Release 3 5 0"},{"location":"release/3.5/release-3-5-0/#osg-software-release-350","text":"Release Date: 2019-08-30 Supported OS Versions: EL7","title":"OSG Software Release 3.5.0"},{"location":"release/3.5/release-3-5-0/#summary-of-changes","text":"This initial release of the OSG 3.5 release series is based on the packages available in OSG 3.4.33 with some additions and subtractions . Additionally, the contents of the upcoming repository have been emptied of packages related to OSG 3.4 and replaced with packages for OSG 3.5. Other notable changes in this release series include dropping support for Enterprise Linux 6 and CREAM CEs . To update to the OSG 3.5 release series, please consult the page on updating between release series . Where are GlideinWMS and HTCondor-CE? HTCondor-CE (including osg-ce metapackages) and GlideinWMS are both absent in OSG 3.5.0: we expect major version updates that may require manual intervention for both of these packages so we are holding their initial releases in this series until they are ready. OSG 3.4 end-of-life As a result of this initial OSG 3.5 release, the end-of-life dates have been set for OSG 3.4 per our policy : regular support will end in February 2020 and critical bug/security support will end in November 2020 .","title":"Summary of changes"},{"location":"release/3.5/release-3-5-0/#package-updates","text":"In addition to the packages that were carried over from OSG 3.4.33, this release contains the following package updates: HTCondor 8.8.4: The current HTCondor stable release . See the manual update instructions before updating to this version. Some highlights from the 8.8 release series include: Automatically add AWS resources to your pool using HTCondor Annex The Python bindings now include submit functionality Added the ability to run a job immediately by replacing a running job HTCondor now tracks and reports GPU utilization Several performance enhancements in the collector The grid universe can create and manage VM instances in Microsoft Azure The MUNGE security method is now supported on all Linux platforms CVMFS 2.6.2: A bug fix release . Note the update recommendations from the developers: As with previous releases, upgrading clients should be seamless just by installing the new package from the repository. As usual, we recommend to update only a few worker nodes first and gradually ramp up once the new version proves to work correctly. Please take special care when upgrading a cvmfs client in NFS mode. For Stratum 1 servers, there should be no running snapshots during the upgrade. For publisher and gateway nodes, all transactions must be closed and no active leases must be present before upgrading. See the known issue with this version below . XCache 1.1.1: This release includes packages for ATLAS and CMS XCaches as well as Stash Origin HTTP/S support. OSG Configure 3.0.0: A major version release , including changes from the OSG Configure 2.4 series and dropping some deprecated features. See the manual update instructions before updating to this version. OSG XRootD 3.5: A meta-package including common configuration across standalone , storage element , and caching installations of XRootD. XRootD LCMAPS 1.7.4: includes default authorization configuration in /etc/xrootd/config.d/40-xrootd-lcmaps.cfg . To use the default configuration, uncomment the # set EnableLcmaps = 1 line in /etc/xrootd/config.d/10-xrootd-lcmaps.cfg . XRootD HDFS 2.1.6: includes default configuration in /etc/xrootd/40-xrootd-hdfs.cfg . MyProxy 6.2.4: Remove usage statistics collection support CCTools 7.0.14 : Bug fix release OSG System Profiler 1.4.3: Remove collection of obsolete information See the known issue with this version below . HTCondor 8.9.2 (upcoming): The current HTCondor development release . Some highlights from the 8.9 release series include: New TOKEN authentication method enables fine-grained authorization control All HTCondor daemons run under a condor_master share a security session An efficient HTTP/S plugin that supports uploads and authentication tokens The HTTP/HTTPS file transfer plugin will timeout and retry transfers HTCondor automatically supports GPU jobs in Docker and Singularity File transfer times are now recorded in the user job log and the job ad A new multi-file box.com file transfer plugin to download files Configuration options for job-log time-stamps (UTC, ISO 8601, sub-second) Several improvements to SSL authentication These JIRA tickets were addressed in this release.","title":"Package updates"},{"location":"release/3.5/release-3-5-0/#package-removals","text":"A new OSG release series , gives us the opportunity to clean up our Yum repositories : either removing packages that are the same version of those available in EPEL; or removing pacakges that are now obsolete in the OSG Software stack. Below, you will find a list of packages that were in OSG 3.4 but have been removed in OSG 3.5. If you believe any of the following packages have been removed in error, please contact us . The following packages were removed from the OSG 3.5 Yum repositories but are available via EPEL repositories : singularity globus-ftp-client globus-gridftp-server-control The following packages are obsolete and have been removed from the OSG 3.5 Yum repositories: autopyfactory CREAM packages: glite-ce-cream-client-api-c (replaced by an empty package to ease updates to 3.5) glite-ce-wsdl glite-build-common-cpp PerfSONAR client tools: bwctl l2util nuttcp owamp perfsonar-tools lcmaps-plugins-scas-client osg-control osg-version osg-vo-map rsv (replaced by an empty package to ease updates to 3.5) xacml","title":"Package removals"},{"location":"release/3.5/release-3-5-0/#known-issues","text":"CVMFS 2.6.2 has a known memory leak when using an /etc/hosts file with lines only containing whitespace ( CVM-1796 ) OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-0/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-0/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-0/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-0/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . avro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osg35.el7 bigtop-jsvc-0.3.0-1.2.osg35.el7 bigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osg35.el7 blahp-1.18.41.bosco-1.osg35.el7 cctools-7.0.14-1.osg35.el7 cigetcert-1.16-2.osg35.el7 cilogon-openid-ca-cert-1.1-4.osg35.el7 condor-8.8.4-1.8.osg35.el7 cvmfs-2.6.2-1.osg35.el7 cvmfs-config-osg-2.4-1.osg35.el7 cvmfs-gateway-0.3.1-1.1.osg35.el7 cvmfs-x509-helper-2.0-3.osg35.el7 frontier-squid-4.8-1.1.osg35.el7 glite-ce-cream-client-api-c-1.15.4-2.5.osg35.el7 globus-gridftp-osg-extensions-0.4-1.osg35.el7 globus-gridftp-server-13.9-1.1.osg35.el7 gratia-probe-1.20.8-1.osg35.el7 gridftp-dsi-posix-1.4-2.osg35.el7 gridftp-hdfs-1.1.1-1.2.osg35.el7 gsi-openssh-7.4p1-2.3.osg35.el7 hadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 igtf-ca-certs-1.101-1.osg35.el7 javascriptrrd-1.1.1-1.osg35.el7 lcas-lcmaps-gt4-interface-0.3.1-1.3.osg35.el7 lcmaps-1.6.6-1.10.osg35.el7 lcmaps-plugins-basic-1.7.0-2.osg35.el7 lcmaps-plugins-verify-proxy-1.5.11-1.1.osg35.el7 lcmaps-plugins-voms-1.7.1-1.6.osg35.el7 llrun-0.1.3-1.3.osg35.el7 myproxy-6.2.4-1.1.osg35.el7 osg-ca-certs-1.83-1.osg35.el7 osg-ca-certs-updater-1.8-1.osg35.el7 osg-ca-scripts-1.2.4-1.osg35.el7 osg-configure-3.0.0-1.osg35.el7 osg-flock-1.1-1.osg35.el7 osg-gridftp-3.5-3.osg35.el7 osg-oasis-15-2.osg35.el7 osg-pki-tools-3.3.0-1.osg35.el7 osg-release-itb-3.5-2.osg35.el7 osg-se-hadoop-3.5-1.osg35.el7 osg-system-profiler-1.4.3-1.osg35.el7 osg-update-vos-1.4.0-1.osg35.el7 osg-wn-client-3.5-1.osg35.el7 osg-xrootd-3.5-3.osg35.el7 pegasus-4.9.1-1.osg35.el7 python-jwt-1.6.1-1.osg35.el7 python-scitokens-1.2.1-2.osg35.el7 rsv-3.19.8-2.osg35.el7 scitokens-cpp-0.3.3-1.osg35.el7 stashcache-client-5.2.0-1.osg35.el7 uberftp-2.8-2.1.osg35.el7 vo-client-94-1.osg35.el7 voms-2.0.14-1.4.osg35.el7 vomsxrd-0.6.0-3.osg35.el7 xcache-1.1.1-1.osg35.el7 xrootd-4.10.0-1.osg35.el7 xrootd-hdfs-2.1.6-1.osg35.el7 xrootd-lcmaps-1.7.4-2.osg35.el7 xrootd-multiuser-0.4.2-4.osg35.el7 xrootd-scitokens-1.0.0-1.1.osg35.el7 zookeeper-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-0/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache avro-doc avro-libs avro-tools bigtop-jsvc bigtop-jsvc-debuginfo bigtop-utils blahp blahp-debuginfo cctools cctools-debuginfo cctools-devel cigetcert cilogon-openid-ca-cert cms-xcache condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-ducc cvmfs-gateway cvmfs-server cvmfs-shrinkwrap cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo glite-ce-cream-client-api-c glite-ce-cream-client-devel globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gridftp-dsi-posix gridftp-dsi-posix-debuginfo gridftp-hdfs gridftp-hdfs-debuginfo gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-nfs3 hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-kms hadoop-kms-server hadoop-libhdfs hadoop-libhdfs-devel hadoop-mapreduce hadoop-yarn igtf-ca-certs javascriptrrd lcas-lcmaps-gt4-interface lcas-lcmaps-gt4-interface-debuginfo lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-plugins-basic lcmaps-plugins-basic-debuginfo lcmaps-plugins-basic-ldap lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo lcmaps-plugins-voms lcmaps-plugins-voms-debuginfo lcmaps-without-gsi lcmaps-without-gsi-devel llrun llrun-debuginfo minicondor myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs osg-ca-certs-updater osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-misc osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-siteinfo osg-configure-slurm osg-configure-squid osg-configure-tests osg-flock osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd osg-oasis osg-pki-tools osg-release osg-release-itb osg-se-hadoop osg-se-hadoop-client osg-se-hadoop-datanode osg-se-hadoop-gridftp osg-se-hadoop-namenode osg-se-hadoop-secondarynamenode osg-system-profiler osg-system-profiler-viewer osg-update-data osg-update-vos osg-wn-client osg-xrootd osg-xrootd-standalone pegasus pegasus-debuginfo python2-condor python2-jwt python2-scitokens python2-xrootd python36-jwt python3-condor rsv rsv-consumers rsv-core rsv-metrics scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel stash-cache stashcache-client stash-origin uberftp uberftp-debuginfo vo-client vo-client-dcache vo-client-lcmaps-voms voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server vomsxrd vomsxrd-debuginfo vomsxrd-devel xcache xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs zookeeper zookeeper-debuginfo zookeeper-native zookeeper-server If you wish to only update the RPMs that changed, the set of RPMs is: atlas-xcache-1.1.1-1.osg35.el7 avro-doc-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osg35.el7 avro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osg35.el7 avro-tools-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osg35.el7 bigtop-jsvc-0.3.0-1.2.osg35.el7 bigtop-jsvc-debuginfo-0.3.0-1.2.osg35.el7 bigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osg35.el7 blahp-1.18.41.bosco-1.osg35.el7 blahp-debuginfo-1.18.41.bosco-1.osg35.el7 cctools-7.0.14-1.osg35.el7 cctools-debuginfo-7.0.14-1.osg35.el7 cctools-devel-7.0.14-1.osg35.el7 cigetcert-1.16-2.osg35.el7 cilogon-openid-ca-cert-1.1-4.osg35.el7 cms-xcache-1.1.1-1.osg35.el7 condor-8.8.4-1.8.osg35.el7 condor-all-8.8.4-1.8.osg35.el7 condor-annex-ec2-8.8.4-1.8.osg35.el7 condor-bosco-8.8.4-1.8.osg35.el7 condor-classads-8.8.4-1.8.osg35.el7 condor-classads-devel-8.8.4-1.8.osg35.el7 condor-debuginfo-8.8.4-1.8.osg35.el7 condor-kbdd-8.8.4-1.8.osg35.el7 condor-procd-8.8.4-1.8.osg35.el7 condor-test-8.8.4-1.8.osg35.el7 condor-vm-gahp-8.8.4-1.8.osg35.el7 cvmfs-2.6.2-1.osg35.el7 cvmfs-config-osg-2.4-1.osg35.el7 cvmfs-devel-2.6.2-1.osg35.el7 cvmfs-ducc-2.6.2-1.osg35.el7 cvmfs-gateway-0.3.1-1.1.osg35.el7 cvmfs-server-2.6.2-1.osg35.el7 cvmfs-shrinkwrap-2.6.2-1.osg35.el7 cvmfs-unittests-2.6.2-1.osg35.el7 cvmfs-x509-helper-2.0-3.osg35.el7 cvmfs-x509-helper-debuginfo-2.0-3.osg35.el7 frontier-squid-4.8-1.1.osg35.el7 frontier-squid-debuginfo-4.8-1.1.osg35.el7 glite-ce-cream-client-api-c-1.15.4-2.5.osg35.el7 glite-ce-cream-client-devel-1.15.4-2.5.osg35.el7 globus-gridftp-osg-extensions-0.4-1.osg35.el7 globus-gridftp-osg-extensions-debuginfo-0.4-1.osg35.el7 globus-gridftp-server-13.9-1.1.osg35.el7 globus-gridftp-server-debuginfo-13.9-1.1.osg35.el7 globus-gridftp-server-devel-13.9-1.1.osg35.el7 globus-gridftp-server-progs-13.9-1.1.osg35.el7 gratia-probe-1.20.8-1.osg35.el7 gratia-probe-common-1.20.8-1.osg35.el7 gratia-probe-condor-1.20.8-1.osg35.el7 gratia-probe-condor-events-1.20.8-1.osg35.el7 gratia-probe-dcache-storage-1.20.8-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.8-1.osg35.el7 gratia-probe-dcache-transfer-1.20.8-1.osg35.el7 gratia-probe-debuginfo-1.20.8-1.osg35.el7 gratia-probe-enstore-storage-1.20.8-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.8-1.osg35.el7 gratia-probe-enstore-transfer-1.20.8-1.osg35.el7 gratia-probe-glideinwms-1.20.8-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.8-1.osg35.el7 gratia-probe-hadoop-storage-1.20.8-1.osg35.el7 gratia-probe-htcondor-ce-1.20.8-1.osg35.el7 gratia-probe-lsf-1.20.8-1.osg35.el7 gratia-probe-metric-1.20.8-1.osg35.el7 gratia-probe-onevm-1.20.8-1.osg35.el7 gratia-probe-pbs-lsf-1.20.8-1.osg35.el7 gratia-probe-services-1.20.8-1.osg35.el7 gratia-probe-sge-1.20.8-1.osg35.el7 gratia-probe-slurm-1.20.8-1.osg35.el7 gratia-probe-xrootd-storage-1.20.8-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.8-1.osg35.el7 gridftp-dsi-posix-1.4-2.osg35.el7 gridftp-dsi-posix-debuginfo-1.4-2.osg35.el7 gridftp-hdfs-1.1.1-1.2.osg35.el7 gridftp-hdfs-debuginfo-1.1.1-1.2.osg35.el7 gsi-openssh-7.4p1-2.3.osg35.el7 gsi-openssh-clients-7.4p1-2.3.osg35.el7 gsi-openssh-debuginfo-7.4p1-2.3.osg35.el7 gsi-openssh-server-7.4p1-2.3.osg35.el7 hadoop-0.20-conf-pseudo-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-0.20-mapreduce-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-client-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-conf-pseudo-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-debuginfo-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-doc-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-datanode-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-fuse-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-journalnode-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-namenode-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-nfs3-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-secondarynamenode-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-hdfs-zkfc-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-httpfs-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-kms-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-kms-server-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-libhdfs-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-libhdfs-devel-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-mapreduce-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 hadoop-yarn-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osg35.el7 igtf-ca-certs-1.101-1.osg35.el7 javascriptrrd-1.1.1-1.osg35.el7 lcas-lcmaps-gt4-interface-0.3.1-1.3.osg35.el7 lcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.3.osg35.el7 lcmaps-1.6.6-1.10.osg35.el7 lcmaps-common-devel-1.6.6-1.10.osg35.el7 lcmaps-db-templates-1.6.6-1.10.osg35.el7 lcmaps-debuginfo-1.6.6-1.10.osg35.el7 lcmaps-devel-1.6.6-1.10.osg35.el7 lcmaps-plugins-basic-1.7.0-2.osg35.el7 lcmaps-plugins-basic-debuginfo-1.7.0-2.osg35.el7 lcmaps-plugins-basic-ldap-1.7.0-2.osg35.el7 lcmaps-plugins-verify-proxy-1.5.11-1.1.osg35.el7 lcmaps-plugins-verify-proxy-debuginfo-1.5.11-1.1.osg35.el7 lcmaps-plugins-voms-1.7.1-1.6.osg35.el7 lcmaps-plugins-voms-debuginfo-1.7.1-1.6.osg35.el7 lcmaps-without-gsi-1.6.6-1.10.osg35.el7 lcmaps-without-gsi-devel-1.6.6-1.10.osg35.el7 llrun-0.1.3-1.3.osg35.el7 llrun-debuginfo-0.1.3-1.3.osg35.el7 minicondor-8.8.4-1.8.osg35.el7 myproxy-6.2.4-1.1.osg35.el7 myproxy-admin-6.2.4-1.1.osg35.el7 myproxy-debuginfo-6.2.4-1.1.osg35.el7 myproxy-devel-6.2.4-1.1.osg35.el7 myproxy-doc-6.2.4-1.1.osg35.el7 myproxy-libs-6.2.4-1.1.osg35.el7 myproxy-server-6.2.4-1.1.osg35.el7 myproxy-voms-6.2.4-1.1.osg35.el7 osg-ca-certs-1.83-1.osg35.el7 osg-ca-certs-updater-1.8-1.osg35.el7 osg-ca-scripts-1.2.4-1.osg35.el7 osg-configure-3.0.0-1.osg35.el7 osg-configure-bosco-3.0.0-1.osg35.el7 osg-configure-ce-3.0.0-1.osg35.el7 osg-configure-condor-3.0.0-1.osg35.el7 osg-configure-gateway-3.0.0-1.osg35.el7 osg-configure-gip-3.0.0-1.osg35.el7 osg-configure-gratia-3.0.0-1.osg35.el7 osg-configure-infoservices-3.0.0-1.osg35.el7 osg-configure-lsf-3.0.0-1.osg35.el7 osg-configure-misc-3.0.0-1.osg35.el7 osg-configure-pbs-3.0.0-1.osg35.el7 osg-configure-rsv-3.0.0-1.osg35.el7 osg-configure-sge-3.0.0-1.osg35.el7 osg-configure-siteinfo-3.0.0-1.osg35.el7 osg-configure-slurm-3.0.0-1.osg35.el7 osg-configure-squid-3.0.0-1.osg35.el7 osg-configure-tests-3.0.0-1.osg35.el7 osg-flock-1.1-1.osg35.el7 osg-gridftp-3.5-3.osg35.el7 osg-gridftp-hdfs-3.5-3.osg35.el7 osg-gridftp-xrootd-3.5-3.osg35.el7 osg-oasis-15-2.osg35.el7 osg-pki-tools-3.3.0-1.osg35.el7 osg-release-itb-3.5-2.osg35.el7 osg-se-hadoop-3.5-1.osg35.el7 osg-se-hadoop-client-3.5-1.osg35.el7 osg-se-hadoop-datanode-3.5-1.osg35.el7 osg-se-hadoop-gridftp-3.5-1.osg35.el7 osg-se-hadoop-namenode-3.5-1.osg35.el7 osg-se-hadoop-secondarynamenode-3.5-1.osg35.el7 osg-system-profiler-1.4.3-1.osg35.el7 osg-system-profiler-viewer-1.4.3-1.osg35.el7 osg-update-data-1.4.0-1.osg35.el7 osg-update-vos-1.4.0-1.osg35.el7 osg-wn-client-3.5-1.osg35.el7 osg-xrootd-3.5-3.osg35.el7 osg-xrootd-standalone-3.5-3.osg35.el7 pegasus-4.9.1-1.osg35.el7 pegasus-debuginfo-4.9.1-1.osg35.el7 python2-condor-8.8.4-1.8.osg35.el7 python2-jwt-1.6.1-1.osg35.el7 python2-scitokens-1.2.1-2.osg35.el7 python2-xrootd-4.10.0-1.osg35.el7 python36-jwt-1.6.1-1.osg35.el7 python3-condor-8.8.4-1.8.osg35.el7 python-jwt-1.6.1-1.osg35.el7 python-scitokens-1.2.1-2.osg35.el7 rsv-3.19.8-2.osg35.el7 rsv-consumers-3.19.8-2.osg35.el7 rsv-core-3.19.8-2.osg35.el7 rsv-metrics-3.19.8-2.osg35.el7 scitokens-cpp-0.3.3-1.osg35.el7 scitokens-cpp-debuginfo-0.3.3-1.osg35.el7 scitokens-cpp-devel-0.3.3-1.osg35.el7 stash-cache-1.1.1-1.osg35.el7 stashcache-client-5.2.0-1.osg35.el7 stash-origin-1.1.1-1.osg35.el7 uberftp-2.8-2.1.osg35.el7 uberftp-debuginfo-2.8-2.1.osg35.el7 vo-client-94-1.osg35.el7 vo-client-dcache-94-1.osg35.el7 vo-client-lcmaps-voms-94-1.osg35.el7 voms-2.0.14-1.4.osg35.el7 voms-clients-cpp-2.0.14-1.4.osg35.el7 voms-debuginfo-2.0.14-1.4.osg35.el7 voms-devel-2.0.14-1.4.osg35.el7 voms-doc-2.0.14-1.4.osg35.el7 voms-server-2.0.14-1.4.osg35.el7 vomsxrd-0.6.0-3.osg35.el7 vomsxrd-debuginfo-0.6.0-3.osg35.el7 vomsxrd-devel-0.6.0-3.osg35.el7 xcache-1.1.1-1.osg35.el7 xrootd-4.10.0-1.osg35.el7 xrootd-client-4.10.0-1.osg35.el7 xrootd-client-devel-4.10.0-1.osg35.el7 xrootd-client-libs-4.10.0-1.osg35.el7 xrootd-debuginfo-4.10.0-1.osg35.el7 xrootd-devel-4.10.0-1.osg35.el7 xrootd-doc-4.10.0-1.osg35.el7 xrootd-fuse-4.10.0-1.osg35.el7 xrootd-hdfs-2.1.6-1.osg35.el7 xrootd-hdfs-debuginfo-2.1.6-1.osg35.el7 xrootd-hdfs-devel-2.1.6-1.osg35.el7 xrootd-lcmaps-1.7.4-2.osg35.el7 xrootd-lcmaps-debuginfo-1.7.4-2.osg35.el7 xrootd-libs-4.10.0-1.osg35.el7 xrootd-multiuser-0.4.2-4.osg35.el7 xrootd-multiuser-debuginfo-0.4.2-4.osg35.el7 xrootd-private-devel-4.10.0-1.osg35.el7 xrootd-scitokens-1.0.0-1.1.osg35.el7 xrootd-scitokens-debuginfo-1.0.0-1.1.osg35.el7 xrootd-selinux-4.10.0-1.osg35.el7 xrootd-server-4.10.0-1.osg35.el7 xrootd-server-devel-4.10.0-1.osg35.el7 xrootd-server-libs-4.10.0-1.osg35.el7 zookeeper-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7 zookeeper-debuginfo-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7 zookeeper-native-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7 zookeeper-server-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-0/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.41.bosco-2.osgup.el7 condor-8.9.2-1.4.osgup.el7","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-0/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glite-ce-cream-client-api-c glite-ce-cream-client-devel minicondor osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd python2-condor If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.41.bosco-2.osgup.el7 blahp-debuginfo-1.18.41.bosco-2.osgup.el7 condor-8.9.2-1.4.osgup.el7 condor-all-8.9.2-1.4.osgup.el7 condor-annex-ec2-8.9.2-1.4.osgup.el7 condor-bosco-8.9.2-1.4.osgup.el7 condor-classads-8.9.2-1.4.osgup.el7 condor-classads-devel-8.9.2-1.4.osgup.el7 condor-debuginfo-8.9.2-1.4.osgup.el7 condor-kbdd-8.9.2-1.4.osgup.el7 condor-procd-8.9.2-1.4.osgup.el7 condor-test-8.9.2-1.4.osgup.el7 condor-vm-gahp-8.9.2-1.4.osgup.el7 minicondor-8.9.2-1.4.osgup.el7 python2-condor-8.9.2-1.4.osgup.el7","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-1/","text":"OSG Software Release 3.5.1 \u00b6 Release Date: 2019-09-19 Supported OS Versions: EL7 Summary of changes \u00b6 This release contains: Updated: MyProxy, GSI-OpenSSH, and Globus GridFTP Server Fixed installation failure due to missing globus-usage dependency Removed usage statistics collection and reporting back to developers These JIRA tickets were addressed in this release. Known Issues \u00b6 CVMFS 2.6.2 has a known memory leak when using an /etc/hosts file with lines only containing whitespace ( CVM-1796 ) OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . globus-gridftp-server-13.11-1.1.osg35.el7 gsi-openssh-7.4p1-4.3.osg35.el7 myproxy-6.2.6-1.1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms If you wish to only update the RPMs that changed, the set of RPMs is: globus-gridftp-server-13.11-1.1.osg35.el7 globus-gridftp-server-debuginfo-13.11-1.1.osg35.el7 globus-gridftp-server-devel-13.11-1.1.osg35.el7 globus-gridftp-server-progs-13.11-1.1.osg35.el7 gsi-openssh-7.4p1-4.3.osg35.el7 gsi-openssh-clients-7.4p1-4.3.osg35.el7 gsi-openssh-debuginfo-7.4p1-4.3.osg35.el7 gsi-openssh-server-7.4p1-4.3.osg35.el7 myproxy-6.2.6-1.1.osg35.el7 myproxy-admin-6.2.6-1.1.osg35.el7 myproxy-debuginfo-6.2.6-1.1.osg35.el7 myproxy-devel-6.2.6-1.1.osg35.el7 myproxy-doc-6.2.6-1.1.osg35.el7 myproxy-libs-6.2.6-1.1.osg35.el7 myproxy-server-6.2.6-1.1.osg35.el7 myproxy-voms-6.2.6-1.1.osg35.el7","title":"Release 3 5 1"},{"location":"release/3.5/release-3-5-1/#osg-software-release-351","text":"Release Date: 2019-09-19 Supported OS Versions: EL7","title":"OSG Software Release 3.5.1"},{"location":"release/3.5/release-3-5-1/#summary-of-changes","text":"This release contains: Updated: MyProxy, GSI-OpenSSH, and Globus GridFTP Server Fixed installation failure due to missing globus-usage dependency Removed usage statistics collection and reporting back to developers These JIRA tickets were addressed in this release.","title":"Summary of changes"},{"location":"release/3.5/release-3-5-1/#known-issues","text":"CVMFS 2.6.2 has a known memory leak when using an /etc/hosts file with lines only containing whitespace ( CVM-1796 ) OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-1/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-1/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-1/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-1/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . globus-gridftp-server-13.11-1.1.osg35.el7 gsi-openssh-7.4p1-4.3.osg35.el7 myproxy-6.2.6-1.1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-1/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms If you wish to only update the RPMs that changed, the set of RPMs is: globus-gridftp-server-13.11-1.1.osg35.el7 globus-gridftp-server-debuginfo-13.11-1.1.osg35.el7 globus-gridftp-server-devel-13.11-1.1.osg35.el7 globus-gridftp-server-progs-13.11-1.1.osg35.el7 gsi-openssh-7.4p1-4.3.osg35.el7 gsi-openssh-clients-7.4p1-4.3.osg35.el7 gsi-openssh-debuginfo-7.4p1-4.3.osg35.el7 gsi-openssh-server-7.4p1-4.3.osg35.el7 myproxy-6.2.6-1.1.osg35.el7 myproxy-admin-6.2.6-1.1.osg35.el7 myproxy-debuginfo-6.2.6-1.1.osg35.el7 myproxy-devel-6.2.6-1.1.osg35.el7 myproxy-doc-6.2.6-1.1.osg35.el7 myproxy-libs-6.2.6-1.1.osg35.el7 myproxy-server-6.2.6-1.1.osg35.el7 myproxy-voms-6.2.6-1.1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-10-2/","text":"OSG Data Release 3.5.10-2 \u00b6 Release Date: 2020-03-04 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains VO Package v100 : Add new cert for GLOW (SOFTWARE-4006) Replace one of the certs for OSG (SOFTWARE-4007) Update voms2.fnal.gov DN for DES, DUNE, Fermilab (SOFTWARE-4012) Map FQANs from Fermilab VO subgroups to the same user as the VO-wide target (SOFTWARE-4005) Drop CDF (SOFTWARE-4012) Drop MIS VO (SOFTWARE-3575) These JIRA tickets were addressed in this release. Containers \u00b6 The latest stable Hosted CE container has been updated to this release. Worker Node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . vo-client-100-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: vo-client-100-1.osg35.el7 vo-client-dcache-100-1.osg35.el7 vo-client-lcmaps-voms-100-1.osg35.el7","title":"Release 3 5 10 2"},{"location":"release/3.5/release-3-5-10-2/#osg-data-release-3510-2","text":"Release Date: 2020-03-04 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.10-2"},{"location":"release/3.5/release-3-5-10-2/#summary-of-changes","text":"This release contains VO Package v100 : Add new cert for GLOW (SOFTWARE-4006) Replace one of the certs for OSG (SOFTWARE-4007) Update voms2.fnal.gov DN for DES, DUNE, Fermilab (SOFTWARE-4012) Map FQANs from Fermilab VO subgroups to the same user as the VO-wide target (SOFTWARE-4005) Drop CDF (SOFTWARE-4012) Drop MIS VO (SOFTWARE-3575) These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-10-2/#containers","text":"The latest stable Hosted CE container has been updated to this release. Worker Node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-10-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-10-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-10-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-10-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . vo-client-100-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-10-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: vo-client-100-1.osg35.el7 vo-client-dcache-100-1.osg35.el7 vo-client-lcmaps-voms-100-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-10/","text":"OSG Software Release 3.5.10 \u00b6 Release Date: 2020-02-20 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: XRootD 4.11.2 : Bug fix release XCache 1.2.1: Fixed problem where plugins were applied to redirectors VO Package v99 : New certificate DN for HCC VOMS UberFTP 2.8-3: OSG fixes incorporated upstream osg-configure 3.1.1: Minor fixes osg-system-profiler 1.5.0: Report XRootD configuration Elide kernel RPM checks These JIRA tickets were addressed in this release. Containers \u00b6 The StashCache container is available and has been tagged as stable in accordance with our Container Release Policy StashCache Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . osg-configure-3.1.1-1.osg35.el7 osg-system-profiler-1.5.0-1.osg35.el7 uberftp-2.8-3.osg35.el7 vo-client-99-1.osg35.el7 xcache-1.2.1-1.osg35.el7 xrootd-4.11.2-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cms-xcache osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-misc osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-siteinfo osg-configure-slurm osg-configure-squid osg-configure-tests osg-system-profiler osg-system-profiler-viewer python2-xrootd stash-cache stash-origin uberftp uberftp-debuginfo vo-client vo-client-dcache vo-client-lcmaps-voms xcache xcache-redirector xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: atlas-xcache-1.2.1-1.osg35.el7 cms-xcache-1.2.1-1.osg35.el7 osg-configure-3.1.1-1.osg35.el7 osg-configure-bosco-3.1.1-1.osg35.el7 osg-configure-ce-3.1.1-1.osg35.el7 osg-configure-condor-3.1.1-1.osg35.el7 osg-configure-gateway-3.1.1-1.osg35.el7 osg-configure-gip-3.1.1-1.osg35.el7 osg-configure-gratia-3.1.1-1.osg35.el7 osg-configure-infoservices-3.1.1-1.osg35.el7 osg-configure-lsf-3.1.1-1.osg35.el7 osg-configure-misc-3.1.1-1.osg35.el7 osg-configure-pbs-3.1.1-1.osg35.el7 osg-configure-rsv-3.1.1-1.osg35.el7 osg-configure-sge-3.1.1-1.osg35.el7 osg-configure-siteinfo-3.1.1-1.osg35.el7 osg-configure-slurm-3.1.1-1.osg35.el7 osg-configure-squid-3.1.1-1.osg35.el7 osg-configure-tests-3.1.1-1.osg35.el7 osg-system-profiler-1.5.0-1.osg35.el7 osg-system-profiler-viewer-1.5.0-1.osg35.el7 python2-xrootd-4.11.2-1.osg35.el7 stash-cache-1.2.1-1.osg35.el7 stash-origin-1.2.1-1.osg35.el7 uberftp-2.8-3.osg35.el7 uberftp-debuginfo-2.8-3.osg35.el7 vo-client-99-1.osg35.el7 vo-client-dcache-99-1.osg35.el7 vo-client-lcmaps-voms-99-1.osg35.el7 xcache-1.2.1-1.osg35.el7 xcache-redirector-1.2.1-1.osg35.el7 xrootd-4.11.2-1.osg35.el7 xrootd-client-4.11.2-1.osg35.el7 xrootd-client-devel-4.11.2-1.osg35.el7 xrootd-client-libs-4.11.2-1.osg35.el7 xrootd-debuginfo-4.11.2-1.osg35.el7 xrootd-devel-4.11.2-1.osg35.el7 xrootd-doc-4.11.2-1.osg35.el7 xrootd-fuse-4.11.2-1.osg35.el7 xrootd-libs-4.11.2-1.osg35.el7 xrootd-private-devel-4.11.2-1.osg35.el7 xrootd-selinux-4.11.2-1.osg35.el7 xrootd-server-4.11.2-1.osg35.el7 xrootd-server-devel-4.11.2-1.osg35.el7 xrootd-server-libs-4.11.2-1.osg35.el7","title":"Release 3 5 10"},{"location":"release/3.5/release-3-5-10/#osg-software-release-3510","text":"Release Date: 2020-02-20 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.10"},{"location":"release/3.5/release-3-5-10/#summary-of-changes","text":"This release contains: XRootD 4.11.2 : Bug fix release XCache 1.2.1: Fixed problem where plugins were applied to redirectors VO Package v99 : New certificate DN for HCC VOMS UberFTP 2.8-3: OSG fixes incorporated upstream osg-configure 3.1.1: Minor fixes osg-system-profiler 1.5.0: Report XRootD configuration Elide kernel RPM checks These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-10/#containers","text":"The StashCache container is available and has been tagged as stable in accordance with our Container Release Policy StashCache","title":"Containers"},{"location":"release/3.5/release-3-5-10/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-10/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-10/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-10/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . osg-configure-3.1.1-1.osg35.el7 osg-system-profiler-1.5.0-1.osg35.el7 uberftp-2.8-3.osg35.el7 vo-client-99-1.osg35.el7 xcache-1.2.1-1.osg35.el7 xrootd-4.11.2-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-10/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cms-xcache osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-misc osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-siteinfo osg-configure-slurm osg-configure-squid osg-configure-tests osg-system-profiler osg-system-profiler-viewer python2-xrootd stash-cache stash-origin uberftp uberftp-debuginfo vo-client vo-client-dcache vo-client-lcmaps-voms xcache xcache-redirector xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: atlas-xcache-1.2.1-1.osg35.el7 cms-xcache-1.2.1-1.osg35.el7 osg-configure-3.1.1-1.osg35.el7 osg-configure-bosco-3.1.1-1.osg35.el7 osg-configure-ce-3.1.1-1.osg35.el7 osg-configure-condor-3.1.1-1.osg35.el7 osg-configure-gateway-3.1.1-1.osg35.el7 osg-configure-gip-3.1.1-1.osg35.el7 osg-configure-gratia-3.1.1-1.osg35.el7 osg-configure-infoservices-3.1.1-1.osg35.el7 osg-configure-lsf-3.1.1-1.osg35.el7 osg-configure-misc-3.1.1-1.osg35.el7 osg-configure-pbs-3.1.1-1.osg35.el7 osg-configure-rsv-3.1.1-1.osg35.el7 osg-configure-sge-3.1.1-1.osg35.el7 osg-configure-siteinfo-3.1.1-1.osg35.el7 osg-configure-slurm-3.1.1-1.osg35.el7 osg-configure-squid-3.1.1-1.osg35.el7 osg-configure-tests-3.1.1-1.osg35.el7 osg-system-profiler-1.5.0-1.osg35.el7 osg-system-profiler-viewer-1.5.0-1.osg35.el7 python2-xrootd-4.11.2-1.osg35.el7 stash-cache-1.2.1-1.osg35.el7 stash-origin-1.2.1-1.osg35.el7 uberftp-2.8-3.osg35.el7 uberftp-debuginfo-2.8-3.osg35.el7 vo-client-99-1.osg35.el7 vo-client-dcache-99-1.osg35.el7 vo-client-lcmaps-voms-99-1.osg35.el7 xcache-1.2.1-1.osg35.el7 xcache-redirector-1.2.1-1.osg35.el7 xrootd-4.11.2-1.osg35.el7 xrootd-client-4.11.2-1.osg35.el7 xrootd-client-devel-4.11.2-1.osg35.el7 xrootd-client-libs-4.11.2-1.osg35.el7 xrootd-debuginfo-4.11.2-1.osg35.el7 xrootd-devel-4.11.2-1.osg35.el7 xrootd-doc-4.11.2-1.osg35.el7 xrootd-fuse-4.11.2-1.osg35.el7 xrootd-libs-4.11.2-1.osg35.el7 xrootd-private-devel-4.11.2-1.osg35.el7 xrootd-selinux-4.11.2-1.osg35.el7 xrootd-server-4.11.2-1.osg35.el7 xrootd-server-devel-4.11.2-1.osg35.el7 xrootd-server-libs-4.11.2-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-11/","text":"OSG Software Release 3.5.11 \u00b6 Release Date: 2020-03-11 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: CVMFS 2.7.1 : Bug Fixes and Improvements Fix client fail-over for redirected stratum 1 sources Add server configuration to support MaxMind GeoDB license requirements oidc-agent 3.3.1 : Update from version 3.2.6 Add support to request tokens with specific audience CCTools 7.0.22 : Bug fix release GSI-OpenSSH 7.4p1-5: Bring up to date with EPEL VO Package v101 Update LZ VOMS server DN Add new GlueX VO DN Retire DOSAR These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.0.22-1.osg35.el7 cvmfs-2.7.1-1.osg35.el7 gsi-openssh-7.4p1-5.1.osg35.el7 oidc-agent-3.3.1-1.1.osg35.el7 osg-oasis-16-2.osg35.el7 vo-client-101-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server oidc-agent oidc-agent-debuginfo osg-oasis vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: cctools-7.0.22-1.osg35.el7 cctools-debuginfo-7.0.22-1.osg35.el7 cctools-devel-7.0.22-1.osg35.el7 cvmfs-2.7.1-1.osg35.el7 cvmfs-devel-2.7.1-1.osg35.el7 cvmfs-ducc-2.7.1-1.osg35.el7 cvmfs-fuse3-2.7.1-1.osg35.el7 cvmfs-server-2.7.1-1.osg35.el7 cvmfs-shrinkwrap-2.7.1-1.osg35.el7 cvmfs-unittests-2.7.1-1.osg35.el7 gsi-openssh-7.4p1-5.1.osg35.el7 gsi-openssh-clients-7.4p1-5.1.osg35.el7 gsi-openssh-debuginfo-7.4p1-5.1.osg35.el7 gsi-openssh-server-7.4p1-5.1.osg35.el7 oidc-agent-3.3.1-1.1.osg35.el7 oidc-agent-debuginfo-3.3.1-1.1.osg35.el7 osg-oasis-16-2.osg35.el7 vo-client-101-1.osg35.el7 vo-client-dcache-101-1.osg35.el7 vo-client-lcmaps-voms-101-1.osg35.el7","title":"Release 3 5 11"},{"location":"release/3.5/release-3-5-11/#osg-software-release-3511","text":"Release Date: 2020-03-11 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.11"},{"location":"release/3.5/release-3-5-11/#summary-of-changes","text":"This release contains: CVMFS 2.7.1 : Bug Fixes and Improvements Fix client fail-over for redirected stratum 1 sources Add server configuration to support MaxMind GeoDB license requirements oidc-agent 3.3.1 : Update from version 3.2.6 Add support to request tokens with specific audience CCTools 7.0.22 : Bug fix release GSI-OpenSSH 7.4p1-5: Bring up to date with EPEL VO Package v101 Update LZ VOMS server DN Add new GlueX VO DN Retire DOSAR These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-11/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-11/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-11/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-11/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-11/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.0.22-1.osg35.el7 cvmfs-2.7.1-1.osg35.el7 gsi-openssh-7.4p1-5.1.osg35.el7 oidc-agent-3.3.1-1.1.osg35.el7 osg-oasis-16-2.osg35.el7 vo-client-101-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-11/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server oidc-agent oidc-agent-debuginfo osg-oasis vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: cctools-7.0.22-1.osg35.el7 cctools-debuginfo-7.0.22-1.osg35.el7 cctools-devel-7.0.22-1.osg35.el7 cvmfs-2.7.1-1.osg35.el7 cvmfs-devel-2.7.1-1.osg35.el7 cvmfs-ducc-2.7.1-1.osg35.el7 cvmfs-fuse3-2.7.1-1.osg35.el7 cvmfs-server-2.7.1-1.osg35.el7 cvmfs-shrinkwrap-2.7.1-1.osg35.el7 cvmfs-unittests-2.7.1-1.osg35.el7 gsi-openssh-7.4p1-5.1.osg35.el7 gsi-openssh-clients-7.4p1-5.1.osg35.el7 gsi-openssh-debuginfo-7.4p1-5.1.osg35.el7 gsi-openssh-server-7.4p1-5.1.osg35.el7 oidc-agent-3.3.1-1.1.osg35.el7 oidc-agent-debuginfo-3.3.1-1.1.osg35.el7 osg-oasis-16-2.osg35.el7 vo-client-101-1.osg35.el7 vo-client-dcache-101-1.osg35.el7 vo-client-lcmaps-voms-101-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-12/","text":"OSG Software Release 3.5.12 \u00b6 Release Date: 2020-03-26 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: XRootD-SciTokens 1.2.0 : Fixes authorization logic error ( Security announcement ) These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . xrootd-scitokens-1.2.0-2.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: xrootd-scitokens xrootd-scitokens-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is: xrootd-scitokens-1.2.0-2.osg35.el7 xrootd-scitokens-debuginfo-1.2.0-2.osg35.el7","title":"Release 3 5 12"},{"location":"release/3.5/release-3-5-12/#osg-software-release-3512","text":"Release Date: 2020-03-26 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.12"},{"location":"release/3.5/release-3-5-12/#summary-of-changes","text":"This release contains: XRootD-SciTokens 1.2.0 : Fixes authorization logic error ( Security announcement ) These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-12/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-12/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-12/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-12/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-12/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . xrootd-scitokens-1.2.0-2.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-12/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: xrootd-scitokens xrootd-scitokens-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is: xrootd-scitokens-1.2.0-2.osg35.el7 xrootd-scitokens-debuginfo-1.2.0-2.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-13/","text":"OSG Software Release 3.5.13 \u00b6 Release Date: 2020-04-02 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: GlideinWMS 3.6.2 Add a portable condor_chirp for user jobs running in the glideins Adding GPU monitor as default for pilots Reduce number of queries from Frontend to User collector Bug fix: Incorrect CERTIFICATE_MAPFILE used when using HTCondor python binding (failed schedd authentication) Bug fix: Broken submission to GCE and AWS Updated CA certificates based on IGTF 1.105 : Discontinued CERN-LCG-IOTA-CA following decommissioning by authority (CERN) Added new G4 intermediates for the GEANT TCS service and supporting self-signed USERTrust RSA Root (EU) Updated AddTrust External CA Root signing policy to support legacy UTN chains for GEANT TCS G4 (EU) HTCondor-CE 4.2.1 : Advertise to the central collector via SSL Pegasus 4.9.3 : Update from 4.9.1 Support for getting images from Singularity Library Updates from Pegasus 4.9.2 globus-gridftp-server-13.20-1.1: Fix transfer logging LCMAPS 1.6.6-1.12: Use VOMS mappings by default scitokens-cpp 0.5.0: Added API to retrieve string lists These JIRA tickets were addressed in this release. Containers \u00b6 The Hosted CE container is available and has been tagged as stable in accordance with our Container Release Policy Hosted CE The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . glideinwms-3.6.2-1.osg35.el7 globus-gridftp-server-13.20-1.1.osg35.el7 htcondor-ce-4.2.1-1.osg35.el7 igtf-ca-certs-1.105-1.osg35.el7 lcmaps-1.6.6-1.12.osg35.el7 osg-ca-certs-1.87-1.osg35.el7 pegasus-4.9.3-1.2.osg35.el7 scitokens-cpp-0.5.0-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-without-gsi lcmaps-without-gsi-devel osg-ca-certs pegasus pegasus-debuginfo scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel If you wish to only update the RPMs that changed, the set of RPMs is: glideinwms-3.6.2-1.osg35.el7 glideinwms-common-tools-3.6.2-1.osg35.el7 glideinwms-condor-common-config-3.6.2-1.osg35.el7 glideinwms-factory-3.6.2-1.osg35.el7 glideinwms-factory-condor-3.6.2-1.osg35.el7 glideinwms-glidecondor-tools-3.6.2-1.osg35.el7 glideinwms-libs-3.6.2-1.osg35.el7 glideinwms-minimal-condor-3.6.2-1.osg35.el7 glideinwms-usercollector-3.6.2-1.osg35.el7 glideinwms-userschedd-3.6.2-1.osg35.el7 glideinwms-vofrontend-3.6.2-1.osg35.el7 glideinwms-vofrontend-standalone-3.6.2-1.osg35.el7 globus-gridftp-server-13.20-1.1.osg35.el7 globus-gridftp-server-debuginfo-13.20-1.1.osg35.el7 globus-gridftp-server-devel-13.20-1.1.osg35.el7 globus-gridftp-server-progs-13.20-1.1.osg35.el7 htcondor-ce-4.2.1-1.osg35.el7 htcondor-ce-bosco-4.2.1-1.osg35.el7 htcondor-ce-client-4.2.1-1.osg35.el7 htcondor-ce-collector-4.2.1-1.osg35.el7 htcondor-ce-condor-4.2.1-1.osg35.el7 htcondor-ce-lsf-4.2.1-1.osg35.el7 htcondor-ce-pbs-4.2.1-1.osg35.el7 htcondor-ce-sge-4.2.1-1.osg35.el7 htcondor-ce-slurm-4.2.1-1.osg35.el7 htcondor-ce-view-4.2.1-1.osg35.el7 igtf-ca-certs-1.105-1.osg35.el7 lcmaps-1.6.6-1.12.osg35.el7 lcmaps-common-devel-1.6.6-1.12.osg35.el7 lcmaps-db-templates-1.6.6-1.12.osg35.el7 lcmaps-debuginfo-1.6.6-1.12.osg35.el7 lcmaps-devel-1.6.6-1.12.osg35.el7 lcmaps-without-gsi-1.6.6-1.12.osg35.el7 lcmaps-without-gsi-devel-1.6.6-1.12.osg35.el7 osg-ca-certs-1.87-1.osg35.el7 pegasus-4.9.3-1.2.osg35.el7 pegasus-debuginfo-4.9.3-1.2.osg35.el7 scitokens-cpp-0.5.0-1.osg35.el7 scitokens-cpp-debuginfo-0.5.0-1.osg35.el7 scitokens-cpp-devel-0.5.0-1.osg35.el7","title":"Release 3 5 13"},{"location":"release/3.5/release-3-5-13/#osg-software-release-3513","text":"Release Date: 2020-04-02 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.13"},{"location":"release/3.5/release-3-5-13/#summary-of-changes","text":"This release contains: GlideinWMS 3.6.2 Add a portable condor_chirp for user jobs running in the glideins Adding GPU monitor as default for pilots Reduce number of queries from Frontend to User collector Bug fix: Incorrect CERTIFICATE_MAPFILE used when using HTCondor python binding (failed schedd authentication) Bug fix: Broken submission to GCE and AWS Updated CA certificates based on IGTF 1.105 : Discontinued CERN-LCG-IOTA-CA following decommissioning by authority (CERN) Added new G4 intermediates for the GEANT TCS service and supporting self-signed USERTrust RSA Root (EU) Updated AddTrust External CA Root signing policy to support legacy UTN chains for GEANT TCS G4 (EU) HTCondor-CE 4.2.1 : Advertise to the central collector via SSL Pegasus 4.9.3 : Update from 4.9.1 Support for getting images from Singularity Library Updates from Pegasus 4.9.2 globus-gridftp-server-13.20-1.1: Fix transfer logging LCMAPS 1.6.6-1.12: Use VOMS mappings by default scitokens-cpp 0.5.0: Added API to retrieve string lists These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-13/#containers","text":"The Hosted CE container is available and has been tagged as stable in accordance with our Container Release Policy Hosted CE The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-13/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-13/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-13/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-13/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . glideinwms-3.6.2-1.osg35.el7 globus-gridftp-server-13.20-1.1.osg35.el7 htcondor-ce-4.2.1-1.osg35.el7 igtf-ca-certs-1.105-1.osg35.el7 lcmaps-1.6.6-1.12.osg35.el7 osg-ca-certs-1.87-1.osg35.el7 pegasus-4.9.3-1.2.osg35.el7 scitokens-cpp-0.5.0-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-13/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-without-gsi lcmaps-without-gsi-devel osg-ca-certs pegasus pegasus-debuginfo scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel If you wish to only update the RPMs that changed, the set of RPMs is: glideinwms-3.6.2-1.osg35.el7 glideinwms-common-tools-3.6.2-1.osg35.el7 glideinwms-condor-common-config-3.6.2-1.osg35.el7 glideinwms-factory-3.6.2-1.osg35.el7 glideinwms-factory-condor-3.6.2-1.osg35.el7 glideinwms-glidecondor-tools-3.6.2-1.osg35.el7 glideinwms-libs-3.6.2-1.osg35.el7 glideinwms-minimal-condor-3.6.2-1.osg35.el7 glideinwms-usercollector-3.6.2-1.osg35.el7 glideinwms-userschedd-3.6.2-1.osg35.el7 glideinwms-vofrontend-3.6.2-1.osg35.el7 glideinwms-vofrontend-standalone-3.6.2-1.osg35.el7 globus-gridftp-server-13.20-1.1.osg35.el7 globus-gridftp-server-debuginfo-13.20-1.1.osg35.el7 globus-gridftp-server-devel-13.20-1.1.osg35.el7 globus-gridftp-server-progs-13.20-1.1.osg35.el7 htcondor-ce-4.2.1-1.osg35.el7 htcondor-ce-bosco-4.2.1-1.osg35.el7 htcondor-ce-client-4.2.1-1.osg35.el7 htcondor-ce-collector-4.2.1-1.osg35.el7 htcondor-ce-condor-4.2.1-1.osg35.el7 htcondor-ce-lsf-4.2.1-1.osg35.el7 htcondor-ce-pbs-4.2.1-1.osg35.el7 htcondor-ce-sge-4.2.1-1.osg35.el7 htcondor-ce-slurm-4.2.1-1.osg35.el7 htcondor-ce-view-4.2.1-1.osg35.el7 igtf-ca-certs-1.105-1.osg35.el7 lcmaps-1.6.6-1.12.osg35.el7 lcmaps-common-devel-1.6.6-1.12.osg35.el7 lcmaps-db-templates-1.6.6-1.12.osg35.el7 lcmaps-debuginfo-1.6.6-1.12.osg35.el7 lcmaps-devel-1.6.6-1.12.osg35.el7 lcmaps-without-gsi-1.6.6-1.12.osg35.el7 lcmaps-without-gsi-devel-1.6.6-1.12.osg35.el7 osg-ca-certs-1.87-1.osg35.el7 pegasus-4.9.3-1.2.osg35.el7 pegasus-debuginfo-4.9.3-1.2.osg35.el7 scitokens-cpp-0.5.0-1.osg35.el7 scitokens-cpp-debuginfo-0.5.0-1.osg35.el7 scitokens-cpp-devel-0.5.0-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-14/","text":"OSG Software Release 3.5.14 \u00b6 Release Date: 2020-04-02 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor 8.8.8 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2020-0001 HTCONDOR-2020-0002 HTCONDOR-2020-0003 HTCONDOR-2020-0004 Upcoming Repository: HTCondor 8.9.6 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2020-0001 HTCONDOR-2020-0002 HTCONDOR-2020-0003 HTCONDOR-2020-0004 Affected nodes These updates affect submit and execute hosts. Please update your submit host first and then your execute nodes. Don't forget to update your HTCondor CE. If you are upgrading from HTCondor 8.6.x, please note that configuration changes may be necessary when updating to HTCondor 8.8.8 These JIRA tickets were addressed in this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.8-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp GenericError: Invalid tagInfo: If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.8-1.osg35.el7 condor-all-8.8.8-1.osg35.el7 condor-annex-ec2-8.8.8-1.osg35.el7 condor-bosco-8.8.8-1.osg35.el7 condor-classads-8.8.8-1.osg35.el7 condor-classads-devel-8.8.8-1.osg35.el7 condor-debuginfo-8.8.8-1.osg35.el7 condor-kbdd-8.8.8-1.osg35.el7 condor-procd-8.8.8-1.osg35.el7 condor-test-8.8.8-1.osg35.el7 condor-vm-gahp-8.8.8-1.osg35.el7 minicondor-8.8.8-1.osg35.el7 python2-condor-8.8.8-1.osg35.el7 python3-condor-8.8.8-1.osg35.el7 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. condor-8.9.6-1.osgup.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.9.6-1.osgup.el7 condor-all-8.9.6-1.osgup.el7 condor-annex-ec2-8.9.6-1.osgup.el7 condor-bosco-8.9.6-1.osgup.el7 condor-classads-8.9.6-1.osgup.el7 condor-classads-devel-8.9.6-1.osgup.el7 condor-debuginfo-8.9.6-1.osgup.el7 condor-kbdd-8.9.6-1.osgup.el7 condor-procd-8.9.6-1.osgup.el7 condor-test-8.9.6-1.osgup.el7 condor-vm-gahp-8.9.6-1.osgup.el7 minicondor-8.9.6-1.osgup.el7 python2-condor-8.9.6-1.osgup.el7 python3-condor-8.9.6-1.osgup.el7","title":"Release 3 5 14"},{"location":"release/3.5/release-3-5-14/#osg-software-release-3514","text":"Release Date: 2020-04-02 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.14"},{"location":"release/3.5/release-3-5-14/#summary-of-changes","text":"This release contains: HTCondor 8.8.8 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2020-0001 HTCONDOR-2020-0002 HTCONDOR-2020-0003 HTCONDOR-2020-0004 Upcoming Repository: HTCondor 8.9.6 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2020-0001 HTCONDOR-2020-0002 HTCONDOR-2020-0003 HTCONDOR-2020-0004 Affected nodes These updates affect submit and execute hosts. Please update your submit host first and then your execute nodes. Don't forget to update your HTCondor CE. If you are upgrading from HTCondor 8.6.x, please note that configuration changes may be necessary when updating to HTCondor 8.8.8 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-14/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-14/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-14/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-14/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.8-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-14/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp GenericError: Invalid tagInfo: If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.8-1.osg35.el7 condor-all-8.8.8-1.osg35.el7 condor-annex-ec2-8.8.8-1.osg35.el7 condor-bosco-8.8.8-1.osg35.el7 condor-classads-8.8.8-1.osg35.el7 condor-classads-devel-8.8.8-1.osg35.el7 condor-debuginfo-8.8.8-1.osg35.el7 condor-kbdd-8.8.8-1.osg35.el7 condor-procd-8.8.8-1.osg35.el7 condor-test-8.8.8-1.osg35.el7 condor-vm-gahp-8.8.8-1.osg35.el7 minicondor-8.8.8-1.osg35.el7 python2-condor-8.8.8-1.osg35.el7 python3-condor-8.8.8-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-14/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. condor-8.9.6-1.osgup.el7","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-14/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.9.6-1.osgup.el7 condor-all-8.9.6-1.osgup.el7 condor-annex-ec2-8.9.6-1.osgup.el7 condor-bosco-8.9.6-1.osgup.el7 condor-classads-8.9.6-1.osgup.el7 condor-classads-devel-8.9.6-1.osgup.el7 condor-debuginfo-8.9.6-1.osgup.el7 condor-kbdd-8.9.6-1.osgup.el7 condor-procd-8.9.6-1.osgup.el7 condor-test-8.9.6-1.osgup.el7 condor-vm-gahp-8.9.6-1.osgup.el7 minicondor-8.9.6-1.osgup.el7 python2-condor-8.9.6-1.osgup.el7 python3-condor-8.9.6-1.osgup.el7","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-15-2/","text":"OSG Data Release 3.5.15-2 \u00b6 Release Date: 2020-04-15 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains VO Package v104 : Update to the new certificate for GlueX VOMS These JIRA tickets were addressed in this release. Containers \u00b6 Worker Node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . vo-client-104-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: vo-client-104-1.osg35.el7 vo-client-dcache-104-1.osg35.el7 vo-client-lcmaps-voms-104-1.osg35.el7","title":"Release 3 5 15 2"},{"location":"release/3.5/release-3-5-15-2/#osg-data-release-3515-2","text":"Release Date: 2020-04-15 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.15-2"},{"location":"release/3.5/release-3-5-15-2/#summary-of-changes","text":"This release contains VO Package v104 : Update to the new certificate for GlueX VOMS These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-15-2/#containers","text":"Worker Node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-15-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-15-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-15-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-15-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . vo-client-104-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-15-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: vo-client-104-1.osg35.el7 vo-client-dcache-104-1.osg35.el7 vo-client-lcmaps-voms-104-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-15-3/","text":"OSG Data Release 3.5.15-3 \u00b6 Release Date: 2020-05-06 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains CA Certificates based on IGTF 1.106 Removed expiring AddTrust External CA Root (US) Updated legacy DutchGrid (Nikhef MS) Root CA (NL) Removed discontinued NCSA-tfca-2013 CA (US) Added TCS G4 ECC trust anchors to experimental area (EU) These JIRA tickets were addressed in this release. Containers \u00b6 Worker Node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . igtf-ca-certs-1.106-1.osg35.el7 osg-ca-certs-1.88-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: igtf-ca-certs-1.106-1.osg35.el7 osg-ca-certs-1.88-1.osg35.el7","title":"Release 3 5 15 3"},{"location":"release/3.5/release-3-5-15-3/#osg-data-release-3515-3","text":"Release Date: 2020-05-06 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.15-3"},{"location":"release/3.5/release-3-5-15-3/#summary-of-changes","text":"This release contains CA Certificates based on IGTF 1.106 Removed expiring AddTrust External CA Root (US) Updated legacy DutchGrid (Nikhef MS) Root CA (NL) Removed discontinued NCSA-tfca-2013 CA (US) Added TCS G4 ECC trust anchors to experimental area (EU) These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-15-3/#containers","text":"Worker Node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-15-3/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-15-3/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-15-3/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-15-3/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . igtf-ca-certs-1.106-1.osg35.el7 osg-ca-certs-1.88-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-15-3/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: igtf-ca-certs-1.106-1.osg35.el7 osg-ca-certs-1.88-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-15/","text":"OSG Software Release 3.5.15 \u00b6 Release Date: 2020-04-08 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Frontier Squid 4.10.3 : Bug fix for negative caching VO Package v103 Added WLCG VOMS Updated certificate for voms1.fnal.gov XRootD 4.11.3 : Bug fix release osg-xrootd 3.5-12: The standalone configuration file only affects the standalone XRootD process These JIRA tickets were addressed in this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . frontier-squid-4.10-3.1.osg35.el7 osg-xrootd-3.5-12.osg35.el7 vo-client-103-1.osg35.el7 xrootd-4.11.3-1.2.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: frontier-squid frontier-squid-debuginfo osg-xrootd osg-xrootd-standalone python2-xrootd vo-client vo-client-dcache vo-client-lcmaps-voms xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: frontier-squid-4.10-3.1.osg35.el7 frontier-squid-debuginfo-4.10-3.1.osg35.el7 osg-xrootd-3.5-12.osg35.el7 osg-xrootd-standalone-3.5-12.osg35.el7 python2-xrootd-4.11.3-1.2.osg35.el7 vo-client-103-1.osg35.el7 vo-client-dcache-103-1.osg35.el7 vo-client-lcmaps-voms-103-1.osg35.el7 xrootd-4.11.3-1.2.osg35.el7 xrootd-client-4.11.3-1.2.osg35.el7 xrootd-client-devel-4.11.3-1.2.osg35.el7 xrootd-client-libs-4.11.3-1.2.osg35.el7 xrootd-debuginfo-4.11.3-1.2.osg35.el7 xrootd-devel-4.11.3-1.2.osg35.el7 xrootd-doc-4.11.3-1.2.osg35.el7 xrootd-fuse-4.11.3-1.2.osg35.el7 xrootd-libs-4.11.3-1.2.osg35.el7 xrootd-private-devel-4.11.3-1.2.osg35.el7 xrootd-selinux-4.11.3-1.2.osg35.el7 xrootd-server-4.11.3-1.2.osg35.el7 xrootd-server-devel-4.11.3-1.2.osg35.el7 xrootd-server-libs-4.11.3-1.2.osg35.el7","title":"Release 3 5 15"},{"location":"release/3.5/release-3-5-15/#osg-software-release-3515","text":"Release Date: 2020-04-08 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.15"},{"location":"release/3.5/release-3-5-15/#summary-of-changes","text":"This release contains: Frontier Squid 4.10.3 : Bug fix for negative caching VO Package v103 Added WLCG VOMS Updated certificate for voms1.fnal.gov XRootD 4.11.3 : Bug fix release osg-xrootd 3.5-12: The standalone configuration file only affects the standalone XRootD process These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-15/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-15/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-15/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-15/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . frontier-squid-4.10-3.1.osg35.el7 osg-xrootd-3.5-12.osg35.el7 vo-client-103-1.osg35.el7 xrootd-4.11.3-1.2.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-15/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: frontier-squid frontier-squid-debuginfo osg-xrootd osg-xrootd-standalone python2-xrootd vo-client vo-client-dcache vo-client-lcmaps-voms xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: frontier-squid-4.10-3.1.osg35.el7 frontier-squid-debuginfo-4.10-3.1.osg35.el7 osg-xrootd-3.5-12.osg35.el7 osg-xrootd-standalone-3.5-12.osg35.el7 python2-xrootd-4.11.3-1.2.osg35.el7 vo-client-103-1.osg35.el7 vo-client-dcache-103-1.osg35.el7 vo-client-lcmaps-voms-103-1.osg35.el7 xrootd-4.11.3-1.2.osg35.el7 xrootd-client-4.11.3-1.2.osg35.el7 xrootd-client-devel-4.11.3-1.2.osg35.el7 xrootd-client-libs-4.11.3-1.2.osg35.el7 xrootd-debuginfo-4.11.3-1.2.osg35.el7 xrootd-devel-4.11.3-1.2.osg35.el7 xrootd-doc-4.11.3-1.2.osg35.el7 xrootd-fuse-4.11.3-1.2.osg35.el7 xrootd-libs-4.11.3-1.2.osg35.el7 xrootd-private-devel-4.11.3-1.2.osg35.el7 xrootd-selinux-4.11.3-1.2.osg35.el7 xrootd-server-4.11.3-1.2.osg35.el7 xrootd-server-devel-4.11.3-1.2.osg35.el7 xrootd-server-libs-4.11.3-1.2.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-16/","text":"OSG Software Release 3.5.16 \u00b6 Release Date: 2020-05-14 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: CVMFS 2.7.2 Optimizes loading of nested catalogs Improves logging when switching hosts Frontier Squid 4.11-2.1 Fixed a bug that caused capital 'L's from appearing at the beginning of log lines under heavy load osg-ce 3.5-5: Fixed Hosted CE Gratia schedd cron hosted-ce-tools 0.7: Added worker node client update timeout to prevent hangs CCTools 7.1.5 : Update from 7.0.22 (minor features and bug fixes) CCTools 7.1.2 CCTools 7.1.0 VO Package v105 : Added EIC VO These JIRA tickets were addressed in this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.1.5-1.osg35.el7 cvmfs-2.7.2-1.osg35.el7 frontier-squid-4.11-2.1.osg35.el7 hosted-ce-tools-0.7-1.osg35.el7 osg-ce-3.5-5.osg35.el7 osg-oasis-16-3.osg35.el7 vo-client-105-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel cvmfs cvmfs-devel cvmfs-ducc cvmfs-server cvmfs-shrinkwrap cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo hosted-ce-tools osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-oasis vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: cctools-7.1.5-1.osg35.el7 cctools-debuginfo-7.1.5-1.osg35.el7 cctools-devel-7.1.5-1.osg35.el7 cvmfs-2.7.2-1.osg35.el7 cvmfs-devel-2.7.2-1.osg35.el7 cvmfs-ducc-2.7.2-1.osg35.el7 cvmfs-fuse3-2.7.2-1.osg35.el7 cvmfs-server-2.7.2-1.osg35.el7 cvmfs-shrinkwrap-2.7.2-1.osg35.el7 cvmfs-unittests-2.7.2-1.osg35.el7 frontier-squid-4.11-2.1.osg35.el7 frontier-squid-debuginfo-4.11-2.1.osg35.el7 hosted-ce-tools-0.7-1.osg35.el7 osg-ce-3.5-5.osg35.el7 osg-ce-bosco-3.5-5.osg35.el7 osg-ce-condor-3.5-5.osg35.el7 osg-ce-lsf-3.5-5.osg35.el7 osg-ce-pbs-3.5-5.osg35.el7 osg-ce-sge-3.5-5.osg35.el7 osg-ce-slurm-3.5-5.osg35.el7 osg-oasis-16-3.osg35.el7 vo-client-105-1.osg35.el7 vo-client-dcache-105-1.osg35.el7 vo-client-lcmaps-voms-105-1.osg35.el7","title":"Release 3 5 16"},{"location":"release/3.5/release-3-5-16/#osg-software-release-3516","text":"Release Date: 2020-05-14 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.16"},{"location":"release/3.5/release-3-5-16/#summary-of-changes","text":"This release contains: CVMFS 2.7.2 Optimizes loading of nested catalogs Improves logging when switching hosts Frontier Squid 4.11-2.1 Fixed a bug that caused capital 'L's from appearing at the beginning of log lines under heavy load osg-ce 3.5-5: Fixed Hosted CE Gratia schedd cron hosted-ce-tools 0.7: Added worker node client update timeout to prevent hangs CCTools 7.1.5 : Update from 7.0.22 (minor features and bug fixes) CCTools 7.1.2 CCTools 7.1.0 VO Package v105 : Added EIC VO These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-16/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-16/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-16/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-16/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.1.5-1.osg35.el7 cvmfs-2.7.2-1.osg35.el7 frontier-squid-4.11-2.1.osg35.el7 hosted-ce-tools-0.7-1.osg35.el7 osg-ce-3.5-5.osg35.el7 osg-oasis-16-3.osg35.el7 vo-client-105-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-16/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel cvmfs cvmfs-devel cvmfs-ducc cvmfs-server cvmfs-shrinkwrap cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo hosted-ce-tools osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-oasis vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: cctools-7.1.5-1.osg35.el7 cctools-debuginfo-7.1.5-1.osg35.el7 cctools-devel-7.1.5-1.osg35.el7 cvmfs-2.7.2-1.osg35.el7 cvmfs-devel-2.7.2-1.osg35.el7 cvmfs-ducc-2.7.2-1.osg35.el7 cvmfs-fuse3-2.7.2-1.osg35.el7 cvmfs-server-2.7.2-1.osg35.el7 cvmfs-shrinkwrap-2.7.2-1.osg35.el7 cvmfs-unittests-2.7.2-1.osg35.el7 frontier-squid-4.11-2.1.osg35.el7 frontier-squid-debuginfo-4.11-2.1.osg35.el7 hosted-ce-tools-0.7-1.osg35.el7 osg-ce-3.5-5.osg35.el7 osg-ce-bosco-3.5-5.osg35.el7 osg-ce-condor-3.5-5.osg35.el7 osg-ce-lsf-3.5-5.osg35.el7 osg-ce-pbs-3.5-5.osg35.el7 osg-ce-sge-3.5-5.osg35.el7 osg-ce-slurm-3.5-5.osg35.el7 osg-oasis-16-3.osg35.el7 vo-client-105-1.osg35.el7 vo-client-dcache-105-1.osg35.el7 vo-client-lcmaps-voms-105-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-17/","text":"OSG Software Release 3.5.17 \u00b6 Release Date: 2020-06-04 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: BLAHP 1.18.46 : better interaction with HTCondor and Slurm Fix an issue where the slurm binpath always returned scontrol Python 3 compatibility Handle extra-quoted arguments to condor_submit.sh Expand env vars in configured slurm_binpath Fix cluster handling in slurm_status.py Introduce blah_job_env_confs for dynamic env var expansion Add EXIT trap to remove barrier file Amended file credits for Matt Farrellee HTCondor 8.8.9 : Bug fix release Proper tracking of maximum memory used by Docker universe jobs Fixed preempting a GPU slot for a GPU job when all GPUs are in use Fixed a Python crash when queue_item_data iterator raises an exception Fixed a bug where slot attribute overrides were ignored Calculates accounting group quota correctly when more than 1 CPU requested Updated HTCondor Annex to accommodate API change for AWS Spot Fleet Fixed a problem where HTCondor would not start on AWS Fargate Fixed where the collector could wait forever for a partial message Fixed streaming output to large files (>2Gb) when using the 32-bit shadow gratia-probe 1.20.13 Fix bug in interacting with Slurm versions earlier than 18 Handle cluster name that contain special characters VO Package v106 : Fixed WLCG VOMS server host name Upcoming: HTCondor 8.9.7 : New feature release Multiple enhancements in the file transfer code Support for more regions in s3:// URLs Much more flexible job router language Jobs may now specify cuda_version to match equally-capable GPUs TOKENS are now called IDTOKENS to differentiate from SCITOKENS Added the ability to blacklist TOKENS via an expression Can simultaneously handle Kerberos and OAUTH credentials The startd supports a remote history query similar to the schedd condor_q -submitters now works with accounting groups Fixed a bug reading service account credentials for Google Compute Engine Upcoming: GlideinWMS 3.7 : New feature release Includes all features and fixes of 3.6.2 Use of HTCondor token-auth for Glideins authentication Secure logging channel Refactored glidein_startup to separate out the code in heredoc sections These JIRA tickets were addressed in this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . blahp-1.18.46-1.osg35.el7 condor-8.8.9-1.1.osg35.el7 gratia-probe-1.20.13-1.osg35.el7 vo-client-106-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer minicondor python2-condor python3-condor vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.46-1.osg35.el7 blahp-debuginfo-1.18.46-1.osg35.el7 condor-8.8.9-1.1.osg35.el7 condor-all-8.8.9-1.1.osg35.el7 condor-annex-ec2-8.8.9-1.1.osg35.el7 condor-bosco-8.8.9-1.1.osg35.el7 condor-classads-8.8.9-1.1.osg35.el7 condor-classads-devel-8.8.9-1.1.osg35.el7 condor-debuginfo-8.8.9-1.1.osg35.el7 condor-kbdd-8.8.9-1.1.osg35.el7 condor-procd-8.8.9-1.1.osg35.el7 condor-test-8.8.9-1.1.osg35.el7 condor-vm-gahp-8.8.9-1.1.osg35.el7 gratia-probe-1.20.13-1.osg35.el7 gratia-probe-common-1.20.13-1.osg35.el7 gratia-probe-condor-1.20.13-1.osg35.el7 gratia-probe-condor-events-1.20.13-1.osg35.el7 gratia-probe-dcache-storage-1.20.13-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.13-1.osg35.el7 gratia-probe-dcache-transfer-1.20.13-1.osg35.el7 gratia-probe-debuginfo-1.20.13-1.osg35.el7 gratia-probe-enstore-storage-1.20.13-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.13-1.osg35.el7 gratia-probe-enstore-transfer-1.20.13-1.osg35.el7 gratia-probe-glideinwms-1.20.13-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.13-1.osg35.el7 gratia-probe-hadoop-storage-1.20.13-1.osg35.el7 gratia-probe-htcondor-ce-1.20.13-1.osg35.el7 gratia-probe-lsf-1.20.13-1.osg35.el7 gratia-probe-metric-1.20.13-1.osg35.el7 gratia-probe-onevm-1.20.13-1.osg35.el7 gratia-probe-pbs-lsf-1.20.13-1.osg35.el7 gratia-probe-services-1.20.13-1.osg35.el7 gratia-probe-sge-1.20.13-1.osg35.el7 gratia-probe-slurm-1.20.13-1.osg35.el7 gratia-probe-xrootd-storage-1.20.13-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.13-1.osg35.el7 minicondor-8.8.9-1.1.osg35.el7 python2-condor-8.8.9-1.1.osg35.el7 python3-condor-8.8.9-1.1.osg35.el7 vo-client-106-1.osg35.el7 vo-client-dcache-106-1.osg35.el7 vo-client-lcmaps-voms-106-1.osg35.el7 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.46-3.osgup.el7 condor-8.9.7-1.1.osgup.el7 glideinwms-3.7-1.osgup.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.46-3.osgup.el7 blahp-debuginfo-1.18.46-3.osgup.el7 condor-8.9.7-1.1.osgup.el7 condor-all-8.9.7-1.1.osgup.el7 condor-annex-ec2-8.9.7-1.1.osgup.el7 condor-bosco-8.9.7-1.1.osgup.el7 condor-classads-8.9.7-1.1.osgup.el7 condor-classads-devel-8.9.7-1.1.osgup.el7 condor-debuginfo-8.9.7-1.1.osgup.el7 condor-kbdd-8.9.7-1.1.osgup.el7 condor-procd-8.9.7-1.1.osgup.el7 condor-test-8.9.7-1.1.osgup.el7 condor-vm-gahp-8.9.7-1.1.osgup.el7 glideinwms-3.7-1.osgup.el7 glideinwms-common-tools-3.7-1.osgup.el7 glideinwms-condor-common-config-3.7-1.osgup.el7 glideinwms-factory-3.7-1.osgup.el7 glideinwms-factory-condor-3.7-1.osgup.el7 glideinwms-glidecondor-tools-3.7-1.osgup.el7 glideinwms-libs-3.7-1.osgup.el7 glideinwms-minimal-condor-3.7-1.osgup.el7 glideinwms-usercollector-3.7-1.osgup.el7 glideinwms-userschedd-3.7-1.osgup.el7 glideinwms-vofrontend-3.7-1.osgup.el7 glideinwms-vofrontend-standalone-3.7-1.osgup.el7 minicondor-8.9.7-1.1.osgup.el7 python2-condor-8.9.7-1.1.osgup.el7 python3-condor-8.9.7-1.1.osgup.el7","title":"Release 3 5 17"},{"location":"release/3.5/release-3-5-17/#osg-software-release-3517","text":"Release Date: 2020-06-04 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.17"},{"location":"release/3.5/release-3-5-17/#summary-of-changes","text":"This release contains: BLAHP 1.18.46 : better interaction with HTCondor and Slurm Fix an issue where the slurm binpath always returned scontrol Python 3 compatibility Handle extra-quoted arguments to condor_submit.sh Expand env vars in configured slurm_binpath Fix cluster handling in slurm_status.py Introduce blah_job_env_confs for dynamic env var expansion Add EXIT trap to remove barrier file Amended file credits for Matt Farrellee HTCondor 8.8.9 : Bug fix release Proper tracking of maximum memory used by Docker universe jobs Fixed preempting a GPU slot for a GPU job when all GPUs are in use Fixed a Python crash when queue_item_data iterator raises an exception Fixed a bug where slot attribute overrides were ignored Calculates accounting group quota correctly when more than 1 CPU requested Updated HTCondor Annex to accommodate API change for AWS Spot Fleet Fixed a problem where HTCondor would not start on AWS Fargate Fixed where the collector could wait forever for a partial message Fixed streaming output to large files (>2Gb) when using the 32-bit shadow gratia-probe 1.20.13 Fix bug in interacting with Slurm versions earlier than 18 Handle cluster name that contain special characters VO Package v106 : Fixed WLCG VOMS server host name Upcoming: HTCondor 8.9.7 : New feature release Multiple enhancements in the file transfer code Support for more regions in s3:// URLs Much more flexible job router language Jobs may now specify cuda_version to match equally-capable GPUs TOKENS are now called IDTOKENS to differentiate from SCITOKENS Added the ability to blacklist TOKENS via an expression Can simultaneously handle Kerberos and OAUTH credentials The startd supports a remote history query similar to the schedd condor_q -submitters now works with accounting groups Fixed a bug reading service account credentials for Google Compute Engine Upcoming: GlideinWMS 3.7 : New feature release Includes all features and fixes of 3.6.2 Use of HTCondor token-auth for Glideins authentication Secure logging channel Refactored glidein_startup to separate out the code in heredoc sections These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-17/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-17/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-17/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-17/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . blahp-1.18.46-1.osg35.el7 condor-8.8.9-1.1.osg35.el7 gratia-probe-1.20.13-1.osg35.el7 vo-client-106-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-17/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer minicondor python2-condor python3-condor vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.46-1.osg35.el7 blahp-debuginfo-1.18.46-1.osg35.el7 condor-8.8.9-1.1.osg35.el7 condor-all-8.8.9-1.1.osg35.el7 condor-annex-ec2-8.8.9-1.1.osg35.el7 condor-bosco-8.8.9-1.1.osg35.el7 condor-classads-8.8.9-1.1.osg35.el7 condor-classads-devel-8.8.9-1.1.osg35.el7 condor-debuginfo-8.8.9-1.1.osg35.el7 condor-kbdd-8.8.9-1.1.osg35.el7 condor-procd-8.8.9-1.1.osg35.el7 condor-test-8.8.9-1.1.osg35.el7 condor-vm-gahp-8.8.9-1.1.osg35.el7 gratia-probe-1.20.13-1.osg35.el7 gratia-probe-common-1.20.13-1.osg35.el7 gratia-probe-condor-1.20.13-1.osg35.el7 gratia-probe-condor-events-1.20.13-1.osg35.el7 gratia-probe-dcache-storage-1.20.13-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.13-1.osg35.el7 gratia-probe-dcache-transfer-1.20.13-1.osg35.el7 gratia-probe-debuginfo-1.20.13-1.osg35.el7 gratia-probe-enstore-storage-1.20.13-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.13-1.osg35.el7 gratia-probe-enstore-transfer-1.20.13-1.osg35.el7 gratia-probe-glideinwms-1.20.13-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.13-1.osg35.el7 gratia-probe-hadoop-storage-1.20.13-1.osg35.el7 gratia-probe-htcondor-ce-1.20.13-1.osg35.el7 gratia-probe-lsf-1.20.13-1.osg35.el7 gratia-probe-metric-1.20.13-1.osg35.el7 gratia-probe-onevm-1.20.13-1.osg35.el7 gratia-probe-pbs-lsf-1.20.13-1.osg35.el7 gratia-probe-services-1.20.13-1.osg35.el7 gratia-probe-sge-1.20.13-1.osg35.el7 gratia-probe-slurm-1.20.13-1.osg35.el7 gratia-probe-xrootd-storage-1.20.13-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.13-1.osg35.el7 minicondor-8.8.9-1.1.osg35.el7 python2-condor-8.8.9-1.1.osg35.el7 python3-condor-8.8.9-1.1.osg35.el7 vo-client-106-1.osg35.el7 vo-client-dcache-106-1.osg35.el7 vo-client-lcmaps-voms-106-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-17/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.46-3.osgup.el7 condor-8.9.7-1.1.osgup.el7 glideinwms-3.7-1.osgup.el7","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-17/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.46-3.osgup.el7 blahp-debuginfo-1.18.46-3.osgup.el7 condor-8.9.7-1.1.osgup.el7 condor-all-8.9.7-1.1.osgup.el7 condor-annex-ec2-8.9.7-1.1.osgup.el7 condor-bosco-8.9.7-1.1.osgup.el7 condor-classads-8.9.7-1.1.osgup.el7 condor-classads-devel-8.9.7-1.1.osgup.el7 condor-debuginfo-8.9.7-1.1.osgup.el7 condor-kbdd-8.9.7-1.1.osgup.el7 condor-procd-8.9.7-1.1.osgup.el7 condor-test-8.9.7-1.1.osgup.el7 condor-vm-gahp-8.9.7-1.1.osgup.el7 glideinwms-3.7-1.osgup.el7 glideinwms-common-tools-3.7-1.osgup.el7 glideinwms-condor-common-config-3.7-1.osgup.el7 glideinwms-factory-3.7-1.osgup.el7 glideinwms-factory-condor-3.7-1.osgup.el7 glideinwms-glidecondor-tools-3.7-1.osgup.el7 glideinwms-libs-3.7-1.osgup.el7 glideinwms-minimal-condor-3.7-1.osgup.el7 glideinwms-usercollector-3.7-1.osgup.el7 glideinwms-userschedd-3.7-1.osgup.el7 glideinwms-vofrontend-3.7-1.osgup.el7 glideinwms-vofrontend-standalone-3.7-1.osgup.el7 minicondor-8.9.7-1.1.osgup.el7 python2-condor-8.9.7-1.1.osgup.el7 python3-condor-8.9.7-1.1.osgup.el7","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-18/","text":"OSG Software Release 3.5.18 \u00b6 Release Date: 2020-06-11 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Frontier Squid 4.11-3.1 : Fixed critical bug that causes server errors to be cached indefinitely which may require caches to be cleared. The bug was introduced in version 4.10-3.1 VOMS 2.0.14-1.6: Allow VOMS libraries to accept certificates with VOMS attributes that only have a top-level component, such as those provided by the WLCG VO XCache 1.4 added xcache-consistency-check advertise to the OSG Central Collector via SSL reduced default XRootD logging verbosity stashcache-client 5.6.1: reads the list of caches from the globally available WLCG-WPAD service; this allows updates to the cache list without a new release of stashcache-client These JIRA tickets were addressed in this release. Containers \u00b6 The Frontier Squid and Hosted CE containers are available and have been tagged as stable in accordance with our Container Release Policy Frontier Squid Hosted CE The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . frontier-squid-4.11-3.1.osg35.el7 stashcache-client-5.6.1-1.osg35.el7 voms-2.0.14-1.6.osg35.el7 xcache-1.4.0-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cms-xcache frontier-squid frontier-squid-debuginfo stash-cache stashcache-client stash-origin voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xcache xcache-consistency-check xcache-redirector If you wish to only update the RPMs that changed, the set of RPMs is: atlas-xcache-1.4.0-1.osg35.el7 cms-xcache-1.4.0-1.osg35.el7 frontier-squid-4.11-3.1.osg35.el7 frontier-squid-debuginfo-4.11-3.1.osg35.el7 stash-cache-1.4.0-1.osg35.el7 stashcache-client-5.6.1-1.osg35.el7 stash-origin-1.4.0-1.osg35.el7 voms-2.0.14-1.6.osg35.el7 voms-clients-cpp-2.0.14-1.6.osg35.el7 voms-debuginfo-2.0.14-1.6.osg35.el7 voms-devel-2.0.14-1.6.osg35.el7 voms-doc-2.0.14-1.6.osg35.el7 voms-server-2.0.14-1.6.osg35.el7 xcache-1.4.0-1.osg35.el7 xcache-consistency-check-1.4.0-1.osg35.el7 xcache-redirector-1.4.0-1.osg35.el7","title":"Release 3 5 18"},{"location":"release/3.5/release-3-5-18/#osg-software-release-3518","text":"Release Date: 2020-06-11 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.18"},{"location":"release/3.5/release-3-5-18/#summary-of-changes","text":"This release contains: Frontier Squid 4.11-3.1 : Fixed critical bug that causes server errors to be cached indefinitely which may require caches to be cleared. The bug was introduced in version 4.10-3.1 VOMS 2.0.14-1.6: Allow VOMS libraries to accept certificates with VOMS attributes that only have a top-level component, such as those provided by the WLCG VO XCache 1.4 added xcache-consistency-check advertise to the OSG Central Collector via SSL reduced default XRootD logging verbosity stashcache-client 5.6.1: reads the list of caches from the globally available WLCG-WPAD service; this allows updates to the cache list without a new release of stashcache-client These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-18/#containers","text":"The Frontier Squid and Hosted CE containers are available and have been tagged as stable in accordance with our Container Release Policy Frontier Squid Hosted CE The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-18/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-18/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-18/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-18/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . frontier-squid-4.11-3.1.osg35.el7 stashcache-client-5.6.1-1.osg35.el7 voms-2.0.14-1.6.osg35.el7 xcache-1.4.0-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-18/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cms-xcache frontier-squid frontier-squid-debuginfo stash-cache stashcache-client stash-origin voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xcache xcache-consistency-check xcache-redirector If you wish to only update the RPMs that changed, the set of RPMs is: atlas-xcache-1.4.0-1.osg35.el7 cms-xcache-1.4.0-1.osg35.el7 frontier-squid-4.11-3.1.osg35.el7 frontier-squid-debuginfo-4.11-3.1.osg35.el7 stash-cache-1.4.0-1.osg35.el7 stashcache-client-5.6.1-1.osg35.el7 stash-origin-1.4.0-1.osg35.el7 voms-2.0.14-1.6.osg35.el7 voms-clients-cpp-2.0.14-1.6.osg35.el7 voms-debuginfo-2.0.14-1.6.osg35.el7 voms-devel-2.0.14-1.6.osg35.el7 voms-doc-2.0.14-1.6.osg35.el7 voms-server-2.0.14-1.6.osg35.el7 xcache-1.4.0-1.osg35.el7 xcache-consistency-check-1.4.0-1.osg35.el7 xcache-redirector-1.4.0-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-19/","text":"OSG Software Release 3.5.19 \u00b6 Release Date: 2020-07-01 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 Note If using scitokens-credmon with HTCondor 8.9, manual adjustments need to be made before upgrading. This release contains: XRootD 4.12.3 Major features to the the xrdcp client: Ability to limit bandwidth usage Ability to resume a transfer Minor new enhancements for the python based bindings Several bug fixes xrootd-lcmaps 1.7.7: Better logging when lcmaps is not used scitokens-credmon 0.7 Conforms (gracefully) to configuration changes in HTCondor 8.9.7 Fixes exception logging that may have caused the CredMon to hang Fixes OAuthCredmon from trying to read LocalCredmon credentials Emits error message when local issuer private key is not found Adds a lookup table for common OAuth providers' user information endpoints Adds a lookup table to set how often tokens are refreshed for common OAuth providers The main condor_credmon_oauth script now uses multiprocessing processes and queue to prevent credmon threads from stalling Upcoming: HTCondor 8.9.7 : New feature release Multiple enhancements in the file transfer code Support for more regions in s3:// URLs Much more flexible job router language Jobs may now specify cuda_version to match equally-capable GPUs TOKENS are now called IDTOKENS to differentiate from SCITOKENS Added the ability to blacklist IDTOKENS via an expression Can simultaneously handle Kerberos and OAUTH credentials The startd supports a remote history query similar to the schedd condor_q -submitters now works with accounting groups Fixed a bug reading service account credentials for Google Compute Engine These JIRA tickets were addressed in this release. Containers \u00b6 The Frontier Squid and Hosted CE containers are available and have been tagged as stable in accordance with our Container Release Policy Stash Cache The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.1.6-1.osg35.el7 scitokens-credmon-0.7-1.osg35.el7 xrootd-4.12.3-1.osg35.el7 xrootd-hdfs-2.1.7-6.osg35.el7 xrootd-lcmaps-1.7.7-2.osg35.el7 xrootd-multiuser-0.4.2-8.osg35.el7 xrootd-scitokens-1.2.0-5.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel python2-scitokens-credmon python2-xrootd scitokens-credmon xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: cctools-7.1.6-1.osg35.el7 cctools-debuginfo-7.1.6-1.osg35.el7 cctools-devel-7.1.6-1.osg35.el7 python2-scitokens-credmon-0.7-1.osg35.el7 python2-xrootd-4.12.3-1.osg35.el7 scitokens-credmon-0.7-1.osg35.el7 xrootd-4.12.3-1.osg35.el7 xrootd-client-4.12.3-1.osg35.el7 xrootd-client-devel-4.12.3-1.osg35.el7 xrootd-client-libs-4.12.3-1.osg35.el7 xrootd-debuginfo-4.12.3-1.osg35.el7 xrootd-devel-4.12.3-1.osg35.el7 xrootd-doc-4.12.3-1.osg35.el7 xrootd-fuse-4.12.3-1.osg35.el7 xrootd-hdfs-2.1.7-6.osg35.el7 xrootd-hdfs-debuginfo-2.1.7-6.osg35.el7 xrootd-hdfs-devel-2.1.7-6.osg35.el7 xrootd-lcmaps-1.7.7-2.osg35.el7 xrootd-lcmaps-debuginfo-1.7.7-2.osg35.el7 xrootd-libs-4.12.3-1.osg35.el7 xrootd-multiuser-0.4.2-8.osg35.el7 xrootd-multiuser-debuginfo-0.4.2-8.osg35.el7 xrootd-private-devel-4.12.3-1.osg35.el7 xrootd-scitokens-1.2.0-5.osg35.el7 xrootd-scitokens-debuginfo-1.2.0-5.osg35.el7 xrootd-selinux-4.12.3-1.osg35.el7 xrootd-server-4.12.3-1.osg35.el7 xrootd-server-devel-4.12.3-1.osg35.el7 xrootd-server-libs-4.12.3-1.osg35.el7 xrootd-voms-4.12.3-1.osg35.el7 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.46-3.osgup.el7 condor-8.9.7-1.1.osgup.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.46-3.osgup.el7 blahp-debuginfo-1.18.46-3.osgup.el7 condor-8.9.7-1.1.osgup.el7 condor-all-8.9.7-1.1.osgup.el7 condor-annex-ec2-8.9.7-1.1.osgup.el7 condor-bosco-8.9.7-1.1.osgup.el7 condor-classads-8.9.7-1.1.osgup.el7 condor-classads-devel-8.9.7-1.1.osgup.el7 condor-debuginfo-8.9.7-1.1.osgup.el7 condor-kbdd-8.9.7-1.1.osgup.el7 condor-procd-8.9.7-1.1.osgup.el7 condor-test-8.9.7-1.1.osgup.el7 condor-vm-gahp-8.9.7-1.1.osgup.el7 minicondor-8.9.7-1.1.osgup.el7 python2-condor-8.9.7-1.1.osgup.el7 python3-condor-8.9.7-1.1.osgup.el7","title":"Release 3 5 19"},{"location":"release/3.5/release-3-5-19/#osg-software-release-3519","text":"Release Date: 2020-07-01 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.19"},{"location":"release/3.5/release-3-5-19/#summary-of-changes","text":"Note If using scitokens-credmon with HTCondor 8.9, manual adjustments need to be made before upgrading. This release contains: XRootD 4.12.3 Major features to the the xrdcp client: Ability to limit bandwidth usage Ability to resume a transfer Minor new enhancements for the python based bindings Several bug fixes xrootd-lcmaps 1.7.7: Better logging when lcmaps is not used scitokens-credmon 0.7 Conforms (gracefully) to configuration changes in HTCondor 8.9.7 Fixes exception logging that may have caused the CredMon to hang Fixes OAuthCredmon from trying to read LocalCredmon credentials Emits error message when local issuer private key is not found Adds a lookup table for common OAuth providers' user information endpoints Adds a lookup table to set how often tokens are refreshed for common OAuth providers The main condor_credmon_oauth script now uses multiprocessing processes and queue to prevent credmon threads from stalling Upcoming: HTCondor 8.9.7 : New feature release Multiple enhancements in the file transfer code Support for more regions in s3:// URLs Much more flexible job router language Jobs may now specify cuda_version to match equally-capable GPUs TOKENS are now called IDTOKENS to differentiate from SCITOKENS Added the ability to blacklist IDTOKENS via an expression Can simultaneously handle Kerberos and OAUTH credentials The startd supports a remote history query similar to the schedd condor_q -submitters now works with accounting groups Fixed a bug reading service account credentials for Google Compute Engine These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-19/#containers","text":"The Frontier Squid and Hosted CE containers are available and have been tagged as stable in accordance with our Container Release Policy Stash Cache The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-19/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-19/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-19/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-19/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.1.6-1.osg35.el7 scitokens-credmon-0.7-1.osg35.el7 xrootd-4.12.3-1.osg35.el7 xrootd-hdfs-2.1.7-6.osg35.el7 xrootd-lcmaps-1.7.7-2.osg35.el7 xrootd-multiuser-0.4.2-8.osg35.el7 xrootd-scitokens-1.2.0-5.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-19/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel python2-scitokens-credmon python2-xrootd scitokens-credmon xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: cctools-7.1.6-1.osg35.el7 cctools-debuginfo-7.1.6-1.osg35.el7 cctools-devel-7.1.6-1.osg35.el7 python2-scitokens-credmon-0.7-1.osg35.el7 python2-xrootd-4.12.3-1.osg35.el7 scitokens-credmon-0.7-1.osg35.el7 xrootd-4.12.3-1.osg35.el7 xrootd-client-4.12.3-1.osg35.el7 xrootd-client-devel-4.12.3-1.osg35.el7 xrootd-client-libs-4.12.3-1.osg35.el7 xrootd-debuginfo-4.12.3-1.osg35.el7 xrootd-devel-4.12.3-1.osg35.el7 xrootd-doc-4.12.3-1.osg35.el7 xrootd-fuse-4.12.3-1.osg35.el7 xrootd-hdfs-2.1.7-6.osg35.el7 xrootd-hdfs-debuginfo-2.1.7-6.osg35.el7 xrootd-hdfs-devel-2.1.7-6.osg35.el7 xrootd-lcmaps-1.7.7-2.osg35.el7 xrootd-lcmaps-debuginfo-1.7.7-2.osg35.el7 xrootd-libs-4.12.3-1.osg35.el7 xrootd-multiuser-0.4.2-8.osg35.el7 xrootd-multiuser-debuginfo-0.4.2-8.osg35.el7 xrootd-private-devel-4.12.3-1.osg35.el7 xrootd-scitokens-1.2.0-5.osg35.el7 xrootd-scitokens-debuginfo-1.2.0-5.osg35.el7 xrootd-selinux-4.12.3-1.osg35.el7 xrootd-server-4.12.3-1.osg35.el7 xrootd-server-devel-4.12.3-1.osg35.el7 xrootd-server-libs-4.12.3-1.osg35.el7 xrootd-voms-4.12.3-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-19/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.46-3.osgup.el7 condor-8.9.7-1.1.osgup.el7","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-19/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.46-3.osgup.el7 blahp-debuginfo-1.18.46-3.osgup.el7 condor-8.9.7-1.1.osgup.el7 condor-all-8.9.7-1.1.osgup.el7 condor-annex-ec2-8.9.7-1.1.osgup.el7 condor-bosco-8.9.7-1.1.osgup.el7 condor-classads-8.9.7-1.1.osgup.el7 condor-classads-devel-8.9.7-1.1.osgup.el7 condor-debuginfo-8.9.7-1.1.osgup.el7 condor-kbdd-8.9.7-1.1.osgup.el7 condor-procd-8.9.7-1.1.osgup.el7 condor-test-8.9.7-1.1.osgup.el7 condor-vm-gahp-8.9.7-1.1.osgup.el7 minicondor-8.9.7-1.1.osgup.el7 python2-condor-8.9.7-1.1.osgup.el7 python3-condor-8.9.7-1.1.osgup.el7","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-2/","text":"OSG Software Release 3.5.2 \u00b6 Release Date: 2019-10-10 Supported OS Versions: EL7 Summary of Changes \u00b6 This release contains: HTCondor-CE 4.0.1 Some manual adjustments are required 4.0.0: Major feature update SciTokens support Disabled job retries Simplified remote CE requirements format for non-HTCondor batch systems Major configuration reorganization 4.0.1: bug fix OSG CE 3.5-2: Carries OSG specific CE configurations CVMFS 2.6.3 : Bug fix release Frontier-Squid 4.8-2 : Add support for shoal-agent CCTools 7.0.18 : Bug fix release LCMAPS 1.6.6-1.9: Rebuilt to ease transition from OSG 3.3 VO Package v96 : Add LHCb VO These JIRA tickets were addressed in this release. The Worker node containers have been updated to this release. Known Issues \u00b6 OSG System Profiler verifies all installed packages, which may result in excessively long run times . Containers \u00b6 Several containers are available and have been tagged as stable in accordance with our Container Release Policy ATLAS XCache CMS XCache Frontier Squid Stash Cache Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.0.18-1.osg35.el7 cvmfs-2.6.3-1.osg35.el7 cvmfs-x509-helper-2.1-2.osg35.el7 frontier-squid-4.8-2.1.osg35.el7 hosted-ce-tools-0.4-1.osg35.el7 htcondor-ce-4.0.1-1.osg35.el7 osg-ce-3.5-2.osg35.el7 osg-oasis-15-4.osg35.el7 vo-client-96-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel cvmfs cvmfs-devel cvmfs-ducc cvmfs-server cvmfs-shrinkwrap cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo hosted-ce-tools htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-oasis vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: cctools-7.0.18-1.osg35.el7 cctools-debuginfo-7.0.18-1.osg35.el7 cctools-devel-7.0.18-1.osg35.el7 cvmfs-2.6.3-1.osg35.el7 cvmfs-devel-2.6.3-1.osg35.el7 cvmfs-ducc-2.6.3-1.osg35.el7 cvmfs-server-2.6.3-1.osg35.el7 cvmfs-shrinkwrap-2.6.3-1.osg35.el7 cvmfs-unittests-2.6.3-1.osg35.el7 cvmfs-x509-helper-2.1-2.osg35.el7 cvmfs-x509-helper-debuginfo-2.1-2.osg35.el7 frontier-squid-4.8-2.1.osg35.el7 frontier-squid-debuginfo-4.8-2.1.osg35.el7 hosted-ce-tools-0.4-1.osg35.el7 htcondor-ce-4.0.1-1.osg35.el7 htcondor-ce-bosco-4.0.1-1.osg35.el7 htcondor-ce-client-4.0.1-1.osg35.el7 htcondor-ce-collector-4.0.1-1.osg35.el7 htcondor-ce-condor-4.0.1-1.osg35.el7 htcondor-ce-lsf-4.0.1-1.osg35.el7 htcondor-ce-pbs-4.0.1-1.osg35.el7 htcondor-ce-sge-4.0.1-1.osg35.el7 htcondor-ce-slurm-4.0.1-1.osg35.el7 htcondor-ce-view-4.0.1-1.osg35.el7 osg-ce-3.5-2.osg35.el7 osg-ce-bosco-3.5-2.osg35.el7 osg-ce-condor-3.5-2.osg35.el7 osg-ce-lsf-3.5-2.osg35.el7 osg-ce-pbs-3.5-2.osg35.el7 osg-ce-sge-3.5-2.osg35.el7 osg-ce-slurm-3.5-2.osg35.el7 osg-oasis-15-4.osg35.el7 vo-client-96-1.osg35.el7 vo-client-dcache-96-1.osg35.el7 vo-client-lcmaps-voms-96-1.osg35.el7","title":"Release 3 5 2"},{"location":"release/3.5/release-3-5-2/#osg-software-release-352","text":"Release Date: 2019-10-10 Supported OS Versions: EL7","title":"OSG Software Release 3.5.2"},{"location":"release/3.5/release-3-5-2/#summary-of-changes","text":"This release contains: HTCondor-CE 4.0.1 Some manual adjustments are required 4.0.0: Major feature update SciTokens support Disabled job retries Simplified remote CE requirements format for non-HTCondor batch systems Major configuration reorganization 4.0.1: bug fix OSG CE 3.5-2: Carries OSG specific CE configurations CVMFS 2.6.3 : Bug fix release Frontier-Squid 4.8-2 : Add support for shoal-agent CCTools 7.0.18 : Bug fix release LCMAPS 1.6.6-1.9: Rebuilt to ease transition from OSG 3.3 VO Package v96 : Add LHCb VO These JIRA tickets were addressed in this release. The Worker node containers have been updated to this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-2/#known-issues","text":"OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-2/#containers","text":"Several containers are available and have been tagged as stable in accordance with our Container Release Policy ATLAS XCache CMS XCache Frontier Squid Stash Cache","title":"Containers"},{"location":"release/3.5/release-3-5-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.0.18-1.osg35.el7 cvmfs-2.6.3-1.osg35.el7 cvmfs-x509-helper-2.1-2.osg35.el7 frontier-squid-4.8-2.1.osg35.el7 hosted-ce-tools-0.4-1.osg35.el7 htcondor-ce-4.0.1-1.osg35.el7 osg-ce-3.5-2.osg35.el7 osg-oasis-15-4.osg35.el7 vo-client-96-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel cvmfs cvmfs-devel cvmfs-ducc cvmfs-server cvmfs-shrinkwrap cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo hosted-ce-tools htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-oasis vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: cctools-7.0.18-1.osg35.el7 cctools-debuginfo-7.0.18-1.osg35.el7 cctools-devel-7.0.18-1.osg35.el7 cvmfs-2.6.3-1.osg35.el7 cvmfs-devel-2.6.3-1.osg35.el7 cvmfs-ducc-2.6.3-1.osg35.el7 cvmfs-server-2.6.3-1.osg35.el7 cvmfs-shrinkwrap-2.6.3-1.osg35.el7 cvmfs-unittests-2.6.3-1.osg35.el7 cvmfs-x509-helper-2.1-2.osg35.el7 cvmfs-x509-helper-debuginfo-2.1-2.osg35.el7 frontier-squid-4.8-2.1.osg35.el7 frontier-squid-debuginfo-4.8-2.1.osg35.el7 hosted-ce-tools-0.4-1.osg35.el7 htcondor-ce-4.0.1-1.osg35.el7 htcondor-ce-bosco-4.0.1-1.osg35.el7 htcondor-ce-client-4.0.1-1.osg35.el7 htcondor-ce-collector-4.0.1-1.osg35.el7 htcondor-ce-condor-4.0.1-1.osg35.el7 htcondor-ce-lsf-4.0.1-1.osg35.el7 htcondor-ce-pbs-4.0.1-1.osg35.el7 htcondor-ce-sge-4.0.1-1.osg35.el7 htcondor-ce-slurm-4.0.1-1.osg35.el7 htcondor-ce-view-4.0.1-1.osg35.el7 osg-ce-3.5-2.osg35.el7 osg-ce-bosco-3.5-2.osg35.el7 osg-ce-condor-3.5-2.osg35.el7 osg-ce-lsf-3.5-2.osg35.el7 osg-ce-pbs-3.5-2.osg35.el7 osg-ce-sge-3.5-2.osg35.el7 osg-ce-slurm-3.5-2.osg35.el7 osg-oasis-15-4.osg35.el7 vo-client-96-1.osg35.el7 vo-client-dcache-96-1.osg35.el7 vo-client-lcmaps-voms-96-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-20/","text":"OSG Software Release 3.5.20 \u00b6 Release Date: 2020-07-23 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor-CE 4.4.0 Compared to version 4.2.1, this release includes the following new features Identify broken job routes upon startup Add configuration option (COMPLETED_JOB_EXPIRATION) for how many days completed jobs may stay in the queue Add the CE registry web application to the Central Collector The registry provides an interface to OSG site administrators of HTCondor-CEs to retrieve an HTCondor IDTOKEN for authenticating pilot job submissions. Add plug-in interface to HTCondor-CE View and separate out OSG-specific code and configuration Bug fixes Fix HTCondor-CE View SchedD query that caused \"Info\" tables to be blank Fix handling of unmapped GSI users in the Central Collector CVMFS 2.7.3 Client / EL8: fix spurious SElinux error message during package upgrade Client: fix cvmfs_config chksetup for squashed NFS alien cache Server: fix handling of .cvmfs_status.json after garbage collection Server: add add-replica -P option to create pass-through replicas Frontier Squid 4.12-2.1 Fixed cache poisoning issue in HTTP request processing Support the \"stdio:\" prefix for access_log scitokens-cpp 0.5.1: Add support for WLCG-style storage write permissions These JIRA tickets were addressed in this release. Containers \u00b6 The Frontier Squid container is available and has been tagged as stable in accordance with our Container Release Policy Frontier Squid The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cvmfs-2.7.3-1.osg35.el7 cvmfs-config-osg-2.4-4.osg35.el7 frontier-squid-4.12-2.1.osg35.el7 htcondor-ce-4.4.0-1.osg35.el7 osg-oasis-16-5.osg35.el7 scitokens-cpp-0.5.1-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cvmfs cvmfs-config-osg cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests frontier-squid htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view osg-oasis scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel If you wish to only update the RPMs that changed, the set of RPMs is: cvmfs-2.7.3-1.osg35.el7 cvmfs-config-osg-2.4-4.osg35.el7 cvmfs-devel-2.7.3-1.osg35.el7 cvmfs-ducc-2.7.3-1.osg35.el7 cvmfs-fuse3-2.7.3-1.osg35.el7 cvmfs-server-2.7.3-1.osg35.el7 cvmfs-shrinkwrap-2.7.3-1.osg35.el7 cvmfs-unittests-2.7.3-1.osg35.el7 frontier-squid-4.12-2.1.osg35.el7 htcondor-ce-4.4.0-1.osg35.el7 htcondor-ce-bosco-4.4.0-1.osg35.el7 htcondor-ce-client-4.4.0-1.osg35.el7 htcondor-ce-collector-4.4.0-1.osg35.el7 htcondor-ce-condor-4.4.0-1.osg35.el7 htcondor-ce-lsf-4.4.0-1.osg35.el7 htcondor-ce-pbs-4.4.0-1.osg35.el7 htcondor-ce-sge-4.4.0-1.osg35.el7 htcondor-ce-slurm-4.4.0-1.osg35.el7 htcondor-ce-view-4.4.0-1.osg35.el7 osg-oasis-16-5.osg35.el7 scitokens-cpp-0.5.1-1.osg35.el7 scitokens-cpp-debuginfo-0.5.1-1.osg35.el7 scitokens-cpp-devel-0.5.1-1.osg35.el7","title":"Release 3 5 20"},{"location":"release/3.5/release-3-5-20/#osg-software-release-3520","text":"Release Date: 2020-07-23 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.20"},{"location":"release/3.5/release-3-5-20/#summary-of-changes","text":"This release contains: HTCondor-CE 4.4.0 Compared to version 4.2.1, this release includes the following new features Identify broken job routes upon startup Add configuration option (COMPLETED_JOB_EXPIRATION) for how many days completed jobs may stay in the queue Add the CE registry web application to the Central Collector The registry provides an interface to OSG site administrators of HTCondor-CEs to retrieve an HTCondor IDTOKEN for authenticating pilot job submissions. Add plug-in interface to HTCondor-CE View and separate out OSG-specific code and configuration Bug fixes Fix HTCondor-CE View SchedD query that caused \"Info\" tables to be blank Fix handling of unmapped GSI users in the Central Collector CVMFS 2.7.3 Client / EL8: fix spurious SElinux error message during package upgrade Client: fix cvmfs_config chksetup for squashed NFS alien cache Server: fix handling of .cvmfs_status.json after garbage collection Server: add add-replica -P option to create pass-through replicas Frontier Squid 4.12-2.1 Fixed cache poisoning issue in HTTP request processing Support the \"stdio:\" prefix for access_log scitokens-cpp 0.5.1: Add support for WLCG-style storage write permissions These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-20/#containers","text":"The Frontier Squid container is available and has been tagged as stable in accordance with our Container Release Policy Frontier Squid The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-20/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-20/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-20/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-20/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cvmfs-2.7.3-1.osg35.el7 cvmfs-config-osg-2.4-4.osg35.el7 frontier-squid-4.12-2.1.osg35.el7 htcondor-ce-4.4.0-1.osg35.el7 osg-oasis-16-5.osg35.el7 scitokens-cpp-0.5.1-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-20/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cvmfs cvmfs-config-osg cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests frontier-squid htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view osg-oasis scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel If you wish to only update the RPMs that changed, the set of RPMs is: cvmfs-2.7.3-1.osg35.el7 cvmfs-config-osg-2.4-4.osg35.el7 cvmfs-devel-2.7.3-1.osg35.el7 cvmfs-ducc-2.7.3-1.osg35.el7 cvmfs-fuse3-2.7.3-1.osg35.el7 cvmfs-server-2.7.3-1.osg35.el7 cvmfs-shrinkwrap-2.7.3-1.osg35.el7 cvmfs-unittests-2.7.3-1.osg35.el7 frontier-squid-4.12-2.1.osg35.el7 htcondor-ce-4.4.0-1.osg35.el7 htcondor-ce-bosco-4.4.0-1.osg35.el7 htcondor-ce-client-4.4.0-1.osg35.el7 htcondor-ce-collector-4.4.0-1.osg35.el7 htcondor-ce-condor-4.4.0-1.osg35.el7 htcondor-ce-lsf-4.4.0-1.osg35.el7 htcondor-ce-pbs-4.4.0-1.osg35.el7 htcondor-ce-sge-4.4.0-1.osg35.el7 htcondor-ce-slurm-4.4.0-1.osg35.el7 htcondor-ce-view-4.4.0-1.osg35.el7 osg-oasis-16-5.osg35.el7 scitokens-cpp-0.5.1-1.osg35.el7 scitokens-cpp-debuginfo-0.5.1-1.osg35.el7 scitokens-cpp-devel-0.5.1-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-21-2/","text":"OSG Data Release 3.5.21-2 \u00b6 Release Date: 2020-08-10 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 CA Certificates based on IGTF 1.107 Retired DarkMatterSecureCA and DarkMatterAssuredCA (AE) Removed superseded PolishGrid CA (PL) Added TCS G4 ECC trust anchors to accredited set (EU) VO Package v107 Update SLAC VOMS server DN These JIRA tickets were addressed in this release. Containers \u00b6 Worker Node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 igtf-ca-certs-1.107-1.osg35.el7 osg-ca-certs-1.89-1.osg35.el7 vo-client-107-1.osg35.el7 Enterprise Linux 8 \u00b6 igtf-ca-certs-1.107-1.osg35.el8 osg-ca-certs-1.89-1.osg35.el8 vo-client-107-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 igtf-ca-certs-1.107-1.osg35.el7 osg-ca-certs-1.89-1.osg35.el7 vo-client-107-1.osg35.el7 vo-client-dcache-107-1.osg35.el7 vo-client-lcmaps-voms-107-1.osg35.el7 Enterprise Linux 8 \u00b6 igtf-ca-certs-1.107-1.osg35.el8 osg-ca-certs-1.89-1.osg35.el8 vo-client-107-1.osg35.el8 vo-client-dcache-107-1.osg35.el8 vo-client-lcmaps-voms-107-1.osg35.el8","title":"Release 3 5 21 2"},{"location":"release/3.5/release-3-5-21-2/#osg-data-release-3521-2","text":"Release Date: 2020-08-10 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.21-2"},{"location":"release/3.5/release-3-5-21-2/#summary-of-changes","text":"CA Certificates based on IGTF 1.107 Retired DarkMatterSecureCA and DarkMatterAssuredCA (AE) Removed superseded PolishGrid CA (PL) Added TCS G4 ECC trust anchors to accredited set (EU) VO Package v107 Update SLAC VOMS server DN These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-21-2/#containers","text":"Worker Node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-21-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-21-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-21-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-21-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-21-2/#enterprise-linux-7","text":"igtf-ca-certs-1.107-1.osg35.el7 osg-ca-certs-1.89-1.osg35.el7 vo-client-107-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-21-2/#enterprise-linux-8","text":"igtf-ca-certs-1.107-1.osg35.el8 osg-ca-certs-1.89-1.osg35.el8 vo-client-107-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-21-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-21-2/#enterprise-linux-7_1","text":"igtf-ca-certs-1.107-1.osg35.el7 osg-ca-certs-1.89-1.osg35.el7 vo-client-107-1.osg35.el7 vo-client-dcache-107-1.osg35.el7 vo-client-lcmaps-voms-107-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-21-2/#enterprise-linux-8_1","text":"igtf-ca-certs-1.107-1.osg35.el8 osg-ca-certs-1.89-1.osg35.el8 vo-client-107-1.osg35.el8 vo-client-dcache-107-1.osg35.el8 vo-client-lcmaps-voms-107-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-21/","text":"OSG Software Release 3.5.21 \u00b6 Release Date: 2020-07-30 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: The worker node client and OASIS are now supported on Enterprise Linux 8 HTCondor-CE 4.4.1 Fixed a race condition that could cause removed jobs to be put on hold Improve performance of the HTCondor-CE View osg-flock 1.1-2: Fix CA requirements to work with osg-ca-scripts osg-pki-tools 3.4.0: Add option to specify Organizational Unit in the CSR osg-system-profiler 1.6.0: Report which Python versions are installed osg-xrootd 3.5-13: Reduce the default amount of logging stashcache-client 6.0.0 : Add Python 3 support These JIRA tickets were addressed in this release. Note To enable OSG repositories on Enterprise Linux 8, follow the OSG repository instructions . Containers \u00b6 The Hosted CE container is available and has been tagged as stable in accordance with our Container Release Policy Hosted CE The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 htcondor-ce-4.4.1-1.osg35.el7 osg-flock-1.1-2.osg35.el7 osg-pki-tools-3.4.0-1.osg35.el7 osg-system-profiler-1.6.0-1.osg35.el7 osg-wn-client-3.5-4.osg35.el7 osg-xrootd-3.5-13.osg35.el7 stashcache-client-6.0.0-1.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.7.3-1.osg35.el8 cvmfs-config-osg-2.4-4.osg35.el8 cvmfs-x509-helper-2.1-2.1.osg35.el8 igtf-ca-certs-1.106-1.osg35.el8 osg-ca-certs-1.88-1.osg35.el8 osg-ca-scripts-1.2.4-1.osg35.el8 osg-oasis-16-5.osg35.el8 osg-system-profiler-1.6.0-1.osg35.el8 osg-update-vos-1.4.0-2.osg35.el8 osg-wn-client-3.5-4.osg35.el8 stashcache-client-6.0.0-1.osg35.el8 vo-client-106-1.osg35.el8 voms-2.1.0-0.14.rc0.1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view osg-flock osg-pki-tools osg-system-profiler osg-system-profiler-viewer osg-wn-client osg-xrootd osg-xrootd-standalone stashcache-client If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 htcondor-ce-4.4.1-1.osg35.el7 htcondor-ce-bosco-4.4.1-1.osg35.el7 htcondor-ce-client-4.4.1-1.osg35.el7 htcondor-ce-collector-4.4.1-1.osg35.el7 htcondor-ce-condor-4.4.1-1.osg35.el7 htcondor-ce-lsf-4.4.1-1.osg35.el7 htcondor-ce-pbs-4.4.1-1.osg35.el7 htcondor-ce-sge-4.4.1-1.osg35.el7 htcondor-ce-slurm-4.4.1-1.osg35.el7 htcondor-ce-view-4.4.1-1.osg35.el7 osg-flock-1.1-2.osg35.el7 osg-pki-tools-3.4.0-1.osg35.el7 osg-system-profiler-1.6.0-1.osg35.el7 osg-system-profiler-viewer-1.6.0-1.osg35.el7 osg-wn-client-3.5-4.osg35.el7 osg-xrootd-3.5-13.osg35.el7 osg-xrootd-standalone-3.5-13.osg35.el7 stashcache-client-6.0.0-1.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.7.3-1.osg35.el8 cvmfs-config-osg-2.4-4.osg35.el8 cvmfs-devel-2.7.3-1.osg35.el8 cvmfs-ducc-2.7.3-1.osg35.el8 cvmfs-fuse3-2.7.3-1.osg35.el8 cvmfs-server-2.7.3-1.osg35.el8 cvmfs-shrinkwrap-2.7.3-1.osg35.el8 cvmfs-unittests-2.7.3-1.osg35.el8 cvmfs-x509-helper-2.1-2.1.osg35.el8 cvmfs-x509-helper-debuginfo-2.1-2.1.osg35.el8 cvmfs-x509-helper-debugsource-2.1-2.1.osg35.el8 igtf-ca-certs-1.106-1.osg35.el8 osg-ca-certs-1.88-1.osg35.el8 osg-ca-scripts-1.2.4-1.osg35.el8 osg-oasis-16-5.osg35.el8 osg-system-profiler-1.6.0-1.osg35.el8 osg-system-profiler-viewer-1.6.0-1.osg35.el8 osg-update-data-1.4.0-2.osg35.el8 osg-update-vos-1.4.0-2.osg35.el8 osg-wn-client-3.5-4.osg35.el8 stashcache-client-6.0.0-1.osg35.el8 vo-client-106-1.osg35.el8 vo-client-dcache-106-1.osg35.el8 vo-client-lcmaps-voms-106-1.osg35.el8 voms-2.1.0-0.14.rc0.1.osg35.el8 voms-clients-cpp-2.1.0-0.14.rc0.1.osg35.el8 voms-clients-cpp-debuginfo-2.1.0-0.14.rc0.1.osg35.el8 voms-debuginfo-2.1.0-0.14.rc0.1.osg35.el8 voms-debugsource-2.1.0-0.14.rc0.1.osg35.el8 voms-devel-2.1.0-0.14.rc0.1.osg35.el8 voms-doc-2.1.0-0.14.rc0.1.osg35.el8 voms-server-2.1.0-0.14.rc0.1.osg35.el8 voms-server-debuginfo-2.1.0-0.14.rc0.1.osg35.el8","title":"Release 3 5 21"},{"location":"release/3.5/release-3-5-21/#osg-software-release-3521","text":"Release Date: 2020-07-30 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.21"},{"location":"release/3.5/release-3-5-21/#summary-of-changes","text":"This release contains: The worker node client and OASIS are now supported on Enterprise Linux 8 HTCondor-CE 4.4.1 Fixed a race condition that could cause removed jobs to be put on hold Improve performance of the HTCondor-CE View osg-flock 1.1-2: Fix CA requirements to work with osg-ca-scripts osg-pki-tools 3.4.0: Add option to specify Organizational Unit in the CSR osg-system-profiler 1.6.0: Report which Python versions are installed osg-xrootd 3.5-13: Reduce the default amount of logging stashcache-client 6.0.0 : Add Python 3 support These JIRA tickets were addressed in this release. Note To enable OSG repositories on Enterprise Linux 8, follow the OSG repository instructions .","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-21/#containers","text":"The Hosted CE container is available and has been tagged as stable in accordance with our Container Release Policy Hosted CE The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-21/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-21/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-21/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-21/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-21/#enterprise-linux-7","text":"htcondor-ce-4.4.1-1.osg35.el7 osg-flock-1.1-2.osg35.el7 osg-pki-tools-3.4.0-1.osg35.el7 osg-system-profiler-1.6.0-1.osg35.el7 osg-wn-client-3.5-4.osg35.el7 osg-xrootd-3.5-13.osg35.el7 stashcache-client-6.0.0-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-21/#enterprise-linux-8","text":"cvmfs-2.7.3-1.osg35.el8 cvmfs-config-osg-2.4-4.osg35.el8 cvmfs-x509-helper-2.1-2.1.osg35.el8 igtf-ca-certs-1.106-1.osg35.el8 osg-ca-certs-1.88-1.osg35.el8 osg-ca-scripts-1.2.4-1.osg35.el8 osg-oasis-16-5.osg35.el8 osg-system-profiler-1.6.0-1.osg35.el8 osg-update-vos-1.4.0-2.osg35.el8 osg-wn-client-3.5-4.osg35.el8 stashcache-client-6.0.0-1.osg35.el8 vo-client-106-1.osg35.el8 voms-2.1.0-0.14.rc0.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-21/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view osg-flock osg-pki-tools osg-system-profiler osg-system-profiler-viewer osg-wn-client osg-xrootd osg-xrootd-standalone stashcache-client If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-21/#enterprise-linux-7_1","text":"htcondor-ce-4.4.1-1.osg35.el7 htcondor-ce-bosco-4.4.1-1.osg35.el7 htcondor-ce-client-4.4.1-1.osg35.el7 htcondor-ce-collector-4.4.1-1.osg35.el7 htcondor-ce-condor-4.4.1-1.osg35.el7 htcondor-ce-lsf-4.4.1-1.osg35.el7 htcondor-ce-pbs-4.4.1-1.osg35.el7 htcondor-ce-sge-4.4.1-1.osg35.el7 htcondor-ce-slurm-4.4.1-1.osg35.el7 htcondor-ce-view-4.4.1-1.osg35.el7 osg-flock-1.1-2.osg35.el7 osg-pki-tools-3.4.0-1.osg35.el7 osg-system-profiler-1.6.0-1.osg35.el7 osg-system-profiler-viewer-1.6.0-1.osg35.el7 osg-wn-client-3.5-4.osg35.el7 osg-xrootd-3.5-13.osg35.el7 osg-xrootd-standalone-3.5-13.osg35.el7 stashcache-client-6.0.0-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-21/#enterprise-linux-8_1","text":"cvmfs-2.7.3-1.osg35.el8 cvmfs-config-osg-2.4-4.osg35.el8 cvmfs-devel-2.7.3-1.osg35.el8 cvmfs-ducc-2.7.3-1.osg35.el8 cvmfs-fuse3-2.7.3-1.osg35.el8 cvmfs-server-2.7.3-1.osg35.el8 cvmfs-shrinkwrap-2.7.3-1.osg35.el8 cvmfs-unittests-2.7.3-1.osg35.el8 cvmfs-x509-helper-2.1-2.1.osg35.el8 cvmfs-x509-helper-debuginfo-2.1-2.1.osg35.el8 cvmfs-x509-helper-debugsource-2.1-2.1.osg35.el8 igtf-ca-certs-1.106-1.osg35.el8 osg-ca-certs-1.88-1.osg35.el8 osg-ca-scripts-1.2.4-1.osg35.el8 osg-oasis-16-5.osg35.el8 osg-system-profiler-1.6.0-1.osg35.el8 osg-system-profiler-viewer-1.6.0-1.osg35.el8 osg-update-data-1.4.0-2.osg35.el8 osg-update-vos-1.4.0-2.osg35.el8 osg-wn-client-3.5-4.osg35.el8 stashcache-client-6.0.0-1.osg35.el8 vo-client-106-1.osg35.el8 vo-client-dcache-106-1.osg35.el8 vo-client-lcmaps-voms-106-1.osg35.el8 voms-2.1.0-0.14.rc0.1.osg35.el8 voms-clients-cpp-2.1.0-0.14.rc0.1.osg35.el8 voms-clients-cpp-debuginfo-2.1.0-0.14.rc0.1.osg35.el8 voms-debuginfo-2.1.0-0.14.rc0.1.osg35.el8 voms-debugsource-2.1.0-0.14.rc0.1.osg35.el8 voms-devel-2.1.0-0.14.rc0.1.osg35.el8 voms-doc-2.1.0-0.14.rc0.1.osg35.el8 voms-server-2.1.0-0.14.rc0.1.osg35.el8 voms-server-debuginfo-2.1.0-0.14.rc0.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-22/","text":"OSG Software Release 3.5.22 \u00b6 Release Date: 2020-08-27 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor 8.8.10 condor_qedit can no longer be used to disrupt the condor_schedd Fixed a bug where the SHARED_PORT_PORT configuration setting was ignored Frontier Squid 4.13-1.1 The release notes contains a couple of relevant security advisories related to cache poisoning. Removed the recursion on the restorecon for SELinux in the %post install step, to avoid taking a long time when going through a large cache. XCache 1.5.2: Added SciTokens support xrootd-scitokens 1.2.2 : Bug fix for allowed issuers gratia-probe 1.20.14 Improved handling of special characters in Slurm cluster names Detect condor/htcondor-ce probe configuration automatically BLAHP 1.18.47 : Added HTCondor submit debugging support oidc-agent 3.3.3: Various bug fixes XRootD plugins: Loosen XRootD version requirements These JIRA tickets were addressed in this release. Note To enable OSG repositories on Enterprise Linux 8, follow the OSG repository instructions . Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 blahp-1.18.47-1.osg35.el7 condor-8.8.10-1.3.osg35.el7 frontier-squid-4.13-1.1.osg35.el7 gratia-probe-1.20.14-1.osg35.el7 oidc-agent-3.3.3-1.1.osg35.el7 xcache-1.5.2-1.osg35.el7 xrootd-hdfs-2.1.7-8.osg35.el7 xrootd-lcmaps-1.7.8-2.osg35.el7 xrootd-multiuser-0.4.3-2.osg35.el7 xrootd-scitokens-1.2.2-1.osg35.el7 Enterprise Linux 8 \u00b6 None RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache blahp blahp-debuginfo cms-xcache condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp frontier-squid gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer minicondor oidc-agent oidc-agent-debuginfo python2-condor python3-condor stash-cache stash-origin xcache xcache-consistency-check xcache-redirector xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-multiuser xrootd-multiuser-debuginfo xrootd-scitokens xrootd-scitokens-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 atlas-xcache-1.5.2-1.osg35.el7 blahp-1.18.47-1.osg35.el7 blahp-debuginfo-1.18.47-1.osg35.el7 cms-xcache-1.5.2-1.osg35.el7 condor-8.8.10-1.3.osg35.el7 condor-all-8.8.10-1.3.osg35.el7 condor-annex-ec2-8.8.10-1.3.osg35.el7 condor-bosco-8.8.10-1.3.osg35.el7 condor-classads-8.8.10-1.3.osg35.el7 condor-classads-devel-8.8.10-1.3.osg35.el7 condor-debuginfo-8.8.10-1.3.osg35.el7 condor-kbdd-8.8.10-1.3.osg35.el7 condor-procd-8.8.10-1.3.osg35.el7 condor-test-8.8.10-1.3.osg35.el7 condor-vm-gahp-8.8.10-1.3.osg35.el7 frontier-squid-4.13-1.1.osg35.el7 gratia-probe-1.20.14-1.osg35.el7 gratia-probe-common-1.20.14-1.osg35.el7 gratia-probe-condor-1.20.14-1.osg35.el7 gratia-probe-condor-events-1.20.14-1.osg35.el7 gratia-probe-dcache-storage-1.20.14-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.14-1.osg35.el7 gratia-probe-dcache-transfer-1.20.14-1.osg35.el7 gratia-probe-debuginfo-1.20.14-1.osg35.el7 gratia-probe-enstore-storage-1.20.14-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.14-1.osg35.el7 gratia-probe-enstore-transfer-1.20.14-1.osg35.el7 gratia-probe-glideinwms-1.20.14-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.14-1.osg35.el7 gratia-probe-hadoop-storage-1.20.14-1.osg35.el7 gratia-probe-htcondor-ce-1.20.14-1.osg35.el7 gratia-probe-lsf-1.20.14-1.osg35.el7 gratia-probe-metric-1.20.14-1.osg35.el7 gratia-probe-onevm-1.20.14-1.osg35.el7 gratia-probe-pbs-lsf-1.20.14-1.osg35.el7 gratia-probe-services-1.20.14-1.osg35.el7 gratia-probe-sge-1.20.14-1.osg35.el7 gratia-probe-slurm-1.20.14-1.osg35.el7 gratia-probe-xrootd-storage-1.20.14-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.14-1.osg35.el7 minicondor-8.8.10-1.3.osg35.el7 oidc-agent-3.3.3-1.1.osg35.el7 oidc-agent-debuginfo-3.3.3-1.1.osg35.el7 python2-condor-8.8.10-1.3.osg35.el7 python3-condor-8.8.10-1.3.osg35.el7 stash-cache-1.5.2-1.osg35.el7 stash-origin-1.5.2-1.osg35.el7 xcache-1.5.2-1.osg35.el7 xcache-consistency-check-1.5.2-1.osg35.el7 xcache-redirector-1.5.2-1.osg35.el7 xrootd-hdfs-2.1.7-8.osg35.el7 xrootd-hdfs-debuginfo-2.1.7-8.osg35.el7 xrootd-hdfs-devel-2.1.7-8.osg35.el7 xrootd-lcmaps-1.7.8-2.osg35.el7 xrootd-lcmaps-debuginfo-1.7.8-2.osg35.el7 xrootd-multiuser-0.4.3-2.osg35.el7 xrootd-multiuser-debuginfo-0.4.3-2.osg35.el7 xrootd-scitokens-1.2.2-1.osg35.el7 xrootd-scitokens-debuginfo-1.2.2-1.osg35.el7 Enterprise Linux 8 \u00b6 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 blahp-1.18.47-2.osgup.el7 Enterprise Linux 8 \u00b6 None Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-1.18.47-2.osgup.el7 blahp-debuginfo-1.18.47-2.osgup.el7 Enterprise Linux 8 \u00b6","title":"Release 3 5 22"},{"location":"release/3.5/release-3-5-22/#osg-software-release-3522","text":"Release Date: 2020-08-27 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.22"},{"location":"release/3.5/release-3-5-22/#summary-of-changes","text":"This release contains: HTCondor 8.8.10 condor_qedit can no longer be used to disrupt the condor_schedd Fixed a bug where the SHARED_PORT_PORT configuration setting was ignored Frontier Squid 4.13-1.1 The release notes contains a couple of relevant security advisories related to cache poisoning. Removed the recursion on the restorecon for SELinux in the %post install step, to avoid taking a long time when going through a large cache. XCache 1.5.2: Added SciTokens support xrootd-scitokens 1.2.2 : Bug fix for allowed issuers gratia-probe 1.20.14 Improved handling of special characters in Slurm cluster names Detect condor/htcondor-ce probe configuration automatically BLAHP 1.18.47 : Added HTCondor submit debugging support oidc-agent 3.3.3: Various bug fixes XRootD plugins: Loosen XRootD version requirements These JIRA tickets were addressed in this release. Note To enable OSG repositories on Enterprise Linux 8, follow the OSG repository instructions .","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-22/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-22/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-22/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-22/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-22/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-22/#enterprise-linux-7","text":"blahp-1.18.47-1.osg35.el7 condor-8.8.10-1.3.osg35.el7 frontier-squid-4.13-1.1.osg35.el7 gratia-probe-1.20.14-1.osg35.el7 oidc-agent-3.3.3-1.1.osg35.el7 xcache-1.5.2-1.osg35.el7 xrootd-hdfs-2.1.7-8.osg35.el7 xrootd-lcmaps-1.7.8-2.osg35.el7 xrootd-multiuser-0.4.3-2.osg35.el7 xrootd-scitokens-1.2.2-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-22/#enterprise-linux-8","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-22/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache blahp blahp-debuginfo cms-xcache condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp frontier-squid gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer minicondor oidc-agent oidc-agent-debuginfo python2-condor python3-condor stash-cache stash-origin xcache xcache-consistency-check xcache-redirector xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-multiuser xrootd-multiuser-debuginfo xrootd-scitokens xrootd-scitokens-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-22/#enterprise-linux-7_1","text":"atlas-xcache-1.5.2-1.osg35.el7 blahp-1.18.47-1.osg35.el7 blahp-debuginfo-1.18.47-1.osg35.el7 cms-xcache-1.5.2-1.osg35.el7 condor-8.8.10-1.3.osg35.el7 condor-all-8.8.10-1.3.osg35.el7 condor-annex-ec2-8.8.10-1.3.osg35.el7 condor-bosco-8.8.10-1.3.osg35.el7 condor-classads-8.8.10-1.3.osg35.el7 condor-classads-devel-8.8.10-1.3.osg35.el7 condor-debuginfo-8.8.10-1.3.osg35.el7 condor-kbdd-8.8.10-1.3.osg35.el7 condor-procd-8.8.10-1.3.osg35.el7 condor-test-8.8.10-1.3.osg35.el7 condor-vm-gahp-8.8.10-1.3.osg35.el7 frontier-squid-4.13-1.1.osg35.el7 gratia-probe-1.20.14-1.osg35.el7 gratia-probe-common-1.20.14-1.osg35.el7 gratia-probe-condor-1.20.14-1.osg35.el7 gratia-probe-condor-events-1.20.14-1.osg35.el7 gratia-probe-dcache-storage-1.20.14-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.14-1.osg35.el7 gratia-probe-dcache-transfer-1.20.14-1.osg35.el7 gratia-probe-debuginfo-1.20.14-1.osg35.el7 gratia-probe-enstore-storage-1.20.14-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.14-1.osg35.el7 gratia-probe-enstore-transfer-1.20.14-1.osg35.el7 gratia-probe-glideinwms-1.20.14-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.14-1.osg35.el7 gratia-probe-hadoop-storage-1.20.14-1.osg35.el7 gratia-probe-htcondor-ce-1.20.14-1.osg35.el7 gratia-probe-lsf-1.20.14-1.osg35.el7 gratia-probe-metric-1.20.14-1.osg35.el7 gratia-probe-onevm-1.20.14-1.osg35.el7 gratia-probe-pbs-lsf-1.20.14-1.osg35.el7 gratia-probe-services-1.20.14-1.osg35.el7 gratia-probe-sge-1.20.14-1.osg35.el7 gratia-probe-slurm-1.20.14-1.osg35.el7 gratia-probe-xrootd-storage-1.20.14-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.14-1.osg35.el7 minicondor-8.8.10-1.3.osg35.el7 oidc-agent-3.3.3-1.1.osg35.el7 oidc-agent-debuginfo-3.3.3-1.1.osg35.el7 python2-condor-8.8.10-1.3.osg35.el7 python3-condor-8.8.10-1.3.osg35.el7 stash-cache-1.5.2-1.osg35.el7 stash-origin-1.5.2-1.osg35.el7 xcache-1.5.2-1.osg35.el7 xcache-consistency-check-1.5.2-1.osg35.el7 xcache-redirector-1.5.2-1.osg35.el7 xrootd-hdfs-2.1.7-8.osg35.el7 xrootd-hdfs-debuginfo-2.1.7-8.osg35.el7 xrootd-hdfs-devel-2.1.7-8.osg35.el7 xrootd-lcmaps-1.7.8-2.osg35.el7 xrootd-lcmaps-debuginfo-1.7.8-2.osg35.el7 xrootd-multiuser-0.4.3-2.osg35.el7 xrootd-multiuser-debuginfo-0.4.3-2.osg35.el7 xrootd-scitokens-1.2.2-1.osg35.el7 xrootd-scitokens-debuginfo-1.2.2-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-22/#enterprise-linux-8_1","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-22/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-22/#enterprise-linux-7_2","text":"blahp-1.18.47-2.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-22/#enterprise-linux-8_2","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-22/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-22/#enterprise-linux-7_3","text":"blahp-1.18.47-2.osgup.el7 blahp-debuginfo-1.18.47-2.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-22/#enterprise-linux-8_3","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-23/","text":"OSG Software Release 3.5.23 \u00b6 Release Date: 2020-09-03 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: xrootd-hdfs 2.1.8: Fixed a bug with loading a library introduced in 2.1.7 osg-release 3.5-4: Minor tweak to spread the load over the mirrors Upcoming: HTCondor 8.9.8 condor_qedit can no longer be used to disrupt the condor_schedd Fixed a bug where the SHARED_PORT_PORT configuration setting was ignored Added htcondor.dags and htcondor.htchirp to the HTCondor Python bindings New condor_watch_q tool that efficiently provides live job status updates Added support for marking a GPU offline while other jobs continue The condor_master command does not return until it is fully started Deprecated several Python interfaces in the Python bindings XRootD 5.0.1 TLS support Starting with version 5.0.0 XRootD protocol supports encryption These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Known Issues \u00b6 There is a known bug in XRootD 5.0.1 that prevents HTTP-TPC from working with X.509 proxies. Sites that utilize HTTP-TPC to move data with FTS should not upgrade to this release. See Gitub xrootd issue #1276 for technical details. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 osg-release-3.5-4.osg35.el7 xrootd-hdfs-2.1.8-1.osg35.el7 Enterprise Linux 8 \u00b6 None RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: osg-release xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 osg-release-3.5-4.osg35.el7 xrootd-hdfs-2.1.8-1.osg35.el7 xrootd-hdfs-debuginfo-2.1.8-1.osg35.el7 xrootd-hdfs-devel-2.1.8-1.osg35.el7 Enterprise Linux 8 \u00b6 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 condor-8.9.8-1.1.osgup.el7 xrootd-5.0.1-1.3.osgup.el7 xrootd-cmstfc-1.5.2-5.osgup.el7 xrootd-hdfs-2.1.8-1.1.osgup.el7 xrootd-lcmaps-1.7.8-3.osgup.el7 xrootd-multiuser-0.4.3-4.osgup.el7 xrootd-rucioN2N-for-Xcache-1.2-3.3.osgup.el7 xrootd-scitokens-1.2.2-1.osgup.el7 Enterprise Linux 8 \u00b6 None Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python2-xrootd python3-condor python3-xrootd xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-cmstfc xrootd-cmstfc-debuginfo xrootd-cmstfc-devel xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-rucioN2N-for-Xcache xrootd-rucioN2N-for-Xcache-debuginfo xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-8.9.8-1.1.osgup.el7 condor-all-8.9.8-1.1.osgup.el7 condor-annex-ec2-8.9.8-1.1.osgup.el7 condor-bosco-8.9.8-1.1.osgup.el7 condor-classads-8.9.8-1.1.osgup.el7 condor-classads-devel-8.9.8-1.1.osgup.el7 condor-debuginfo-8.9.8-1.1.osgup.el7 condor-kbdd-8.9.8-1.1.osgup.el7 condor-procd-8.9.8-1.1.osgup.el7 condor-test-8.9.8-1.1.osgup.el7 condor-vm-gahp-8.9.8-1.1.osgup.el7 minicondor-8.9.8-1.1.osgup.el7 python2-condor-8.9.8-1.1.osgup.el7 python2-xrootd-5.0.1-1.3.osgup.el7 python3-condor-8.9.8-1.1.osgup.el7 python3-xrootd-5.0.1-1.3.osgup.el7 xrootd-5.0.1-1.3.osgup.el7 xrootd-client-5.0.1-1.3.osgup.el7 xrootd-client-compat-5.0.1-1.3.osgup.el7 xrootd-client-devel-5.0.1-1.3.osgup.el7 xrootd-client-libs-5.0.1-1.3.osgup.el7 xrootd-cmstfc-1.5.2-5.osgup.el7 xrootd-cmstfc-debuginfo-1.5.2-5.osgup.el7 xrootd-cmstfc-devel-1.5.2-5.osgup.el7 xrootd-debuginfo-5.0.1-1.3.osgup.el7 xrootd-devel-5.0.1-1.3.osgup.el7 xrootd-doc-5.0.1-1.3.osgup.el7 xrootd-fuse-5.0.1-1.3.osgup.el7 xrootd-hdfs-2.1.8-1.1.osgup.el7 xrootd-hdfs-debuginfo-2.1.8-1.1.osgup.el7 xrootd-hdfs-devel-2.1.8-1.1.osgup.el7 xrootd-lcmaps-1.7.8-3.osgup.el7 xrootd-lcmaps-debuginfo-1.7.8-3.osgup.el7 xrootd-libs-5.0.1-1.3.osgup.el7 xrootd-multiuser-0.4.3-4.osgup.el7 xrootd-multiuser-debuginfo-0.4.3-4.osgup.el7 xrootd-private-devel-5.0.1-1.3.osgup.el7 xrootd-rucioN2N-for-Xcache-1.2-3.3.osgup.el7 xrootd-rucioN2N-for-Xcache-debuginfo-1.2-3.3.osgup.el7 xrootd-scitokens-1.2.2-1.osgup.el7 xrootd-scitokens-debuginfo-1.2.2-1.osgup.el7 xrootd-selinux-5.0.1-1.3.osgup.el7 xrootd-server-5.0.1-1.3.osgup.el7 xrootd-server-compat-5.0.1-1.3.osgup.el7 xrootd-server-devel-5.0.1-1.3.osgup.el7 xrootd-server-libs-5.0.1-1.3.osgup.el7 xrootd-voms-5.0.1-1.3.osgup.el7 Enterprise Linux 8 \u00b6","title":"Release 3 5 23"},{"location":"release/3.5/release-3-5-23/#osg-software-release-3523","text":"Release Date: 2020-09-03 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.23"},{"location":"release/3.5/release-3-5-23/#summary-of-changes","text":"This release contains: xrootd-hdfs 2.1.8: Fixed a bug with loading a library introduced in 2.1.7 osg-release 3.5-4: Minor tweak to spread the load over the mirrors Upcoming: HTCondor 8.9.8 condor_qedit can no longer be used to disrupt the condor_schedd Fixed a bug where the SHARED_PORT_PORT configuration setting was ignored Added htcondor.dags and htcondor.htchirp to the HTCondor Python bindings New condor_watch_q tool that efficiently provides live job status updates Added support for marking a GPU offline while other jobs continue The condor_master command does not return until it is fully started Deprecated several Python interfaces in the Python bindings XRootD 5.0.1 TLS support Starting with version 5.0.0 XRootD protocol supports encryption These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-23/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-23/#known-issues","text":"There is a known bug in XRootD 5.0.1 that prevents HTTP-TPC from working with X.509 proxies. Sites that utilize HTTP-TPC to move data with FTS should not upgrade to this release. See Gitub xrootd issue #1276 for technical details.","title":"Known Issues"},{"location":"release/3.5/release-3-5-23/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-23/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-23/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-23/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-23/#enterprise-linux-7","text":"osg-release-3.5-4.osg35.el7 xrootd-hdfs-2.1.8-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-23/#enterprise-linux-8","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-23/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: osg-release xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-23/#enterprise-linux-7_1","text":"osg-release-3.5-4.osg35.el7 xrootd-hdfs-2.1.8-1.osg35.el7 xrootd-hdfs-debuginfo-2.1.8-1.osg35.el7 xrootd-hdfs-devel-2.1.8-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-23/#enterprise-linux-8_1","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-23/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-23/#enterprise-linux-7_2","text":"condor-8.9.8-1.1.osgup.el7 xrootd-5.0.1-1.3.osgup.el7 xrootd-cmstfc-1.5.2-5.osgup.el7 xrootd-hdfs-2.1.8-1.1.osgup.el7 xrootd-lcmaps-1.7.8-3.osgup.el7 xrootd-multiuser-0.4.3-4.osgup.el7 xrootd-rucioN2N-for-Xcache-1.2-3.3.osgup.el7 xrootd-scitokens-1.2.2-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-23/#enterprise-linux-8_2","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-23/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python2-xrootd python3-condor python3-xrootd xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-cmstfc xrootd-cmstfc-debuginfo xrootd-cmstfc-devel xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-rucioN2N-for-Xcache xrootd-rucioN2N-for-Xcache-debuginfo xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-23/#enterprise-linux-7_3","text":"condor-8.9.8-1.1.osgup.el7 condor-all-8.9.8-1.1.osgup.el7 condor-annex-ec2-8.9.8-1.1.osgup.el7 condor-bosco-8.9.8-1.1.osgup.el7 condor-classads-8.9.8-1.1.osgup.el7 condor-classads-devel-8.9.8-1.1.osgup.el7 condor-debuginfo-8.9.8-1.1.osgup.el7 condor-kbdd-8.9.8-1.1.osgup.el7 condor-procd-8.9.8-1.1.osgup.el7 condor-test-8.9.8-1.1.osgup.el7 condor-vm-gahp-8.9.8-1.1.osgup.el7 minicondor-8.9.8-1.1.osgup.el7 python2-condor-8.9.8-1.1.osgup.el7 python2-xrootd-5.0.1-1.3.osgup.el7 python3-condor-8.9.8-1.1.osgup.el7 python3-xrootd-5.0.1-1.3.osgup.el7 xrootd-5.0.1-1.3.osgup.el7 xrootd-client-5.0.1-1.3.osgup.el7 xrootd-client-compat-5.0.1-1.3.osgup.el7 xrootd-client-devel-5.0.1-1.3.osgup.el7 xrootd-client-libs-5.0.1-1.3.osgup.el7 xrootd-cmstfc-1.5.2-5.osgup.el7 xrootd-cmstfc-debuginfo-1.5.2-5.osgup.el7 xrootd-cmstfc-devel-1.5.2-5.osgup.el7 xrootd-debuginfo-5.0.1-1.3.osgup.el7 xrootd-devel-5.0.1-1.3.osgup.el7 xrootd-doc-5.0.1-1.3.osgup.el7 xrootd-fuse-5.0.1-1.3.osgup.el7 xrootd-hdfs-2.1.8-1.1.osgup.el7 xrootd-hdfs-debuginfo-2.1.8-1.1.osgup.el7 xrootd-hdfs-devel-2.1.8-1.1.osgup.el7 xrootd-lcmaps-1.7.8-3.osgup.el7 xrootd-lcmaps-debuginfo-1.7.8-3.osgup.el7 xrootd-libs-5.0.1-1.3.osgup.el7 xrootd-multiuser-0.4.3-4.osgup.el7 xrootd-multiuser-debuginfo-0.4.3-4.osgup.el7 xrootd-private-devel-5.0.1-1.3.osgup.el7 xrootd-rucioN2N-for-Xcache-1.2-3.3.osgup.el7 xrootd-rucioN2N-for-Xcache-debuginfo-1.2-3.3.osgup.el7 xrootd-scitokens-1.2.2-1.osgup.el7 xrootd-scitokens-debuginfo-1.2.2-1.osgup.el7 xrootd-selinux-5.0.1-1.3.osgup.el7 xrootd-server-5.0.1-1.3.osgup.el7 xrootd-server-compat-5.0.1-1.3.osgup.el7 xrootd-server-devel-5.0.1-1.3.osgup.el7 xrootd-server-libs-5.0.1-1.3.osgup.el7 xrootd-voms-5.0.1-1.3.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-23/#enterprise-linux-8_3","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-24/","text":"OSG Software Release 3.5.24 \u00b6 Release Date: 2020-09-17 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: VO Package v108 Adding the KAGRA VO Updating the VOMS of VIRGO CVMFS 2.7.4 Server: fix ingestion of tarballs with leading slashes Server: fix name clash with certain, concurrently hosted repository names Gateway: fix statistics counting when renaming trees with nested catalogs stashcache-client 6.1.0: Fixes to avoid overloading origin servers hosted-ce-tools 0.8.2: Fix CA update failure when using mixed OS versions CCTools 7.1.7 : Bug fix release (EL7 Only) Packages new to Enterprise Linux 8: Frontier-Squid 4.13-1.1 stashcache-client 6.1.0 hosted-ce-tools 0.8.2 oidc-agent 3.3.3-1.1 These JIRA tickets were addressed in this release. Containers \u00b6 The following containers are available and have been tagged as stable in accordance with our Container Release Policy CMS XCache Frontier Squid Hosted CE Stash Cache Stash Origin The Worker node containers have been updated to this release. Known Issues \u00b6 There is a known bug in XRootD 5.0.1 that prevents HTTP-TPC from working with X.509 proxies. Sites that utilize HTTP-TPC to move data with FTS should not upgrade to this release. See Gitub xrootd issue #1276 for technical details. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 cctools-7.1.7-1.osg35.el7 cvmfs-2.7.4-1.osg35.el7 hosted-ce-tools-0.8-2.osg35.el7 osg-oasis-16-6.osg35.el7 stashcache-client-6.1.0-1.osg35.el7 vo-client-108-1.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.7.4-1.osg35.el8 frontier-squid-4.13-1.1.osg35.el8 hosted-ce-tools-0.8-2.osg35.el8 oidc-agent-3.3.3-1.1.osg35.el8 osg-oasis-16-6.osg35.el8 osg-release-3.5-4.osg35.el8 stashcache-client-6.1.0-1.osg35.el8 vo-client-108-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests hosted-ce-tools osg-oasis stashcache-client vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 cctools-7.1.7-1.osg35.el7 cctools-debuginfo-7.1.7-1.osg35.el7 cctools-devel-7.1.7-1.osg35.el7 cvmfs-2.7.4-1.osg35.el7 cvmfs-devel-2.7.4-1.osg35.el7 cvmfs-ducc-2.7.4-1.osg35.el7 cvmfs-fuse3-2.7.4-1.osg35.el7 cvmfs-server-2.7.4-1.osg35.el7 cvmfs-shrinkwrap-2.7.4-1.osg35.el7 cvmfs-unittests-2.7.4-1.osg35.el7 hosted-ce-tools-0.8-2.osg35.el7 osg-oasis-16-6.osg35.el7 stashcache-client-6.1.0-1.osg35.el7 vo-client-108-1.osg35.el7 vo-client-dcache-108-1.osg35.el7 vo-client-lcmaps-voms-108-1.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.7.4-1.osg35.el8 cvmfs-devel-2.7.4-1.osg35.el8 cvmfs-ducc-2.7.4-1.osg35.el8 cvmfs-fuse3-2.7.4-1.osg35.el8 cvmfs-server-2.7.4-1.osg35.el8 cvmfs-shrinkwrap-2.7.4-1.osg35.el8 cvmfs-unittests-2.7.4-1.osg35.el8 frontier-squid-4.13-1.1.osg35.el8 hosted-ce-tools-0.8-2.osg35.el8 oidc-agent-3.3.3-1.1.osg35.el8 oidc-agent-debuginfo-3.3.3-1.1.osg35.el8 oidc-agent-debugsource-3.3.3-1.1.osg35.el8 osg-oasis-16-6.osg35.el8 osg-release-3.5-4.osg35.el8 stashcache-client-6.1.0-1.osg35.el8 vo-client-108-1.osg35.el8 vo-client-dcache-108-1.osg35.el8 vo-client-lcmaps-voms-108-1.osg35.el8","title":"Release 3 5 24"},{"location":"release/3.5/release-3-5-24/#osg-software-release-3524","text":"Release Date: 2020-09-17 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.24"},{"location":"release/3.5/release-3-5-24/#summary-of-changes","text":"This release contains: VO Package v108 Adding the KAGRA VO Updating the VOMS of VIRGO CVMFS 2.7.4 Server: fix ingestion of tarballs with leading slashes Server: fix name clash with certain, concurrently hosted repository names Gateway: fix statistics counting when renaming trees with nested catalogs stashcache-client 6.1.0: Fixes to avoid overloading origin servers hosted-ce-tools 0.8.2: Fix CA update failure when using mixed OS versions CCTools 7.1.7 : Bug fix release (EL7 Only) Packages new to Enterprise Linux 8: Frontier-Squid 4.13-1.1 stashcache-client 6.1.0 hosted-ce-tools 0.8.2 oidc-agent 3.3.3-1.1 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-24/#containers","text":"The following containers are available and have been tagged as stable in accordance with our Container Release Policy CMS XCache Frontier Squid Hosted CE Stash Cache Stash Origin The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-24/#known-issues","text":"There is a known bug in XRootD 5.0.1 that prevents HTTP-TPC from working with X.509 proxies. Sites that utilize HTTP-TPC to move data with FTS should not upgrade to this release. See Gitub xrootd issue #1276 for technical details.","title":"Known Issues"},{"location":"release/3.5/release-3-5-24/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-24/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-24/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-24/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-24/#enterprise-linux-7","text":"cctools-7.1.7-1.osg35.el7 cvmfs-2.7.4-1.osg35.el7 hosted-ce-tools-0.8-2.osg35.el7 osg-oasis-16-6.osg35.el7 stashcache-client-6.1.0-1.osg35.el7 vo-client-108-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-24/#enterprise-linux-8","text":"cvmfs-2.7.4-1.osg35.el8 frontier-squid-4.13-1.1.osg35.el8 hosted-ce-tools-0.8-2.osg35.el8 oidc-agent-3.3.3-1.1.osg35.el8 osg-oasis-16-6.osg35.el8 osg-release-3.5-4.osg35.el8 stashcache-client-6.1.0-1.osg35.el8 vo-client-108-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-24/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cctools cctools-debuginfo cctools-devel cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests hosted-ce-tools osg-oasis stashcache-client vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-24/#enterprise-linux-7_1","text":"cctools-7.1.7-1.osg35.el7 cctools-debuginfo-7.1.7-1.osg35.el7 cctools-devel-7.1.7-1.osg35.el7 cvmfs-2.7.4-1.osg35.el7 cvmfs-devel-2.7.4-1.osg35.el7 cvmfs-ducc-2.7.4-1.osg35.el7 cvmfs-fuse3-2.7.4-1.osg35.el7 cvmfs-server-2.7.4-1.osg35.el7 cvmfs-shrinkwrap-2.7.4-1.osg35.el7 cvmfs-unittests-2.7.4-1.osg35.el7 hosted-ce-tools-0.8-2.osg35.el7 osg-oasis-16-6.osg35.el7 stashcache-client-6.1.0-1.osg35.el7 vo-client-108-1.osg35.el7 vo-client-dcache-108-1.osg35.el7 vo-client-lcmaps-voms-108-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-24/#enterprise-linux-8_1","text":"cvmfs-2.7.4-1.osg35.el8 cvmfs-devel-2.7.4-1.osg35.el8 cvmfs-ducc-2.7.4-1.osg35.el8 cvmfs-fuse3-2.7.4-1.osg35.el8 cvmfs-server-2.7.4-1.osg35.el8 cvmfs-shrinkwrap-2.7.4-1.osg35.el8 cvmfs-unittests-2.7.4-1.osg35.el8 frontier-squid-4.13-1.1.osg35.el8 hosted-ce-tools-0.8-2.osg35.el8 oidc-agent-3.3.3-1.1.osg35.el8 oidc-agent-debuginfo-3.3.3-1.1.osg35.el8 oidc-agent-debugsource-3.3.3-1.1.osg35.el8 osg-oasis-16-6.osg35.el8 osg-release-3.5-4.osg35.el8 stashcache-client-6.1.0-1.osg35.el8 vo-client-108-1.osg35.el8 vo-client-dcache-108-1.osg35.el8 vo-client-lcmaps-voms-108-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-25-2/","text":"OSG Data Release 3.5.25-2 \u00b6 Release Date: 2020-10-29 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 VO Package v109 Update DN and issuer for WLCG VOMS Server These JIRA tickets were addressed in this release. Containers \u00b6 Worker Node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 vo-client-109-1.osg35.el7 Enterprise Linux 8 \u00b6 vo-client-109-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 vo-client-109-1.osg35.el7 vo-client-dcache-109-1.osg35.el7 vo-client-lcmaps-voms-109-1.osg35.el7 Enterprise Linux 8 \u00b6 vo-client-109-1.osg35.el8 vo-client-dcache-109-1.osg35.el8 vo-client-lcmaps-voms-109-1.osg35.el8","title":"Release 3 5 25 2"},{"location":"release/3.5/release-3-5-25-2/#osg-data-release-3525-2","text":"Release Date: 2020-10-29 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.25-2"},{"location":"release/3.5/release-3-5-25-2/#summary-of-changes","text":"VO Package v109 Update DN and issuer for WLCG VOMS Server These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-25-2/#containers","text":"Worker Node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-25-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-25-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-25-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-25-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-25-2/#enterprise-linux-7","text":"vo-client-109-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-25-2/#enterprise-linux-8","text":"vo-client-109-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-25-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-25-2/#enterprise-linux-7_1","text":"vo-client-109-1.osg35.el7 vo-client-dcache-109-1.osg35.el7 vo-client-lcmaps-voms-109-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-25-2/#enterprise-linux-8_1","text":"vo-client-109-1.osg35.el8 vo-client-dcache-109-1.osg35.el8 vo-client-lcmaps-voms-109-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-25/","text":"OSG Software Release 3.5.25 \u00b6 Release Date: 2020-10-08 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: GlideinWMS 3.6.5 Upgrade from 3.6.2 Improved Singularity support HTCondor's Python based condor_chip in the PATH Support for EL8 worker nodes BLAHP 1.18.48 Add efficient querying of LSF job statuses Improve logic when proxy refresh is disabled These JIRA tickets were addressed in this release. Containers \u00b6 The following containers are available and have been tagged as stable in accordance with our Container Release Policy CMS XCache Hosted CE Stash Cache The Worker node containers have been updated to this release. Known Issues \u00b6 There is a known bug in XRootD 5.0.1 that prevents HTTP-TPC from working with X.509 proxies. Sites that utilize HTTP-TPC to move data with FTS should not upgrade to this release. See Gitub xrootd issue #1276 for technical details. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 blahp-1.18.48-2.osg35.el7 glideinwms-3.6.5-1.osg35.el7 Enterprise Linux 8 \u00b6 None RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-1.18.48-2.osg35.el7 blahp-debuginfo-1.18.48-2.osg35.el7 glideinwms-3.6.5-1.osg35.el7 glideinwms-common-tools-3.6.5-1.osg35.el7 glideinwms-condor-common-config-3.6.5-1.osg35.el7 glideinwms-factory-3.6.5-1.osg35.el7 glideinwms-factory-condor-3.6.5-1.osg35.el7 glideinwms-glidecondor-tools-3.6.5-1.osg35.el7 glideinwms-libs-3.6.5-1.osg35.el7 glideinwms-minimal-condor-3.6.5-1.osg35.el7 glideinwms-usercollector-3.6.5-1.osg35.el7 glideinwms-userschedd-3.6.5-1.osg35.el7 glideinwms-vofrontend-3.6.5-1.osg35.el7 glideinwms-vofrontend-standalone-3.6.5-1.osg35.el7 Enterprise Linux 8 \u00b6 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 blahp-1.18.48-2.osgup.el7 Enterprise Linux 8 \u00b6 None Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-1.18.48-2.osgup.el7 blahp-debuginfo-1.18.48-2.osgup.el7 Enterprise Linux 8 \u00b6","title":"Release 3 5 25"},{"location":"release/3.5/release-3-5-25/#osg-software-release-3525","text":"Release Date: 2020-10-08 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.25"},{"location":"release/3.5/release-3-5-25/#summary-of-changes","text":"This release contains: GlideinWMS 3.6.5 Upgrade from 3.6.2 Improved Singularity support HTCondor's Python based condor_chip in the PATH Support for EL8 worker nodes BLAHP 1.18.48 Add efficient querying of LSF job statuses Improve logic when proxy refresh is disabled These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-25/#containers","text":"The following containers are available and have been tagged as stable in accordance with our Container Release Policy CMS XCache Hosted CE Stash Cache The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-25/#known-issues","text":"There is a known bug in XRootD 5.0.1 that prevents HTTP-TPC from working with X.509 proxies. Sites that utilize HTTP-TPC to move data with FTS should not upgrade to this release. See Gitub xrootd issue #1276 for technical details.","title":"Known Issues"},{"location":"release/3.5/release-3-5-25/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-25/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-25/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-25/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-25/#enterprise-linux-7","text":"blahp-1.18.48-2.osg35.el7 glideinwms-3.6.5-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-25/#enterprise-linux-8","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-25/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-25/#enterprise-linux-7_1","text":"blahp-1.18.48-2.osg35.el7 blahp-debuginfo-1.18.48-2.osg35.el7 glideinwms-3.6.5-1.osg35.el7 glideinwms-common-tools-3.6.5-1.osg35.el7 glideinwms-condor-common-config-3.6.5-1.osg35.el7 glideinwms-factory-3.6.5-1.osg35.el7 glideinwms-factory-condor-3.6.5-1.osg35.el7 glideinwms-glidecondor-tools-3.6.5-1.osg35.el7 glideinwms-libs-3.6.5-1.osg35.el7 glideinwms-minimal-condor-3.6.5-1.osg35.el7 glideinwms-usercollector-3.6.5-1.osg35.el7 glideinwms-userschedd-3.6.5-1.osg35.el7 glideinwms-vofrontend-3.6.5-1.osg35.el7 glideinwms-vofrontend-standalone-3.6.5-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-25/#enterprise-linux-8_1","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-25/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-25/#enterprise-linux-7_2","text":"blahp-1.18.48-2.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-25/#enterprise-linux-8_2","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-25/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-25/#enterprise-linux-7_3","text":"blahp-1.18.48-2.osgup.el7 blahp-debuginfo-1.18.48-2.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-25/#enterprise-linux-8_3","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-26/","text":"OSG Software Release 3.5.26 \u00b6 Release Date: 2020-11-05 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: gfal2 2.18.1: compatible with XRootD-4 to correct installation failures HTCondor 8.8.11 HTCondor now properly tracks usage over vanilla universe checkpoints New ClassAd equality and inequality operators in the Python bindings Fixed a bug where removing in-use routes could crash the job router Fixed a bug where condor_chirp would abort after success on Windows Fixed a bug where using MACHINE_RESOURCE_NAMES could crash the startd Improved condor c-gahp to prioritize commands over file transfers Fixed a rare crash in the schedd when running many local universe jobs With GSI, avoid unnecessary reverse DNS lookup when HOST_ALIAS is set Fix a bug that could cause grid universe jobs to fail upon proxy refresh CVMFS 2.7.5 [client] fix rare crash when kernel meta-data caches operate close to 4GB [client] let mount helper detect when CVMFS_HTTP_PROXY is defined but empty [client] add CVMFS_CLIENT_PROFILE and CVMFS_USE_CDN to the list of known parameters in cvmfs_config osg-flock 1.2: Updated to use HTCondor IDTOKENs python-scitokens: Add Python 3 implementation scitokens-credmon 0.8.1 The Credmon should work with Python 3 now The Credmon subprocess child no longer stalls when the parent goes away The Credmon signals readiness to the condor_master The Credmon Webserver no longer depends on an older version of requests_oauthlib The LocalCredmon has the right parameters when calling \"should_renew()\" so it no longer refreshes every loop The OAuthCredmon no longer excepts when computing token lifetimes in the case of missing metadata Upcoming: XRootD 5.0.2 New Features [Xcache] Phase 1 of checksum integrity implementation (a.k.a pgread). [Monitoring] Implement extensive g-stream enhancements. Major bug fixes [Server] Avoid POSC (persistent-on-succesful-close) deletion when file creation fails because it exists. Minor bug fixes [OFS] Correct missparsing '+cksio' option to avoid config failure. [XRootD] Correct flag reset code for esoteric ssq monitor option. These JIRA tickets were addressed in this release. Containers \u00b6 The following container is available and has been tagged as stable in accordance with our Container Release Policy Stash Origin The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 condor-8.8.11-1.osg35.el7 cvmfs-2.7.5-1.osg35.el7 cvmfs-config-osg-2.5-1.osg35.el7 gfal2-2.18.1-1.osg35.el7 osg-flock-1.2-2.osg35.el7 osg-oasis-16-7.osg35.el7 python-scitokens-1.2.4-3.osg35.el7 scitokens-credmon-0.8.1-1.2.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.7.5-1.osg35.el8 cvmfs-config-osg-2.5-1.osg35.el8 osg-oasis-16-7.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests gfal2 gfal2-all gfal2-debuginfo gfal2-devel gfal2-doc gfal2-plugin-dcap gfal2-plugin-file gfal2-plugin-gridftp gfal2-plugin-http gfal2-plugin-lfc gfal2-plugin-mock gfal2-plugin-rfio gfal2-plugin-sftp gfal2-plugin-srm gfal2-plugin-xrootd minicondor osg-flock osg-oasis python2-condor python2-scitokens python2-scitokens-credmon python3-condor python3-scitokens python-scitokens scitokens-credmon If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-8.8.11-1.osg35.el7 condor-all-8.8.11-1.osg35.el7 condor-annex-ec2-8.8.11-1.osg35.el7 condor-bosco-8.8.11-1.osg35.el7 condor-classads-8.8.11-1.osg35.el7 condor-classads-devel-8.8.11-1.osg35.el7 condor-debuginfo-8.8.11-1.osg35.el7 condor-kbdd-8.8.11-1.osg35.el7 condor-procd-8.8.11-1.osg35.el7 condor-test-8.8.11-1.osg35.el7 condor-vm-gahp-8.8.11-1.osg35.el7 cvmfs-2.7.5-1.osg35.el7 cvmfs-config-osg-2.5-1.osg35.el7 cvmfs-devel-2.7.5-1.osg35.el7 cvmfs-ducc-2.7.5-1.osg35.el7 cvmfs-fuse3-2.7.5-1.osg35.el7 cvmfs-server-2.7.5-1.osg35.el7 cvmfs-shrinkwrap-2.7.5-1.osg35.el7 cvmfs-unittests-2.7.5-1.osg35.el7 gfal2-2.18.1-1.osg35.el7 gfal2-all-2.18.1-1.osg35.el7 gfal2-debuginfo-2.18.1-1.osg35.el7 gfal2-devel-2.18.1-1.osg35.el7 gfal2-doc-2.18.1-1.osg35.el7 gfal2-plugin-dcap-2.18.1-1.osg35.el7 gfal2-plugin-file-2.18.1-1.osg35.el7 gfal2-plugin-gridftp-2.18.1-1.osg35.el7 gfal2-plugin-http-2.18.1-1.osg35.el7 gfal2-plugin-lfc-2.18.1-1.osg35.el7 gfal2-plugin-mock-2.18.1-1.osg35.el7 gfal2-plugin-rfio-2.18.1-1.osg35.el7 gfal2-plugin-sftp-2.18.1-1.osg35.el7 gfal2-plugin-srm-2.18.1-1.osg35.el7 gfal2-plugin-xrootd-2.18.1-1.osg35.el7 minicondor-8.8.11-1.osg35.el7 osg-flock-1.2-2.osg35.el7 osg-oasis-16-7.osg35.el7 python2-condor-8.8.11-1.osg35.el7 python2-scitokens-1.2.4-3.osg35.el7 python2-scitokens-credmon-0.8.1-1.2.osg35.el7 python3-condor-8.8.11-1.osg35.el7 python3-scitokens-1.2.4-3.osg35.el7 python-scitokens-1.2.4-3.osg35.el7 scitokens-credmon-0.8.1-1.2.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.7.5-1.osg35.el8 cvmfs-config-osg-2.5-1.osg35.el8 cvmfs-devel-2.7.5-1.osg35.el8 cvmfs-ducc-2.7.5-1.osg35.el8 cvmfs-fuse3-2.7.5-1.osg35.el8 cvmfs-server-2.7.5-1.osg35.el8 cvmfs-shrinkwrap-2.7.5-1.osg35.el8 cvmfs-unittests-2.7.5-1.osg35.el8 osg-oasis-16-7.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 xrootd-5.0.2-1.osgup.el7 Enterprise Linux 8 \u00b6 None Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: python2-xrootd python3-xrootd xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 python2-xrootd-5.0.2-1.osgup.el7 python3-xrootd-5.0.2-1.osgup.el7 xrootd-5.0.2-1.osgup.el7 xrootd-client-5.0.2-1.osgup.el7 xrootd-client-compat-5.0.2-1.osgup.el7 xrootd-client-devel-5.0.2-1.osgup.el7 xrootd-client-libs-5.0.2-1.osgup.el7 xrootd-debuginfo-5.0.2-1.osgup.el7 xrootd-devel-5.0.2-1.osgup.el7 xrootd-doc-5.0.2-1.osgup.el7 xrootd-fuse-5.0.2-1.osgup.el7 xrootd-libs-5.0.2-1.osgup.el7 xrootd-private-devel-5.0.2-1.osgup.el7 xrootd-selinux-5.0.2-1.osgup.el7 xrootd-server-5.0.2-1.osgup.el7 xrootd-server-compat-5.0.2-1.osgup.el7 xrootd-server-devel-5.0.2-1.osgup.el7 xrootd-server-libs-5.0.2-1.osgup.el7 xrootd-voms-5.0.2-1.osgup.el7 Enterprise Linux 8 \u00b6","title":"Release 3 5 26"},{"location":"release/3.5/release-3-5-26/#osg-software-release-3526","text":"Release Date: 2020-11-05 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.26"},{"location":"release/3.5/release-3-5-26/#summary-of-changes","text":"This release contains: gfal2 2.18.1: compatible with XRootD-4 to correct installation failures HTCondor 8.8.11 HTCondor now properly tracks usage over vanilla universe checkpoints New ClassAd equality and inequality operators in the Python bindings Fixed a bug where removing in-use routes could crash the job router Fixed a bug where condor_chirp would abort after success on Windows Fixed a bug where using MACHINE_RESOURCE_NAMES could crash the startd Improved condor c-gahp to prioritize commands over file transfers Fixed a rare crash in the schedd when running many local universe jobs With GSI, avoid unnecessary reverse DNS lookup when HOST_ALIAS is set Fix a bug that could cause grid universe jobs to fail upon proxy refresh CVMFS 2.7.5 [client] fix rare crash when kernel meta-data caches operate close to 4GB [client] let mount helper detect when CVMFS_HTTP_PROXY is defined but empty [client] add CVMFS_CLIENT_PROFILE and CVMFS_USE_CDN to the list of known parameters in cvmfs_config osg-flock 1.2: Updated to use HTCondor IDTOKENs python-scitokens: Add Python 3 implementation scitokens-credmon 0.8.1 The Credmon should work with Python 3 now The Credmon subprocess child no longer stalls when the parent goes away The Credmon signals readiness to the condor_master The Credmon Webserver no longer depends on an older version of requests_oauthlib The LocalCredmon has the right parameters when calling \"should_renew()\" so it no longer refreshes every loop The OAuthCredmon no longer excepts when computing token lifetimes in the case of missing metadata Upcoming: XRootD 5.0.2 New Features [Xcache] Phase 1 of checksum integrity implementation (a.k.a pgread). [Monitoring] Implement extensive g-stream enhancements. Major bug fixes [Server] Avoid POSC (persistent-on-succesful-close) deletion when file creation fails because it exists. Minor bug fixes [OFS] Correct missparsing '+cksio' option to avoid config failure. [XRootD] Correct flag reset code for esoteric ssq monitor option. These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-26/#containers","text":"The following container is available and has been tagged as stable in accordance with our Container Release Policy Stash Origin The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-26/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-26/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-26/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-26/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-26/#enterprise-linux-7","text":"condor-8.8.11-1.osg35.el7 cvmfs-2.7.5-1.osg35.el7 cvmfs-config-osg-2.5-1.osg35.el7 gfal2-2.18.1-1.osg35.el7 osg-flock-1.2-2.osg35.el7 osg-oasis-16-7.osg35.el7 python-scitokens-1.2.4-3.osg35.el7 scitokens-credmon-0.8.1-1.2.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-26/#enterprise-linux-8","text":"cvmfs-2.7.5-1.osg35.el8 cvmfs-config-osg-2.5-1.osg35.el8 osg-oasis-16-7.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-26/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests gfal2 gfal2-all gfal2-debuginfo gfal2-devel gfal2-doc gfal2-plugin-dcap gfal2-plugin-file gfal2-plugin-gridftp gfal2-plugin-http gfal2-plugin-lfc gfal2-plugin-mock gfal2-plugin-rfio gfal2-plugin-sftp gfal2-plugin-srm gfal2-plugin-xrootd minicondor osg-flock osg-oasis python2-condor python2-scitokens python2-scitokens-credmon python3-condor python3-scitokens python-scitokens scitokens-credmon If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-26/#enterprise-linux-7_1","text":"condor-8.8.11-1.osg35.el7 condor-all-8.8.11-1.osg35.el7 condor-annex-ec2-8.8.11-1.osg35.el7 condor-bosco-8.8.11-1.osg35.el7 condor-classads-8.8.11-1.osg35.el7 condor-classads-devel-8.8.11-1.osg35.el7 condor-debuginfo-8.8.11-1.osg35.el7 condor-kbdd-8.8.11-1.osg35.el7 condor-procd-8.8.11-1.osg35.el7 condor-test-8.8.11-1.osg35.el7 condor-vm-gahp-8.8.11-1.osg35.el7 cvmfs-2.7.5-1.osg35.el7 cvmfs-config-osg-2.5-1.osg35.el7 cvmfs-devel-2.7.5-1.osg35.el7 cvmfs-ducc-2.7.5-1.osg35.el7 cvmfs-fuse3-2.7.5-1.osg35.el7 cvmfs-server-2.7.5-1.osg35.el7 cvmfs-shrinkwrap-2.7.5-1.osg35.el7 cvmfs-unittests-2.7.5-1.osg35.el7 gfal2-2.18.1-1.osg35.el7 gfal2-all-2.18.1-1.osg35.el7 gfal2-debuginfo-2.18.1-1.osg35.el7 gfal2-devel-2.18.1-1.osg35.el7 gfal2-doc-2.18.1-1.osg35.el7 gfal2-plugin-dcap-2.18.1-1.osg35.el7 gfal2-plugin-file-2.18.1-1.osg35.el7 gfal2-plugin-gridftp-2.18.1-1.osg35.el7 gfal2-plugin-http-2.18.1-1.osg35.el7 gfal2-plugin-lfc-2.18.1-1.osg35.el7 gfal2-plugin-mock-2.18.1-1.osg35.el7 gfal2-plugin-rfio-2.18.1-1.osg35.el7 gfal2-plugin-sftp-2.18.1-1.osg35.el7 gfal2-plugin-srm-2.18.1-1.osg35.el7 gfal2-plugin-xrootd-2.18.1-1.osg35.el7 minicondor-8.8.11-1.osg35.el7 osg-flock-1.2-2.osg35.el7 osg-oasis-16-7.osg35.el7 python2-condor-8.8.11-1.osg35.el7 python2-scitokens-1.2.4-3.osg35.el7 python2-scitokens-credmon-0.8.1-1.2.osg35.el7 python3-condor-8.8.11-1.osg35.el7 python3-scitokens-1.2.4-3.osg35.el7 python-scitokens-1.2.4-3.osg35.el7 scitokens-credmon-0.8.1-1.2.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-26/#enterprise-linux-8_1","text":"cvmfs-2.7.5-1.osg35.el8 cvmfs-config-osg-2.5-1.osg35.el8 cvmfs-devel-2.7.5-1.osg35.el8 cvmfs-ducc-2.7.5-1.osg35.el8 cvmfs-fuse3-2.7.5-1.osg35.el8 cvmfs-server-2.7.5-1.osg35.el8 cvmfs-shrinkwrap-2.7.5-1.osg35.el8 cvmfs-unittests-2.7.5-1.osg35.el8 osg-oasis-16-7.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-26/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-26/#enterprise-linux-7_2","text":"xrootd-5.0.2-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-26/#enterprise-linux-8_2","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-26/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: python2-xrootd python3-xrootd xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-26/#enterprise-linux-7_3","text":"python2-xrootd-5.0.2-1.osgup.el7 python3-xrootd-5.0.2-1.osgup.el7 xrootd-5.0.2-1.osgup.el7 xrootd-client-5.0.2-1.osgup.el7 xrootd-client-compat-5.0.2-1.osgup.el7 xrootd-client-devel-5.0.2-1.osgup.el7 xrootd-client-libs-5.0.2-1.osgup.el7 xrootd-debuginfo-5.0.2-1.osgup.el7 xrootd-devel-5.0.2-1.osgup.el7 xrootd-doc-5.0.2-1.osgup.el7 xrootd-fuse-5.0.2-1.osgup.el7 xrootd-libs-5.0.2-1.osgup.el7 xrootd-private-devel-5.0.2-1.osgup.el7 xrootd-selinux-5.0.2-1.osgup.el7 xrootd-server-5.0.2-1.osgup.el7 xrootd-server-compat-5.0.2-1.osgup.el7 xrootd-server-devel-5.0.2-1.osgup.el7 xrootd-server-libs-5.0.2-1.osgup.el7 xrootd-voms-5.0.2-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-26/#enterprise-linux-8_3","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-27/","text":"OSG Software Release 3.5.27 \u00b6 Release Date: 2020-11-12 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: gfal2 2.18.1-1.1: Fixes issue preventing osg-wn-client installations HTCondor 8.9.9 The RPM packages require globus, munge, scitokens, and voms from EPEL Improved cgroup memory policy settings that set both hard and soft limit Cgroup memory usage reporting no longer includes the kernel buffer cache Numerous Python binding improvements, see version history Can create a manifest of files on the execute node at job start and finish Added provisioner nodes to DAGMan, allowing users to provision resources DAGMan can now produce .dot graphs without running the workflow HTCondor now properly tracks usage over vanilla universe checkpoints New ClassAd equality and inequality operators in the Python bindings Fixed a bug where removing in-use routes could crash the job router Fixed a bug where condor_chirp would abort after success on Windows Fixed a bug where using MACHINE_RESOURCE_NAMES could crash the startd Improved condor c-gahp to prioritize commands over file transfers Fixed a rare crash in the schedd when running many local universe jobs With GSI, avoid unnecessary reverse DNS lookup when HOST_ALIAS is set Fix a bug that could cause grid universe jobs to fail upon proxy refresh GlideinWMS 3.7.1 Includes all features and fixes of 3.6.5 Improved Singularity support HTCondor's Python based condor_chip in the PATH Support for EL8 worker nodes Configuration changes to make more compatible with HTCondor 8.9 on initial install SciTokens authentication between Factory and CE IDTokens authentication between Factory and Frontend Bug fix: Factory reconfigs and startups do not require manual creation of directories Bug fix: Factory builds Condor Tarballs to send with glideins correctly These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 gfal2-2.18.1-1.1.osg35.el7 Enterprise Linux 8 \u00b6 None RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: gfal2 gfal2-all gfal2-debuginfo gfal2-devel gfal2-doc gfal2-plugin-dcap gfal2-plugin-file gfal2-plugin-gridftp gfal2-plugin-http gfal2-plugin-lfc gfal2-plugin-mock gfal2-plugin-rfio gfal2-plugin-sftp gfal2-plugin-srm gfal2-plugin-xrootd If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 gfal2-2.18.1-1.1.osg35.el7 gfal2-all-2.18.1-1.1.osg35.el7 gfal2-debuginfo-2.18.1-1.1.osg35.el7 gfal2-devel-2.18.1-1.1.osg35.el7 gfal2-doc-2.18.1-1.1.osg35.el7 gfal2-plugin-dcap-2.18.1-1.1.osg35.el7 gfal2-plugin-file-2.18.1-1.1.osg35.el7 gfal2-plugin-gridftp-2.18.1-1.1.osg35.el7 gfal2-plugin-http-2.18.1-1.1.osg35.el7 gfal2-plugin-lfc-2.18.1-1.1.osg35.el7 gfal2-plugin-mock-2.18.1-1.1.osg35.el7 gfal2-plugin-rfio-2.18.1-1.1.osg35.el7 gfal2-plugin-sftp-2.18.1-1.1.osg35.el7 gfal2-plugin-srm-2.18.1-1.1.osg35.el7 gfal2-plugin-xrootd-2.18.1-1.1.osg35.el7 Enterprise Linux 8 \u00b6 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 condor-8.9.9-1.osgup.el7 glideinwms-3.7.1-1.osgup.el7 xrootd-cmstfc-1.5.2-6.osgup.el7 Enterprise Linux 8 \u00b6 None Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone minicondor python2-condor python3-condor xrootd-cmstfc xrootd-cmstfc-debuginfo xrootd-cmstfc-devel If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-8.9.9-1.osgup.el7 condor-all-8.9.9-1.osgup.el7 condor-annex-ec2-8.9.9-1.osgup.el7 condor-bosco-8.9.9-1.osgup.el7 condor-classads-8.9.9-1.osgup.el7 condor-classads-devel-8.9.9-1.osgup.el7 condor-credmon-oauth-8.9.9-1.osgup.el7 condor-debuginfo-8.9.9-1.osgup.el7 condor-kbdd-8.9.9-1.osgup.el7 condor-procd-8.9.9-1.osgup.el7 condor-test-8.9.9-1.osgup.el7 condor-vm-gahp-8.9.9-1.osgup.el7 glideinwms-3.7.1-1.osgup.el7 glideinwms-common-tools-3.7.1-1.osgup.el7 glideinwms-condor-common-config-3.7.1-1.osgup.el7 glideinwms-factory-3.7.1-1.osgup.el7 glideinwms-factory-condor-3.7.1-1.osgup.el7 glideinwms-glidecondor-tools-3.7.1-1.osgup.el7 glideinwms-libs-3.7.1-1.osgup.el7 glideinwms-minimal-condor-3.7.1-1.osgup.el7 glideinwms-usercollector-3.7.1-1.osgup.el7 glideinwms-userschedd-3.7.1-1.osgup.el7 glideinwms-vofrontend-3.7.1-1.osgup.el7 glideinwms-vofrontend-standalone-3.7.1-1.osgup.el7 minicondor-8.9.9-1.osgup.el7 python2-condor-8.9.9-1.osgup.el7 python3-condor-8.9.9-1.osgup.el7 xrootd-cmstfc-1.5.2-6.osgup.el7 xrootd-cmstfc-debuginfo-1.5.2-6.osgup.el7 xrootd-cmstfc-devel-1.5.2-6.osgup.el7 Enterprise Linux 8 \u00b6","title":"Release 3 5 27"},{"location":"release/3.5/release-3-5-27/#osg-software-release-3527","text":"Release Date: 2020-11-12 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.27"},{"location":"release/3.5/release-3-5-27/#summary-of-changes","text":"This release contains: gfal2 2.18.1-1.1: Fixes issue preventing osg-wn-client installations HTCondor 8.9.9 The RPM packages require globus, munge, scitokens, and voms from EPEL Improved cgroup memory policy settings that set both hard and soft limit Cgroup memory usage reporting no longer includes the kernel buffer cache Numerous Python binding improvements, see version history Can create a manifest of files on the execute node at job start and finish Added provisioner nodes to DAGMan, allowing users to provision resources DAGMan can now produce .dot graphs without running the workflow HTCondor now properly tracks usage over vanilla universe checkpoints New ClassAd equality and inequality operators in the Python bindings Fixed a bug where removing in-use routes could crash the job router Fixed a bug where condor_chirp would abort after success on Windows Fixed a bug where using MACHINE_RESOURCE_NAMES could crash the startd Improved condor c-gahp to prioritize commands over file transfers Fixed a rare crash in the schedd when running many local universe jobs With GSI, avoid unnecessary reverse DNS lookup when HOST_ALIAS is set Fix a bug that could cause grid universe jobs to fail upon proxy refresh GlideinWMS 3.7.1 Includes all features and fixes of 3.6.5 Improved Singularity support HTCondor's Python based condor_chip in the PATH Support for EL8 worker nodes Configuration changes to make more compatible with HTCondor 8.9 on initial install SciTokens authentication between Factory and CE IDTokens authentication between Factory and Frontend Bug fix: Factory reconfigs and startups do not require manual creation of directories Bug fix: Factory builds Condor Tarballs to send with glideins correctly These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-27/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-27/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-27/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-27/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-27/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-27/#enterprise-linux-7","text":"gfal2-2.18.1-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-27/#enterprise-linux-8","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-27/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: gfal2 gfal2-all gfal2-debuginfo gfal2-devel gfal2-doc gfal2-plugin-dcap gfal2-plugin-file gfal2-plugin-gridftp gfal2-plugin-http gfal2-plugin-lfc gfal2-plugin-mock gfal2-plugin-rfio gfal2-plugin-sftp gfal2-plugin-srm gfal2-plugin-xrootd If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-27/#enterprise-linux-7_1","text":"gfal2-2.18.1-1.1.osg35.el7 gfal2-all-2.18.1-1.1.osg35.el7 gfal2-debuginfo-2.18.1-1.1.osg35.el7 gfal2-devel-2.18.1-1.1.osg35.el7 gfal2-doc-2.18.1-1.1.osg35.el7 gfal2-plugin-dcap-2.18.1-1.1.osg35.el7 gfal2-plugin-file-2.18.1-1.1.osg35.el7 gfal2-plugin-gridftp-2.18.1-1.1.osg35.el7 gfal2-plugin-http-2.18.1-1.1.osg35.el7 gfal2-plugin-lfc-2.18.1-1.1.osg35.el7 gfal2-plugin-mock-2.18.1-1.1.osg35.el7 gfal2-plugin-rfio-2.18.1-1.1.osg35.el7 gfal2-plugin-sftp-2.18.1-1.1.osg35.el7 gfal2-plugin-srm-2.18.1-1.1.osg35.el7 gfal2-plugin-xrootd-2.18.1-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-27/#enterprise-linux-8_1","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-27/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-27/#enterprise-linux-7_2","text":"condor-8.9.9-1.osgup.el7 glideinwms-3.7.1-1.osgup.el7 xrootd-cmstfc-1.5.2-6.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-27/#enterprise-linux-8_2","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-27/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone minicondor python2-condor python3-condor xrootd-cmstfc xrootd-cmstfc-debuginfo xrootd-cmstfc-devel If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-27/#enterprise-linux-7_3","text":"condor-8.9.9-1.osgup.el7 condor-all-8.9.9-1.osgup.el7 condor-annex-ec2-8.9.9-1.osgup.el7 condor-bosco-8.9.9-1.osgup.el7 condor-classads-8.9.9-1.osgup.el7 condor-classads-devel-8.9.9-1.osgup.el7 condor-credmon-oauth-8.9.9-1.osgup.el7 condor-debuginfo-8.9.9-1.osgup.el7 condor-kbdd-8.9.9-1.osgup.el7 condor-procd-8.9.9-1.osgup.el7 condor-test-8.9.9-1.osgup.el7 condor-vm-gahp-8.9.9-1.osgup.el7 glideinwms-3.7.1-1.osgup.el7 glideinwms-common-tools-3.7.1-1.osgup.el7 glideinwms-condor-common-config-3.7.1-1.osgup.el7 glideinwms-factory-3.7.1-1.osgup.el7 glideinwms-factory-condor-3.7.1-1.osgup.el7 glideinwms-glidecondor-tools-3.7.1-1.osgup.el7 glideinwms-libs-3.7.1-1.osgup.el7 glideinwms-minimal-condor-3.7.1-1.osgup.el7 glideinwms-usercollector-3.7.1-1.osgup.el7 glideinwms-userschedd-3.7.1-1.osgup.el7 glideinwms-vofrontend-3.7.1-1.osgup.el7 glideinwms-vofrontend-standalone-3.7.1-1.osgup.el7 minicondor-8.9.9-1.osgup.el7 python2-condor-8.9.9-1.osgup.el7 python3-condor-8.9.9-1.osgup.el7 xrootd-cmstfc-1.5.2-6.osgup.el7 xrootd-cmstfc-debuginfo-1.5.2-6.osgup.el7 xrootd-cmstfc-devel-1.5.2-6.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-27/#enterprise-linux-8_3","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-28-2/","text":"OSG Data Release 3.5.28-2 \u00b6 Release Date: 2020-12-15 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 CA Certificates based on IGTF 1.108 Added DigitalTrust classic IGTF specific and public trust IGTF CAs (AE) Updated PCS MyProxy SLCS CRL URL location (US) These JIRA tickets were addressed in this release. Containers \u00b6 Worker Node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 igtf-ca-certs-1.108-1.osg35.el7 osg-ca-certs-1.91-1.osg35.el7 Enterprise Linux 8 \u00b6 igtf-ca-certs-1.108-1.osg35.el7 osg-ca-certs-1.91-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 igtf-ca-certs-1.108-1.osg35.el7 osg-ca-certs-1.91-1.osg35.el7 Enterprise Linux 8 \u00b6 igtf-ca-certs-1.108-1.osg35.el8 osg-ca-certs-1.91-1.osg35.el8","title":"Release 3 5 28 2"},{"location":"release/3.5/release-3-5-28-2/#osg-data-release-3528-2","text":"Release Date: 2020-12-15 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.28-2"},{"location":"release/3.5/release-3-5-28-2/#summary-of-changes","text":"CA Certificates based on IGTF 1.108 Added DigitalTrust classic IGTF specific and public trust IGTF CAs (AE) Updated PCS MyProxy SLCS CRL URL location (US) These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-28-2/#containers","text":"Worker Node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-28-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-28-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-28-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-28-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-28-2/#enterprise-linux-7","text":"igtf-ca-certs-1.108-1.osg35.el7 osg-ca-certs-1.91-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-28-2/#enterprise-linux-8","text":"igtf-ca-certs-1.108-1.osg35.el7 osg-ca-certs-1.91-1.osg35.el7","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-28-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-28-2/#enterprise-linux-7_1","text":"igtf-ca-certs-1.108-1.osg35.el7 osg-ca-certs-1.91-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-28-2/#enterprise-linux-8_1","text":"igtf-ca-certs-1.108-1.osg35.el8 osg-ca-certs-1.91-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-28/","text":"OSG Software Release 3.5.28 \u00b6 Release Date: 2020-12-10 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: osg-ca-certs 1.90: Add new Let's Encrypt intermediate CAs (EL7 and EL8) (fixes issues with newly issued host certficates) htgettoken 1.0: get OIDC bearer tokens by interacting with Hashicorp vault XRootD 4.12.5 (EL7 Only) [XrdCl] Fix regression in recursive copy (introduced in f6723e00) [VOMS] Do not touch the Entity.name field; especially converting spaces [VOMS] Fix improper collection of multi-VO cert attributes [RPM] Refine xrootd-voms obsoletes/provides for vomsxrd [RPM] Remove libXrdSecgsiVOMS-4.so from xrootd-libs pkg [pfc] Make sure v4 does not try to read v5 cinfo files [XrdHttp] Shutdown the connection in the case of an unrecognized HTTP first line. HTCondor 8.8.12 (EL7 Only) Added a family of version comparison functions to ClassAds Increased default Globus proxy key length to meet current NIST guidance Upcoming: HTCondor 8.9.10 (EL7 Only) Fix bug where negotiator stopped making matches when group quotas are used Support OAuth, SciTokens, and Kerberos credentials in local universe jobs The Python schedd.submit method now takes a Submit object DAGMan can now optionally run a script when a job goes on hold DAGMan now provides a method for inline jobs to share submit descriptions Can now add arbitrary tags to condor annex instances Runs the \"singularity test\" before running the a singularity job These JIRA tickets were addressed in this release. Containers \u00b6 The following container is available and has been tagged as stable in accordance with our Container Release Policy CMS XCache Hosted CE The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 blahp-1.18.48-2.1.osg35.el7 condor-8.8.12-1.1.osg35.el7 hosted-ce-tools-0.9-1.osg35.el7 htgettoken-1.0-1.osg35.el7 osg-ca-certs-1.90-1.osg35.el7 xrootd-4.12.5-1.osg35.el7 Enterprise Linux 8 \u00b6 htgettoken-1.0-1.osg35.el8 osg-ca-certs-1.90-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp hosted-ce-tools htgettoken minicondor osg-ca-certs python2-condor python2-xrootd python3-condor xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-1.18.48-2.1.osg35.el7 blahp-debuginfo-1.18.48-2.1.osg35.el7 condor-8.8.12-1.1.osg35.el7 condor-all-8.8.12-1.1.osg35.el7 condor-annex-ec2-8.8.12-1.1.osg35.el7 condor-bosco-8.8.12-1.1.osg35.el7 condor-classads-8.8.12-1.1.osg35.el7 condor-classads-devel-8.8.12-1.1.osg35.el7 condor-debuginfo-8.8.12-1.1.osg35.el7 condor-kbdd-8.8.12-1.1.osg35.el7 condor-procd-8.8.12-1.1.osg35.el7 condor-test-8.8.12-1.1.osg35.el7 condor-vm-gahp-8.8.12-1.1.osg35.el7 hosted-ce-tools-0.9-1.osg35.el7 htgettoken-1.0-1.osg35.el7 minicondor-8.8.12-1.1.osg35.el7 osg-ca-certs-1.90-1.osg35.el7 python2-condor-8.8.12-1.1.osg35.el7 python2-xrootd-4.12.5-1.osg35.el7 python3-condor-8.8.12-1.1.osg35.el7 xrootd-4.12.5-1.osg35.el7 xrootd-client-4.12.5-1.osg35.el7 xrootd-client-devel-4.12.5-1.osg35.el7 xrootd-client-libs-4.12.5-1.osg35.el7 xrootd-debuginfo-4.12.5-1.osg35.el7 xrootd-devel-4.12.5-1.osg35.el7 xrootd-doc-4.12.5-1.osg35.el7 xrootd-fuse-4.12.5-1.osg35.el7 xrootd-libs-4.12.5-1.osg35.el7 xrootd-private-devel-4.12.5-1.osg35.el7 xrootd-selinux-4.12.5-1.osg35.el7 xrootd-server-4.12.5-1.osg35.el7 xrootd-server-devel-4.12.5-1.osg35.el7 xrootd-server-libs-4.12.5-1.osg35.el7 xrootd-voms-4.12.5-1.osg35.el7 Enterprise Linux 8 \u00b6 htgettoken-1.0-1.osg35.el8 osg-ca-certs-1.90-1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 blahp-1.18.48-2.2.osgup.el7 condor-8.9.10-1.osgup.el7 Enterprise Linux 8 \u00b6 None Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-1.18.48-2.2.osgup.el7 blahp-debuginfo-1.18.48-2.2.osgup.el7 condor-8.9.10-1.osgup.el7 condor-all-8.9.10-1.osgup.el7 condor-annex-ec2-8.9.10-1.osgup.el7 condor-bosco-8.9.10-1.osgup.el7 condor-classads-8.9.10-1.osgup.el7 condor-classads-devel-8.9.10-1.osgup.el7 condor-credmon-oauth-8.9.10-1.osgup.el7 condor-debuginfo-8.9.10-1.osgup.el7 condor-kbdd-8.9.10-1.osgup.el7 condor-procd-8.9.10-1.osgup.el7 condor-test-8.9.10-1.osgup.el7 condor-vm-gahp-8.9.10-1.osgup.el7 minicondor-8.9.10-1.osgup.el7 python2-condor-8.9.10-1.osgup.el7 python3-condor-8.9.10-1.osgup.el7 Enterprise Linux 8 \u00b6","title":"Release 3 5 28"},{"location":"release/3.5/release-3-5-28/#osg-software-release-3528","text":"Release Date: 2020-12-10 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.28"},{"location":"release/3.5/release-3-5-28/#summary-of-changes","text":"This release contains: osg-ca-certs 1.90: Add new Let's Encrypt intermediate CAs (EL7 and EL8) (fixes issues with newly issued host certficates) htgettoken 1.0: get OIDC bearer tokens by interacting with Hashicorp vault XRootD 4.12.5 (EL7 Only) [XrdCl] Fix regression in recursive copy (introduced in f6723e00) [VOMS] Do not touch the Entity.name field; especially converting spaces [VOMS] Fix improper collection of multi-VO cert attributes [RPM] Refine xrootd-voms obsoletes/provides for vomsxrd [RPM] Remove libXrdSecgsiVOMS-4.so from xrootd-libs pkg [pfc] Make sure v4 does not try to read v5 cinfo files [XrdHttp] Shutdown the connection in the case of an unrecognized HTTP first line. HTCondor 8.8.12 (EL7 Only) Added a family of version comparison functions to ClassAds Increased default Globus proxy key length to meet current NIST guidance Upcoming: HTCondor 8.9.10 (EL7 Only) Fix bug where negotiator stopped making matches when group quotas are used Support OAuth, SciTokens, and Kerberos credentials in local universe jobs The Python schedd.submit method now takes a Submit object DAGMan can now optionally run a script when a job goes on hold DAGMan now provides a method for inline jobs to share submit descriptions Can now add arbitrary tags to condor annex instances Runs the \"singularity test\" before running the a singularity job These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-28/#containers","text":"The following container is available and has been tagged as stable in accordance with our Container Release Policy CMS XCache Hosted CE The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-28/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-28/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-28/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-28/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-28/#enterprise-linux-7","text":"blahp-1.18.48-2.1.osg35.el7 condor-8.8.12-1.1.osg35.el7 hosted-ce-tools-0.9-1.osg35.el7 htgettoken-1.0-1.osg35.el7 osg-ca-certs-1.90-1.osg35.el7 xrootd-4.12.5-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-28/#enterprise-linux-8","text":"htgettoken-1.0-1.osg35.el8 osg-ca-certs-1.90-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-28/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp hosted-ce-tools htgettoken minicondor osg-ca-certs python2-condor python2-xrootd python3-condor xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-28/#enterprise-linux-7_1","text":"blahp-1.18.48-2.1.osg35.el7 blahp-debuginfo-1.18.48-2.1.osg35.el7 condor-8.8.12-1.1.osg35.el7 condor-all-8.8.12-1.1.osg35.el7 condor-annex-ec2-8.8.12-1.1.osg35.el7 condor-bosco-8.8.12-1.1.osg35.el7 condor-classads-8.8.12-1.1.osg35.el7 condor-classads-devel-8.8.12-1.1.osg35.el7 condor-debuginfo-8.8.12-1.1.osg35.el7 condor-kbdd-8.8.12-1.1.osg35.el7 condor-procd-8.8.12-1.1.osg35.el7 condor-test-8.8.12-1.1.osg35.el7 condor-vm-gahp-8.8.12-1.1.osg35.el7 hosted-ce-tools-0.9-1.osg35.el7 htgettoken-1.0-1.osg35.el7 minicondor-8.8.12-1.1.osg35.el7 osg-ca-certs-1.90-1.osg35.el7 python2-condor-8.8.12-1.1.osg35.el7 python2-xrootd-4.12.5-1.osg35.el7 python3-condor-8.8.12-1.1.osg35.el7 xrootd-4.12.5-1.osg35.el7 xrootd-client-4.12.5-1.osg35.el7 xrootd-client-devel-4.12.5-1.osg35.el7 xrootd-client-libs-4.12.5-1.osg35.el7 xrootd-debuginfo-4.12.5-1.osg35.el7 xrootd-devel-4.12.5-1.osg35.el7 xrootd-doc-4.12.5-1.osg35.el7 xrootd-fuse-4.12.5-1.osg35.el7 xrootd-libs-4.12.5-1.osg35.el7 xrootd-private-devel-4.12.5-1.osg35.el7 xrootd-selinux-4.12.5-1.osg35.el7 xrootd-server-4.12.5-1.osg35.el7 xrootd-server-devel-4.12.5-1.osg35.el7 xrootd-server-libs-4.12.5-1.osg35.el7 xrootd-voms-4.12.5-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-28/#enterprise-linux-8_1","text":"htgettoken-1.0-1.osg35.el8 osg-ca-certs-1.90-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-28/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-28/#enterprise-linux-7_2","text":"blahp-1.18.48-2.2.osgup.el7 condor-8.9.10-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-28/#enterprise-linux-8_2","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-28/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-28/#enterprise-linux-7_3","text":"blahp-1.18.48-2.2.osgup.el7 blahp-debuginfo-1.18.48-2.2.osgup.el7 condor-8.9.10-1.osgup.el7 condor-all-8.9.10-1.osgup.el7 condor-annex-ec2-8.9.10-1.osgup.el7 condor-bosco-8.9.10-1.osgup.el7 condor-classads-8.9.10-1.osgup.el7 condor-classads-devel-8.9.10-1.osgup.el7 condor-credmon-oauth-8.9.10-1.osgup.el7 condor-debuginfo-8.9.10-1.osgup.el7 condor-kbdd-8.9.10-1.osgup.el7 condor-procd-8.9.10-1.osgup.el7 condor-test-8.9.10-1.osgup.el7 condor-vm-gahp-8.9.10-1.osgup.el7 minicondor-8.9.10-1.osgup.el7 python2-condor-8.9.10-1.osgup.el7 python3-condor-8.9.10-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-28/#enterprise-linux-8_3","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-29/","text":"OSG Software Release 3.5.29 \u00b6 Release Date: 2021-01-21 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Updated CA certificates based on IGTF 1.109 : Removed discontinued DM private IGTF classic CAs (AE) Removed obsolete QuoVadis-Root-CA1, under which no ICAs are left (BM) Updated QV Grid ICA G2 intermediary following its re-issuance (BM) osg-configure 3.11.0 Added Pilot sections for sites to request different types of pilots from the factory Make fetch-crl success optional (though give a warning on failure) Don't try to resolve the Squid server, since it only needs to be visible from the workers, not the CE htgettoken 1.1 : Additional options to integrate with HTCondor Upcoming: GlideinWMS 3.7.2 (EL7 Only) Can now run custom script at the end of glidein execution Prevent failures when CE disappears from OSG Collector Bug fix: Frontend no longer over-generates tokens for entries Bug fix: Factory does not remove glideins when idle limit is hit These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 htgettoken-1.1-1.osg35.el7 igtf-ca-certs-1.109-1.osg35.el7 osg-ca-certs-1.92-1.osg35.el7 osg-configure-3.11.0-1.osg35.el7 Enterprise Linux 8 \u00b6 htgettoken-1.1-1.osg35.el8 igtf-ca-certs-1.109-1.osg35.el8 osg-ca-certs-1.92-1.osg35.el8 osg-configure-3.11.0-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: htgettoken igtf-ca-certs osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-misc osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-siteinfo osg-configure-slurm osg-configure-squid osg-configure-tests If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 htgettoken-1.1-1.osg35.el7 igtf-ca-certs-1.109-1.osg35.el7 osg-ca-certs-1.92-1.osg35.el7 osg-configure-3.11.0-1.osg35.el7 osg-configure-bosco-3.11.0-1.osg35.el7 osg-configure-ce-3.11.0-1.osg35.el7 osg-configure-condor-3.11.0-1.osg35.el7 osg-configure-gateway-3.11.0-1.osg35.el7 osg-configure-gip-3.11.0-1.osg35.el7 osg-configure-gratia-3.11.0-1.osg35.el7 osg-configure-infoservices-3.11.0-1.osg35.el7 osg-configure-lsf-3.11.0-1.osg35.el7 osg-configure-misc-3.11.0-1.osg35.el7 osg-configure-pbs-3.11.0-1.osg35.el7 osg-configure-rsv-3.11.0-1.osg35.el7 osg-configure-sge-3.11.0-1.osg35.el7 osg-configure-siteinfo-3.11.0-1.osg35.el7 osg-configure-slurm-3.11.0-1.osg35.el7 osg-configure-squid-3.11.0-1.osg35.el7 osg-configure-tests-3.11.0-1.osg35.el7 Enterprise Linux 8 \u00b6 htgettoken-1.1-1.osg35.el8 igtf-ca-certs-1.109-1.osg35.el8 osg-ca-certs-1.92-1.osg35.el8 osg-configure-3.11.0-1.osg35.el8 osg-configure-bosco-3.11.0-1.osg35.el8 osg-configure-ce-3.11.0-1.osg35.el8 osg-configure-condor-3.11.0-1.osg35.el8 osg-configure-gateway-3.11.0-1.osg35.el8 osg-configure-gip-3.11.0-1.osg35.el8 osg-configure-gratia-3.11.0-1.osg35.el8 osg-configure-infoservices-3.11.0-1.osg35.el8 osg-configure-lsf-3.11.0-1.osg35.el8 osg-configure-misc-3.11.0-1.osg35.el8 osg-configure-pbs-3.11.0-1.osg35.el8 osg-configure-rsv-3.11.0-1.osg35.el8 osg-configure-sge-3.11.0-1.osg35.el8 osg-configure-siteinfo-3.11.0-1.osg35.el8 osg-configure-slurm-3.11.0-1.osg35.el8 osg-configure-squid-3.11.0-1.osg35.el8 osg-configure-tests-3.11.0-1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 glideinwms-3.7.2-1.osgup.el7 Enterprise Linux 8 \u00b6 None Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 glideinwms-3.7.2-1.osgup.el7 glideinwms-common-tools-3.7.2-1.osgup.el7 glideinwms-condor-common-config-3.7.2-1.osgup.el7 glideinwms-factory-3.7.2-1.osgup.el7 glideinwms-factory-condor-3.7.2-1.osgup.el7 glideinwms-glidecondor-tools-3.7.2-1.osgup.el7 glideinwms-libs-3.7.2-1.osgup.el7 glideinwms-minimal-condor-3.7.2-1.osgup.el7 glideinwms-usercollector-3.7.2-1.osgup.el7 glideinwms-userschedd-3.7.2-1.osgup.el7 glideinwms-vofrontend-3.7.2-1.osgup.el7 glideinwms-vofrontend-standalone-3.7.2-1.osgup.el7 Enterprise Linux 8 \u00b6","title":"Release 3 5 29"},{"location":"release/3.5/release-3-5-29/#osg-software-release-3529","text":"Release Date: 2021-01-21 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.29"},{"location":"release/3.5/release-3-5-29/#summary-of-changes","text":"This release contains: Updated CA certificates based on IGTF 1.109 : Removed discontinued DM private IGTF classic CAs (AE) Removed obsolete QuoVadis-Root-CA1, under which no ICAs are left (BM) Updated QV Grid ICA G2 intermediary following its re-issuance (BM) osg-configure 3.11.0 Added Pilot sections for sites to request different types of pilots from the factory Make fetch-crl success optional (though give a warning on failure) Don't try to resolve the Squid server, since it only needs to be visible from the workers, not the CE htgettoken 1.1 : Additional options to integrate with HTCondor Upcoming: GlideinWMS 3.7.2 (EL7 Only) Can now run custom script at the end of glidein execution Prevent failures when CE disappears from OSG Collector Bug fix: Frontend no longer over-generates tokens for entries Bug fix: Factory does not remove glideins when idle limit is hit These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-29/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-29/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-29/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-29/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-29/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-29/#enterprise-linux-7","text":"htgettoken-1.1-1.osg35.el7 igtf-ca-certs-1.109-1.osg35.el7 osg-ca-certs-1.92-1.osg35.el7 osg-configure-3.11.0-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-29/#enterprise-linux-8","text":"htgettoken-1.1-1.osg35.el8 igtf-ca-certs-1.109-1.osg35.el8 osg-ca-certs-1.92-1.osg35.el8 osg-configure-3.11.0-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-29/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: htgettoken igtf-ca-certs osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-misc osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-siteinfo osg-configure-slurm osg-configure-squid osg-configure-tests If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-29/#enterprise-linux-7_1","text":"htgettoken-1.1-1.osg35.el7 igtf-ca-certs-1.109-1.osg35.el7 osg-ca-certs-1.92-1.osg35.el7 osg-configure-3.11.0-1.osg35.el7 osg-configure-bosco-3.11.0-1.osg35.el7 osg-configure-ce-3.11.0-1.osg35.el7 osg-configure-condor-3.11.0-1.osg35.el7 osg-configure-gateway-3.11.0-1.osg35.el7 osg-configure-gip-3.11.0-1.osg35.el7 osg-configure-gratia-3.11.0-1.osg35.el7 osg-configure-infoservices-3.11.0-1.osg35.el7 osg-configure-lsf-3.11.0-1.osg35.el7 osg-configure-misc-3.11.0-1.osg35.el7 osg-configure-pbs-3.11.0-1.osg35.el7 osg-configure-rsv-3.11.0-1.osg35.el7 osg-configure-sge-3.11.0-1.osg35.el7 osg-configure-siteinfo-3.11.0-1.osg35.el7 osg-configure-slurm-3.11.0-1.osg35.el7 osg-configure-squid-3.11.0-1.osg35.el7 osg-configure-tests-3.11.0-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-29/#enterprise-linux-8_1","text":"htgettoken-1.1-1.osg35.el8 igtf-ca-certs-1.109-1.osg35.el8 osg-ca-certs-1.92-1.osg35.el8 osg-configure-3.11.0-1.osg35.el8 osg-configure-bosco-3.11.0-1.osg35.el8 osg-configure-ce-3.11.0-1.osg35.el8 osg-configure-condor-3.11.0-1.osg35.el8 osg-configure-gateway-3.11.0-1.osg35.el8 osg-configure-gip-3.11.0-1.osg35.el8 osg-configure-gratia-3.11.0-1.osg35.el8 osg-configure-infoservices-3.11.0-1.osg35.el8 osg-configure-lsf-3.11.0-1.osg35.el8 osg-configure-misc-3.11.0-1.osg35.el8 osg-configure-pbs-3.11.0-1.osg35.el8 osg-configure-rsv-3.11.0-1.osg35.el8 osg-configure-sge-3.11.0-1.osg35.el8 osg-configure-siteinfo-3.11.0-1.osg35.el8 osg-configure-slurm-3.11.0-1.osg35.el8 osg-configure-squid-3.11.0-1.osg35.el8 osg-configure-tests-3.11.0-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-29/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-29/#enterprise-linux-7_2","text":"glideinwms-3.7.2-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-29/#enterprise-linux-8_2","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-29/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-29/#enterprise-linux-7_3","text":"glideinwms-3.7.2-1.osgup.el7 glideinwms-common-tools-3.7.2-1.osgup.el7 glideinwms-condor-common-config-3.7.2-1.osgup.el7 glideinwms-factory-3.7.2-1.osgup.el7 glideinwms-factory-condor-3.7.2-1.osgup.el7 glideinwms-glidecondor-tools-3.7.2-1.osgup.el7 glideinwms-libs-3.7.2-1.osgup.el7 glideinwms-minimal-condor-3.7.2-1.osgup.el7 glideinwms-usercollector-3.7.2-1.osgup.el7 glideinwms-userschedd-3.7.2-1.osgup.el7 glideinwms-vofrontend-3.7.2-1.osgup.el7 glideinwms-vofrontend-standalone-3.7.2-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-29/#enterprise-linux-8_3","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-3/","text":"OSG Software Release 3.5.3 \u00b6 Release Date: 2019-10-17 Supported OS Versions: EL7 Summary of Changes \u00b6 This release contains: GlideinWMS 3.6 : New stable version Manual adjustments required when updating a GlideinWMS factory Compatible with HTCondor 8.6.x, 8.8.x, and 8.9.x oidc-agent 3.2.6 : Tools for managing OpenID Connect tokens scitokens-cpp 0.3.4 : Add support for Identity and Access Management (IAM) XRootD 4.10.1 : Make third party client check bogus-response proof HTCondor 8.8.5 Major upgrade from version 8.6.13 OSG Upgrade Instructions HTCondor Upgrade Release Notes Enhanced security with default configuration added by OSG bosco_cluster pulls tarball via HTTPS Added support for customizations to remote BOSCO installations osg-configure 3.1.0 : Add support for bosco_cluster override directories gratia-probe 1.20.11: Updates to work better with Slurm Upcoming repository: HTCondor 8.9.3 TOKEN and SSL authentication methods are now enabled by default The job and global event logs use ISO 8601 formatted dates by default Added Google Drive multifile transfer plugin Added upload capability to Box multifile transfer plugin Added Python bindings to submit a DAG Python 'JobEventLog' can be pickled to facilitate intermittent readers 2x matchmaking speed for partitionable slots with simple START expressions Improved the performance of the condor_schedd under heavy load Reduced the memory footprint of condor_dagman Initial implementation to record the circumstances of a job's termination These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Known Issues \u00b6 OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.5-1.6.osg35.el7 glideinwms-3.6-1.osg35.el7 gratia-probe-1.20.11-1.osg35.el7 oidc-agent-3.2.6-1.osg35.el7 osg-configure-3.1.0-1.osg35.el7 scitokens-cpp-0.3.4-1.osg35.el7 xrootd-4.10.1-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer minicondor oidc-agent oidc-agent-debuginfo osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-misc osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-siteinfo osg-configure-slurm osg-configure-squid osg-configure-tests python2-condor python2-xrootd python3-condor scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.5-1.6.osg35.el7 condor-all-8.8.5-1.6.osg35.el7 condor-annex-ec2-8.8.5-1.6.osg35.el7 condor-bosco-8.8.5-1.6.osg35.el7 condor-classads-8.8.5-1.6.osg35.el7 condor-classads-devel-8.8.5-1.6.osg35.el7 condor-debuginfo-8.8.5-1.6.osg35.el7 condor-kbdd-8.8.5-1.6.osg35.el7 condor-procd-8.8.5-1.6.osg35.el7 condor-test-8.8.5-1.6.osg35.el7 condor-vm-gahp-8.8.5-1.6.osg35.el7 glideinwms-3.6-1.osg35.el7 glideinwms-common-tools-3.6-1.osg35.el7 glideinwms-condor-common-config-3.6-1.osg35.el7 glideinwms-factory-3.6-1.osg35.el7 glideinwms-factory-condor-3.6-1.osg35.el7 glideinwms-glidecondor-tools-3.6-1.osg35.el7 glideinwms-libs-3.6-1.osg35.el7 glideinwms-minimal-condor-3.6-1.osg35.el7 glideinwms-usercollector-3.6-1.osg35.el7 glideinwms-userschedd-3.6-1.osg35.el7 glideinwms-vofrontend-3.6-1.osg35.el7 glideinwms-vofrontend-standalone-3.6-1.osg35.el7 gratia-probe-1.20.11-1.osg35.el7 gratia-probe-common-1.20.11-1.osg35.el7 gratia-probe-condor-1.20.11-1.osg35.el7 gratia-probe-condor-events-1.20.11-1.osg35.el7 gratia-probe-dcache-storage-1.20.11-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.11-1.osg35.el7 gratia-probe-dcache-transfer-1.20.11-1.osg35.el7 gratia-probe-debuginfo-1.20.11-1.osg35.el7 gratia-probe-enstore-storage-1.20.11-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.11-1.osg35.el7 gratia-probe-enstore-transfer-1.20.11-1.osg35.el7 gratia-probe-glideinwms-1.20.11-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.11-1.osg35.el7 gratia-probe-hadoop-storage-1.20.11-1.osg35.el7 gratia-probe-htcondor-ce-1.20.11-1.osg35.el7 gratia-probe-lsf-1.20.11-1.osg35.el7 gratia-probe-metric-1.20.11-1.osg35.el7 gratia-probe-onevm-1.20.11-1.osg35.el7 gratia-probe-pbs-lsf-1.20.11-1.osg35.el7 gratia-probe-services-1.20.11-1.osg35.el7 gratia-probe-sge-1.20.11-1.osg35.el7 gratia-probe-slurm-1.20.11-1.osg35.el7 gratia-probe-xrootd-storage-1.20.11-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.11-1.osg35.el7 minicondor-8.8.5-1.6.osg35.el7 oidc-agent-3.2.6-1.osg35.el7 oidc-agent-debuginfo-3.2.6-1.osg35.el7 osg-configure-3.1.0-1.osg35.el7 osg-configure-bosco-3.1.0-1.osg35.el7 osg-configure-ce-3.1.0-1.osg35.el7 osg-configure-condor-3.1.0-1.osg35.el7 osg-configure-gateway-3.1.0-1.osg35.el7 osg-configure-gip-3.1.0-1.osg35.el7 osg-configure-gratia-3.1.0-1.osg35.el7 osg-configure-infoservices-3.1.0-1.osg35.el7 osg-configure-lsf-3.1.0-1.osg35.el7 osg-configure-misc-3.1.0-1.osg35.el7 osg-configure-pbs-3.1.0-1.osg35.el7 osg-configure-rsv-3.1.0-1.osg35.el7 osg-configure-sge-3.1.0-1.osg35.el7 osg-configure-siteinfo-3.1.0-1.osg35.el7 osg-configure-slurm-3.1.0-1.osg35.el7 osg-configure-squid-3.1.0-1.osg35.el7 osg-configure-tests-3.1.0-1.osg35.el7 python2-condor-8.8.5-1.6.osg35.el7 python2-xrootd-4.10.1-1.osg35.el7 python3-condor-8.8.5-1.6.osg35.el7 scitokens-cpp-0.3.4-1.osg35.el7 scitokens-cpp-debuginfo-0.3.4-1.osg35.el7 scitokens-cpp-devel-0.3.4-1.osg35.el7 xrootd-4.10.1-1.osg35.el7 xrootd-client-4.10.1-1.osg35.el7 xrootd-client-devel-4.10.1-1.osg35.el7 xrootd-client-libs-4.10.1-1.osg35.el7 xrootd-debuginfo-4.10.1-1.osg35.el7 xrootd-devel-4.10.1-1.osg35.el7 xrootd-doc-4.10.1-1.osg35.el7 xrootd-fuse-4.10.1-1.osg35.el7 xrootd-libs-4.10.1-1.osg35.el7 xrootd-private-devel-4.10.1-1.osg35.el7 xrootd-selinux-4.10.1-1.osg35.el7 xrootd-server-4.10.1-1.osg35.el7 xrootd-server-devel-4.10.1-1.osg35.el7 xrootd-server-libs-4.10.1-1.osg35.el7 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. condor-8.9.3-1.1.osgup.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.9.3-1.1.osgup.el7 condor-all-8.9.3-1.1.osgup.el7 condor-annex-ec2-8.9.3-1.1.osgup.el7 condor-bosco-8.9.3-1.1.osgup.el7 condor-classads-8.9.3-1.1.osgup.el7 condor-classads-devel-8.9.3-1.1.osgup.el7 condor-debuginfo-8.9.3-1.1.osgup.el7 condor-kbdd-8.9.3-1.1.osgup.el7 condor-procd-8.9.3-1.1.osgup.el7 condor-test-8.9.3-1.1.osgup.el7 condor-vm-gahp-8.9.3-1.1.osgup.el7 minicondor-8.9.3-1.1.osgup.el7 python2-condor-8.9.3-1.1.osgup.el7 python3-condor-8.9.3-1.1.osgup.el7","title":"Release 3 5 3"},{"location":"release/3.5/release-3-5-3/#osg-software-release-353","text":"Release Date: 2019-10-17 Supported OS Versions: EL7","title":"OSG Software Release 3.5.3"},{"location":"release/3.5/release-3-5-3/#summary-of-changes","text":"This release contains: GlideinWMS 3.6 : New stable version Manual adjustments required when updating a GlideinWMS factory Compatible with HTCondor 8.6.x, 8.8.x, and 8.9.x oidc-agent 3.2.6 : Tools for managing OpenID Connect tokens scitokens-cpp 0.3.4 : Add support for Identity and Access Management (IAM) XRootD 4.10.1 : Make third party client check bogus-response proof HTCondor 8.8.5 Major upgrade from version 8.6.13 OSG Upgrade Instructions HTCondor Upgrade Release Notes Enhanced security with default configuration added by OSG bosco_cluster pulls tarball via HTTPS Added support for customizations to remote BOSCO installations osg-configure 3.1.0 : Add support for bosco_cluster override directories gratia-probe 1.20.11: Updates to work better with Slurm Upcoming repository: HTCondor 8.9.3 TOKEN and SSL authentication methods are now enabled by default The job and global event logs use ISO 8601 formatted dates by default Added Google Drive multifile transfer plugin Added upload capability to Box multifile transfer plugin Added Python bindings to submit a DAG Python 'JobEventLog' can be pickled to facilitate intermittent readers 2x matchmaking speed for partitionable slots with simple START expressions Improved the performance of the condor_schedd under heavy load Reduced the memory footprint of condor_dagman Initial implementation to record the circumstances of a job's termination These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-3/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-3/#known-issues","text":"OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-3/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-3/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-3/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-3/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.5-1.6.osg35.el7 glideinwms-3.6-1.osg35.el7 gratia-probe-1.20.11-1.osg35.el7 oidc-agent-3.2.6-1.osg35.el7 osg-configure-3.1.0-1.osg35.el7 scitokens-cpp-0.3.4-1.osg35.el7 xrootd-4.10.1-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-3/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer minicondor oidc-agent oidc-agent-debuginfo osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-misc osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-siteinfo osg-configure-slurm osg-configure-squid osg-configure-tests python2-condor python2-xrootd python3-condor scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.5-1.6.osg35.el7 condor-all-8.8.5-1.6.osg35.el7 condor-annex-ec2-8.8.5-1.6.osg35.el7 condor-bosco-8.8.5-1.6.osg35.el7 condor-classads-8.8.5-1.6.osg35.el7 condor-classads-devel-8.8.5-1.6.osg35.el7 condor-debuginfo-8.8.5-1.6.osg35.el7 condor-kbdd-8.8.5-1.6.osg35.el7 condor-procd-8.8.5-1.6.osg35.el7 condor-test-8.8.5-1.6.osg35.el7 condor-vm-gahp-8.8.5-1.6.osg35.el7 glideinwms-3.6-1.osg35.el7 glideinwms-common-tools-3.6-1.osg35.el7 glideinwms-condor-common-config-3.6-1.osg35.el7 glideinwms-factory-3.6-1.osg35.el7 glideinwms-factory-condor-3.6-1.osg35.el7 glideinwms-glidecondor-tools-3.6-1.osg35.el7 glideinwms-libs-3.6-1.osg35.el7 glideinwms-minimal-condor-3.6-1.osg35.el7 glideinwms-usercollector-3.6-1.osg35.el7 glideinwms-userschedd-3.6-1.osg35.el7 glideinwms-vofrontend-3.6-1.osg35.el7 glideinwms-vofrontend-standalone-3.6-1.osg35.el7 gratia-probe-1.20.11-1.osg35.el7 gratia-probe-common-1.20.11-1.osg35.el7 gratia-probe-condor-1.20.11-1.osg35.el7 gratia-probe-condor-events-1.20.11-1.osg35.el7 gratia-probe-dcache-storage-1.20.11-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.11-1.osg35.el7 gratia-probe-dcache-transfer-1.20.11-1.osg35.el7 gratia-probe-debuginfo-1.20.11-1.osg35.el7 gratia-probe-enstore-storage-1.20.11-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.11-1.osg35.el7 gratia-probe-enstore-transfer-1.20.11-1.osg35.el7 gratia-probe-glideinwms-1.20.11-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.11-1.osg35.el7 gratia-probe-hadoop-storage-1.20.11-1.osg35.el7 gratia-probe-htcondor-ce-1.20.11-1.osg35.el7 gratia-probe-lsf-1.20.11-1.osg35.el7 gratia-probe-metric-1.20.11-1.osg35.el7 gratia-probe-onevm-1.20.11-1.osg35.el7 gratia-probe-pbs-lsf-1.20.11-1.osg35.el7 gratia-probe-services-1.20.11-1.osg35.el7 gratia-probe-sge-1.20.11-1.osg35.el7 gratia-probe-slurm-1.20.11-1.osg35.el7 gratia-probe-xrootd-storage-1.20.11-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.11-1.osg35.el7 minicondor-8.8.5-1.6.osg35.el7 oidc-agent-3.2.6-1.osg35.el7 oidc-agent-debuginfo-3.2.6-1.osg35.el7 osg-configure-3.1.0-1.osg35.el7 osg-configure-bosco-3.1.0-1.osg35.el7 osg-configure-ce-3.1.0-1.osg35.el7 osg-configure-condor-3.1.0-1.osg35.el7 osg-configure-gateway-3.1.0-1.osg35.el7 osg-configure-gip-3.1.0-1.osg35.el7 osg-configure-gratia-3.1.0-1.osg35.el7 osg-configure-infoservices-3.1.0-1.osg35.el7 osg-configure-lsf-3.1.0-1.osg35.el7 osg-configure-misc-3.1.0-1.osg35.el7 osg-configure-pbs-3.1.0-1.osg35.el7 osg-configure-rsv-3.1.0-1.osg35.el7 osg-configure-sge-3.1.0-1.osg35.el7 osg-configure-siteinfo-3.1.0-1.osg35.el7 osg-configure-slurm-3.1.0-1.osg35.el7 osg-configure-squid-3.1.0-1.osg35.el7 osg-configure-tests-3.1.0-1.osg35.el7 python2-condor-8.8.5-1.6.osg35.el7 python2-xrootd-4.10.1-1.osg35.el7 python3-condor-8.8.5-1.6.osg35.el7 scitokens-cpp-0.3.4-1.osg35.el7 scitokens-cpp-debuginfo-0.3.4-1.osg35.el7 scitokens-cpp-devel-0.3.4-1.osg35.el7 xrootd-4.10.1-1.osg35.el7 xrootd-client-4.10.1-1.osg35.el7 xrootd-client-devel-4.10.1-1.osg35.el7 xrootd-client-libs-4.10.1-1.osg35.el7 xrootd-debuginfo-4.10.1-1.osg35.el7 xrootd-devel-4.10.1-1.osg35.el7 xrootd-doc-4.10.1-1.osg35.el7 xrootd-fuse-4.10.1-1.osg35.el7 xrootd-libs-4.10.1-1.osg35.el7 xrootd-private-devel-4.10.1-1.osg35.el7 xrootd-selinux-4.10.1-1.osg35.el7 xrootd-server-4.10.1-1.osg35.el7 xrootd-server-devel-4.10.1-1.osg35.el7 xrootd-server-libs-4.10.1-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-3/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. condor-8.9.3-1.1.osgup.el7","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-3/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.9.3-1.1.osgup.el7 condor-all-8.9.3-1.1.osgup.el7 condor-annex-ec2-8.9.3-1.1.osgup.el7 condor-bosco-8.9.3-1.1.osgup.el7 condor-classads-8.9.3-1.1.osgup.el7 condor-classads-devel-8.9.3-1.1.osgup.el7 condor-debuginfo-8.9.3-1.1.osgup.el7 condor-kbdd-8.9.3-1.1.osgup.el7 condor-procd-8.9.3-1.1.osgup.el7 condor-test-8.9.3-1.1.osgup.el7 condor-vm-gahp-8.9.3-1.1.osgup.el7 minicondor-8.9.3-1.1.osgup.el7 python2-condor-8.9.3-1.1.osgup.el7 python3-condor-8.9.3-1.1.osgup.el7","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-30/","text":"OSG Software Release 3.5.30 \u00b6 Release Date: 2021-01-27 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Upcoming Repository: HTCondor 8.9.11 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0001 HTCONDOR-2021-0002 These JIRA tickets were addressed in this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 None Enterprise Linux 8 \u00b6 None RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: None If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 Enterprise Linux 8 \u00b6 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 condor-8.9.11-1.osgup.el7 Enterprise Linux 8 \u00b6 None Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-8.9.11-1.osgup.el7 condor-all-8.9.11-1.osgup.el7 condor-annex-ec2-8.9.11-1.osgup.el7 condor-bosco-8.9.11-1.osgup.el7 condor-classads-8.9.11-1.osgup.el7 condor-classads-devel-8.9.11-1.osgup.el7 condor-credmon-oauth-8.9.11-1.osgup.el7 condor-debuginfo-8.9.11-1.osgup.el7 condor-kbdd-8.9.11-1.osgup.el7 condor-procd-8.9.11-1.osgup.el7 condor-test-8.9.11-1.osgup.el7 condor-vm-gahp-8.9.11-1.osgup.el7 minicondor-8.9.11-1.osgup.el7 python2-condor-8.9.11-1.osgup.el7 python3-condor-8.9.11-1.osgup.el7 Enterprise Linux 8 \u00b6","title":"Release 3 5 30"},{"location":"release/3.5/release-3-5-30/#osg-software-release-3530","text":"Release Date: 2021-01-27 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.30"},{"location":"release/3.5/release-3-5-30/#summary-of-changes","text":"This release contains: Upcoming Repository: HTCondor 8.9.11 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0001 HTCONDOR-2021-0002 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-30/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-30/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-30/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-30/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-30/#enterprise-linux-7","text":"None","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-30/#enterprise-linux-8","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-30/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: None If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-30/#enterprise-linux-7_1","text":"","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-30/#enterprise-linux-8_1","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-30/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-30/#enterprise-linux-7_2","text":"condor-8.9.11-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-30/#enterprise-linux-8_2","text":"None","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-30/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-30/#enterprise-linux-7_3","text":"condor-8.9.11-1.osgup.el7 condor-all-8.9.11-1.osgup.el7 condor-annex-ec2-8.9.11-1.osgup.el7 condor-bosco-8.9.11-1.osgup.el7 condor-classads-8.9.11-1.osgup.el7 condor-classads-devel-8.9.11-1.osgup.el7 condor-credmon-oauth-8.9.11-1.osgup.el7 condor-debuginfo-8.9.11-1.osgup.el7 condor-kbdd-8.9.11-1.osgup.el7 condor-procd-8.9.11-1.osgup.el7 condor-test-8.9.11-1.osgup.el7 condor-vm-gahp-8.9.11-1.osgup.el7 minicondor-8.9.11-1.osgup.el7 python2-condor-8.9.11-1.osgup.el7 python3-condor-8.9.11-1.osgup.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-30/#enterprise-linux-8_3","text":"","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-31/","text":"OSG Software Release 3.5.31 \u00b6 Release Date: 2021-02-04 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: CVMFS 2.8.0 A new \u201cservice container\u201d aimed at easier kubernetes deployments Support for macOS 11 Big Sur Support for Windows Services for Linux (WSL2) Parallelized garbage collection for greatly reduced GC durations Support for generating podman image store meta-data in DUCC Ability to show the diff of the current transaction using the cvmfs_server diff --worktree command Two new experimental features: \u201ctemplate transactions\u201d and ephemeral publish containers XRootD 4.12.6 : Bug fix release osg-ca-certs 1.94: Added cross-signing Let's Encrypt root CA osg-release 3.5-5: Make upcoming repositories OSG version specific osg-flock 1.3: Enable schedd audit log python-scitokens 1.3.1 Added option to specify the lifetime of generated tokens Fix dependency change of behavior in PyJWT lcmaps 1.6.6: Initial version on EL8 These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 cvmfs-2.8.0-1.osg35.el7 osg-ca-certs-1.94-1.osg35.el7 osg-flock-1.3-1.osg35.el7 osg-oasis-17-1.osg35.el7 osg-release-3.5-5.osg35.el7 python-scitokens-1.3.1-1.osg35.el7 xrootd-4.12.6-1.1.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.8.0-1.osg35.el8 lcmaps-1.6.6-1.14.osg35.el8 lcmaps-plugins-basic-1.7.0-2.osg35.el8 lcmaps-plugins-verify-proxy-1.5.11-1.1.osg35.el8 lcmaps-plugins-voms-1.7.1-1.6.osg35.el8 osg-ca-certs-1.94-1.osg35.el8 osg-oasis-17-1.osg35.el8 osg-release-3.5-5.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests osg-ca-certs osg-flock osg-oasis osg-release python2-scitokens python2-xrootd python3-scitokens python-scitokens xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 cvmfs-2.8.0-1.osg35.el7 cvmfs-devel-2.8.0-1.osg35.el7 cvmfs-ducc-2.8.0-1.osg35.el7 cvmfs-fuse3-2.8.0-1.osg35.el7 cvmfs-server-2.8.0-1.osg35.el7 cvmfs-shrinkwrap-2.8.0-1.osg35.el7 cvmfs-unittests-2.8.0-1.osg35.el7 osg-ca-certs-1.94-1.osg35.el7 osg-flock-1.3-1.osg35.el7 osg-oasis-17-1.osg35.el7 osg-release-3.5-5.osg35.el7 python2-scitokens-1.3.1-1.osg35.el7 python2-xrootd-4.12.6-1.1.osg35.el7 python3-scitokens-1.3.1-1.osg35.el7 python-scitokens-1.3.1-1.osg35.el7 xrootd-4.12.6-1.1.osg35.el7 xrootd-client-4.12.6-1.1.osg35.el7 xrootd-client-devel-4.12.6-1.1.osg35.el7 xrootd-client-libs-4.12.6-1.1.osg35.el7 xrootd-debuginfo-4.12.6-1.1.osg35.el7 xrootd-devel-4.12.6-1.1.osg35.el7 xrootd-doc-4.12.6-1.1.osg35.el7 xrootd-fuse-4.12.6-1.1.osg35.el7 xrootd-libs-4.12.6-1.1.osg35.el7 xrootd-private-devel-4.12.6-1.1.osg35.el7 xrootd-selinux-4.12.6-1.1.osg35.el7 xrootd-server-4.12.6-1.1.osg35.el7 xrootd-server-devel-4.12.6-1.1.osg35.el7 xrootd-server-libs-4.12.6-1.1.osg35.el7 xrootd-voms-4.12.6-1.1.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.8.0-1.osg35.el8 cvmfs-devel-2.8.0-1.osg35.el8 cvmfs-ducc-2.8.0-1.osg35.el8 cvmfs-fuse3-2.8.0-1.osg35.el8 cvmfs-server-2.8.0-1.osg35.el8 cvmfs-shrinkwrap-2.8.0-1.osg35.el8 cvmfs-unittests-2.8.0-1.osg35.el8 lcmaps-1.6.6-1.14.osg35.el8 lcmaps-common-devel-1.6.6-1.14.osg35.el8 lcmaps-db-templates-1.6.6-1.14.osg35.el8 lcmaps-debuginfo-1.6.6-1.14.osg35.el8 lcmaps-debugsource-1.6.6-1.14.osg35.el8 lcmaps-devel-1.6.6-1.14.osg35.el8 lcmaps-plugins-basic-1.7.0-2.osg35.el8 lcmaps-plugins-basic-debuginfo-1.7.0-2.osg35.el8 lcmaps-plugins-basic-debugsource-1.7.0-2.osg35.el8 lcmaps-plugins-basic-ldap-1.7.0-2.osg35.el8 lcmaps-plugins-basic-ldap-debuginfo-1.7.0-2.osg35.el8 lcmaps-plugins-verify-proxy-1.5.11-1.1.osg35.el8 lcmaps-plugins-verify-proxy-debuginfo-1.5.11-1.1.osg35.el8 lcmaps-plugins-verify-proxy-debugsource-1.5.11-1.1.osg35.el8 lcmaps-plugins-voms-1.7.1-1.6.osg35.el8 lcmaps-plugins-voms-debuginfo-1.7.1-1.6.osg35.el8 lcmaps-plugins-voms-debugsource-1.7.1-1.6.osg35.el8 lcmaps-without-gsi-1.6.6-1.14.osg35.el8 lcmaps-without-gsi-debuginfo-1.6.6-1.14.osg35.el8 lcmaps-without-gsi-devel-1.6.6-1.14.osg35.el8 osg-ca-certs-1.94-1.osg35.el8 osg-oasis-17-1.osg35.el8 osg-release-3.5-5.osg35.el8","title":"Release 3 5 31"},{"location":"release/3.5/release-3-5-31/#osg-software-release-3531","text":"Release Date: 2021-02-04 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.31"},{"location":"release/3.5/release-3-5-31/#summary-of-changes","text":"This release contains: CVMFS 2.8.0 A new \u201cservice container\u201d aimed at easier kubernetes deployments Support for macOS 11 Big Sur Support for Windows Services for Linux (WSL2) Parallelized garbage collection for greatly reduced GC durations Support for generating podman image store meta-data in DUCC Ability to show the diff of the current transaction using the cvmfs_server diff --worktree command Two new experimental features: \u201ctemplate transactions\u201d and ephemeral publish containers XRootD 4.12.6 : Bug fix release osg-ca-certs 1.94: Added cross-signing Let's Encrypt root CA osg-release 3.5-5: Make upcoming repositories OSG version specific osg-flock 1.3: Enable schedd audit log python-scitokens 1.3.1 Added option to specify the lifetime of generated tokens Fix dependency change of behavior in PyJWT lcmaps 1.6.6: Initial version on EL8 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-31/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-31/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-31/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-31/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-31/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-31/#enterprise-linux-7","text":"cvmfs-2.8.0-1.osg35.el7 osg-ca-certs-1.94-1.osg35.el7 osg-flock-1.3-1.osg35.el7 osg-oasis-17-1.osg35.el7 osg-release-3.5-5.osg35.el7 python-scitokens-1.3.1-1.osg35.el7 xrootd-4.12.6-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-31/#enterprise-linux-8","text":"cvmfs-2.8.0-1.osg35.el8 lcmaps-1.6.6-1.14.osg35.el8 lcmaps-plugins-basic-1.7.0-2.osg35.el8 lcmaps-plugins-verify-proxy-1.5.11-1.1.osg35.el8 lcmaps-plugins-voms-1.7.1-1.6.osg35.el8 osg-ca-certs-1.94-1.osg35.el8 osg-oasis-17-1.osg35.el8 osg-release-3.5-5.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-31/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests osg-ca-certs osg-flock osg-oasis osg-release python2-scitokens python2-xrootd python3-scitokens python-scitokens xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-31/#enterprise-linux-7_1","text":"cvmfs-2.8.0-1.osg35.el7 cvmfs-devel-2.8.0-1.osg35.el7 cvmfs-ducc-2.8.0-1.osg35.el7 cvmfs-fuse3-2.8.0-1.osg35.el7 cvmfs-server-2.8.0-1.osg35.el7 cvmfs-shrinkwrap-2.8.0-1.osg35.el7 cvmfs-unittests-2.8.0-1.osg35.el7 osg-ca-certs-1.94-1.osg35.el7 osg-flock-1.3-1.osg35.el7 osg-oasis-17-1.osg35.el7 osg-release-3.5-5.osg35.el7 python2-scitokens-1.3.1-1.osg35.el7 python2-xrootd-4.12.6-1.1.osg35.el7 python3-scitokens-1.3.1-1.osg35.el7 python-scitokens-1.3.1-1.osg35.el7 xrootd-4.12.6-1.1.osg35.el7 xrootd-client-4.12.6-1.1.osg35.el7 xrootd-client-devel-4.12.6-1.1.osg35.el7 xrootd-client-libs-4.12.6-1.1.osg35.el7 xrootd-debuginfo-4.12.6-1.1.osg35.el7 xrootd-devel-4.12.6-1.1.osg35.el7 xrootd-doc-4.12.6-1.1.osg35.el7 xrootd-fuse-4.12.6-1.1.osg35.el7 xrootd-libs-4.12.6-1.1.osg35.el7 xrootd-private-devel-4.12.6-1.1.osg35.el7 xrootd-selinux-4.12.6-1.1.osg35.el7 xrootd-server-4.12.6-1.1.osg35.el7 xrootd-server-devel-4.12.6-1.1.osg35.el7 xrootd-server-libs-4.12.6-1.1.osg35.el7 xrootd-voms-4.12.6-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-31/#enterprise-linux-8_1","text":"cvmfs-2.8.0-1.osg35.el8 cvmfs-devel-2.8.0-1.osg35.el8 cvmfs-ducc-2.8.0-1.osg35.el8 cvmfs-fuse3-2.8.0-1.osg35.el8 cvmfs-server-2.8.0-1.osg35.el8 cvmfs-shrinkwrap-2.8.0-1.osg35.el8 cvmfs-unittests-2.8.0-1.osg35.el8 lcmaps-1.6.6-1.14.osg35.el8 lcmaps-common-devel-1.6.6-1.14.osg35.el8 lcmaps-db-templates-1.6.6-1.14.osg35.el8 lcmaps-debuginfo-1.6.6-1.14.osg35.el8 lcmaps-debugsource-1.6.6-1.14.osg35.el8 lcmaps-devel-1.6.6-1.14.osg35.el8 lcmaps-plugins-basic-1.7.0-2.osg35.el8 lcmaps-plugins-basic-debuginfo-1.7.0-2.osg35.el8 lcmaps-plugins-basic-debugsource-1.7.0-2.osg35.el8 lcmaps-plugins-basic-ldap-1.7.0-2.osg35.el8 lcmaps-plugins-basic-ldap-debuginfo-1.7.0-2.osg35.el8 lcmaps-plugins-verify-proxy-1.5.11-1.1.osg35.el8 lcmaps-plugins-verify-proxy-debuginfo-1.5.11-1.1.osg35.el8 lcmaps-plugins-verify-proxy-debugsource-1.5.11-1.1.osg35.el8 lcmaps-plugins-voms-1.7.1-1.6.osg35.el8 lcmaps-plugins-voms-debuginfo-1.7.1-1.6.osg35.el8 lcmaps-plugins-voms-debugsource-1.7.1-1.6.osg35.el8 lcmaps-without-gsi-1.6.6-1.14.osg35.el8 lcmaps-without-gsi-debuginfo-1.6.6-1.14.osg35.el8 lcmaps-without-gsi-devel-1.6.6-1.14.osg35.el8 osg-ca-certs-1.94-1.osg35.el8 osg-oasis-17-1.osg35.el8 osg-release-3.5-5.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-32/","text":"OSG Software Release 3.5.32 \u00b6 Release Date: 2021-03-25 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor 8.9.11-1.1 (Upcoming, EL7 only) Fixes a potential SchedD crash when using malformed tokens condor_watch_q now works on DAGs blahp-1.18.48-2.4 a rebuild against HTCondor 8.9.11 osg-release-3.5-6 including the OSG 3.6 RPM signing key to ease updates between release series vo-client-110-1 with updated WeNMR VOMS information osg-scitokens-mapfile-1-1 containing a new HTCondor-CE mapfile for VO token issuers vault-1.6.2-1 and htvault-config-0.5-1 for managing tokens cvmfs-gateway-1.2.0-1 : note the upstream documentation for updating from version 0.2.5 These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 cvmfs-gateway-1.2.0-1.osg35.el7 htvault-config-0.5-1.osg35.el7 osg-ce-3.5-6.osg35.el7 osg-release-3.5-7.osg35.el7 osg-scitokens-mapfile-1-1.osg35.el7 vault-1.6.2-1.osg35.el7 vo-client-110-1.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-gateway-1.2.0-1.osg35.el8 htvault-config-0.5-1.osg35.el8 osg-ce-3.5-6.osg35.el8 osg-release-3.5-7.osg35.el8 osg-scitokens-mapfile-1-1.osg35.el8 vault-1.6.2-1.osg35.el8 vo-client-110-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cvmfs-gateway htvault-config osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-release osg-scitokens-mapfile vault vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 cvmfs-gateway-1.2.0-1.osg35.el7.x86_64 htvault-config-0.5-1.osg35.el7.x86_64 osg-ce-3.5-6.osg35.el7.x86_64 osg-ce-bosco-3.5-6.osg35.el7.x86_64 osg-ce-condor-3.5-6.osg35.el7.x86_64 osg-ce-lsf-3.5-6.osg35.el7.x86_64 osg-ce-pbs-3.5-6.osg35.el7.x86_64 osg-ce-sge-3.5-6.osg35.el7.x86_64 osg-ce-slurm-3.5-6.osg35.el7.x86_64 osg-release-3.5-7.osg35.el7.noarch osg-scitokens-mapfile-1-1.osg35.el7.x86_64 vault-1.6.2-1.osg35.el7.x86_64 vo-client-110-1.osg35.el7.noarch vo-client-dcache-110-1.osg35.el7.noarch vo-client-lcmaps-voms-110-1.osg35.el7.noarch Enterprise Linux 8 \u00b6 cvmfs-gateway-1.2.0-1.osg35.el8.x86_64 htvault-config-0.5-1.osg35.el8.x86_64 osg-ce-3.5-6.osg35.el8.x86_64 osg-ce-bosco-3.5-6.osg35.el8.x86_64 osg-ce-condor-3.5-6.osg35.el8.x86_64 osg-ce-lsf-3.5-6.osg35.el8.x86_64 osg-ce-pbs-3.5-6.osg35.el8.x86_64 osg-ce-sge-3.5-6.osg35.el8.x86_64 osg-ce-slurm-3.5-6.osg35.el8.x86_64 osg-release-3.5-7.osg35.el8.noarch osg-scitokens-mapfile-1-1.osg35.el8.x86_64 vault-1.6.2-1.osg35.el8.x86_64 vo-client-110-1.osg35.el8.noarch vo-client-dcache-110-1.osg35.el8.noarch vo-client-lcmaps-voms-110-1.osg35.el8.noarch Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 blahp-1.18.48-2.4.osg35up.el7 condor-8.9.11-1.1.osg35up.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-1.18.48-2.4.osg35up.el7.x86_64 blahp-debuginfo-1.18.48-2.4.osg35up.el7.x86_64 condor-8.9.11-1.1.osg35up.el7.x86_64 condor-all-8.9.11-1.1.osg35up.el7.x86_64 condor-annex-ec2-8.9.11-1.1.osg35up.el7.x86_64 condor-bosco-8.9.11-1.1.osg35up.el7.x86_64 condor-classads-8.9.11-1.1.osg35up.el7.x86_64 condor-classads-devel-8.9.11-1.1.osg35up.el7.x86_64 condor-credmon-oauth-8.9.11-1.1.osg35up.el7.x86_64 condor-debuginfo-8.9.11-1.1.osg35up.el7.x86_64 condor-kbdd-8.9.11-1.1.osg35up.el7.x86_64 condor-procd-8.9.11-1.1.osg35up.el7.x86_64 condor-test-8.9.11-1.1.osg35up.el7.x86_64 condor-vm-gahp-8.9.11-1.1.osg35up.el7.x86_64 minicondor-8.9.11-1.1.osg35up.el7.x86_64 python2-condor-8.9.11-1.1.osg35up.el7.x86_64 python3-condor-8.9.11-1.1.osg35up.el7.x86_64","title":"Release 3 5 32"},{"location":"release/3.5/release-3-5-32/#osg-software-release-3532","text":"Release Date: 2021-03-25 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.32"},{"location":"release/3.5/release-3-5-32/#summary-of-changes","text":"This release contains: HTCondor 8.9.11-1.1 (Upcoming, EL7 only) Fixes a potential SchedD crash when using malformed tokens condor_watch_q now works on DAGs blahp-1.18.48-2.4 a rebuild against HTCondor 8.9.11 osg-release-3.5-6 including the OSG 3.6 RPM signing key to ease updates between release series vo-client-110-1 with updated WeNMR VOMS information osg-scitokens-mapfile-1-1 containing a new HTCondor-CE mapfile for VO token issuers vault-1.6.2-1 and htvault-config-0.5-1 for managing tokens cvmfs-gateway-1.2.0-1 : note the upstream documentation for updating from version 0.2.5 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-32/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-32/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-32/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-32/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-32/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-32/#enterprise-linux-7","text":"cvmfs-gateway-1.2.0-1.osg35.el7 htvault-config-0.5-1.osg35.el7 osg-ce-3.5-6.osg35.el7 osg-release-3.5-7.osg35.el7 osg-scitokens-mapfile-1-1.osg35.el7 vault-1.6.2-1.osg35.el7 vo-client-110-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-32/#enterprise-linux-8","text":"cvmfs-gateway-1.2.0-1.osg35.el8 htvault-config-0.5-1.osg35.el8 osg-ce-3.5-6.osg35.el8 osg-release-3.5-7.osg35.el8 osg-scitokens-mapfile-1-1.osg35.el8 vault-1.6.2-1.osg35.el8 vo-client-110-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-32/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cvmfs-gateway htvault-config osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-release osg-scitokens-mapfile vault vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-32/#enterprise-linux-7_1","text":"cvmfs-gateway-1.2.0-1.osg35.el7.x86_64 htvault-config-0.5-1.osg35.el7.x86_64 osg-ce-3.5-6.osg35.el7.x86_64 osg-ce-bosco-3.5-6.osg35.el7.x86_64 osg-ce-condor-3.5-6.osg35.el7.x86_64 osg-ce-lsf-3.5-6.osg35.el7.x86_64 osg-ce-pbs-3.5-6.osg35.el7.x86_64 osg-ce-sge-3.5-6.osg35.el7.x86_64 osg-ce-slurm-3.5-6.osg35.el7.x86_64 osg-release-3.5-7.osg35.el7.noarch osg-scitokens-mapfile-1-1.osg35.el7.x86_64 vault-1.6.2-1.osg35.el7.x86_64 vo-client-110-1.osg35.el7.noarch vo-client-dcache-110-1.osg35.el7.noarch vo-client-lcmaps-voms-110-1.osg35.el7.noarch","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-32/#enterprise-linux-8_1","text":"cvmfs-gateway-1.2.0-1.osg35.el8.x86_64 htvault-config-0.5-1.osg35.el8.x86_64 osg-ce-3.5-6.osg35.el8.x86_64 osg-ce-bosco-3.5-6.osg35.el8.x86_64 osg-ce-condor-3.5-6.osg35.el8.x86_64 osg-ce-lsf-3.5-6.osg35.el8.x86_64 osg-ce-pbs-3.5-6.osg35.el8.x86_64 osg-ce-sge-3.5-6.osg35.el8.x86_64 osg-ce-slurm-3.5-6.osg35.el8.x86_64 osg-release-3.5-7.osg35.el8.noarch osg-scitokens-mapfile-1-1.osg35.el8.x86_64 vault-1.6.2-1.osg35.el8.x86_64 vo-client-110-1.osg35.el8.noarch vo-client-dcache-110-1.osg35.el8.noarch vo-client-lcmaps-voms-110-1.osg35.el8.noarch","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-32/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-32/#enterprise-linux-7_2","text":"blahp-1.18.48-2.4.osg35up.el7 condor-8.9.11-1.1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-32/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-32/#enterprise-linux-7_3","text":"blahp-1.18.48-2.4.osg35up.el7.x86_64 blahp-debuginfo-1.18.48-2.4.osg35up.el7.x86_64 condor-8.9.11-1.1.osg35up.el7.x86_64 condor-all-8.9.11-1.1.osg35up.el7.x86_64 condor-annex-ec2-8.9.11-1.1.osg35up.el7.x86_64 condor-bosco-8.9.11-1.1.osg35up.el7.x86_64 condor-classads-8.9.11-1.1.osg35up.el7.x86_64 condor-classads-devel-8.9.11-1.1.osg35up.el7.x86_64 condor-credmon-oauth-8.9.11-1.1.osg35up.el7.x86_64 condor-debuginfo-8.9.11-1.1.osg35up.el7.x86_64 condor-kbdd-8.9.11-1.1.osg35up.el7.x86_64 condor-procd-8.9.11-1.1.osg35up.el7.x86_64 condor-test-8.9.11-1.1.osg35up.el7.x86_64 condor-vm-gahp-8.9.11-1.1.osg35up.el7.x86_64 minicondor-8.9.11-1.1.osg35up.el7.x86_64 python2-condor-8.9.11-1.1.osg35up.el7.x86_64 python3-condor-8.9.11-1.1.osg35up.el7.x86_64","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-33/","text":"OSG Software Release 3.5.33 \u00b6 Release Date: 2021-04-01 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 Known issues with XRootD 5.1.1 The XRootD team is evaluating solutions for a memory leak in the HTTP Third-Party Copy (HTTP-TPC) use case related to libcurl and NSS . These leaks appear to exist in libcurl for all versions of XRootD and their impact depends on the transfer load at each site. Incompatibility with the multi-user plugin : users of the XRootD multi-user plugin will be unable to update to XRootD 5.1.x until a fixed version of XRootD multi-user is released into the OSG repositories In some cases, XCaches using the Rucio plug-in may crash due to malformed URLs generated by the plug-in. This release contains the following changes to the osg-upcoming repository: XRootD 5.1.1 (EL7 only): an update from the previously available version, XRootD 5.0.2 See this section for detailed update instructions The XRootD SciTokens plug-in ( xrootd-scitokens ) has been merged into the XRootD code-base so its version now matches the xrootd package version See the upstream release notes for details XCache 2.0.0 Updated configuration for XRootD 5.1 compatible chaining of authorization libraries ( SOFTWARE-4431 ) Added requirement for XRootD 5.1 ( SOFTWARE-4431 ) XRootD HDFS 2.2.0 Fixed a bug where checksums were written on HDFS write failures, potentially confusing clients ( #34 ) Added support for new extended attribute API, allowing token plugins to properly set the username needed by HDFS ( #35 ) XRootD CMS TFC 1.5.2-6 : rebuilt against XRootD 5.1.1 XRootD Rucio plug-in 1.2-3.3 : rebuilt against XRootD 5.1.1 These JIRA tickets were addressed in this release. Containers \u00b6 The following container images have been updated to contain the new packages listed above: opensciencegrid/atlas-xcache:release opensciencegrid/cms-xcache:release opensciencegrid/stash-cache:release opensciencegrid/stash-origin:release opensciencegrid/xrootd-standalone:release Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 xcache-2.0.0-1.osgup.el7 xrootd-5.1.1-1.3.osg35up.el7 xrootd-cmstfc-1.5.2-6.osg35up.el7 xrootd-hdfs-2.2.0-1.osg35up.el7 xrootd-rucioN2N-for-Xcache-1.2-3.3.osg35up.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cms-xcache python2-xrootd python36-xrootd Signatures: stash-cache stash-origin xcache xcache-consistency-check xcache-redirector xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-cmstfc xrootd-cmstfc-debuginfo xrootd-cmstfc-devel xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-libs xrootd-private-devel xrootd-rucioN2N-for-Xcache xrootd-rucioN2N-for-Xcache-debuginfo xrootd-scitokens xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 atlas-xcache-2.0.0-1.osgup.el7 cms-xcache-2.0.0-1.osgup.el7 python2-xrootd-5.1.1-1.3.osg35up.el7 python36-xrootd-5.1.1-1.3.osg35up.el7 stash-cache-2.0.0-1.osgup.el7 stash-origin-2.0.0-1.osgup.el7 xcache-2.0.0-1.osgup.el7 xcache-consistency-check-2.0.0-1.osgup.el7 xcache-redirector-2.0.0-1.osgup.el7 xrootd-5.1.1-1.3.osg35up.el7 xrootd-client-5.1.1-1.3.osg35up.el7 xrootd-client-compat-5.1.1-1.3.osg35up.el7 xrootd-client-devel-5.1.1-1.3.osg35up.el7 xrootd-client-libs-5.1.1-1.3.osg35up.el7 xrootd-cmstfc-1.5.2-6.osg35up.el7 xrootd-cmstfc-debuginfo-1.5.2-6.osg35up.el7 xrootd-cmstfc-devel-1.5.2-6.osg35up.el7 xrootd-debuginfo-5.1.1-1.3.osg35up.el7 xrootd-devel-5.1.1-1.3.osg35up.el7 xrootd-doc-5.1.1-1.3.osg35up.el7 xrootd-fuse-5.1.1-1.3.osg35up.el7 xrootd-hdfs-2.2.0-1.osg35up.el7 xrootd-hdfs-debuginfo-2.2.0-1.osg35up.el7 xrootd-hdfs-devel-2.2.0-1.osg35up.el7 xrootd-libs-5.1.1-1.3.osg35up.el7 xrootd-private-devel-5.1.1-1.3.osg35up.el7 xrootd-rucioN2N-for-Xcache-1.2-3.3.osg35up.el7 xrootd-rucioN2N-for-Xcache-debuginfo-1.2-3.3.osg35up.el7 xrootd-scitokens-5.1.1-1.3.osg35up.el7 xrootd-selinux-5.1.1-1.3.osg35up.el7 xrootd-server-5.1.1-1.3.osg35up.el7 xrootd-server-compat-5.1.1-1.3.osg35up.el7 xrootd-server-devel-5.1.1-1.3.osg35up.el7 xrootd-server-libs-5.1.1-1.3.osg35up.el7 xrootd-voms-5.1.1-1.3.osg35up.el7","title":"OSG 3.5.33"},{"location":"release/3.5/release-3-5-33/#osg-software-release-3533","text":"Release Date: 2021-04-01 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.33"},{"location":"release/3.5/release-3-5-33/#summary-of-changes","text":"Known issues with XRootD 5.1.1 The XRootD team is evaluating solutions for a memory leak in the HTTP Third-Party Copy (HTTP-TPC) use case related to libcurl and NSS . These leaks appear to exist in libcurl for all versions of XRootD and their impact depends on the transfer load at each site. Incompatibility with the multi-user plugin : users of the XRootD multi-user plugin will be unable to update to XRootD 5.1.x until a fixed version of XRootD multi-user is released into the OSG repositories In some cases, XCaches using the Rucio plug-in may crash due to malformed URLs generated by the plug-in. This release contains the following changes to the osg-upcoming repository: XRootD 5.1.1 (EL7 only): an update from the previously available version, XRootD 5.0.2 See this section for detailed update instructions The XRootD SciTokens plug-in ( xrootd-scitokens ) has been merged into the XRootD code-base so its version now matches the xrootd package version See the upstream release notes for details XCache 2.0.0 Updated configuration for XRootD 5.1 compatible chaining of authorization libraries ( SOFTWARE-4431 ) Added requirement for XRootD 5.1 ( SOFTWARE-4431 ) XRootD HDFS 2.2.0 Fixed a bug where checksums were written on HDFS write failures, potentially confusing clients ( #34 ) Added support for new extended attribute API, allowing token plugins to properly set the username needed by HDFS ( #35 ) XRootD CMS TFC 1.5.2-6 : rebuilt against XRootD 5.1.1 XRootD Rucio plug-in 1.2-3.3 : rebuilt against XRootD 5.1.1 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-33/#containers","text":"The following container images have been updated to contain the new packages listed above: opensciencegrid/atlas-xcache:release opensciencegrid/cms-xcache:release opensciencegrid/stash-cache:release opensciencegrid/stash-origin:release opensciencegrid/xrootd-standalone:release","title":"Containers"},{"location":"release/3.5/release-3-5-33/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series .","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-33/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-33/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-33/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-33/#enterprise-linux-7","text":"xcache-2.0.0-1.osgup.el7 xrootd-5.1.1-1.3.osg35up.el7 xrootd-cmstfc-1.5.2-6.osg35up.el7 xrootd-hdfs-2.2.0-1.osg35up.el7 xrootd-rucioN2N-for-Xcache-1.2-3.3.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-33/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cms-xcache python2-xrootd python36-xrootd Signatures: stash-cache stash-origin xcache xcache-consistency-check xcache-redirector xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-cmstfc xrootd-cmstfc-debuginfo xrootd-cmstfc-devel xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-libs xrootd-private-devel xrootd-rucioN2N-for-Xcache xrootd-rucioN2N-for-Xcache-debuginfo xrootd-scitokens xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-33/#enterprise-linux-7_1","text":"atlas-xcache-2.0.0-1.osgup.el7 cms-xcache-2.0.0-1.osgup.el7 python2-xrootd-5.1.1-1.3.osg35up.el7 python36-xrootd-5.1.1-1.3.osg35up.el7 stash-cache-2.0.0-1.osgup.el7 stash-origin-2.0.0-1.osgup.el7 xcache-2.0.0-1.osgup.el7 xcache-consistency-check-2.0.0-1.osgup.el7 xcache-redirector-2.0.0-1.osgup.el7 xrootd-5.1.1-1.3.osg35up.el7 xrootd-client-5.1.1-1.3.osg35up.el7 xrootd-client-compat-5.1.1-1.3.osg35up.el7 xrootd-client-devel-5.1.1-1.3.osg35up.el7 xrootd-client-libs-5.1.1-1.3.osg35up.el7 xrootd-cmstfc-1.5.2-6.osg35up.el7 xrootd-cmstfc-debuginfo-1.5.2-6.osg35up.el7 xrootd-cmstfc-devel-1.5.2-6.osg35up.el7 xrootd-debuginfo-5.1.1-1.3.osg35up.el7 xrootd-devel-5.1.1-1.3.osg35up.el7 xrootd-doc-5.1.1-1.3.osg35up.el7 xrootd-fuse-5.1.1-1.3.osg35up.el7 xrootd-hdfs-2.2.0-1.osg35up.el7 xrootd-hdfs-debuginfo-2.2.0-1.osg35up.el7 xrootd-hdfs-devel-2.2.0-1.osg35up.el7 xrootd-libs-5.1.1-1.3.osg35up.el7 xrootd-private-devel-5.1.1-1.3.osg35up.el7 xrootd-rucioN2N-for-Xcache-1.2-3.3.osg35up.el7 xrootd-rucioN2N-for-Xcache-debuginfo-1.2-3.3.osg35up.el7 xrootd-scitokens-5.1.1-1.3.osg35up.el7 xrootd-selinux-5.1.1-1.3.osg35up.el7 xrootd-server-5.1.1-1.3.osg35up.el7 xrootd-server-compat-5.1.1-1.3.osg35up.el7 xrootd-server-devel-5.1.1-1.3.osg35up.el7 xrootd-server-libs-5.1.1-1.3.osg35up.el7 xrootd-voms-5.1.1-1.3.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-34/","text":"OSG Software Release 3.5.34 \u00b6 Release Date: 2021-04-22 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: CVMFS 2.8.1 : Bug fix release gratia-probe 1.23.2: Converted to use Python 3 These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 cvmfs-2.8.1-1.osg35.el7 gratia-probe-1.23.2-1.osg35.el7 osg-oasis-17-2.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.8.1-1.osg35.el8 gratia-probe-1.23.2-1.osg35.el8 osg-oasis-17-2.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-osg-pilot-container gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer osg-oasis If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 cvmfs-2.8.1-1.osg35.el7 cvmfs-devel-2.8.1-1.osg35.el7 cvmfs-ducc-2.8.1-1.osg35.el7 cvmfs-fuse3-2.8.1-1.osg35.el7 cvmfs-server-2.8.1-1.osg35.el7 cvmfs-shrinkwrap-2.8.1-1.osg35.el7 cvmfs-unittests-2.8.1-1.osg35.el7 gratia-probe-1.23.2-1.osg35.el7 gratia-probe-common-1.23.2-1.osg35.el7 gratia-probe-condor-1.23.2-1.osg35.el7 gratia-probe-condor-events-1.23.2-1.osg35.el7 gratia-probe-dcache-storage-1.23.2-1.osg35.el7 gratia-probe-dcache-storagegroup-1.23.2-1.osg35.el7 gratia-probe-dcache-transfer-1.23.2-1.osg35.el7 gratia-probe-enstore-storage-1.23.2-1.osg35.el7 gratia-probe-enstore-tapedrive-1.23.2-1.osg35.el7 gratia-probe-enstore-transfer-1.23.2-1.osg35.el7 gratia-probe-glideinwms-1.23.2-1.osg35.el7 gratia-probe-gridftp-transfer-1.23.2-1.osg35.el7 gratia-probe-hadoop-storage-1.23.2-1.osg35.el7 gratia-probe-htcondor-ce-1.23.2-1.osg35.el7 gratia-probe-lsf-1.23.2-1.osg35.el7 gratia-probe-metric-1.23.2-1.osg35.el7 gratia-probe-onevm-1.23.2-1.osg35.el7 gratia-probe-osg-pilot-container-1.23.2-1.osg35.el7 gratia-probe-pbs-lsf-1.23.2-1.osg35.el7 gratia-probe-services-1.23.2-1.osg35.el7 gratia-probe-sge-1.23.2-1.osg35.el7 gratia-probe-slurm-1.23.2-1.osg35.el7 gratia-probe-xrootd-storage-1.23.2-1.osg35.el7 gratia-probe-xrootd-transfer-1.23.2-1.osg35.el7 osg-oasis-17-2.osg35.el7 Enterprise Linux 8 \u00b6 cvmfs-2.8.1-1.osg35.el8 cvmfs-devel-2.8.1-1.osg35.el8 cvmfs-ducc-2.8.1-1.osg35.el8 cvmfs-fuse3-2.8.1-1.osg35.el8 cvmfs-server-2.8.1-1.osg35.el8 cvmfs-shrinkwrap-2.8.1-1.osg35.el8 cvmfs-unittests-2.8.1-1.osg35.el8 gratia-probe-1.23.2-1.osg35.el8 gratia-probe-common-1.23.2-1.osg35.el8 gratia-probe-condor-1.23.2-1.osg35.el8 gratia-probe-condor-events-1.23.2-1.osg35.el8 gratia-probe-dcache-storage-1.23.2-1.osg35.el8 gratia-probe-dcache-storagegroup-1.23.2-1.osg35.el8 gratia-probe-dcache-transfer-1.23.2-1.osg35.el8 gratia-probe-enstore-storage-1.23.2-1.osg35.el8 gratia-probe-enstore-tapedrive-1.23.2-1.osg35.el8 gratia-probe-enstore-transfer-1.23.2-1.osg35.el8 gratia-probe-glideinwms-1.23.2-1.osg35.el8 gratia-probe-gridftp-transfer-1.23.2-1.osg35.el8 gratia-probe-hadoop-storage-1.23.2-1.osg35.el8 gratia-probe-htcondor-ce-1.23.2-1.osg35.el8 gratia-probe-lsf-1.23.2-1.osg35.el8 gratia-probe-metric-1.23.2-1.osg35.el8 gratia-probe-onevm-1.23.2-1.osg35.el8 gratia-probe-osg-pilot-container-1.23.2-1.osg35.el8 gratia-probe-pbs-lsf-1.23.2-1.osg35.el8 gratia-probe-services-1.23.2-1.osg35.el8 gratia-probe-sge-1.23.2-1.osg35.el8 gratia-probe-slurm-1.23.2-1.osg35.el8 gratia-probe-xrootd-storage-1.23.2-1.osg35.el8 gratia-probe-xrootd-transfer-1.23.2-1.osg35.el8 osg-oasis-17-2.osg35.el8","title":"Release 3 5 34"},{"location":"release/3.5/release-3-5-34/#osg-software-release-3534","text":"Release Date: 2021-04-22 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.34"},{"location":"release/3.5/release-3-5-34/#summary-of-changes","text":"This release contains: CVMFS 2.8.1 : Bug fix release gratia-probe 1.23.2: Converted to use Python 3 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-34/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-34/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-34/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-34/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-34/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-34/#enterprise-linux-7","text":"cvmfs-2.8.1-1.osg35.el7 gratia-probe-1.23.2-1.osg35.el7 osg-oasis-17-2.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-34/#enterprise-linux-8","text":"cvmfs-2.8.1-1.osg35.el8 gratia-probe-1.23.2-1.osg35.el8 osg-oasis-17-2.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-34/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-osg-pilot-container gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer osg-oasis If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-34/#enterprise-linux-7_1","text":"cvmfs-2.8.1-1.osg35.el7 cvmfs-devel-2.8.1-1.osg35.el7 cvmfs-ducc-2.8.1-1.osg35.el7 cvmfs-fuse3-2.8.1-1.osg35.el7 cvmfs-server-2.8.1-1.osg35.el7 cvmfs-shrinkwrap-2.8.1-1.osg35.el7 cvmfs-unittests-2.8.1-1.osg35.el7 gratia-probe-1.23.2-1.osg35.el7 gratia-probe-common-1.23.2-1.osg35.el7 gratia-probe-condor-1.23.2-1.osg35.el7 gratia-probe-condor-events-1.23.2-1.osg35.el7 gratia-probe-dcache-storage-1.23.2-1.osg35.el7 gratia-probe-dcache-storagegroup-1.23.2-1.osg35.el7 gratia-probe-dcache-transfer-1.23.2-1.osg35.el7 gratia-probe-enstore-storage-1.23.2-1.osg35.el7 gratia-probe-enstore-tapedrive-1.23.2-1.osg35.el7 gratia-probe-enstore-transfer-1.23.2-1.osg35.el7 gratia-probe-glideinwms-1.23.2-1.osg35.el7 gratia-probe-gridftp-transfer-1.23.2-1.osg35.el7 gratia-probe-hadoop-storage-1.23.2-1.osg35.el7 gratia-probe-htcondor-ce-1.23.2-1.osg35.el7 gratia-probe-lsf-1.23.2-1.osg35.el7 gratia-probe-metric-1.23.2-1.osg35.el7 gratia-probe-onevm-1.23.2-1.osg35.el7 gratia-probe-osg-pilot-container-1.23.2-1.osg35.el7 gratia-probe-pbs-lsf-1.23.2-1.osg35.el7 gratia-probe-services-1.23.2-1.osg35.el7 gratia-probe-sge-1.23.2-1.osg35.el7 gratia-probe-slurm-1.23.2-1.osg35.el7 gratia-probe-xrootd-storage-1.23.2-1.osg35.el7 gratia-probe-xrootd-transfer-1.23.2-1.osg35.el7 osg-oasis-17-2.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-34/#enterprise-linux-8_1","text":"cvmfs-2.8.1-1.osg35.el8 cvmfs-devel-2.8.1-1.osg35.el8 cvmfs-ducc-2.8.1-1.osg35.el8 cvmfs-fuse3-2.8.1-1.osg35.el8 cvmfs-server-2.8.1-1.osg35.el8 cvmfs-shrinkwrap-2.8.1-1.osg35.el8 cvmfs-unittests-2.8.1-1.osg35.el8 gratia-probe-1.23.2-1.osg35.el8 gratia-probe-common-1.23.2-1.osg35.el8 gratia-probe-condor-1.23.2-1.osg35.el8 gratia-probe-condor-events-1.23.2-1.osg35.el8 gratia-probe-dcache-storage-1.23.2-1.osg35.el8 gratia-probe-dcache-storagegroup-1.23.2-1.osg35.el8 gratia-probe-dcache-transfer-1.23.2-1.osg35.el8 gratia-probe-enstore-storage-1.23.2-1.osg35.el8 gratia-probe-enstore-tapedrive-1.23.2-1.osg35.el8 gratia-probe-enstore-transfer-1.23.2-1.osg35.el8 gratia-probe-glideinwms-1.23.2-1.osg35.el8 gratia-probe-gridftp-transfer-1.23.2-1.osg35.el8 gratia-probe-hadoop-storage-1.23.2-1.osg35.el8 gratia-probe-htcondor-ce-1.23.2-1.osg35.el8 gratia-probe-lsf-1.23.2-1.osg35.el8 gratia-probe-metric-1.23.2-1.osg35.el8 gratia-probe-onevm-1.23.2-1.osg35.el8 gratia-probe-osg-pilot-container-1.23.2-1.osg35.el8 gratia-probe-pbs-lsf-1.23.2-1.osg35.el8 gratia-probe-services-1.23.2-1.osg35.el8 gratia-probe-sge-1.23.2-1.osg35.el8 gratia-probe-slurm-1.23.2-1.osg35.el8 gratia-probe-xrootd-storage-1.23.2-1.osg35.el8 gratia-probe-xrootd-transfer-1.23.2-1.osg35.el8 osg-oasis-17-2.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-35/","text":"OSG Software Release 3.5.35 \u00b6 Release Date: 2021-05-13 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Frontier Squid 4.15-1.2 Closes multiple security vulnerabilities Updated CA certificates based on IGTF 1.110 Removed INFN-CA-2015 that has disappeared operationally (IT) osg-ca-certs 1.96 Fixed Let's Encrypt signing policy to accept cross-signing chain These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the latest CA certificates. The OSG Frontier Squid image contains the security fixes. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 frontier-squid-4.15-1.2.osg35.el7 igtf-ca-certs-1.110-1.osg35.el7 osg-ca-certs-1.96-1.osg35.el7 Enterprise Linux 8 \u00b6 cilogon-openid-ca-cert-1.1-4.osg35.el8 frontier-squid-4.15-1.2.osg35.el8 igtf-ca-certs-1.110-1.osg35.el8 osg-ca-certs-1.96-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: frontier-squid igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 frontier-squid-4.15-1.2.osg35.el7 igtf-ca-certs-1.110-1.osg35.el7 osg-ca-certs-1.96-1.osg35.el7 Enterprise Linux 8 \u00b6 cilogon-openid-ca-cert-1.1-4.osg35.el8 frontier-squid-4.15-1.2.osg35.el8 igtf-ca-certs-1.110-1.osg35.el8 osg-ca-certs-1.96-1.osg35.el8","title":"Release 3 5 35"},{"location":"release/3.5/release-3-5-35/#osg-software-release-3535","text":"Release Date: 2021-05-13 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.35"},{"location":"release/3.5/release-3-5-35/#summary-of-changes","text":"This release contains: Frontier Squid 4.15-1.2 Closes multiple security vulnerabilities Updated CA certificates based on IGTF 1.110 Removed INFN-CA-2015 that has disappeared operationally (IT) osg-ca-certs 1.96 Fixed Let's Encrypt signing policy to accept cross-signing chain These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-35/#containers","text":"The OSG Docker images have been updated to contain the latest CA certificates. The OSG Frontier Squid image contains the security fixes.","title":"Containers"},{"location":"release/3.5/release-3-5-35/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-35/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-35/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-35/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-35/#enterprise-linux-7","text":"frontier-squid-4.15-1.2.osg35.el7 igtf-ca-certs-1.110-1.osg35.el7 osg-ca-certs-1.96-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-35/#enterprise-linux-8","text":"cilogon-openid-ca-cert-1.1-4.osg35.el8 frontier-squid-4.15-1.2.osg35.el8 igtf-ca-certs-1.110-1.osg35.el8 osg-ca-certs-1.96-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-35/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: frontier-squid igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-35/#enterprise-linux-7_1","text":"frontier-squid-4.15-1.2.osg35.el7 igtf-ca-certs-1.110-1.osg35.el7 osg-ca-certs-1.96-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-35/#enterprise-linux-8_1","text":"cilogon-openid-ca-cert-1.1-4.osg35.el8 frontier-squid-4.15-1.2.osg35.el8 igtf-ca-certs-1.110-1.osg35.el8 osg-ca-certs-1.96-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-36-2/","text":"OSG Data Release 3.5.36-2 \u00b6 Release Date: 2021-05-25 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 CA Certificates based on IGTF 1.111 Removed discontinued NERSC-SLCS CA (US) Removed discontinued MYIFAM CA (MY) These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the latest CA certificates. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 igtf-ca-certs-1.111-1.osg35.el7 osg-ca-certs-1.97-1.osg35.el7 Enterprise Linux 8 \u00b6 igtf-ca-certs-1.111-1.osg35.el8 osg-ca-certs-1.97-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 igtf-ca-certs-1.111-1.osg35.el7 osg-ca-certs-1.97-1.osg35.el7 Enterprise Linux 8 \u00b6 igtf-ca-certs-1.111-1.osg35.el8 osg-ca-certs-1.97-1.osg35.el8","title":"Release 3 5 36 2"},{"location":"release/3.5/release-3-5-36-2/#osg-data-release-3536-2","text":"Release Date: 2021-05-25 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.36-2"},{"location":"release/3.5/release-3-5-36-2/#summary-of-changes","text":"CA Certificates based on IGTF 1.111 Removed discontinued NERSC-SLCS CA (US) Removed discontinued MYIFAM CA (MY) These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-36-2/#containers","text":"The OSG Docker images have been updated to contain the latest CA certificates.","title":"Containers"},{"location":"release/3.5/release-3-5-36-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-36-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-36-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-36-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-36-2/#enterprise-linux-7","text":"igtf-ca-certs-1.111-1.osg35.el7 osg-ca-certs-1.97-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-36-2/#enterprise-linux-8","text":"igtf-ca-certs-1.111-1.osg35.el8 osg-ca-certs-1.97-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-36-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-36-2/#enterprise-linux-7_1","text":"igtf-ca-certs-1.111-1.osg35.el7 osg-ca-certs-1.97-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-36-2/#enterprise-linux-8_1","text":"igtf-ca-certs-1.111-1.osg35.el8 osg-ca-certs-1.97-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-36/","text":"OSG Software Release 3.5.36 \u00b6 Release Date: 2021-05-17 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor 8.8.13 : Bug fix release osg-scitokens-mapfile 3: Updated to support HTCondor-CE 5.1.0 osg-ce: now requires osg-scitokens-mapfile vault 1.7.1: Update to latest upstream release htvault-config 1.1: Uses yaml configuration files htgettoken 1.2: improved error message handling and bug fixes Upcoming HTCondor-CE 5.1.0 Support for Job Router Transform configuration syntax Credential mapping changes Converted to Python 3 Please consult the upgrade documentation GlideinWMS 3.7.3 Known Issue GlideinWMS 3.7.3: submissions from 3.6.5 frontends to 3.7.3 factory go on hold HTCondor-CE 5.1.0: batch system max walltime requests are always set to 3 days. Details and workaround can be found in the upstream bug tracker . These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the latest CA certificates. The OSG Frontier Squid image contains the security fixes. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 blahp-1.18.48-2.2.osg35.el7 condor-8.8.13-1.osg35.el7 htgettoken-1.2-1.osg35.el7 htvault-config-1.1-1.osg35.el7 osg-ce-3.5-7.osg35.el7 osg-scitokens-mapfile-3-1.osg35.el7 vault-1.7.1-1.osg35.el7 Enterprise Linux 8 \u00b6 blahp-1.18.48-2.2.osg35.el8 condor-8.8.13-1.osg35.el8 htgettoken-1.2-1.osg35.el8 htvault-config-1.1-1.osg35.el8 osg-ce-3.5-7.osg35.el8 osg-scitokens-mapfile-3-1.osg35.el8 vault-1.7.1-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp htgettoken htvault-config minicondor osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-scitokens-mapfile python2-condor python3-condor vault If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-1.18.48-2.2.osg35.el7 blahp-debuginfo-1.18.48-2.2.osg35.el7 condor-8.8.13-1.osg35.el7 condor-all-8.8.13-1.osg35.el7 condor-annex-ec2-8.8.13-1.osg35.el7 condor-bosco-8.8.13-1.osg35.el7 condor-classads-8.8.13-1.osg35.el7 condor-classads-devel-8.8.13-1.osg35.el7 condor-debuginfo-8.8.13-1.osg35.el7 condor-kbdd-8.8.13-1.osg35.el7 condor-procd-8.8.13-1.osg35.el7 condor-test-8.8.13-1.osg35.el7 condor-vm-gahp-8.8.13-1.osg35.el7 htgettoken-1.2-1.osg35.el7 htvault-config-1.1-1.osg35.el7 minicondor-8.8.13-1.osg35.el7 osg-ce-3.5-7.osg35.el7 osg-ce-bosco-3.5-7.osg35.el7 osg-ce-condor-3.5-7.osg35.el7 osg-ce-lsf-3.5-7.osg35.el7 osg-ce-pbs-3.5-7.osg35.el7 osg-ce-sge-3.5-7.osg35.el7 osg-ce-slurm-3.5-7.osg35.el7 osg-scitokens-mapfile-3-1.osg35.el7 python2-condor-8.8.13-1.osg35.el7 python3-condor-8.8.13-1.osg35.el7 vault-1.7.1-1.osg35.el7 Enterprise Linux 8 \u00b6 blahp-1.18.48-2.2.osg35.el8 blahp-debuginfo-1.18.48-2.2.osg35.el8 blahp-debugsource-1.18.48-2.2.osg35.el8 condor-8.8.13-1.osg35.el8 condor-all-8.8.13-1.osg35.el8 condor-annex-ec2-8.8.13-1.osg35.el8 condor-bosco-8.8.13-1.osg35.el8 condor-bosco-debuginfo-8.8.13-1.osg35.el8 condor-classads-8.8.13-1.osg35.el8 condor-classads-debuginfo-8.8.13-1.osg35.el8 condor-classads-devel-8.8.13-1.osg35.el8 condor-classads-devel-debuginfo-8.8.13-1.osg35.el8 condor-debuginfo-8.8.13-1.osg35.el8 condor-debugsource-8.8.13-1.osg35.el8 condor-kbdd-8.8.13-1.osg35.el8 condor-kbdd-debuginfo-8.8.13-1.osg35.el8 condor-procd-8.8.13-1.osg35.el8 condor-procd-debuginfo-8.8.13-1.osg35.el8 condor-test-8.8.13-1.osg35.el8 condor-test-debuginfo-8.8.13-1.osg35.el8 condor-vm-gahp-8.8.13-1.osg35.el8 condor-vm-gahp-debuginfo-8.8.13-1.osg35.el8 htgettoken-1.2-1.osg35.el8 htvault-config-1.1-1.osg35.el8 minicondor-8.8.13-1.osg35.el8 osg-ce-3.5-7.osg35.el8 osg-ce-bosco-3.5-7.osg35.el8 osg-ce-condor-3.5-7.osg35.el8 osg-ce-lsf-3.5-7.osg35.el8 osg-ce-pbs-3.5-7.osg35.el8 osg-ce-sge-3.5-7.osg35.el8 osg-ce-slurm-3.5-7.osg35.el8 osg-scitokens-mapfile-3-1.osg35.el8 python3-condor-8.8.13-1.osg35.el8 python3-condor-debuginfo-8.8.13-1.osg35.el8 vault-1.7.1-1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 blahp-2.0.2-1.osg35up.el7 condor-9.0.0-1.5.osg35up.el7 glideinwms-3.7.3-1.osg35up.el7 htcondor-ce-5.1.0-1.osg35up.el7 Enterprise Linux 8 \u00b6 blahp-2.0.2-1.osg35up.el8 condor-9.0.0-1.5.osg35up.el8 htcondor-ce-5.1.0-1.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-factory-core glideinwms-factory-httpd glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-core glideinwms-vofrontend-httpd glideinwms-vofrontend-standalone htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-2.0.2-1.osg35up.el7 blahp-debuginfo-2.0.2-1.osg35up.el7 condor-9.0.0-1.5.osg35up.el7 condor-all-9.0.0-1.5.osg35up.el7 condor-annex-ec2-9.0.0-1.5.osg35up.el7 condor-bosco-9.0.0-1.5.osg35up.el7 condor-classads-9.0.0-1.5.osg35up.el7 condor-classads-devel-9.0.0-1.5.osg35up.el7 condor-credmon-oauth-9.0.0-1.5.osg35up.el7 condor-credmon-vault-9.0.0-1.5.osg35up.el7 condor-debuginfo-9.0.0-1.5.osg35up.el7 condor-devel-9.0.0-1.5.osg35up.el7 condor-kbdd-9.0.0-1.5.osg35up.el7 condor-procd-9.0.0-1.5.osg35up.el7 condor-test-9.0.0-1.5.osg35up.el7 condor-vm-gahp-9.0.0-1.5.osg35up.el7 glideinwms-3.7.3-1.osg35up.el7 glideinwms-common-tools-3.7.3-1.osg35up.el7 glideinwms-condor-common-config-3.7.3-1.osg35up.el7 glideinwms-factory-3.7.3-1.osg35up.el7 glideinwms-factory-condor-3.7.3-1.osg35up.el7 glideinwms-factory-core-3.7.3-1.osg35up.el7 glideinwms-factory-httpd-3.7.3-1.osg35up.el7 glideinwms-glidecondor-tools-3.7.3-1.osg35up.el7 glideinwms-libs-3.7.3-1.osg35up.el7 glideinwms-minimal-condor-3.7.3-1.osg35up.el7 glideinwms-usercollector-3.7.3-1.osg35up.el7 glideinwms-userschedd-3.7.3-1.osg35up.el7 glideinwms-vofrontend-3.7.3-1.osg35up.el7 glideinwms-vofrontend-core-3.7.3-1.osg35up.el7 glideinwms-vofrontend-httpd-3.7.3-1.osg35up.el7 glideinwms-vofrontend-standalone-3.7.3-1.osg35up.el7 htcondor-ce-5.1.0-1.osg35up.el7 htcondor-ce-bosco-5.1.0-1.osg35up.el7 htcondor-ce-client-5.1.0-1.osg35up.el7 htcondor-ce-collector-5.1.0-1.osg35up.el7 htcondor-ce-condor-5.1.0-1.osg35up.el7 htcondor-ce-lsf-5.1.0-1.osg35up.el7 htcondor-ce-pbs-5.1.0-1.osg35up.el7 htcondor-ce-sge-5.1.0-1.osg35up.el7 htcondor-ce-slurm-5.1.0-1.osg35up.el7 htcondor-ce-view-5.1.0-1.osg35up.el7 minicondor-9.0.0-1.5.osg35up.el7 python2-condor-9.0.0-1.5.osg35up.el7 python3-condor-9.0.0-1.5.osg35up.el7 Enterprise Linux 8 \u00b6 blahp-2.0.2-1.osg35up.el8 blahp-debuginfo-2.0.2-1.osg35up.el8 blahp-debugsource-2.0.2-1.osg35up.el8 condor-9.0.0-1.5.osg35up.el8 condor-all-9.0.0-1.5.osg35up.el8 condor-annex-ec2-9.0.0-1.5.osg35up.el8 condor-bosco-9.0.0-1.5.osg35up.el8 condor-bosco-debuginfo-9.0.0-1.5.osg35up.el8 condor-classads-9.0.0-1.5.osg35up.el8 condor-classads-debuginfo-9.0.0-1.5.osg35up.el8 condor-classads-devel-9.0.0-1.5.osg35up.el8 condor-classads-devel-debuginfo-9.0.0-1.5.osg35up.el8 condor-credmon-vault-9.0.0-1.5.osg35up.el8 condor-debuginfo-9.0.0-1.5.osg35up.el8 condor-debugsource-9.0.0-1.5.osg35up.el8 condor-devel-9.0.0-1.5.osg35up.el8 condor-kbdd-9.0.0-1.5.osg35up.el8 condor-kbdd-debuginfo-9.0.0-1.5.osg35up.el8 condor-procd-9.0.0-1.5.osg35up.el8 condor-procd-debuginfo-9.0.0-1.5.osg35up.el8 condor-test-9.0.0-1.5.osg35up.el8 condor-test-debuginfo-9.0.0-1.5.osg35up.el8 condor-vm-gahp-9.0.0-1.5.osg35up.el8 condor-vm-gahp-debuginfo-9.0.0-1.5.osg35up.el8 htcondor-ce-5.1.0-1.osg35up.el8 htcondor-ce-bosco-5.1.0-1.osg35up.el8 htcondor-ce-client-5.1.0-1.osg35up.el8 htcondor-ce-collector-5.1.0-1.osg35up.el8 htcondor-ce-condor-5.1.0-1.osg35up.el8 htcondor-ce-lsf-5.1.0-1.osg35up.el8 htcondor-ce-pbs-5.1.0-1.osg35up.el8 htcondor-ce-sge-5.1.0-1.osg35up.el8 htcondor-ce-slurm-5.1.0-1.osg35up.el8 htcondor-ce-view-5.1.0-1.osg35up.el8 minicondor-9.0.0-1.5.osg35up.el8 python3-condor-9.0.0-1.5.osg35up.el8 python3-condor-debuginfo-9.0.0-1.5.osg35up.el8","title":"Release 3 5 36"},{"location":"release/3.5/release-3-5-36/#osg-software-release-3536","text":"Release Date: 2021-05-17 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.36"},{"location":"release/3.5/release-3-5-36/#summary-of-changes","text":"This release contains: HTCondor 8.8.13 : Bug fix release osg-scitokens-mapfile 3: Updated to support HTCondor-CE 5.1.0 osg-ce: now requires osg-scitokens-mapfile vault 1.7.1: Update to latest upstream release htvault-config 1.1: Uses yaml configuration files htgettoken 1.2: improved error message handling and bug fixes Upcoming HTCondor-CE 5.1.0 Support for Job Router Transform configuration syntax Credential mapping changes Converted to Python 3 Please consult the upgrade documentation GlideinWMS 3.7.3 Known Issue GlideinWMS 3.7.3: submissions from 3.6.5 frontends to 3.7.3 factory go on hold HTCondor-CE 5.1.0: batch system max walltime requests are always set to 3 days. Details and workaround can be found in the upstream bug tracker . These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-36/#containers","text":"The OSG Docker images have been updated to contain the latest CA certificates. The OSG Frontier Squid image contains the security fixes.","title":"Containers"},{"location":"release/3.5/release-3-5-36/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-36/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-36/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-36/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-36/#enterprise-linux-7","text":"blahp-1.18.48-2.2.osg35.el7 condor-8.8.13-1.osg35.el7 htgettoken-1.2-1.osg35.el7 htvault-config-1.1-1.osg35.el7 osg-ce-3.5-7.osg35.el7 osg-scitokens-mapfile-3-1.osg35.el7 vault-1.7.1-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-36/#enterprise-linux-8","text":"blahp-1.18.48-2.2.osg35.el8 condor-8.8.13-1.osg35.el8 htgettoken-1.2-1.osg35.el8 htvault-config-1.1-1.osg35.el8 osg-ce-3.5-7.osg35.el8 osg-scitokens-mapfile-3-1.osg35.el8 vault-1.7.1-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-36/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp htgettoken htvault-config minicondor osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-scitokens-mapfile python2-condor python3-condor vault If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-36/#enterprise-linux-7_1","text":"blahp-1.18.48-2.2.osg35.el7 blahp-debuginfo-1.18.48-2.2.osg35.el7 condor-8.8.13-1.osg35.el7 condor-all-8.8.13-1.osg35.el7 condor-annex-ec2-8.8.13-1.osg35.el7 condor-bosco-8.8.13-1.osg35.el7 condor-classads-8.8.13-1.osg35.el7 condor-classads-devel-8.8.13-1.osg35.el7 condor-debuginfo-8.8.13-1.osg35.el7 condor-kbdd-8.8.13-1.osg35.el7 condor-procd-8.8.13-1.osg35.el7 condor-test-8.8.13-1.osg35.el7 condor-vm-gahp-8.8.13-1.osg35.el7 htgettoken-1.2-1.osg35.el7 htvault-config-1.1-1.osg35.el7 minicondor-8.8.13-1.osg35.el7 osg-ce-3.5-7.osg35.el7 osg-ce-bosco-3.5-7.osg35.el7 osg-ce-condor-3.5-7.osg35.el7 osg-ce-lsf-3.5-7.osg35.el7 osg-ce-pbs-3.5-7.osg35.el7 osg-ce-sge-3.5-7.osg35.el7 osg-ce-slurm-3.5-7.osg35.el7 osg-scitokens-mapfile-3-1.osg35.el7 python2-condor-8.8.13-1.osg35.el7 python3-condor-8.8.13-1.osg35.el7 vault-1.7.1-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-36/#enterprise-linux-8_1","text":"blahp-1.18.48-2.2.osg35.el8 blahp-debuginfo-1.18.48-2.2.osg35.el8 blahp-debugsource-1.18.48-2.2.osg35.el8 condor-8.8.13-1.osg35.el8 condor-all-8.8.13-1.osg35.el8 condor-annex-ec2-8.8.13-1.osg35.el8 condor-bosco-8.8.13-1.osg35.el8 condor-bosco-debuginfo-8.8.13-1.osg35.el8 condor-classads-8.8.13-1.osg35.el8 condor-classads-debuginfo-8.8.13-1.osg35.el8 condor-classads-devel-8.8.13-1.osg35.el8 condor-classads-devel-debuginfo-8.8.13-1.osg35.el8 condor-debuginfo-8.8.13-1.osg35.el8 condor-debugsource-8.8.13-1.osg35.el8 condor-kbdd-8.8.13-1.osg35.el8 condor-kbdd-debuginfo-8.8.13-1.osg35.el8 condor-procd-8.8.13-1.osg35.el8 condor-procd-debuginfo-8.8.13-1.osg35.el8 condor-test-8.8.13-1.osg35.el8 condor-test-debuginfo-8.8.13-1.osg35.el8 condor-vm-gahp-8.8.13-1.osg35.el8 condor-vm-gahp-debuginfo-8.8.13-1.osg35.el8 htgettoken-1.2-1.osg35.el8 htvault-config-1.1-1.osg35.el8 minicondor-8.8.13-1.osg35.el8 osg-ce-3.5-7.osg35.el8 osg-ce-bosco-3.5-7.osg35.el8 osg-ce-condor-3.5-7.osg35.el8 osg-ce-lsf-3.5-7.osg35.el8 osg-ce-pbs-3.5-7.osg35.el8 osg-ce-sge-3.5-7.osg35.el8 osg-ce-slurm-3.5-7.osg35.el8 osg-scitokens-mapfile-3-1.osg35.el8 python3-condor-8.8.13-1.osg35.el8 python3-condor-debuginfo-8.8.13-1.osg35.el8 vault-1.7.1-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-36/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-36/#enterprise-linux-7_2","text":"blahp-2.0.2-1.osg35up.el7 condor-9.0.0-1.5.osg35up.el7 glideinwms-3.7.3-1.osg35up.el7 htcondor-ce-5.1.0-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-36/#enterprise-linux-8_2","text":"blahp-2.0.2-1.osg35up.el8 condor-9.0.0-1.5.osg35up.el8 htcondor-ce-5.1.0-1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-36/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-factory-core glideinwms-factory-httpd glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-core glideinwms-vofrontend-httpd glideinwms-vofrontend-standalone htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-36/#enterprise-linux-7_3","text":"blahp-2.0.2-1.osg35up.el7 blahp-debuginfo-2.0.2-1.osg35up.el7 condor-9.0.0-1.5.osg35up.el7 condor-all-9.0.0-1.5.osg35up.el7 condor-annex-ec2-9.0.0-1.5.osg35up.el7 condor-bosco-9.0.0-1.5.osg35up.el7 condor-classads-9.0.0-1.5.osg35up.el7 condor-classads-devel-9.0.0-1.5.osg35up.el7 condor-credmon-oauth-9.0.0-1.5.osg35up.el7 condor-credmon-vault-9.0.0-1.5.osg35up.el7 condor-debuginfo-9.0.0-1.5.osg35up.el7 condor-devel-9.0.0-1.5.osg35up.el7 condor-kbdd-9.0.0-1.5.osg35up.el7 condor-procd-9.0.0-1.5.osg35up.el7 condor-test-9.0.0-1.5.osg35up.el7 condor-vm-gahp-9.0.0-1.5.osg35up.el7 glideinwms-3.7.3-1.osg35up.el7 glideinwms-common-tools-3.7.3-1.osg35up.el7 glideinwms-condor-common-config-3.7.3-1.osg35up.el7 glideinwms-factory-3.7.3-1.osg35up.el7 glideinwms-factory-condor-3.7.3-1.osg35up.el7 glideinwms-factory-core-3.7.3-1.osg35up.el7 glideinwms-factory-httpd-3.7.3-1.osg35up.el7 glideinwms-glidecondor-tools-3.7.3-1.osg35up.el7 glideinwms-libs-3.7.3-1.osg35up.el7 glideinwms-minimal-condor-3.7.3-1.osg35up.el7 glideinwms-usercollector-3.7.3-1.osg35up.el7 glideinwms-userschedd-3.7.3-1.osg35up.el7 glideinwms-vofrontend-3.7.3-1.osg35up.el7 glideinwms-vofrontend-core-3.7.3-1.osg35up.el7 glideinwms-vofrontend-httpd-3.7.3-1.osg35up.el7 glideinwms-vofrontend-standalone-3.7.3-1.osg35up.el7 htcondor-ce-5.1.0-1.osg35up.el7 htcondor-ce-bosco-5.1.0-1.osg35up.el7 htcondor-ce-client-5.1.0-1.osg35up.el7 htcondor-ce-collector-5.1.0-1.osg35up.el7 htcondor-ce-condor-5.1.0-1.osg35up.el7 htcondor-ce-lsf-5.1.0-1.osg35up.el7 htcondor-ce-pbs-5.1.0-1.osg35up.el7 htcondor-ce-sge-5.1.0-1.osg35up.el7 htcondor-ce-slurm-5.1.0-1.osg35up.el7 htcondor-ce-view-5.1.0-1.osg35up.el7 minicondor-9.0.0-1.5.osg35up.el7 python2-condor-9.0.0-1.5.osg35up.el7 python3-condor-9.0.0-1.5.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-36/#enterprise-linux-8_3","text":"blahp-2.0.2-1.osg35up.el8 blahp-debuginfo-2.0.2-1.osg35up.el8 blahp-debugsource-2.0.2-1.osg35up.el8 condor-9.0.0-1.5.osg35up.el8 condor-all-9.0.0-1.5.osg35up.el8 condor-annex-ec2-9.0.0-1.5.osg35up.el8 condor-bosco-9.0.0-1.5.osg35up.el8 condor-bosco-debuginfo-9.0.0-1.5.osg35up.el8 condor-classads-9.0.0-1.5.osg35up.el8 condor-classads-debuginfo-9.0.0-1.5.osg35up.el8 condor-classads-devel-9.0.0-1.5.osg35up.el8 condor-classads-devel-debuginfo-9.0.0-1.5.osg35up.el8 condor-credmon-vault-9.0.0-1.5.osg35up.el8 condor-debuginfo-9.0.0-1.5.osg35up.el8 condor-debugsource-9.0.0-1.5.osg35up.el8 condor-devel-9.0.0-1.5.osg35up.el8 condor-kbdd-9.0.0-1.5.osg35up.el8 condor-kbdd-debuginfo-9.0.0-1.5.osg35up.el8 condor-procd-9.0.0-1.5.osg35up.el8 condor-procd-debuginfo-9.0.0-1.5.osg35up.el8 condor-test-9.0.0-1.5.osg35up.el8 condor-test-debuginfo-9.0.0-1.5.osg35up.el8 condor-vm-gahp-9.0.0-1.5.osg35up.el8 condor-vm-gahp-debuginfo-9.0.0-1.5.osg35up.el8 htcondor-ce-5.1.0-1.osg35up.el8 htcondor-ce-bosco-5.1.0-1.osg35up.el8 htcondor-ce-client-5.1.0-1.osg35up.el8 htcondor-ce-collector-5.1.0-1.osg35up.el8 htcondor-ce-condor-5.1.0-1.osg35up.el8 htcondor-ce-lsf-5.1.0-1.osg35up.el8 htcondor-ce-pbs-5.1.0-1.osg35up.el8 htcondor-ce-sge-5.1.0-1.osg35up.el8 htcondor-ce-slurm-5.1.0-1.osg35up.el8 htcondor-ce-view-5.1.0-1.osg35up.el8 minicondor-9.0.0-1.5.osg35up.el8 python3-condor-9.0.0-1.5.osg35up.el8 python3-condor-debuginfo-9.0.0-1.5.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-37/","text":"OSG Software Release 3.5.37 \u00b6 Release Date: 2021-06-03 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor-CE 4.5.2 : Upgrade from version 4.4.1 HTCondor-CE 4.5.0 Version History : New feature release HTCondor-CE 4.5.1 Version History : Bug fix release HTCondor-CE 4.5.2 Version History : Bug fix release gratia-probe 1.23.3: Fix problem that could cause pilot hours to be zero for non-HTCondor batch systems vault 1.7.2 : Security update; fixes CVE-2021-32923. (OSG configuration not vulnerable) osg-gridftp for Enterprise Linux 8 Upcoming GlideinWMS 3.7.4 : Fixed compatibility issue between 3.7 factories and 3.6.5 frontends Known Issue HTCondor-CE 5.1.0: batch system max walltime requests are always set to 3 days. Details and workaround can be found in the upstream bug tracker . These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the latest CA certificates. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 globus-gridftp-server-13.20-1.3.osg35.el7 gratia-probe-1.23.3-1.osg35.el7 htcondor-ce-4.5.2-1.osg35.el7 vault-1.7.2-1.osg35.el7 Enterprise Linux 8 \u00b6 globus-gridftp-osg-extensions-0.4-1.osg35.el8 globus-gridftp-server-13.20-1.3.osg35.el8 gratia-probe-1.23.3-1.osg35.el8 gridftp-dsi-posix-1.4-3.osg35.el8 lcas-lcmaps-gt4-interface-0.3.1-1.3.osg35.el8 osg-gridftp-3.5-4.osg35.el8 vault-1.7.2-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-osg-pilot-container gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view vault If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 globus-gridftp-server-13.20-1.3.osg35.el7 globus-gridftp-server-debuginfo-13.20-1.3.osg35.el7 globus-gridftp-server-devel-13.20-1.3.osg35.el7 globus-gridftp-server-progs-13.20-1.3.osg35.el7 gratia-probe-1.23.3-1.osg35.el7 gratia-probe-common-1.23.3-1.osg35.el7 gratia-probe-condor-1.23.3-1.osg35.el7 gratia-probe-condor-events-1.23.3-1.osg35.el7 gratia-probe-dcache-storage-1.23.3-1.osg35.el7 gratia-probe-dcache-storagegroup-1.23.3-1.osg35.el7 gratia-probe-dcache-transfer-1.23.3-1.osg35.el7 gratia-probe-enstore-storage-1.23.3-1.osg35.el7 gratia-probe-enstore-tapedrive-1.23.3-1.osg35.el7 gratia-probe-enstore-transfer-1.23.3-1.osg35.el7 gratia-probe-glideinwms-1.23.3-1.osg35.el7 gratia-probe-gridftp-transfer-1.23.3-1.osg35.el7 gratia-probe-hadoop-storage-1.23.3-1.osg35.el7 gratia-probe-htcondor-ce-1.23.3-1.osg35.el7 gratia-probe-lsf-1.23.3-1.osg35.el7 gratia-probe-metric-1.23.3-1.osg35.el7 gratia-probe-onevm-1.23.3-1.osg35.el7 gratia-probe-osg-pilot-container-1.23.3-1.osg35.el7 gratia-probe-pbs-lsf-1.23.3-1.osg35.el7 gratia-probe-services-1.23.3-1.osg35.el7 gratia-probe-sge-1.23.3-1.osg35.el7 gratia-probe-slurm-1.23.3-1.osg35.el7 gratia-probe-xrootd-storage-1.23.3-1.osg35.el7 gratia-probe-xrootd-transfer-1.23.3-1.osg35.el7 htcondor-ce-4.5.2-1.osg35.el7 htcondor-ce-bosco-4.5.2-1.osg35.el7 htcondor-ce-client-4.5.2-1.osg35.el7 htcondor-ce-collector-4.5.2-1.osg35.el7 htcondor-ce-condor-4.5.2-1.osg35.el7 htcondor-ce-lsf-4.5.2-1.osg35.el7 htcondor-ce-pbs-4.5.2-1.osg35.el7 htcondor-ce-sge-4.5.2-1.osg35.el7 htcondor-ce-slurm-4.5.2-1.osg35.el7 htcondor-ce-view-4.5.2-1.osg35.el7 vault-1.7.2-1.osg35.el7 Enterprise Linux 8 \u00b6 globus-gridftp-osg-extensions-0.4-1.osg35.el8 globus-gridftp-osg-extensions-debuginfo-0.4-1.osg35.el8 globus-gridftp-osg-extensions-debugsource-0.4-1.osg35.el8 globus-gridftp-server-13.20-1.3.osg35.el8 globus-gridftp-server-debuginfo-13.20-1.3.osg35.el8 globus-gridftp-server-debugsource-13.20-1.3.osg35.el8 globus-gridftp-server-devel-13.20-1.3.osg35.el8 globus-gridftp-server-progs-13.20-1.3.osg35.el8 globus-gridftp-server-progs-debuginfo-13.20-1.3.osg35.el8 gratia-probe-1.23.3-1.osg35.el8 gratia-probe-common-1.23.3-1.osg35.el8 gratia-probe-condor-1.23.3-1.osg35.el8 gratia-probe-condor-events-1.23.3-1.osg35.el8 gratia-probe-dcache-storage-1.23.3-1.osg35.el8 gratia-probe-dcache-storagegroup-1.23.3-1.osg35.el8 gratia-probe-dcache-transfer-1.23.3-1.osg35.el8 gratia-probe-enstore-storage-1.23.3-1.osg35.el8 gratia-probe-enstore-tapedrive-1.23.3-1.osg35.el8 gratia-probe-enstore-transfer-1.23.3-1.osg35.el8 gratia-probe-glideinwms-1.23.3-1.osg35.el8 gratia-probe-gridftp-transfer-1.23.3-1.osg35.el8 gratia-probe-hadoop-storage-1.23.3-1.osg35.el8 gratia-probe-htcondor-ce-1.23.3-1.osg35.el8 gratia-probe-lsf-1.23.3-1.osg35.el8 gratia-probe-metric-1.23.3-1.osg35.el8 gratia-probe-onevm-1.23.3-1.osg35.el8 gratia-probe-osg-pilot-container-1.23.3-1.osg35.el8 gratia-probe-pbs-lsf-1.23.3-1.osg35.el8 gratia-probe-services-1.23.3-1.osg35.el8 gratia-probe-sge-1.23.3-1.osg35.el8 gratia-probe-slurm-1.23.3-1.osg35.el8 gratia-probe-xrootd-storage-1.23.3-1.osg35.el8 gratia-probe-xrootd-transfer-1.23.3-1.osg35.el8 gridftp-dsi-posix-1.4-3.osg35.el8 lcas-lcmaps-gt4-interface-0.3.1-1.3.osg35.el8 lcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.3.osg35.el8 lcas-lcmaps-gt4-interface-debugsource-0.3.1-1.3.osg35.el8 osg-gridftp-3.5-4.osg35.el8 osg-gridftp-hdfs-3.5-4.osg35.el8 osg-gridftp-xrootd-3.5-4.osg35.el8 vault-1.7.2-1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 glideinwms-3.7.4-1.osg35up.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-factory-core glideinwms-factory-httpd glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-core glideinwms-vofrontend-httpd glideinwms-vofrontend-standalone If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 glideinwms-3.7.4-1.osg35up.el7 glideinwms-common-tools-3.7.4-1.osg35up.el7 glideinwms-condor-common-config-3.7.4-1.osg35up.el7 glideinwms-factory-3.7.4-1.osg35up.el7 glideinwms-factory-condor-3.7.4-1.osg35up.el7 glideinwms-factory-core-3.7.4-1.osg35up.el7 glideinwms-factory-httpd-3.7.4-1.osg35up.el7 glideinwms-glidecondor-tools-3.7.4-1.osg35up.el7 glideinwms-libs-3.7.4-1.osg35up.el7 glideinwms-minimal-condor-3.7.4-1.osg35up.el7 glideinwms-usercollector-3.7.4-1.osg35up.el7 glideinwms-userschedd-3.7.4-1.osg35up.el7 glideinwms-vofrontend-3.7.4-1.osg35up.el7 glideinwms-vofrontend-core-3.7.4-1.osg35up.el7 glideinwms-vofrontend-httpd-3.7.4-1.osg35up.el7 glideinwms-vofrontend-standalone-3.7.4-1.osg35up.el7","title":"Release 3 5 37"},{"location":"release/3.5/release-3-5-37/#osg-software-release-3537","text":"Release Date: 2021-06-03 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.37"},{"location":"release/3.5/release-3-5-37/#summary-of-changes","text":"This release contains: HTCondor-CE 4.5.2 : Upgrade from version 4.4.1 HTCondor-CE 4.5.0 Version History : New feature release HTCondor-CE 4.5.1 Version History : Bug fix release HTCondor-CE 4.5.2 Version History : Bug fix release gratia-probe 1.23.3: Fix problem that could cause pilot hours to be zero for non-HTCondor batch systems vault 1.7.2 : Security update; fixes CVE-2021-32923. (OSG configuration not vulnerable) osg-gridftp for Enterprise Linux 8 Upcoming GlideinWMS 3.7.4 : Fixed compatibility issue between 3.7 factories and 3.6.5 frontends Known Issue HTCondor-CE 5.1.0: batch system max walltime requests are always set to 3 days. Details and workaround can be found in the upstream bug tracker . These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-37/#containers","text":"The OSG Docker images have been updated to contain the latest CA certificates.","title":"Containers"},{"location":"release/3.5/release-3-5-37/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-37/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-37/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-37/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-37/#enterprise-linux-7","text":"globus-gridftp-server-13.20-1.3.osg35.el7 gratia-probe-1.23.3-1.osg35.el7 htcondor-ce-4.5.2-1.osg35.el7 vault-1.7.2-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-37/#enterprise-linux-8","text":"globus-gridftp-osg-extensions-0.4-1.osg35.el8 globus-gridftp-server-13.20-1.3.osg35.el8 gratia-probe-1.23.3-1.osg35.el8 gridftp-dsi-posix-1.4-3.osg35.el8 lcas-lcmaps-gt4-interface-0.3.1-1.3.osg35.el8 osg-gridftp-3.5-4.osg35.el8 vault-1.7.2-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-37/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-osg-pilot-container gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view vault If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-37/#enterprise-linux-7_1","text":"globus-gridftp-server-13.20-1.3.osg35.el7 globus-gridftp-server-debuginfo-13.20-1.3.osg35.el7 globus-gridftp-server-devel-13.20-1.3.osg35.el7 globus-gridftp-server-progs-13.20-1.3.osg35.el7 gratia-probe-1.23.3-1.osg35.el7 gratia-probe-common-1.23.3-1.osg35.el7 gratia-probe-condor-1.23.3-1.osg35.el7 gratia-probe-condor-events-1.23.3-1.osg35.el7 gratia-probe-dcache-storage-1.23.3-1.osg35.el7 gratia-probe-dcache-storagegroup-1.23.3-1.osg35.el7 gratia-probe-dcache-transfer-1.23.3-1.osg35.el7 gratia-probe-enstore-storage-1.23.3-1.osg35.el7 gratia-probe-enstore-tapedrive-1.23.3-1.osg35.el7 gratia-probe-enstore-transfer-1.23.3-1.osg35.el7 gratia-probe-glideinwms-1.23.3-1.osg35.el7 gratia-probe-gridftp-transfer-1.23.3-1.osg35.el7 gratia-probe-hadoop-storage-1.23.3-1.osg35.el7 gratia-probe-htcondor-ce-1.23.3-1.osg35.el7 gratia-probe-lsf-1.23.3-1.osg35.el7 gratia-probe-metric-1.23.3-1.osg35.el7 gratia-probe-onevm-1.23.3-1.osg35.el7 gratia-probe-osg-pilot-container-1.23.3-1.osg35.el7 gratia-probe-pbs-lsf-1.23.3-1.osg35.el7 gratia-probe-services-1.23.3-1.osg35.el7 gratia-probe-sge-1.23.3-1.osg35.el7 gratia-probe-slurm-1.23.3-1.osg35.el7 gratia-probe-xrootd-storage-1.23.3-1.osg35.el7 gratia-probe-xrootd-transfer-1.23.3-1.osg35.el7 htcondor-ce-4.5.2-1.osg35.el7 htcondor-ce-bosco-4.5.2-1.osg35.el7 htcondor-ce-client-4.5.2-1.osg35.el7 htcondor-ce-collector-4.5.2-1.osg35.el7 htcondor-ce-condor-4.5.2-1.osg35.el7 htcondor-ce-lsf-4.5.2-1.osg35.el7 htcondor-ce-pbs-4.5.2-1.osg35.el7 htcondor-ce-sge-4.5.2-1.osg35.el7 htcondor-ce-slurm-4.5.2-1.osg35.el7 htcondor-ce-view-4.5.2-1.osg35.el7 vault-1.7.2-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-37/#enterprise-linux-8_1","text":"globus-gridftp-osg-extensions-0.4-1.osg35.el8 globus-gridftp-osg-extensions-debuginfo-0.4-1.osg35.el8 globus-gridftp-osg-extensions-debugsource-0.4-1.osg35.el8 globus-gridftp-server-13.20-1.3.osg35.el8 globus-gridftp-server-debuginfo-13.20-1.3.osg35.el8 globus-gridftp-server-debugsource-13.20-1.3.osg35.el8 globus-gridftp-server-devel-13.20-1.3.osg35.el8 globus-gridftp-server-progs-13.20-1.3.osg35.el8 globus-gridftp-server-progs-debuginfo-13.20-1.3.osg35.el8 gratia-probe-1.23.3-1.osg35.el8 gratia-probe-common-1.23.3-1.osg35.el8 gratia-probe-condor-1.23.3-1.osg35.el8 gratia-probe-condor-events-1.23.3-1.osg35.el8 gratia-probe-dcache-storage-1.23.3-1.osg35.el8 gratia-probe-dcache-storagegroup-1.23.3-1.osg35.el8 gratia-probe-dcache-transfer-1.23.3-1.osg35.el8 gratia-probe-enstore-storage-1.23.3-1.osg35.el8 gratia-probe-enstore-tapedrive-1.23.3-1.osg35.el8 gratia-probe-enstore-transfer-1.23.3-1.osg35.el8 gratia-probe-glideinwms-1.23.3-1.osg35.el8 gratia-probe-gridftp-transfer-1.23.3-1.osg35.el8 gratia-probe-hadoop-storage-1.23.3-1.osg35.el8 gratia-probe-htcondor-ce-1.23.3-1.osg35.el8 gratia-probe-lsf-1.23.3-1.osg35.el8 gratia-probe-metric-1.23.3-1.osg35.el8 gratia-probe-onevm-1.23.3-1.osg35.el8 gratia-probe-osg-pilot-container-1.23.3-1.osg35.el8 gratia-probe-pbs-lsf-1.23.3-1.osg35.el8 gratia-probe-services-1.23.3-1.osg35.el8 gratia-probe-sge-1.23.3-1.osg35.el8 gratia-probe-slurm-1.23.3-1.osg35.el8 gratia-probe-xrootd-storage-1.23.3-1.osg35.el8 gratia-probe-xrootd-transfer-1.23.3-1.osg35.el8 gridftp-dsi-posix-1.4-3.osg35.el8 lcas-lcmaps-gt4-interface-0.3.1-1.3.osg35.el8 lcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.3.osg35.el8 lcas-lcmaps-gt4-interface-debugsource-0.3.1-1.3.osg35.el8 osg-gridftp-3.5-4.osg35.el8 osg-gridftp-hdfs-3.5-4.osg35.el8 osg-gridftp-xrootd-3.5-4.osg35.el8 vault-1.7.2-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-37/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-37/#enterprise-linux-7_2","text":"glideinwms-3.7.4-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-37/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-factory-core glideinwms-factory-httpd glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-core glideinwms-vofrontend-httpd glideinwms-vofrontend-standalone If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-37/#enterprise-linux-7_3","text":"glideinwms-3.7.4-1.osg35up.el7 glideinwms-common-tools-3.7.4-1.osg35up.el7 glideinwms-condor-common-config-3.7.4-1.osg35up.el7 glideinwms-factory-3.7.4-1.osg35up.el7 glideinwms-factory-condor-3.7.4-1.osg35up.el7 glideinwms-factory-core-3.7.4-1.osg35up.el7 glideinwms-factory-httpd-3.7.4-1.osg35up.el7 glideinwms-glidecondor-tools-3.7.4-1.osg35up.el7 glideinwms-libs-3.7.4-1.osg35up.el7 glideinwms-minimal-condor-3.7.4-1.osg35up.el7 glideinwms-usercollector-3.7.4-1.osg35up.el7 glideinwms-userschedd-3.7.4-1.osg35up.el7 glideinwms-vofrontend-3.7.4-1.osg35up.el7 glideinwms-vofrontend-core-3.7.4-1.osg35up.el7 glideinwms-vofrontend-httpd-3.7.4-1.osg35up.el7 glideinwms-vofrontend-standalone-3.7.4-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-38-2/","text":"OSG Data Release 3.5.38-2 \u00b6 Release Date: 2021-06-16 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 VO Package v113 Added new CLAS12/EIC VO certificates Retired old CLAS12/EIC VO certificates These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the latest VO package. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 vo-client-113-1.osg35.el7 Enterprise Linux 8 \u00b6 vo-client-113-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: vo-client If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 vo-client-113-1.osg35.el7 Enterprise Linux 8 \u00b6 vo-client-113-1.osg35.el7","title":"Release 3 5 38 2"},{"location":"release/3.5/release-3-5-38-2/#osg-data-release-3538-2","text":"Release Date: 2021-06-16 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.38-2"},{"location":"release/3.5/release-3-5-38-2/#summary-of-changes","text":"VO Package v113 Added new CLAS12/EIC VO certificates Retired old CLAS12/EIC VO certificates These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-38-2/#containers","text":"The OSG Docker images have been updated to contain the latest VO package.","title":"Containers"},{"location":"release/3.5/release-3-5-38-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-38-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-38-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-38-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-38-2/#enterprise-linux-7","text":"vo-client-113-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-38-2/#enterprise-linux-8","text":"vo-client-113-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-38-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: vo-client If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-38-2/#enterprise-linux-7_1","text":"vo-client-113-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-38-2/#enterprise-linux-8_1","text":"vo-client-113-1.osg35.el7","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-38/","text":"OSG Software Release 3.5.38 \u00b6 Release Date: 2021-06-10 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor 8.8.13-1.1: Ensure that completed job ads retain local batch system job IDs Upcoming XRootD 5.2.0 Fix scaling issue with HTTP-TPC (caused by inefficient CA handling) Many other features and bug fixes xrootd-hdfs 2.2.0-1.1: Bug fix for token authentication Known Issue HTCondor-CE 5.1.0: batch system max walltime requests are always set to 3 days. Details and workaround can be found in the upstream bug tracker . These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the latest CA certificates. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 condor-8.8.13-1.1.osg35.el7 Enterprise Linux 8 \u00b6 condor-8.8.13-1.1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-8.8.13-1.1.osg35.el7 condor-all-8.8.13-1.1.osg35.el7 condor-annex-ec2-8.8.13-1.1.osg35.el7 condor-bosco-8.8.13-1.1.osg35.el7 condor-classads-8.8.13-1.1.osg35.el7 condor-classads-devel-8.8.13-1.1.osg35.el7 condor-debuginfo-8.8.13-1.1.osg35.el7 condor-kbdd-8.8.13-1.1.osg35.el7 condor-procd-8.8.13-1.1.osg35.el7 condor-test-8.8.13-1.1.osg35.el7 condor-vm-gahp-8.8.13-1.1.osg35.el7 minicondor-8.8.13-1.1.osg35.el7 python2-condor-8.8.13-1.1.osg35.el7 python3-condor-8.8.13-1.1.osg35.el7 Enterprise Linux 8 \u00b6 condor-8.8.13-1.1.osg35.el8 condor-all-8.8.13-1.1.osg35.el8 condor-annex-ec2-8.8.13-1.1.osg35.el8 condor-bosco-8.8.13-1.1.osg35.el8 condor-bosco-debuginfo-8.8.13-1.1.osg35.el8 condor-classads-8.8.13-1.1.osg35.el8 condor-classads-debuginfo-8.8.13-1.1.osg35.el8 condor-classads-devel-8.8.13-1.1.osg35.el8 condor-classads-devel-debuginfo-8.8.13-1.1.osg35.el8 condor-debuginfo-8.8.13-1.1.osg35.el8 condor-debugsource-8.8.13-1.1.osg35.el8 condor-kbdd-8.8.13-1.1.osg35.el8 condor-kbdd-debuginfo-8.8.13-1.1.osg35.el8 condor-procd-8.8.13-1.1.osg35.el8 condor-procd-debuginfo-8.8.13-1.1.osg35.el8 condor-test-8.8.13-1.1.osg35.el8 condor-test-debuginfo-8.8.13-1.1.osg35.el8 condor-vm-gahp-8.8.13-1.1.osg35.el8 condor-vm-gahp-debuginfo-8.8.13-1.1.osg35.el8 minicondor-8.8.13-1.1.osg35.el8 python3-condor-8.8.13-1.1.osg35.el8 python3-condor-debuginfo-8.8.13-1.1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 osg-xrootd-3.5.upcoming-1.osg35up.el7 xrootd-5.2.0-1.1.osg35up.el7 xrootd-hdfs-2.2.0-1.1.osg35up.el7 xrootd-multiuser-1.0.0-1.osg35up.el7 xrootd-scitokens-1.2.2-1.osg35up.el7 Enterprise Linux 8 \u00b6 osg-xrootd-3.5.upcoming-1.osg35up.el8 xrootd-5.2.0-1.1.osg35up.el8 xrootd-cmstfc-1.5.2-6.osg35up.el8 xrootd-lcmaps-1.7.8-3.osgup.el8 xrootd-multiuser-1.0.0-1.osg35up.el8 xrootd-rucioN2N-for-Xcache-1.2-3.3.osg35up.el8 xrootd-scitokens-1.2.2-1.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: osg-xrootd osg-xrootd-standalone python2-xrootd python36-xrootd xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-scitokens xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 osg-xrootd-3.5.upcoming-1.osg35up.el7 osg-xrootd-standalone-3.5.upcoming-1.osg35up.el7 python2-xrootd-5.2.0-1.1.osg35up.el7 python36-xrootd-5.2.0-1.1.osg35up.el7 xrootd-5.2.0-1.1.osg35up.el7 xrootd-client-5.2.0-1.1.osg35up.el7 xrootd-client-compat-5.2.0-1.1.osg35up.el7 xrootd-client-devel-5.2.0-1.1.osg35up.el7 xrootd-client-libs-5.2.0-1.1.osg35up.el7 xrootd-debuginfo-5.2.0-1.1.osg35up.el7 xrootd-devel-5.2.0-1.1.osg35up.el7 xrootd-doc-5.2.0-1.1.osg35up.el7 xrootd-fuse-5.2.0-1.1.osg35up.el7 xrootd-hdfs-2.2.0-1.1.osg35up.el7 xrootd-hdfs-debuginfo-2.2.0-1.1.osg35up.el7 xrootd-hdfs-devel-2.2.0-1.1.osg35up.el7 xrootd-libs-5.2.0-1.1.osg35up.el7 xrootd-multiuser-1.0.0-1.osg35up.el7 xrootd-multiuser-debuginfo-1.0.0-1.osg35up.el7 xrootd-private-devel-5.2.0-1.1.osg35up.el7 xrootd-scitokens-1.2.2-1.osg35up.el7 xrootd-scitokens-5.2.0-1.1.osg35up.el7 xrootd-scitokens-debuginfo-1.2.2-1.osg35up.el7 xrootd-selinux-5.2.0-1.1.osg35up.el7 xrootd-server-5.2.0-1.1.osg35up.el7 xrootd-server-compat-5.2.0-1.1.osg35up.el7 xrootd-server-devel-5.2.0-1.1.osg35up.el7 xrootd-server-libs-5.2.0-1.1.osg35up.el7 xrootd-voms-5.2.0-1.1.osg35up.el7 Enterprise Linux 8 \u00b6 osg-xrootd-3.5.upcoming-1.osg35up.el8 osg-xrootd-standalone-3.5.upcoming-1.osg35up.el8 python3-xrootd-5.2.0-1.1.osg35up.el8 python3-xrootd-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-5.2.0-1.1.osg35up.el8 xrootd-client-5.2.0-1.1.osg35up.el8 xrootd-client-compat-5.2.0-1.1.osg35up.el8 xrootd-client-compat-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-client-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-client-devel-5.2.0-1.1.osg35up.el8 xrootd-client-devel-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-client-libs-5.2.0-1.1.osg35up.el8 xrootd-client-libs-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-cmstfc-1.5.2-6.osg35up.el8 xrootd-cmstfc-debuginfo-1.5.2-6.osg35up.el8 xrootd-cmstfc-debugsource-1.5.2-6.osg35up.el8 xrootd-cmstfc-devel-1.5.2-6.osg35up.el8 xrootd-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-debugsource-5.2.0-1.1.osg35up.el8 xrootd-devel-5.2.0-1.1.osg35up.el8 xrootd-doc-5.2.0-1.1.osg35up.el8 xrootd-fuse-5.2.0-1.1.osg35up.el8 xrootd-fuse-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-lcmaps-1.7.8-3.osgup.el8 xrootd-lcmaps-debuginfo-1.7.8-3.osgup.el8 xrootd-lcmaps-debugsource-1.7.8-3.osgup.el8 xrootd-libs-5.2.0-1.1.osg35up.el8 xrootd-libs-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-multiuser-1.0.0-1.osg35up.el8 xrootd-multiuser-debuginfo-1.0.0-1.osg35up.el8 xrootd-multiuser-debugsource-1.0.0-1.osg35up.el8 xrootd-private-devel-5.2.0-1.1.osg35up.el8 xrootd-rucioN2N-for-Xcache-1.2-3.3.osg35up.el8 xrootd-rucioN2N-for-Xcache-debuginfo-1.2-3.3.osg35up.el8 xrootd-rucioN2N-for-Xcache-debugsource-1.2-3.3.osg35up.el8 xrootd-scitokens-1.2.2-1.osg35up.el8 xrootd-scitokens-5.2.0-1.1.osg35up.el8 xrootd-scitokens-debuginfo-1.2.2-1.osg35up.el8 xrootd-scitokens-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-scitokens-debugsource-1.2.2-1.osg35up.el8 xrootd-selinux-5.2.0-1.1.osg35up.el8 xrootd-server-5.2.0-1.1.osg35up.el8 xrootd-server-compat-5.2.0-1.1.osg35up.el8 xrootd-server-compat-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-server-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-server-devel-5.2.0-1.1.osg35up.el8 xrootd-server-libs-5.2.0-1.1.osg35up.el8 xrootd-server-libs-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-voms-5.2.0-1.1.osg35up.el8 xrootd-voms-debuginfo-5.2.0-1.1.osg35up.el8","title":"Release 3 5 38"},{"location":"release/3.5/release-3-5-38/#osg-software-release-3538","text":"Release Date: 2021-06-10 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.38"},{"location":"release/3.5/release-3-5-38/#summary-of-changes","text":"This release contains: HTCondor 8.8.13-1.1: Ensure that completed job ads retain local batch system job IDs Upcoming XRootD 5.2.0 Fix scaling issue with HTTP-TPC (caused by inefficient CA handling) Many other features and bug fixes xrootd-hdfs 2.2.0-1.1: Bug fix for token authentication Known Issue HTCondor-CE 5.1.0: batch system max walltime requests are always set to 3 days. Details and workaround can be found in the upstream bug tracker . These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-38/#containers","text":"The OSG Docker images have been updated to contain the latest CA certificates.","title":"Containers"},{"location":"release/3.5/release-3-5-38/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-38/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-38/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-38/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-38/#enterprise-linux-7","text":"condor-8.8.13-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-38/#enterprise-linux-8","text":"condor-8.8.13-1.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-38/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-38/#enterprise-linux-7_1","text":"condor-8.8.13-1.1.osg35.el7 condor-all-8.8.13-1.1.osg35.el7 condor-annex-ec2-8.8.13-1.1.osg35.el7 condor-bosco-8.8.13-1.1.osg35.el7 condor-classads-8.8.13-1.1.osg35.el7 condor-classads-devel-8.8.13-1.1.osg35.el7 condor-debuginfo-8.8.13-1.1.osg35.el7 condor-kbdd-8.8.13-1.1.osg35.el7 condor-procd-8.8.13-1.1.osg35.el7 condor-test-8.8.13-1.1.osg35.el7 condor-vm-gahp-8.8.13-1.1.osg35.el7 minicondor-8.8.13-1.1.osg35.el7 python2-condor-8.8.13-1.1.osg35.el7 python3-condor-8.8.13-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-38/#enterprise-linux-8_1","text":"condor-8.8.13-1.1.osg35.el8 condor-all-8.8.13-1.1.osg35.el8 condor-annex-ec2-8.8.13-1.1.osg35.el8 condor-bosco-8.8.13-1.1.osg35.el8 condor-bosco-debuginfo-8.8.13-1.1.osg35.el8 condor-classads-8.8.13-1.1.osg35.el8 condor-classads-debuginfo-8.8.13-1.1.osg35.el8 condor-classads-devel-8.8.13-1.1.osg35.el8 condor-classads-devel-debuginfo-8.8.13-1.1.osg35.el8 condor-debuginfo-8.8.13-1.1.osg35.el8 condor-debugsource-8.8.13-1.1.osg35.el8 condor-kbdd-8.8.13-1.1.osg35.el8 condor-kbdd-debuginfo-8.8.13-1.1.osg35.el8 condor-procd-8.8.13-1.1.osg35.el8 condor-procd-debuginfo-8.8.13-1.1.osg35.el8 condor-test-8.8.13-1.1.osg35.el8 condor-test-debuginfo-8.8.13-1.1.osg35.el8 condor-vm-gahp-8.8.13-1.1.osg35.el8 condor-vm-gahp-debuginfo-8.8.13-1.1.osg35.el8 minicondor-8.8.13-1.1.osg35.el8 python3-condor-8.8.13-1.1.osg35.el8 python3-condor-debuginfo-8.8.13-1.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-38/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-38/#enterprise-linux-7_2","text":"osg-xrootd-3.5.upcoming-1.osg35up.el7 xrootd-5.2.0-1.1.osg35up.el7 xrootd-hdfs-2.2.0-1.1.osg35up.el7 xrootd-multiuser-1.0.0-1.osg35up.el7 xrootd-scitokens-1.2.2-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-38/#enterprise-linux-8_2","text":"osg-xrootd-3.5.upcoming-1.osg35up.el8 xrootd-5.2.0-1.1.osg35up.el8 xrootd-cmstfc-1.5.2-6.osg35up.el8 xrootd-lcmaps-1.7.8-3.osgup.el8 xrootd-multiuser-1.0.0-1.osg35up.el8 xrootd-rucioN2N-for-Xcache-1.2-3.3.osg35up.el8 xrootd-scitokens-1.2.2-1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-38/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: osg-xrootd osg-xrootd-standalone python2-xrootd python36-xrootd xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-scitokens xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-38/#enterprise-linux-7_3","text":"osg-xrootd-3.5.upcoming-1.osg35up.el7 osg-xrootd-standalone-3.5.upcoming-1.osg35up.el7 python2-xrootd-5.2.0-1.1.osg35up.el7 python36-xrootd-5.2.0-1.1.osg35up.el7 xrootd-5.2.0-1.1.osg35up.el7 xrootd-client-5.2.0-1.1.osg35up.el7 xrootd-client-compat-5.2.0-1.1.osg35up.el7 xrootd-client-devel-5.2.0-1.1.osg35up.el7 xrootd-client-libs-5.2.0-1.1.osg35up.el7 xrootd-debuginfo-5.2.0-1.1.osg35up.el7 xrootd-devel-5.2.0-1.1.osg35up.el7 xrootd-doc-5.2.0-1.1.osg35up.el7 xrootd-fuse-5.2.0-1.1.osg35up.el7 xrootd-hdfs-2.2.0-1.1.osg35up.el7 xrootd-hdfs-debuginfo-2.2.0-1.1.osg35up.el7 xrootd-hdfs-devel-2.2.0-1.1.osg35up.el7 xrootd-libs-5.2.0-1.1.osg35up.el7 xrootd-multiuser-1.0.0-1.osg35up.el7 xrootd-multiuser-debuginfo-1.0.0-1.osg35up.el7 xrootd-private-devel-5.2.0-1.1.osg35up.el7 xrootd-scitokens-1.2.2-1.osg35up.el7 xrootd-scitokens-5.2.0-1.1.osg35up.el7 xrootd-scitokens-debuginfo-1.2.2-1.osg35up.el7 xrootd-selinux-5.2.0-1.1.osg35up.el7 xrootd-server-5.2.0-1.1.osg35up.el7 xrootd-server-compat-5.2.0-1.1.osg35up.el7 xrootd-server-devel-5.2.0-1.1.osg35up.el7 xrootd-server-libs-5.2.0-1.1.osg35up.el7 xrootd-voms-5.2.0-1.1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-38/#enterprise-linux-8_3","text":"osg-xrootd-3.5.upcoming-1.osg35up.el8 osg-xrootd-standalone-3.5.upcoming-1.osg35up.el8 python3-xrootd-5.2.0-1.1.osg35up.el8 python3-xrootd-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-5.2.0-1.1.osg35up.el8 xrootd-client-5.2.0-1.1.osg35up.el8 xrootd-client-compat-5.2.0-1.1.osg35up.el8 xrootd-client-compat-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-client-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-client-devel-5.2.0-1.1.osg35up.el8 xrootd-client-devel-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-client-libs-5.2.0-1.1.osg35up.el8 xrootd-client-libs-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-cmstfc-1.5.2-6.osg35up.el8 xrootd-cmstfc-debuginfo-1.5.2-6.osg35up.el8 xrootd-cmstfc-debugsource-1.5.2-6.osg35up.el8 xrootd-cmstfc-devel-1.5.2-6.osg35up.el8 xrootd-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-debugsource-5.2.0-1.1.osg35up.el8 xrootd-devel-5.2.0-1.1.osg35up.el8 xrootd-doc-5.2.0-1.1.osg35up.el8 xrootd-fuse-5.2.0-1.1.osg35up.el8 xrootd-fuse-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-lcmaps-1.7.8-3.osgup.el8 xrootd-lcmaps-debuginfo-1.7.8-3.osgup.el8 xrootd-lcmaps-debugsource-1.7.8-3.osgup.el8 xrootd-libs-5.2.0-1.1.osg35up.el8 xrootd-libs-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-multiuser-1.0.0-1.osg35up.el8 xrootd-multiuser-debuginfo-1.0.0-1.osg35up.el8 xrootd-multiuser-debugsource-1.0.0-1.osg35up.el8 xrootd-private-devel-5.2.0-1.1.osg35up.el8 xrootd-rucioN2N-for-Xcache-1.2-3.3.osg35up.el8 xrootd-rucioN2N-for-Xcache-debuginfo-1.2-3.3.osg35up.el8 xrootd-rucioN2N-for-Xcache-debugsource-1.2-3.3.osg35up.el8 xrootd-scitokens-1.2.2-1.osg35up.el8 xrootd-scitokens-5.2.0-1.1.osg35up.el8 xrootd-scitokens-debuginfo-1.2.2-1.osg35up.el8 xrootd-scitokens-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-scitokens-debugsource-1.2.2-1.osg35up.el8 xrootd-selinux-5.2.0-1.1.osg35up.el8 xrootd-server-5.2.0-1.1.osg35up.el8 xrootd-server-compat-5.2.0-1.1.osg35up.el8 xrootd-server-compat-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-server-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-server-devel-5.2.0-1.1.osg35up.el8 xrootd-server-libs-5.2.0-1.1.osg35up.el8 xrootd-server-libs-debuginfo-5.2.0-1.1.osg35up.el8 xrootd-voms-5.2.0-1.1.osg35up.el8 xrootd-voms-debuginfo-5.2.0-1.1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-39/","text":"OSG Software Release 3.5.39 \u00b6 Release Date: 2021-06-24 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: scitokens-cpp 0.6.2: Fix WLCG token compatibility for the HTCondor READ scope Upcoming HTCondor 9.0.1-1.2 : Bug fix release Fix problem where X.509 proxy refresh kills job when using AES encryption Fix problem when jobs require a different machine after a failure Fix problem where a job matched a machine it can't use, delaying job start Fix exit code and retry checking when a job exits because of a signal Fix a memory leak in the job router when a job is removed via job policy Fixed the back-end support for the 'bosco_cluster --add' command HTCondor-CE 5.1.1 Improve restart time of HTCondor-CE View Fix bug that caused HTCondor-CE to ignore incoming BatchRuntime requests Fixed error that occurred during RPM installation of non-HTCondor batch systems regarding missing file batch_gahp These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the new software. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 scitokens-cpp-0.6.2-1.osg35.el7 Enterprise Linux 8 \u00b6 scitokens-cpp-0.6.2-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 scitokens-cpp-0.6.2-1.osg35.el7 scitokens-cpp-debuginfo-0.6.2-1.osg35.el7 scitokens-cpp-devel-0.6.2-1.osg35.el7 Enterprise Linux 8 \u00b6 scitokens-cpp-0.6.2-1.osg35.el8 scitokens-cpp-debuginfo-0.6.2-1.osg35.el8 scitokens-cpp-debugsource-0.6.2-1.osg35.el8 scitokens-cpp-devel-0.6.2-1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 condor-9.0.1-1.1.osg35up.el7 htcondor-ce-5.1.1-1.1.osg35up.el7 Enterprise Linux 8 \u00b6 condor-9.0.1-1.1.osg35up.el8 htcondor-ce-5.1.1-1.1.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-9.0.1-1.1.osg35up.el7 condor-all-9.0.1-1.1.osg35up.el7 condor-annex-ec2-9.0.1-1.1.osg35up.el7 condor-bosco-9.0.1-1.1.osg35up.el7 condor-classads-9.0.1-1.1.osg35up.el7 condor-classads-devel-9.0.1-1.1.osg35up.el7 condor-credmon-oauth-9.0.1-1.1.osg35up.el7 condor-credmon-vault-9.0.1-1.1.osg35up.el7 condor-debuginfo-9.0.1-1.1.osg35up.el7 condor-devel-9.0.1-1.1.osg35up.el7 condor-kbdd-9.0.1-1.1.osg35up.el7 condor-procd-9.0.1-1.1.osg35up.el7 condor-test-9.0.1-1.1.osg35up.el7 condor-vm-gahp-9.0.1-1.1.osg35up.el7 htcondor-ce-5.1.1-1.1.osg35up.el7 htcondor-ce-bosco-5.1.1-1.1.osg35up.el7 htcondor-ce-client-5.1.1-1.1.osg35up.el7 htcondor-ce-collector-5.1.1-1.1.osg35up.el7 htcondor-ce-condor-5.1.1-1.1.osg35up.el7 htcondor-ce-lsf-5.1.1-1.1.osg35up.el7 htcondor-ce-pbs-5.1.1-1.1.osg35up.el7 htcondor-ce-sge-5.1.1-1.1.osg35up.el7 htcondor-ce-slurm-5.1.1-1.1.osg35up.el7 htcondor-ce-view-5.1.1-1.1.osg35up.el7 minicondor-9.0.1-1.1.osg35up.el7 python2-condor-9.0.1-1.1.osg35up.el7 python3-condor-9.0.1-1.1.osg35up.el7 Enterprise Linux 8 \u00b6 condor-9.0.1-1.1.osg35up.el8 condor-all-9.0.1-1.1.osg35up.el8 condor-annex-ec2-9.0.1-1.1.osg35up.el8 condor-bosco-9.0.1-1.1.osg35up.el8 condor-bosco-debuginfo-9.0.1-1.1.osg35up.el8 condor-classads-9.0.1-1.1.osg35up.el8 condor-classads-debuginfo-9.0.1-1.1.osg35up.el8 condor-classads-devel-9.0.1-1.1.osg35up.el8 condor-classads-devel-debuginfo-9.0.1-1.1.osg35up.el8 condor-credmon-vault-9.0.1-1.1.osg35up.el8 condor-debuginfo-9.0.1-1.1.osg35up.el8 condor-debugsource-9.0.1-1.1.osg35up.el8 condor-devel-9.0.1-1.1.osg35up.el8 condor-kbdd-9.0.1-1.1.osg35up.el8 condor-kbdd-debuginfo-9.0.1-1.1.osg35up.el8 condor-procd-9.0.1-1.1.osg35up.el8 condor-procd-debuginfo-9.0.1-1.1.osg35up.el8 condor-test-9.0.1-1.1.osg35up.el8 condor-test-debuginfo-9.0.1-1.1.osg35up.el8 condor-vm-gahp-9.0.1-1.1.osg35up.el8 condor-vm-gahp-debuginfo-9.0.1-1.1.osg35up.el8 htcondor-ce-5.1.1-1.1.osg35up.el8 htcondor-ce-bosco-5.1.1-1.1.osg35up.el8 htcondor-ce-client-5.1.1-1.1.osg35up.el8 htcondor-ce-collector-5.1.1-1.1.osg35up.el8 htcondor-ce-condor-5.1.1-1.1.osg35up.el8 htcondor-ce-lsf-5.1.1-1.1.osg35up.el8 htcondor-ce-pbs-5.1.1-1.1.osg35up.el8 htcondor-ce-sge-5.1.1-1.1.osg35up.el8 htcondor-ce-slurm-5.1.1-1.1.osg35up.el8 htcondor-ce-view-5.1.1-1.1.osg35up.el8 minicondor-9.0.1-1.1.osg35up.el8 python3-condor-9.0.1-1.1.osg35up.el8 python3-condor-debuginfo-9.0.1-1.1.osg35up.el8","title":"Release 3 5 39"},{"location":"release/3.5/release-3-5-39/#osg-software-release-3539","text":"Release Date: 2021-06-24 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.39"},{"location":"release/3.5/release-3-5-39/#summary-of-changes","text":"This release contains: scitokens-cpp 0.6.2: Fix WLCG token compatibility for the HTCondor READ scope Upcoming HTCondor 9.0.1-1.2 : Bug fix release Fix problem where X.509 proxy refresh kills job when using AES encryption Fix problem when jobs require a different machine after a failure Fix problem where a job matched a machine it can't use, delaying job start Fix exit code and retry checking when a job exits because of a signal Fix a memory leak in the job router when a job is removed via job policy Fixed the back-end support for the 'bosco_cluster --add' command HTCondor-CE 5.1.1 Improve restart time of HTCondor-CE View Fix bug that caused HTCondor-CE to ignore incoming BatchRuntime requests Fixed error that occurred during RPM installation of non-HTCondor batch systems regarding missing file batch_gahp These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-39/#containers","text":"The OSG Docker images have been updated to contain the new software.","title":"Containers"},{"location":"release/3.5/release-3-5-39/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-39/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-39/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-39/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-39/#enterprise-linux-7","text":"scitokens-cpp-0.6.2-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-39/#enterprise-linux-8","text":"scitokens-cpp-0.6.2-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-39/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-39/#enterprise-linux-7_1","text":"scitokens-cpp-0.6.2-1.osg35.el7 scitokens-cpp-debuginfo-0.6.2-1.osg35.el7 scitokens-cpp-devel-0.6.2-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-39/#enterprise-linux-8_1","text":"scitokens-cpp-0.6.2-1.osg35.el8 scitokens-cpp-debuginfo-0.6.2-1.osg35.el8 scitokens-cpp-debugsource-0.6.2-1.osg35.el8 scitokens-cpp-devel-0.6.2-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-39/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-39/#enterprise-linux-7_2","text":"condor-9.0.1-1.1.osg35up.el7 htcondor-ce-5.1.1-1.1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-39/#enterprise-linux-8_2","text":"condor-9.0.1-1.1.osg35up.el8 htcondor-ce-5.1.1-1.1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-39/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-39/#enterprise-linux-7_3","text":"condor-9.0.1-1.1.osg35up.el7 condor-all-9.0.1-1.1.osg35up.el7 condor-annex-ec2-9.0.1-1.1.osg35up.el7 condor-bosco-9.0.1-1.1.osg35up.el7 condor-classads-9.0.1-1.1.osg35up.el7 condor-classads-devel-9.0.1-1.1.osg35up.el7 condor-credmon-oauth-9.0.1-1.1.osg35up.el7 condor-credmon-vault-9.0.1-1.1.osg35up.el7 condor-debuginfo-9.0.1-1.1.osg35up.el7 condor-devel-9.0.1-1.1.osg35up.el7 condor-kbdd-9.0.1-1.1.osg35up.el7 condor-procd-9.0.1-1.1.osg35up.el7 condor-test-9.0.1-1.1.osg35up.el7 condor-vm-gahp-9.0.1-1.1.osg35up.el7 htcondor-ce-5.1.1-1.1.osg35up.el7 htcondor-ce-bosco-5.1.1-1.1.osg35up.el7 htcondor-ce-client-5.1.1-1.1.osg35up.el7 htcondor-ce-collector-5.1.1-1.1.osg35up.el7 htcondor-ce-condor-5.1.1-1.1.osg35up.el7 htcondor-ce-lsf-5.1.1-1.1.osg35up.el7 htcondor-ce-pbs-5.1.1-1.1.osg35up.el7 htcondor-ce-sge-5.1.1-1.1.osg35up.el7 htcondor-ce-slurm-5.1.1-1.1.osg35up.el7 htcondor-ce-view-5.1.1-1.1.osg35up.el7 minicondor-9.0.1-1.1.osg35up.el7 python2-condor-9.0.1-1.1.osg35up.el7 python3-condor-9.0.1-1.1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-39/#enterprise-linux-8_3","text":"condor-9.0.1-1.1.osg35up.el8 condor-all-9.0.1-1.1.osg35up.el8 condor-annex-ec2-9.0.1-1.1.osg35up.el8 condor-bosco-9.0.1-1.1.osg35up.el8 condor-bosco-debuginfo-9.0.1-1.1.osg35up.el8 condor-classads-9.0.1-1.1.osg35up.el8 condor-classads-debuginfo-9.0.1-1.1.osg35up.el8 condor-classads-devel-9.0.1-1.1.osg35up.el8 condor-classads-devel-debuginfo-9.0.1-1.1.osg35up.el8 condor-credmon-vault-9.0.1-1.1.osg35up.el8 condor-debuginfo-9.0.1-1.1.osg35up.el8 condor-debugsource-9.0.1-1.1.osg35up.el8 condor-devel-9.0.1-1.1.osg35up.el8 condor-kbdd-9.0.1-1.1.osg35up.el8 condor-kbdd-debuginfo-9.0.1-1.1.osg35up.el8 condor-procd-9.0.1-1.1.osg35up.el8 condor-procd-debuginfo-9.0.1-1.1.osg35up.el8 condor-test-9.0.1-1.1.osg35up.el8 condor-test-debuginfo-9.0.1-1.1.osg35up.el8 condor-vm-gahp-9.0.1-1.1.osg35up.el8 condor-vm-gahp-debuginfo-9.0.1-1.1.osg35up.el8 htcondor-ce-5.1.1-1.1.osg35up.el8 htcondor-ce-bosco-5.1.1-1.1.osg35up.el8 htcondor-ce-client-5.1.1-1.1.osg35up.el8 htcondor-ce-collector-5.1.1-1.1.osg35up.el8 htcondor-ce-condor-5.1.1-1.1.osg35up.el8 htcondor-ce-lsf-5.1.1-1.1.osg35up.el8 htcondor-ce-pbs-5.1.1-1.1.osg35up.el8 htcondor-ce-sge-5.1.1-1.1.osg35up.el8 htcondor-ce-slurm-5.1.1-1.1.osg35up.el8 htcondor-ce-view-5.1.1-1.1.osg35up.el8 minicondor-9.0.1-1.1.osg35up.el8 python3-condor-9.0.1-1.1.osg35up.el8 python3-condor-debuginfo-9.0.1-1.1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-4/","text":"OSG Software Release 3.5.4 \u00b6 Release Date: 2019-10-23 Supported OS Versions: EL7 Summary of Changes \u00b6 This release contains: HTCondor 8.8.5-1.7: Addressed issue when updating from OSG 3.4 StashCache-Client 5.5.0 : Update from 5.2.0: Improved SciTokens support plus bug fixes Updated CA certificates based on IGTF 1.102 : Added CESNET-CA-4 ICA accredited classic CA for issuer roll-over (CZ) VO Package v97 : Added CLAS12 VO for Jefferson Lab These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Known Issues \u00b6 OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.5-1.7.osg35.el7 igtf-ca-certs-1.102-1.osg35.el7 osg-ca-certs-1.84-1.osg35.el7 stashcache-client-5.5.0-2.osg35.el7 vo-client-97-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp igtf-ca-certs minicondor osg-ca-certs python2-condor python3-condor stashcache-client vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.5-1.7.osg35.el7 condor-all-8.8.5-1.7.osg35.el7 condor-annex-ec2-8.8.5-1.7.osg35.el7 condor-bosco-8.8.5-1.7.osg35.el7 condor-classads-8.8.5-1.7.osg35.el7 condor-classads-devel-8.8.5-1.7.osg35.el7 condor-debuginfo-8.8.5-1.7.osg35.el7 condor-kbdd-8.8.5-1.7.osg35.el7 condor-procd-8.8.5-1.7.osg35.el7 condor-test-8.8.5-1.7.osg35.el7 condor-vm-gahp-8.8.5-1.7.osg35.el7 igtf-ca-certs-1.102-1.osg35.el7 minicondor-8.8.5-1.7.osg35.el7 osg-ca-certs-1.84-1.osg35.el7 python2-condor-8.8.5-1.7.osg35.el7 python3-condor-8.8.5-1.7.osg35.el7 stashcache-client-5.5.0-2.osg35.el7 vo-client-97-1.osg35.el7 vo-client-dcache-97-1.osg35.el7 vo-client-lcmaps-voms-97-1.osg35.el7","title":"Release 3 5 4"},{"location":"release/3.5/release-3-5-4/#osg-software-release-354","text":"Release Date: 2019-10-23 Supported OS Versions: EL7","title":"OSG Software Release 3.5.4"},{"location":"release/3.5/release-3-5-4/#summary-of-changes","text":"This release contains: HTCondor 8.8.5-1.7: Addressed issue when updating from OSG 3.4 StashCache-Client 5.5.0 : Update from 5.2.0: Improved SciTokens support plus bug fixes Updated CA certificates based on IGTF 1.102 : Added CESNET-CA-4 ICA accredited classic CA for issuer roll-over (CZ) VO Package v97 : Added CLAS12 VO for Jefferson Lab These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-4/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-4/#known-issues","text":"OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-4/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-4/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-4/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-4/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.5-1.7.osg35.el7 igtf-ca-certs-1.102-1.osg35.el7 osg-ca-certs-1.84-1.osg35.el7 stashcache-client-5.5.0-2.osg35.el7 vo-client-97-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-4/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp igtf-ca-certs minicondor osg-ca-certs python2-condor python3-condor stashcache-client vo-client vo-client-dcache vo-client-lcmaps-voms If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.5-1.7.osg35.el7 condor-all-8.8.5-1.7.osg35.el7 condor-annex-ec2-8.8.5-1.7.osg35.el7 condor-bosco-8.8.5-1.7.osg35.el7 condor-classads-8.8.5-1.7.osg35.el7 condor-classads-devel-8.8.5-1.7.osg35.el7 condor-debuginfo-8.8.5-1.7.osg35.el7 condor-kbdd-8.8.5-1.7.osg35.el7 condor-procd-8.8.5-1.7.osg35.el7 condor-test-8.8.5-1.7.osg35.el7 condor-vm-gahp-8.8.5-1.7.osg35.el7 igtf-ca-certs-1.102-1.osg35.el7 minicondor-8.8.5-1.7.osg35.el7 osg-ca-certs-1.84-1.osg35.el7 python2-condor-8.8.5-1.7.osg35.el7 python3-condor-8.8.5-1.7.osg35.el7 stashcache-client-5.5.0-2.osg35.el7 vo-client-97-1.osg35.el7 vo-client-dcache-97-1.osg35.el7 vo-client-lcmaps-voms-97-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-40-2/","text":"OSG Data Release 3.5.40-2 \u00b6 Release Date: 2021-07-15 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 VO Package v114 Fix typographic error in CLAS12 and EIC VOMS certificate issuers Add LSC files for CERN VO IAM endpoints These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the latest VO package. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 vo-client-114-1.osg35.el7 Enterprise Linux 8 \u00b6 vo-client-114-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: vo-client If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 vo-client-114-1.osg35.el7 Enterprise Linux 8 \u00b6 vo-client-114-1.osg35.el7","title":"Release 3 5 40 2"},{"location":"release/3.5/release-3-5-40-2/#osg-data-release-3540-2","text":"Release Date: 2021-07-15 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.40-2"},{"location":"release/3.5/release-3-5-40-2/#summary-of-changes","text":"VO Package v114 Fix typographic error in CLAS12 and EIC VOMS certificate issuers Add LSC files for CERN VO IAM endpoints These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-40-2/#containers","text":"The OSG Docker images have been updated to contain the latest VO package.","title":"Containers"},{"location":"release/3.5/release-3-5-40-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-40-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-40-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-40-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-40-2/#enterprise-linux-7","text":"vo-client-114-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-40-2/#enterprise-linux-8","text":"vo-client-114-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-40-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: vo-client If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-40-2/#enterprise-linux-7_1","text":"vo-client-114-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-40-2/#enterprise-linux-8_1","text":"vo-client-114-1.osg35.el7","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-40/","text":"OSG Software Release 3.5.40 \u00b6 Release Date: 2021-07-01 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Frontier Squid 4.15-2.1 : Fix log rotation when not compressing Vault 1.7.3 : Bug fix release htvault-config 1.2: Updated to match vault 1.7.3 Updates on Enterprise Linux 8 XRootD 4.12.6 and plugins osg-flock 1.3 Upcoming xrootd-multiuser 1.1.0: Administrator can specify the default umask again These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the new software. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 frontier-squid-4.15-2.1.osg35.el7 gfal2-2.18.1-1.2.osg35.el7 htvault-config-1.2-1.osg35.el7 vault-1.7.3-1.osg35.el7 Enterprise Linux 8 \u00b6 frontier-squid-4.15-2.1.osg35.el8 gfal2-2.18.1-1.2.osg35.el8 htvault-config-1.2-1.osg35.el8 osg-flock-1.3-1.osg35.el8 vault-1.7.3-1.osg35.el8 xrootd-4.12.6-1.1.osg35.el8 xrootd-lcmaps-1.7.8-2.osg35.el8 xrootd-multiuser-0.4.4-2.osg35.el8 xrootd-scitokens-1.2.2-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: frontier-squid gfal2 gfal2-all gfal2-debuginfo gfal2-devel gfal2-doc gfal2-plugin-dcap gfal2-plugin-file gfal2-plugin-gridftp gfal2-plugin-http gfal2-plugin-lfc gfal2-plugin-mock gfal2-plugin-rfio gfal2-plugin-sftp gfal2-plugin-srm gfal2-plugin-xrootd htvault-config vault If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 frontier-squid-4.15-2.1.osg35.el7 gfal2-2.18.1-1.2.osg35.el7 gfal2-all-2.18.1-1.2.osg35.el7 gfal2-debuginfo-2.18.1-1.2.osg35.el7 gfal2-devel-2.18.1-1.2.osg35.el7 gfal2-doc-2.18.1-1.2.osg35.el7 gfal2-plugin-dcap-2.18.1-1.2.osg35.el7 gfal2-plugin-file-2.18.1-1.2.osg35.el7 gfal2-plugin-gridftp-2.18.1-1.2.osg35.el7 gfal2-plugin-http-2.18.1-1.2.osg35.el7 gfal2-plugin-lfc-2.18.1-1.2.osg35.el7 gfal2-plugin-mock-2.18.1-1.2.osg35.el7 gfal2-plugin-rfio-2.18.1-1.2.osg35.el7 gfal2-plugin-sftp-2.18.1-1.2.osg35.el7 gfal2-plugin-srm-2.18.1-1.2.osg35.el7 gfal2-plugin-xrootd-2.18.1-1.2.osg35.el7 htvault-config-1.2-1.osg35.el7 vault-1.7.3-1.osg35.el7 Enterprise Linux 8 \u00b6 frontier-squid-4.15-2.1.osg35.el8 gfal2-2.18.1-1.2.osg35.el8 gfal2-all-2.18.1-1.2.osg35.el8 gfal2-debuginfo-2.18.1-1.2.osg35.el8 gfal2-debugsource-2.18.1-1.2.osg35.el8 gfal2-devel-2.18.1-1.2.osg35.el8 gfal2-doc-2.18.1-1.2.osg35.el8 gfal2-plugin-dcap-2.18.1-1.2.osg35.el8 gfal2-plugin-dcap-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-file-2.18.1-1.2.osg35.el8 gfal2-plugin-file-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-gridftp-2.18.1-1.2.osg35.el8 gfal2-plugin-gridftp-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-http-2.18.1-1.2.osg35.el8 gfal2-plugin-http-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-mock-2.18.1-1.2.osg35.el8 gfal2-plugin-mock-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-sftp-2.18.1-1.2.osg35.el8 gfal2-plugin-sftp-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-srm-2.18.1-1.2.osg35.el8 gfal2-plugin-srm-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-xrootd-2.18.1-1.2.osg35.el8 gfal2-plugin-xrootd-debuginfo-2.18.1-1.2.osg35.el8 htvault-config-1.2-1.osg35.el8 osg-flock-1.3-1.osg35.el8 python2-xrootd-4.12.6-1.1.osg35.el8 python2-xrootd-debuginfo-4.12.6-1.1.osg35.el8 python3-xrootd-4.12.6-1.1.osg35.el8 python3-xrootd-debuginfo-4.12.6-1.1.osg35.el8 vault-1.7.3-1.osg35.el8 xrootd-4.12.6-1.1.osg35.el8 xrootd-client-4.12.6-1.1.osg35.el8 xrootd-client-debuginfo-4.12.6-1.1.osg35.el8 xrootd-client-devel-4.12.6-1.1.osg35.el8 xrootd-client-devel-debuginfo-4.12.6-1.1.osg35.el8 xrootd-client-libs-4.12.6-1.1.osg35.el8 xrootd-client-libs-debuginfo-4.12.6-1.1.osg35.el8 xrootd-debuginfo-4.12.6-1.1.osg35.el8 xrootd-debugsource-4.12.6-1.1.osg35.el8 xrootd-devel-4.12.6-1.1.osg35.el8 xrootd-doc-4.12.6-1.1.osg35.el8 xrootd-fuse-4.12.6-1.1.osg35.el8 xrootd-fuse-debuginfo-4.12.6-1.1.osg35.el8 xrootd-lcmaps-1.7.8-2.osg35.el8 xrootd-lcmaps-debuginfo-1.7.8-2.osg35.el8 xrootd-lcmaps-debugsource-1.7.8-2.osg35.el8 xrootd-libs-4.12.6-1.1.osg35.el8 xrootd-libs-debuginfo-4.12.6-1.1.osg35.el8 xrootd-multiuser-0.4.4-2.osg35.el8 xrootd-multiuser-debuginfo-0.4.4-2.osg35.el8 xrootd-multiuser-debugsource-0.4.4-2.osg35.el8 xrootd-private-devel-4.12.6-1.1.osg35.el8 xrootd-scitokens-1.2.2-1.osg35.el8 xrootd-scitokens-debuginfo-1.2.2-1.osg35.el8 xrootd-scitokens-debugsource-1.2.2-1.osg35.el8 xrootd-selinux-4.12.6-1.1.osg35.el8 xrootd-server-4.12.6-1.1.osg35.el8 xrootd-server-debuginfo-4.12.6-1.1.osg35.el8 xrootd-server-devel-4.12.6-1.1.osg35.el8 xrootd-server-libs-4.12.6-1.1.osg35.el8 xrootd-server-libs-debuginfo-4.12.6-1.1.osg35.el8 xrootd-voms-4.12.6-1.1.osg35.el8 xrootd-voms-debuginfo-4.12.6-1.1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 xrootd-multiuser-1.1.0-1.osg35up.el7 Enterprise Linux 8 \u00b6 xrootd-multiuser-1.1.0-1.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: xrootd-multiuser xrootd-multiuser-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 xrootd-multiuser-1.1.0-1.osg35up.el7 xrootd-multiuser-debuginfo-1.1.0-1.osg35up.el7 Enterprise Linux 8 \u00b6 xrootd-multiuser-1.1.0-1.osg35up.el8 xrootd-multiuser-debuginfo-1.1.0-1.osg35up.el8 xrootd-multiuser-debugsource-1.1.0-1.osg35up.el8","title":"Release 3 5 40"},{"location":"release/3.5/release-3-5-40/#osg-software-release-3540","text":"Release Date: 2021-07-01 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.40"},{"location":"release/3.5/release-3-5-40/#summary-of-changes","text":"This release contains: Frontier Squid 4.15-2.1 : Fix log rotation when not compressing Vault 1.7.3 : Bug fix release htvault-config 1.2: Updated to match vault 1.7.3 Updates on Enterprise Linux 8 XRootD 4.12.6 and plugins osg-flock 1.3 Upcoming xrootd-multiuser 1.1.0: Administrator can specify the default umask again These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-40/#containers","text":"The OSG Docker images have been updated to contain the new software.","title":"Containers"},{"location":"release/3.5/release-3-5-40/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-40/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-40/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-40/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-40/#enterprise-linux-7","text":"frontier-squid-4.15-2.1.osg35.el7 gfal2-2.18.1-1.2.osg35.el7 htvault-config-1.2-1.osg35.el7 vault-1.7.3-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-40/#enterprise-linux-8","text":"frontier-squid-4.15-2.1.osg35.el8 gfal2-2.18.1-1.2.osg35.el8 htvault-config-1.2-1.osg35.el8 osg-flock-1.3-1.osg35.el8 vault-1.7.3-1.osg35.el8 xrootd-4.12.6-1.1.osg35.el8 xrootd-lcmaps-1.7.8-2.osg35.el8 xrootd-multiuser-0.4.4-2.osg35.el8 xrootd-scitokens-1.2.2-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-40/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: frontier-squid gfal2 gfal2-all gfal2-debuginfo gfal2-devel gfal2-doc gfal2-plugin-dcap gfal2-plugin-file gfal2-plugin-gridftp gfal2-plugin-http gfal2-plugin-lfc gfal2-plugin-mock gfal2-plugin-rfio gfal2-plugin-sftp gfal2-plugin-srm gfal2-plugin-xrootd htvault-config vault If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-40/#enterprise-linux-7_1","text":"frontier-squid-4.15-2.1.osg35.el7 gfal2-2.18.1-1.2.osg35.el7 gfal2-all-2.18.1-1.2.osg35.el7 gfal2-debuginfo-2.18.1-1.2.osg35.el7 gfal2-devel-2.18.1-1.2.osg35.el7 gfal2-doc-2.18.1-1.2.osg35.el7 gfal2-plugin-dcap-2.18.1-1.2.osg35.el7 gfal2-plugin-file-2.18.1-1.2.osg35.el7 gfal2-plugin-gridftp-2.18.1-1.2.osg35.el7 gfal2-plugin-http-2.18.1-1.2.osg35.el7 gfal2-plugin-lfc-2.18.1-1.2.osg35.el7 gfal2-plugin-mock-2.18.1-1.2.osg35.el7 gfal2-plugin-rfio-2.18.1-1.2.osg35.el7 gfal2-plugin-sftp-2.18.1-1.2.osg35.el7 gfal2-plugin-srm-2.18.1-1.2.osg35.el7 gfal2-plugin-xrootd-2.18.1-1.2.osg35.el7 htvault-config-1.2-1.osg35.el7 vault-1.7.3-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-40/#enterprise-linux-8_1","text":"frontier-squid-4.15-2.1.osg35.el8 gfal2-2.18.1-1.2.osg35.el8 gfal2-all-2.18.1-1.2.osg35.el8 gfal2-debuginfo-2.18.1-1.2.osg35.el8 gfal2-debugsource-2.18.1-1.2.osg35.el8 gfal2-devel-2.18.1-1.2.osg35.el8 gfal2-doc-2.18.1-1.2.osg35.el8 gfal2-plugin-dcap-2.18.1-1.2.osg35.el8 gfal2-plugin-dcap-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-file-2.18.1-1.2.osg35.el8 gfal2-plugin-file-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-gridftp-2.18.1-1.2.osg35.el8 gfal2-plugin-gridftp-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-http-2.18.1-1.2.osg35.el8 gfal2-plugin-http-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-mock-2.18.1-1.2.osg35.el8 gfal2-plugin-mock-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-sftp-2.18.1-1.2.osg35.el8 gfal2-plugin-sftp-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-srm-2.18.1-1.2.osg35.el8 gfal2-plugin-srm-debuginfo-2.18.1-1.2.osg35.el8 gfal2-plugin-xrootd-2.18.1-1.2.osg35.el8 gfal2-plugin-xrootd-debuginfo-2.18.1-1.2.osg35.el8 htvault-config-1.2-1.osg35.el8 osg-flock-1.3-1.osg35.el8 python2-xrootd-4.12.6-1.1.osg35.el8 python2-xrootd-debuginfo-4.12.6-1.1.osg35.el8 python3-xrootd-4.12.6-1.1.osg35.el8 python3-xrootd-debuginfo-4.12.6-1.1.osg35.el8 vault-1.7.3-1.osg35.el8 xrootd-4.12.6-1.1.osg35.el8 xrootd-client-4.12.6-1.1.osg35.el8 xrootd-client-debuginfo-4.12.6-1.1.osg35.el8 xrootd-client-devel-4.12.6-1.1.osg35.el8 xrootd-client-devel-debuginfo-4.12.6-1.1.osg35.el8 xrootd-client-libs-4.12.6-1.1.osg35.el8 xrootd-client-libs-debuginfo-4.12.6-1.1.osg35.el8 xrootd-debuginfo-4.12.6-1.1.osg35.el8 xrootd-debugsource-4.12.6-1.1.osg35.el8 xrootd-devel-4.12.6-1.1.osg35.el8 xrootd-doc-4.12.6-1.1.osg35.el8 xrootd-fuse-4.12.6-1.1.osg35.el8 xrootd-fuse-debuginfo-4.12.6-1.1.osg35.el8 xrootd-lcmaps-1.7.8-2.osg35.el8 xrootd-lcmaps-debuginfo-1.7.8-2.osg35.el8 xrootd-lcmaps-debugsource-1.7.8-2.osg35.el8 xrootd-libs-4.12.6-1.1.osg35.el8 xrootd-libs-debuginfo-4.12.6-1.1.osg35.el8 xrootd-multiuser-0.4.4-2.osg35.el8 xrootd-multiuser-debuginfo-0.4.4-2.osg35.el8 xrootd-multiuser-debugsource-0.4.4-2.osg35.el8 xrootd-private-devel-4.12.6-1.1.osg35.el8 xrootd-scitokens-1.2.2-1.osg35.el8 xrootd-scitokens-debuginfo-1.2.2-1.osg35.el8 xrootd-scitokens-debugsource-1.2.2-1.osg35.el8 xrootd-selinux-4.12.6-1.1.osg35.el8 xrootd-server-4.12.6-1.1.osg35.el8 xrootd-server-debuginfo-4.12.6-1.1.osg35.el8 xrootd-server-devel-4.12.6-1.1.osg35.el8 xrootd-server-libs-4.12.6-1.1.osg35.el8 xrootd-server-libs-debuginfo-4.12.6-1.1.osg35.el8 xrootd-voms-4.12.6-1.1.osg35.el8 xrootd-voms-debuginfo-4.12.6-1.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-40/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-40/#enterprise-linux-7_2","text":"xrootd-multiuser-1.1.0-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-40/#enterprise-linux-8_2","text":"xrootd-multiuser-1.1.0-1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-40/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: xrootd-multiuser xrootd-multiuser-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-40/#enterprise-linux-7_3","text":"xrootd-multiuser-1.1.0-1.osg35up.el7 xrootd-multiuser-debuginfo-1.1.0-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-40/#enterprise-linux-8_3","text":"xrootd-multiuser-1.1.0-1.osg35up.el8 xrootd-multiuser-debuginfo-1.1.0-1.osg35up.el8 xrootd-multiuser-debugsource-1.1.0-1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-41/","text":"OSG Software Release 3.5.41 \u00b6 Release Date: 2021-07-22 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Upcoming HTCondor 9.0.2-1.1 : Bug fix release HTCondor can be setup to use only FIPS 140-2 approved security functions If the Singularity test fails, the job returns to the idle state Can divide GPU memory, when making multiple GPU entries for a single GPU Startd and Schedd cron job maximum line length increased to 64k bytes Added first class submit keywords for SciTokens Fixed MUNGE authentication blahp 2.1.0 : Bug fix release Fix bug where GPU request was not passed onto the batch script Fix issue where proxy symlinks were not cleaned up by not creating them Fix bug where output files are overwritten if no transfer output remap Added support for passing in extra submit arguments from the job ad XRootD 5.3.0 : Bug fix release Fixes related to HTTP-TPC, SSS, and xrootdfs Known Issue HTCondor-CE 5.3.0: caches or proxy servers may crash under heavy load Add \"xrootd.async off\" to your configuration file to avoid the issue These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the new software. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 blahp-2.1.0-1.osg35up.el7 condor-9.0.2-1.osg35up.el7 xrootd-5.3.0-1.1.osg35up.el7 Enterprise Linux 8 \u00b6 blahp-2.1.0-1.osg35up.el8 condor-9.0.2-1.osg35up.el8 xrootd-5.3.0-1.1.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python2-xrootd python36-xrootd python3-condor xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-scitokens xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 blahp-2.1.0-1.osg35up.el7 blahp-debuginfo-2.1.0-1.osg35up.el7 condor-9.0.2-1.osg35up.el7 condor-all-9.0.2-1.osg35up.el7 condor-annex-ec2-9.0.2-1.osg35up.el7 condor-bosco-9.0.2-1.osg35up.el7 condor-classads-9.0.2-1.osg35up.el7 condor-classads-devel-9.0.2-1.osg35up.el7 condor-credmon-oauth-9.0.2-1.osg35up.el7 condor-credmon-vault-9.0.2-1.osg35up.el7 condor-debuginfo-9.0.2-1.osg35up.el7 condor-devel-9.0.2-1.osg35up.el7 condor-kbdd-9.0.2-1.osg35up.el7 condor-procd-9.0.2-1.osg35up.el7 condor-test-9.0.2-1.osg35up.el7 condor-vm-gahp-9.0.2-1.osg35up.el7 minicondor-9.0.2-1.osg35up.el7 python2-condor-9.0.2-1.osg35up.el7 python2-xrootd-5.3.0-1.1.osg35up.el7 python36-xrootd-5.3.0-1.1.osg35up.el7 python3-condor-9.0.2-1.osg35up.el7 xrootd-5.3.0-1.1.osg35up.el7 xrootd-client-5.3.0-1.1.osg35up.el7 xrootd-client-compat-5.3.0-1.1.osg35up.el7 xrootd-client-devel-5.3.0-1.1.osg35up.el7 xrootd-client-libs-5.3.0-1.1.osg35up.el7 xrootd-debuginfo-5.3.0-1.1.osg35up.el7 xrootd-devel-5.3.0-1.1.osg35up.el7 xrootd-doc-5.3.0-1.1.osg35up.el7 xrootd-fuse-5.3.0-1.1.osg35up.el7 xrootd-libs-5.3.0-1.1.osg35up.el7 xrootd-private-devel-5.3.0-1.1.osg35up.el7 xrootd-scitokens-5.3.0-1.1.osg35up.el7 xrootd-selinux-5.3.0-1.1.osg35up.el7 xrootd-server-5.3.0-1.1.osg35up.el7 xrootd-server-compat-5.3.0-1.1.osg35up.el7 xrootd-server-devel-5.3.0-1.1.osg35up.el7 xrootd-server-libs-5.3.0-1.1.osg35up.el7 xrootd-voms-5.3.0-1.1.osg35up.el7 Enterprise Linux 8 \u00b6 blahp-2.1.0-1.osg35up.el8 blahp-debuginfo-2.1.0-1.osg35up.el8 blahp-debugsource-2.1.0-1.osg35up.el8 condor-9.0.2-1.osg35up.el8 condor-all-9.0.2-1.osg35up.el8 condor-annex-ec2-9.0.2-1.osg35up.el8 condor-bosco-9.0.2-1.osg35up.el8 condor-bosco-debuginfo-9.0.2-1.osg35up.el8 condor-classads-9.0.2-1.osg35up.el8 condor-classads-debuginfo-9.0.2-1.osg35up.el8 condor-classads-devel-9.0.2-1.osg35up.el8 condor-classads-devel-debuginfo-9.0.2-1.osg35up.el8 condor-credmon-vault-9.0.2-1.osg35up.el8 condor-debuginfo-9.0.2-1.osg35up.el8 condor-debugsource-9.0.2-1.osg35up.el8 condor-devel-9.0.2-1.osg35up.el8 condor-kbdd-9.0.2-1.osg35up.el8 condor-kbdd-debuginfo-9.0.2-1.osg35up.el8 condor-procd-9.0.2-1.osg35up.el8 condor-procd-debuginfo-9.0.2-1.osg35up.el8 condor-test-9.0.2-1.osg35up.el8 condor-test-debuginfo-9.0.2-1.osg35up.el8 condor-vm-gahp-9.0.2-1.osg35up.el8 condor-vm-gahp-debuginfo-9.0.2-1.osg35up.el8 minicondor-9.0.2-1.osg35up.el8 python3-condor-9.0.2-1.osg35up.el8 python3-condor-debuginfo-9.0.2-1.osg35up.el8 python3-xrootd-5.3.0-1.1.osg35up.el8 python3-xrootd-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-5.3.0-1.1.osg35up.el8 xrootd-client-5.3.0-1.1.osg35up.el8 xrootd-client-compat-5.3.0-1.1.osg35up.el8 xrootd-client-compat-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-client-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-client-devel-5.3.0-1.1.osg35up.el8 xrootd-client-devel-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-client-libs-5.3.0-1.1.osg35up.el8 xrootd-client-libs-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-debugsource-5.3.0-1.1.osg35up.el8 xrootd-devel-5.3.0-1.1.osg35up.el8 xrootd-doc-5.3.0-1.1.osg35up.el8 xrootd-fuse-5.3.0-1.1.osg35up.el8 xrootd-fuse-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-libs-5.3.0-1.1.osg35up.el8 xrootd-libs-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-private-devel-5.3.0-1.1.osg35up.el8 xrootd-scitokens-5.3.0-1.1.osg35up.el8 xrootd-scitokens-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-selinux-5.3.0-1.1.osg35up.el8 xrootd-server-5.3.0-1.1.osg35up.el8 xrootd-server-compat-5.3.0-1.1.osg35up.el8 xrootd-server-compat-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-server-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-server-devel-5.3.0-1.1.osg35up.el8 xrootd-server-libs-5.3.0-1.1.osg35up.el8 xrootd-server-libs-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-voms-5.3.0-1.1.osg35up.el8 xrootd-voms-debuginfo-5.3.0-1.1.osg35up.el8","title":"Release 3 5 41"},{"location":"release/3.5/release-3-5-41/#osg-software-release-3541","text":"Release Date: 2021-07-22 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.41"},{"location":"release/3.5/release-3-5-41/#summary-of-changes","text":"This release contains: Upcoming HTCondor 9.0.2-1.1 : Bug fix release HTCondor can be setup to use only FIPS 140-2 approved security functions If the Singularity test fails, the job returns to the idle state Can divide GPU memory, when making multiple GPU entries for a single GPU Startd and Schedd cron job maximum line length increased to 64k bytes Added first class submit keywords for SciTokens Fixed MUNGE authentication blahp 2.1.0 : Bug fix release Fix bug where GPU request was not passed onto the batch script Fix issue where proxy symlinks were not cleaned up by not creating them Fix bug where output files are overwritten if no transfer output remap Added support for passing in extra submit arguments from the job ad XRootD 5.3.0 : Bug fix release Fixes related to HTTP-TPC, SSS, and xrootdfs Known Issue HTCondor-CE 5.3.0: caches or proxy servers may crash under heavy load Add \"xrootd.async off\" to your configuration file to avoid the issue These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-41/#containers","text":"The OSG Docker images have been updated to contain the new software.","title":"Containers"},{"location":"release/3.5/release-3-5-41/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-41/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-41/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-41/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-41/#enterprise-linux-7","text":"blahp-2.1.0-1.osg35up.el7 condor-9.0.2-1.osg35up.el7 xrootd-5.3.0-1.1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-41/#enterprise-linux-8","text":"blahp-2.1.0-1.osg35up.el8 condor-9.0.2-1.osg35up.el8 xrootd-5.3.0-1.1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-41/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python2-xrootd python36-xrootd python3-condor xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-scitokens xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-41/#enterprise-linux-7_1","text":"blahp-2.1.0-1.osg35up.el7 blahp-debuginfo-2.1.0-1.osg35up.el7 condor-9.0.2-1.osg35up.el7 condor-all-9.0.2-1.osg35up.el7 condor-annex-ec2-9.0.2-1.osg35up.el7 condor-bosco-9.0.2-1.osg35up.el7 condor-classads-9.0.2-1.osg35up.el7 condor-classads-devel-9.0.2-1.osg35up.el7 condor-credmon-oauth-9.0.2-1.osg35up.el7 condor-credmon-vault-9.0.2-1.osg35up.el7 condor-debuginfo-9.0.2-1.osg35up.el7 condor-devel-9.0.2-1.osg35up.el7 condor-kbdd-9.0.2-1.osg35up.el7 condor-procd-9.0.2-1.osg35up.el7 condor-test-9.0.2-1.osg35up.el7 condor-vm-gahp-9.0.2-1.osg35up.el7 minicondor-9.0.2-1.osg35up.el7 python2-condor-9.0.2-1.osg35up.el7 python2-xrootd-5.3.0-1.1.osg35up.el7 python36-xrootd-5.3.0-1.1.osg35up.el7 python3-condor-9.0.2-1.osg35up.el7 xrootd-5.3.0-1.1.osg35up.el7 xrootd-client-5.3.0-1.1.osg35up.el7 xrootd-client-compat-5.3.0-1.1.osg35up.el7 xrootd-client-devel-5.3.0-1.1.osg35up.el7 xrootd-client-libs-5.3.0-1.1.osg35up.el7 xrootd-debuginfo-5.3.0-1.1.osg35up.el7 xrootd-devel-5.3.0-1.1.osg35up.el7 xrootd-doc-5.3.0-1.1.osg35up.el7 xrootd-fuse-5.3.0-1.1.osg35up.el7 xrootd-libs-5.3.0-1.1.osg35up.el7 xrootd-private-devel-5.3.0-1.1.osg35up.el7 xrootd-scitokens-5.3.0-1.1.osg35up.el7 xrootd-selinux-5.3.0-1.1.osg35up.el7 xrootd-server-5.3.0-1.1.osg35up.el7 xrootd-server-compat-5.3.0-1.1.osg35up.el7 xrootd-server-devel-5.3.0-1.1.osg35up.el7 xrootd-server-libs-5.3.0-1.1.osg35up.el7 xrootd-voms-5.3.0-1.1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-41/#enterprise-linux-8_1","text":"blahp-2.1.0-1.osg35up.el8 blahp-debuginfo-2.1.0-1.osg35up.el8 blahp-debugsource-2.1.0-1.osg35up.el8 condor-9.0.2-1.osg35up.el8 condor-all-9.0.2-1.osg35up.el8 condor-annex-ec2-9.0.2-1.osg35up.el8 condor-bosco-9.0.2-1.osg35up.el8 condor-bosco-debuginfo-9.0.2-1.osg35up.el8 condor-classads-9.0.2-1.osg35up.el8 condor-classads-debuginfo-9.0.2-1.osg35up.el8 condor-classads-devel-9.0.2-1.osg35up.el8 condor-classads-devel-debuginfo-9.0.2-1.osg35up.el8 condor-credmon-vault-9.0.2-1.osg35up.el8 condor-debuginfo-9.0.2-1.osg35up.el8 condor-debugsource-9.0.2-1.osg35up.el8 condor-devel-9.0.2-1.osg35up.el8 condor-kbdd-9.0.2-1.osg35up.el8 condor-kbdd-debuginfo-9.0.2-1.osg35up.el8 condor-procd-9.0.2-1.osg35up.el8 condor-procd-debuginfo-9.0.2-1.osg35up.el8 condor-test-9.0.2-1.osg35up.el8 condor-test-debuginfo-9.0.2-1.osg35up.el8 condor-vm-gahp-9.0.2-1.osg35up.el8 condor-vm-gahp-debuginfo-9.0.2-1.osg35up.el8 minicondor-9.0.2-1.osg35up.el8 python3-condor-9.0.2-1.osg35up.el8 python3-condor-debuginfo-9.0.2-1.osg35up.el8 python3-xrootd-5.3.0-1.1.osg35up.el8 python3-xrootd-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-5.3.0-1.1.osg35up.el8 xrootd-client-5.3.0-1.1.osg35up.el8 xrootd-client-compat-5.3.0-1.1.osg35up.el8 xrootd-client-compat-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-client-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-client-devel-5.3.0-1.1.osg35up.el8 xrootd-client-devel-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-client-libs-5.3.0-1.1.osg35up.el8 xrootd-client-libs-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-debugsource-5.3.0-1.1.osg35up.el8 xrootd-devel-5.3.0-1.1.osg35up.el8 xrootd-doc-5.3.0-1.1.osg35up.el8 xrootd-fuse-5.3.0-1.1.osg35up.el8 xrootd-fuse-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-libs-5.3.0-1.1.osg35up.el8 xrootd-libs-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-private-devel-5.3.0-1.1.osg35up.el8 xrootd-scitokens-5.3.0-1.1.osg35up.el8 xrootd-scitokens-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-selinux-5.3.0-1.1.osg35up.el8 xrootd-server-5.3.0-1.1.osg35up.el8 xrootd-server-compat-5.3.0-1.1.osg35up.el8 xrootd-server-compat-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-server-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-server-devel-5.3.0-1.1.osg35up.el8 xrootd-server-libs-5.3.0-1.1.osg35up.el8 xrootd-server-libs-debuginfo-5.3.0-1.1.osg35up.el8 xrootd-voms-5.3.0-1.1.osg35up.el8 xrootd-voms-debuginfo-5.3.0-1.1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-42/","text":"OSG Software Release 3.5.42 \u00b6 Release Date: 2021-07-27 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor 8.8.14 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 Upcoming HTCondor 9.0.3 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 HTCONDOR-2021-0004 These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the new software. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 condor-8.8.14-1.1.osg35.el7 Enterprise Linux 8 \u00b6 condor-8.8.14-1.1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-8.8.14-1.1.osg35.el7 condor-all-8.8.14-1.1.osg35.el7 condor-annex-ec2-8.8.14-1.1.osg35.el7 condor-bosco-8.8.14-1.1.osg35.el7 condor-classads-8.8.14-1.1.osg35.el7 condor-classads-devel-8.8.14-1.1.osg35.el7 condor-debuginfo-8.8.14-1.1.osg35.el7 condor-kbdd-8.8.14-1.1.osg35.el7 condor-procd-8.8.14-1.1.osg35.el7 condor-test-8.8.14-1.1.osg35.el7 condor-vm-gahp-8.8.14-1.1.osg35.el7 minicondor-8.8.14-1.1.osg35.el7 python2-condor-8.8.14-1.1.osg35.el7 python3-condor-8.8.14-1.1.osg35.el7 Enterprise Linux 8 \u00b6 condor-8.8.14-1.1.osg35.el8 condor-all-8.8.14-1.1.osg35.el8 condor-annex-ec2-8.8.14-1.1.osg35.el8 condor-bosco-8.8.14-1.1.osg35.el8 condor-bosco-debuginfo-8.8.14-1.1.osg35.el8 condor-classads-8.8.14-1.1.osg35.el8 condor-classads-debuginfo-8.8.14-1.1.osg35.el8 condor-classads-devel-8.8.14-1.1.osg35.el8 condor-classads-devel-debuginfo-8.8.14-1.1.osg35.el8 condor-debuginfo-8.8.14-1.1.osg35.el8 condor-debugsource-8.8.14-1.1.osg35.el8 condor-kbdd-8.8.14-1.1.osg35.el8 condor-kbdd-debuginfo-8.8.14-1.1.osg35.el8 condor-procd-8.8.14-1.1.osg35.el8 condor-procd-debuginfo-8.8.14-1.1.osg35.el8 condor-test-8.8.14-1.1.osg35.el8 condor-test-debuginfo-8.8.14-1.1.osg35.el8 condor-vm-gahp-8.8.14-1.1.osg35.el8 condor-vm-gahp-debuginfo-8.8.14-1.1.osg35.el8 minicondor-8.8.14-1.1.osg35.el8 python3-condor-8.8.14-1.1.osg35.el8 python3-condor-debuginfo-8.8.14-1.1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 condor-9.0.3-1.osg35up.el7 Enterprise Linux 8 \u00b6 condor-9.0.3-1.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-9.0.3-1.osg35up.el7 condor-all-9.0.3-1.osg35up.el7 condor-annex-ec2-9.0.3-1.osg35up.el7 condor-bosco-9.0.3-1.osg35up.el7 condor-classads-9.0.3-1.osg35up.el7 condor-classads-devel-9.0.3-1.osg35up.el7 condor-credmon-oauth-9.0.3-1.osg35up.el7 condor-credmon-vault-9.0.3-1.osg35up.el7 condor-debuginfo-9.0.3-1.osg35up.el7 condor-devel-9.0.3-1.osg35up.el7 condor-kbdd-9.0.3-1.osg35up.el7 condor-procd-9.0.3-1.osg35up.el7 condor-test-9.0.3-1.osg35up.el7 condor-vm-gahp-9.0.3-1.osg35up.el7 minicondor-9.0.3-1.osg35up.el7 python2-condor-9.0.3-1.osg35up.el7 python3-condor-9.0.3-1.osg35up.el7 Enterprise Linux 8 \u00b6 condor-9.0.3-1.osg35up.el8 condor-all-9.0.3-1.osg35up.el8 condor-annex-ec2-9.0.3-1.osg35up.el8 condor-bosco-9.0.3-1.osg35up.el8 condor-bosco-debuginfo-9.0.3-1.osg35up.el8 condor-classads-9.0.3-1.osg35up.el8 condor-classads-debuginfo-9.0.3-1.osg35up.el8 condor-classads-devel-9.0.3-1.osg35up.el8 condor-classads-devel-debuginfo-9.0.3-1.osg35up.el8 condor-credmon-vault-9.0.3-1.osg35up.el8 condor-debuginfo-9.0.3-1.osg35up.el8 condor-debugsource-9.0.3-1.osg35up.el8 condor-devel-9.0.3-1.osg35up.el8 condor-kbdd-9.0.3-1.osg35up.el8 condor-kbdd-debuginfo-9.0.3-1.osg35up.el8 condor-procd-9.0.3-1.osg35up.el8 condor-procd-debuginfo-9.0.3-1.osg35up.el8 condor-test-9.0.3-1.osg35up.el8 condor-test-debuginfo-9.0.3-1.osg35up.el8 condor-vm-gahp-9.0.3-1.osg35up.el8 condor-vm-gahp-debuginfo-9.0.3-1.osg35up.el8 minicondor-9.0.3-1.osg35up.el8 python3-condor-9.0.3-1.osg35up.el8 python3-condor-debuginfo-9.0.3-1.osg35up.el8","title":"Release 3 5 42"},{"location":"release/3.5/release-3-5-42/#osg-software-release-3542","text":"Release Date: 2021-07-27 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.42"},{"location":"release/3.5/release-3-5-42/#summary-of-changes","text":"This release contains: HTCondor 8.8.14 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 Upcoming HTCondor 9.0.3 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 HTCONDOR-2021-0004 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-42/#containers","text":"The OSG Docker images have been updated to contain the new software.","title":"Containers"},{"location":"release/3.5/release-3-5-42/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-42/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-42/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-42/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-42/#enterprise-linux-7","text":"condor-8.8.14-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-42/#enterprise-linux-8","text":"condor-8.8.14-1.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-42/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-42/#enterprise-linux-7_1","text":"condor-8.8.14-1.1.osg35.el7 condor-all-8.8.14-1.1.osg35.el7 condor-annex-ec2-8.8.14-1.1.osg35.el7 condor-bosco-8.8.14-1.1.osg35.el7 condor-classads-8.8.14-1.1.osg35.el7 condor-classads-devel-8.8.14-1.1.osg35.el7 condor-debuginfo-8.8.14-1.1.osg35.el7 condor-kbdd-8.8.14-1.1.osg35.el7 condor-procd-8.8.14-1.1.osg35.el7 condor-test-8.8.14-1.1.osg35.el7 condor-vm-gahp-8.8.14-1.1.osg35.el7 minicondor-8.8.14-1.1.osg35.el7 python2-condor-8.8.14-1.1.osg35.el7 python3-condor-8.8.14-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-42/#enterprise-linux-8_1","text":"condor-8.8.14-1.1.osg35.el8 condor-all-8.8.14-1.1.osg35.el8 condor-annex-ec2-8.8.14-1.1.osg35.el8 condor-bosco-8.8.14-1.1.osg35.el8 condor-bosco-debuginfo-8.8.14-1.1.osg35.el8 condor-classads-8.8.14-1.1.osg35.el8 condor-classads-debuginfo-8.8.14-1.1.osg35.el8 condor-classads-devel-8.8.14-1.1.osg35.el8 condor-classads-devel-debuginfo-8.8.14-1.1.osg35.el8 condor-debuginfo-8.8.14-1.1.osg35.el8 condor-debugsource-8.8.14-1.1.osg35.el8 condor-kbdd-8.8.14-1.1.osg35.el8 condor-kbdd-debuginfo-8.8.14-1.1.osg35.el8 condor-procd-8.8.14-1.1.osg35.el8 condor-procd-debuginfo-8.8.14-1.1.osg35.el8 condor-test-8.8.14-1.1.osg35.el8 condor-test-debuginfo-8.8.14-1.1.osg35.el8 condor-vm-gahp-8.8.14-1.1.osg35.el8 condor-vm-gahp-debuginfo-8.8.14-1.1.osg35.el8 minicondor-8.8.14-1.1.osg35.el8 python3-condor-8.8.14-1.1.osg35.el8 python3-condor-debuginfo-8.8.14-1.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-42/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-42/#enterprise-linux-7_2","text":"condor-9.0.3-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-42/#enterprise-linux-8_2","text":"condor-9.0.3-1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-42/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-42/#enterprise-linux-7_3","text":"condor-9.0.3-1.osg35up.el7 condor-all-9.0.3-1.osg35up.el7 condor-annex-ec2-9.0.3-1.osg35up.el7 condor-bosco-9.0.3-1.osg35up.el7 condor-classads-9.0.3-1.osg35up.el7 condor-classads-devel-9.0.3-1.osg35up.el7 condor-credmon-oauth-9.0.3-1.osg35up.el7 condor-credmon-vault-9.0.3-1.osg35up.el7 condor-debuginfo-9.0.3-1.osg35up.el7 condor-devel-9.0.3-1.osg35up.el7 condor-kbdd-9.0.3-1.osg35up.el7 condor-procd-9.0.3-1.osg35up.el7 condor-test-9.0.3-1.osg35up.el7 condor-vm-gahp-9.0.3-1.osg35up.el7 minicondor-9.0.3-1.osg35up.el7 python2-condor-9.0.3-1.osg35up.el7 python3-condor-9.0.3-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-42/#enterprise-linux-8_3","text":"condor-9.0.3-1.osg35up.el8 condor-all-9.0.3-1.osg35up.el8 condor-annex-ec2-9.0.3-1.osg35up.el8 condor-bosco-9.0.3-1.osg35up.el8 condor-bosco-debuginfo-9.0.3-1.osg35up.el8 condor-classads-9.0.3-1.osg35up.el8 condor-classads-debuginfo-9.0.3-1.osg35up.el8 condor-classads-devel-9.0.3-1.osg35up.el8 condor-classads-devel-debuginfo-9.0.3-1.osg35up.el8 condor-credmon-vault-9.0.3-1.osg35up.el8 condor-debuginfo-9.0.3-1.osg35up.el8 condor-debugsource-9.0.3-1.osg35up.el8 condor-devel-9.0.3-1.osg35up.el8 condor-kbdd-9.0.3-1.osg35up.el8 condor-kbdd-debuginfo-9.0.3-1.osg35up.el8 condor-procd-9.0.3-1.osg35up.el8 condor-procd-debuginfo-9.0.3-1.osg35up.el8 condor-test-9.0.3-1.osg35up.el8 condor-test-debuginfo-9.0.3-1.osg35up.el8 condor-vm-gahp-9.0.3-1.osg35up.el8 condor-vm-gahp-debuginfo-9.0.3-1.osg35up.el8 minicondor-9.0.3-1.osg35up.el8 python3-condor-9.0.3-1.osg35up.el8 python3-condor-debuginfo-9.0.3-1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-43/","text":"OSG Software Release 3.5.43 \u00b6 Release Date: 2021-07-30 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor 8.8.15 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 Upcoming HTCondor 9.0.4 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 HTCONDOR-2021-0004 These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the new software. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 condor-8.8.15-1.1.osg35.el7 Enterprise Linux 8 \u00b6 condor-8.8.15-1.1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-8.8.15-1.1.osg35.el7 condor-all-8.8.15-1.1.osg35.el7 condor-annex-ec2-8.8.15-1.1.osg35.el7 condor-bosco-8.8.15-1.1.osg35.el7 condor-classads-8.8.15-1.1.osg35.el7 condor-classads-devel-8.8.15-1.1.osg35.el7 condor-debuginfo-8.8.15-1.1.osg35.el7 condor-kbdd-8.8.15-1.1.osg35.el7 condor-procd-8.8.15-1.1.osg35.el7 condor-test-8.8.15-1.1.osg35.el7 condor-vm-gahp-8.8.15-1.1.osg35.el7 minicondor-8.8.15-1.1.osg35.el7 python2-condor-8.8.15-1.1.osg35.el7 python3-condor-8.8.15-1.1.osg35.el7 Enterprise Linux 8 \u00b6 condor-8.8.15-1.1.osg35.el8 condor-all-8.8.15-1.1.osg35.el8 condor-annex-ec2-8.8.15-1.1.osg35.el8 condor-bosco-8.8.15-1.1.osg35.el8 condor-bosco-debuginfo-8.8.15-1.1.osg35.el8 condor-classads-8.8.15-1.1.osg35.el8 condor-classads-debuginfo-8.8.15-1.1.osg35.el8 condor-classads-devel-8.8.15-1.1.osg35.el8 condor-classads-devel-debuginfo-8.8.15-1.1.osg35.el8 condor-debuginfo-8.8.15-1.1.osg35.el8 condor-debugsource-8.8.15-1.1.osg35.el8 condor-kbdd-8.8.15-1.1.osg35.el8 condor-kbdd-debuginfo-8.8.15-1.1.osg35.el8 condor-procd-8.8.15-1.1.osg35.el8 condor-procd-debuginfo-8.8.15-1.1.osg35.el8 condor-test-8.8.15-1.1.osg35.el8 condor-test-debuginfo-8.8.15-1.1.osg35.el8 condor-vm-gahp-8.8.15-1.1.osg35.el8 condor-vm-gahp-debuginfo-8.8.15-1.1.osg35.el8 minicondor-8.8.15-1.1.osg35.el8 python3-condor-8.8.15-1.1.osg35.el8 python3-condor-debuginfo-8.8.15-1.1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 condor-9.0.4-1.osg35up.el7 Enterprise Linux 8 \u00b6 condor-9.0.4-1.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 condor-9.0.4-1.osg35up.el7 condor-all-9.0.4-1.osg35up.el7 condor-annex-ec2-9.0.4-1.osg35up.el7 condor-bosco-9.0.4-1.osg35up.el7 condor-classads-9.0.4-1.osg35up.el7 condor-classads-devel-9.0.4-1.osg35up.el7 condor-credmon-oauth-9.0.4-1.osg35up.el7 condor-credmon-vault-9.0.4-1.osg35up.el7 condor-debuginfo-9.0.4-1.osg35up.el7 condor-devel-9.0.4-1.osg35up.el7 condor-kbdd-9.0.4-1.osg35up.el7 condor-procd-9.0.4-1.osg35up.el7 condor-test-9.0.4-1.osg35up.el7 condor-vm-gahp-9.0.4-1.osg35up.el7 minicondor-9.0.4-1.osg35up.el7 python2-condor-9.0.4-1.osg35up.el7 python3-condor-9.0.4-1.osg35up.el7 Enterprise Linux 8 \u00b6 condor-9.0.4-1.osg35up.el8 condor-all-9.0.4-1.osg35up.el8 condor-annex-ec2-9.0.4-1.osg35up.el8 condor-bosco-9.0.4-1.osg35up.el8 condor-bosco-debuginfo-9.0.4-1.osg35up.el8 condor-classads-9.0.4-1.osg35up.el8 condor-classads-debuginfo-9.0.4-1.osg35up.el8 condor-classads-devel-9.0.4-1.osg35up.el8 condor-classads-devel-debuginfo-9.0.4-1.osg35up.el8 condor-credmon-vault-9.0.4-1.osg35up.el8 condor-debuginfo-9.0.4-1.osg35up.el8 condor-debugsource-9.0.4-1.osg35up.el8 condor-devel-9.0.4-1.osg35up.el8 condor-kbdd-9.0.4-1.osg35up.el8 condor-kbdd-debuginfo-9.0.4-1.osg35up.el8 condor-procd-9.0.4-1.osg35up.el8 condor-procd-debuginfo-9.0.4-1.osg35up.el8 condor-test-9.0.4-1.osg35up.el8 condor-test-debuginfo-9.0.4-1.osg35up.el8 condor-vm-gahp-9.0.4-1.osg35up.el8 condor-vm-gahp-debuginfo-9.0.4-1.osg35up.el8 minicondor-9.0.4-1.osg35up.el8 python3-condor-9.0.4-1.osg35up.el8 python3-condor-debuginfo-9.0.4-1.osg35up.el8","title":"Release 3 5 43"},{"location":"release/3.5/release-3-5-43/#osg-software-release-3543","text":"Release Date: 2021-07-30 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.43"},{"location":"release/3.5/release-3-5-43/#summary-of-changes","text":"This release contains: HTCondor 8.8.15 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 Upcoming HTCondor 9.0.4 Security Release. This release contains fixes for important security issues. More details on the security issues are in the vulnerability reports: HTCONDOR-2021-0003 HTCONDOR-2021-0004 These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-43/#containers","text":"The OSG Docker images have been updated to contain the new software.","title":"Containers"},{"location":"release/3.5/release-3-5-43/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-43/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-43/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-43/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-43/#enterprise-linux-7","text":"condor-8.8.15-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-43/#enterprise-linux-8","text":"condor-8.8.15-1.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-43/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-43/#enterprise-linux-7_1","text":"condor-8.8.15-1.1.osg35.el7 condor-all-8.8.15-1.1.osg35.el7 condor-annex-ec2-8.8.15-1.1.osg35.el7 condor-bosco-8.8.15-1.1.osg35.el7 condor-classads-8.8.15-1.1.osg35.el7 condor-classads-devel-8.8.15-1.1.osg35.el7 condor-debuginfo-8.8.15-1.1.osg35.el7 condor-kbdd-8.8.15-1.1.osg35.el7 condor-procd-8.8.15-1.1.osg35.el7 condor-test-8.8.15-1.1.osg35.el7 condor-vm-gahp-8.8.15-1.1.osg35.el7 minicondor-8.8.15-1.1.osg35.el7 python2-condor-8.8.15-1.1.osg35.el7 python3-condor-8.8.15-1.1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-43/#enterprise-linux-8_1","text":"condor-8.8.15-1.1.osg35.el8 condor-all-8.8.15-1.1.osg35.el8 condor-annex-ec2-8.8.15-1.1.osg35.el8 condor-bosco-8.8.15-1.1.osg35.el8 condor-bosco-debuginfo-8.8.15-1.1.osg35.el8 condor-classads-8.8.15-1.1.osg35.el8 condor-classads-debuginfo-8.8.15-1.1.osg35.el8 condor-classads-devel-8.8.15-1.1.osg35.el8 condor-classads-devel-debuginfo-8.8.15-1.1.osg35.el8 condor-debuginfo-8.8.15-1.1.osg35.el8 condor-debugsource-8.8.15-1.1.osg35.el8 condor-kbdd-8.8.15-1.1.osg35.el8 condor-kbdd-debuginfo-8.8.15-1.1.osg35.el8 condor-procd-8.8.15-1.1.osg35.el8 condor-procd-debuginfo-8.8.15-1.1.osg35.el8 condor-test-8.8.15-1.1.osg35.el8 condor-test-debuginfo-8.8.15-1.1.osg35.el8 condor-vm-gahp-8.8.15-1.1.osg35.el8 condor-vm-gahp-debuginfo-8.8.15-1.1.osg35.el8 minicondor-8.8.15-1.1.osg35.el8 python3-condor-8.8.15-1.1.osg35.el8 python3-condor-debuginfo-8.8.15-1.1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-43/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-43/#enterprise-linux-7_2","text":"condor-9.0.4-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-43/#enterprise-linux-8_2","text":"condor-9.0.4-1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-43/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-credmon-oauth condor-credmon-vault condor-debuginfo condor-devel condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-43/#enterprise-linux-7_3","text":"condor-9.0.4-1.osg35up.el7 condor-all-9.0.4-1.osg35up.el7 condor-annex-ec2-9.0.4-1.osg35up.el7 condor-bosco-9.0.4-1.osg35up.el7 condor-classads-9.0.4-1.osg35up.el7 condor-classads-devel-9.0.4-1.osg35up.el7 condor-credmon-oauth-9.0.4-1.osg35up.el7 condor-credmon-vault-9.0.4-1.osg35up.el7 condor-debuginfo-9.0.4-1.osg35up.el7 condor-devel-9.0.4-1.osg35up.el7 condor-kbdd-9.0.4-1.osg35up.el7 condor-procd-9.0.4-1.osg35up.el7 condor-test-9.0.4-1.osg35up.el7 condor-vm-gahp-9.0.4-1.osg35up.el7 minicondor-9.0.4-1.osg35up.el7 python2-condor-9.0.4-1.osg35up.el7 python3-condor-9.0.4-1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-43/#enterprise-linux-8_3","text":"condor-9.0.4-1.osg35up.el8 condor-all-9.0.4-1.osg35up.el8 condor-annex-ec2-9.0.4-1.osg35up.el8 condor-bosco-9.0.4-1.osg35up.el8 condor-bosco-debuginfo-9.0.4-1.osg35up.el8 condor-classads-9.0.4-1.osg35up.el8 condor-classads-debuginfo-9.0.4-1.osg35up.el8 condor-classads-devel-9.0.4-1.osg35up.el8 condor-classads-devel-debuginfo-9.0.4-1.osg35up.el8 condor-credmon-vault-9.0.4-1.osg35up.el8 condor-debuginfo-9.0.4-1.osg35up.el8 condor-debugsource-9.0.4-1.osg35up.el8 condor-devel-9.0.4-1.osg35up.el8 condor-kbdd-9.0.4-1.osg35up.el8 condor-kbdd-debuginfo-9.0.4-1.osg35up.el8 condor-procd-9.0.4-1.osg35up.el8 condor-procd-debuginfo-9.0.4-1.osg35up.el8 condor-test-9.0.4-1.osg35up.el8 condor-test-debuginfo-9.0.4-1.osg35up.el8 condor-vm-gahp-9.0.4-1.osg35up.el8 condor-vm-gahp-debuginfo-9.0.4-1.osg35up.el8 minicondor-9.0.4-1.osg35up.el8 python3-condor-9.0.4-1.osg35up.el8 python3-condor-debuginfo-9.0.4-1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-44/","text":"OSG Software Release 3.5.44 \u00b6 Release Date: 2021-08-05 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: VOMS 2.0.16-1.2 (EL7) and VOMS 2.1.0-0.14.rc2.2 (EL8) Add IAM and TLS SNI support htvault-config 1.4 and htgettoken 1.3 Improved security through more fine-grained vault tokens and detailed logging Miscellaneous improvements Upcoming XCache 2.1.0-3: Update to scripts to Python 3 Known Issue HTCondor-CE 5.3.0: caches or proxy servers may crash under heavy load Add \"xrootd.async off\" to your configuration file to avoid the issue These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the new software. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 htgettoken-1.3-1.osg35.el7 htvault-config-1.4-1.osg35.el7 voms-2.0.16-1.2.osg35.el7 Enterprise Linux 8 \u00b6 htgettoken-1.3-1.osg35.el8 htvault-config-1.4-1.osg35.el8 voms-2.1.0-0.14.rc2.2.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: htgettoken htvault-config voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 htgettoken-1.3-1.osg35.el7 htvault-config-1.4-1.osg35.el7 voms-2.0.16-1.2.osg35.el7 voms-clients-cpp-2.0.16-1.2.osg35.el7 voms-debuginfo-2.0.16-1.2.osg35.el7 voms-devel-2.0.16-1.2.osg35.el7 voms-doc-2.0.16-1.2.osg35.el7 voms-server-2.0.16-1.2.osg35.el7 Enterprise Linux 8 \u00b6 htgettoken-1.3-1.osg35.el8 htvault-config-1.4-1.osg35.el8 voms-2.1.0-0.14.rc2.2.osg35.el8 voms-clients-cpp-2.1.0-0.14.rc2.2.osg35.el8 voms-clients-cpp-debuginfo-2.1.0-0.14.rc2.2.osg35.el8 voms-debuginfo-2.1.0-0.14.rc2.2.osg35.el8 voms-debugsource-2.1.0-0.14.rc2.2.osg35.el8 voms-devel-2.1.0-0.14.rc2.2.osg35.el8 voms-doc-2.1.0-0.14.rc2.2.osg35.el8 voms-server-2.1.0-0.14.rc2.2.osg35.el8 voms-server-debuginfo-2.1.0-0.14.rc2.2.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 xcache-2.0.1-3.osg35up.el7 Enterprise Linux 8 \u00b6 xcache-2.0.1-3.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cms-xcache stash-cache stash-origin xcache xcache-consistency-check xcache-redirector If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 atlas-xcache-2.0.1-3.osg35up.el7 cms-xcache-2.0.1-3.osg35up.el7 stash-cache-2.0.1-3.osg35up.el7 stash-origin-2.0.1-3.osg35up.el7 xcache-2.0.1-3.osg35up.el7 xcache-consistency-check-2.0.1-3.osg35up.el7 xcache-redirector-2.0.1-3.osg35up.el7 Enterprise Linux 8 \u00b6 atlas-xcache-2.0.1-3.osg35up.el8 cms-xcache-2.0.1-3.osg35up.el8 stash-cache-2.0.1-3.osg35up.el8 stash-origin-2.0.1-3.osg35up.el8 xcache-2.0.1-3.osg35up.el8 xcache-consistency-check-2.0.1-3.osg35up.el8 xcache-redirector-2.0.1-3.osg35up.el8","title":"Release 3 5 44"},{"location":"release/3.5/release-3-5-44/#osg-software-release-3544","text":"Release Date: 2021-08-05 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.44"},{"location":"release/3.5/release-3-5-44/#summary-of-changes","text":"This release contains: VOMS 2.0.16-1.2 (EL7) and VOMS 2.1.0-0.14.rc2.2 (EL8) Add IAM and TLS SNI support htvault-config 1.4 and htgettoken 1.3 Improved security through more fine-grained vault tokens and detailed logging Miscellaneous improvements Upcoming XCache 2.1.0-3: Update to scripts to Python 3 Known Issue HTCondor-CE 5.3.0: caches or proxy servers may crash under heavy load Add \"xrootd.async off\" to your configuration file to avoid the issue These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-44/#containers","text":"The OSG Docker images have been updated to contain the new software.","title":"Containers"},{"location":"release/3.5/release-3-5-44/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-44/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-44/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-44/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-44/#enterprise-linux-7","text":"htgettoken-1.3-1.osg35.el7 htvault-config-1.4-1.osg35.el7 voms-2.0.16-1.2.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-44/#enterprise-linux-8","text":"htgettoken-1.3-1.osg35.el8 htvault-config-1.4-1.osg35.el8 voms-2.1.0-0.14.rc2.2.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-44/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: htgettoken htvault-config voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-44/#enterprise-linux-7_1","text":"htgettoken-1.3-1.osg35.el7 htvault-config-1.4-1.osg35.el7 voms-2.0.16-1.2.osg35.el7 voms-clients-cpp-2.0.16-1.2.osg35.el7 voms-debuginfo-2.0.16-1.2.osg35.el7 voms-devel-2.0.16-1.2.osg35.el7 voms-doc-2.0.16-1.2.osg35.el7 voms-server-2.0.16-1.2.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-44/#enterprise-linux-8_1","text":"htgettoken-1.3-1.osg35.el8 htvault-config-1.4-1.osg35.el8 voms-2.1.0-0.14.rc2.2.osg35.el8 voms-clients-cpp-2.1.0-0.14.rc2.2.osg35.el8 voms-clients-cpp-debuginfo-2.1.0-0.14.rc2.2.osg35.el8 voms-debuginfo-2.1.0-0.14.rc2.2.osg35.el8 voms-debugsource-2.1.0-0.14.rc2.2.osg35.el8 voms-devel-2.1.0-0.14.rc2.2.osg35.el8 voms-doc-2.1.0-0.14.rc2.2.osg35.el8 voms-server-2.1.0-0.14.rc2.2.osg35.el8 voms-server-debuginfo-2.1.0-0.14.rc2.2.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-44/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-44/#enterprise-linux-7_2","text":"xcache-2.0.1-3.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-44/#enterprise-linux-8_2","text":"xcache-2.0.1-3.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-44/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cms-xcache stash-cache stash-origin xcache xcache-consistency-check xcache-redirector If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-44/#enterprise-linux-7_3","text":"atlas-xcache-2.0.1-3.osg35up.el7 cms-xcache-2.0.1-3.osg35up.el7 stash-cache-2.0.1-3.osg35up.el7 stash-origin-2.0.1-3.osg35up.el7 xcache-2.0.1-3.osg35up.el7 xcache-consistency-check-2.0.1-3.osg35up.el7 xcache-redirector-2.0.1-3.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-44/#enterprise-linux-8_3","text":"atlas-xcache-2.0.1-3.osg35up.el8 cms-xcache-2.0.1-3.osg35up.el8 stash-cache-2.0.1-3.osg35up.el8 stash-origin-2.0.1-3.osg35up.el8 xcache-2.0.1-3.osg35up.el8 xcache-consistency-check-2.0.1-3.osg35up.el8 xcache-redirector-2.0.1-3.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-45-2/","text":"OSG Data Release 3.5.45-2 \u00b6 Release Date: 2021-08-16 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 CA Certificates based on IGTF 1.112 Updated ANSPGrid CA with extended validity date (BR) These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the latest CA certificates. Updating to the New Release \u00b6 To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 igtf-ca-certs-1.112-1.osg35.el7 osg-ca-certs-1.98-1.osg35.el7 Enterprise Linux 8 \u00b6 igtf-ca-certs-1.112-1.osg35.el8 osg-ca-certs-1.98-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 igtf-ca-certs-1.112-1.osg35.el7 osg-ca-certs-1.98-1.osg35.el7 Enterprise Linux 8 \u00b6 igtf-ca-certs-1.112-1.osg35.el8 osg-ca-certs-1.98-1.osg35.el8","title":"Release 3 5 45 2"},{"location":"release/3.5/release-3-5-45-2/#osg-data-release-3545-2","text":"Release Date: 2021-08-16 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.45-2"},{"location":"release/3.5/release-3-5-45-2/#summary-of-changes","text":"CA Certificates based on IGTF 1.112 Updated ANSPGrid CA with extended validity date (BR) These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-45-2/#containers","text":"The OSG Docker images have been updated to contain the latest CA certificates.","title":"Containers"},{"location":"release/3.5/release-3-5-45-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series from an earlier release series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-45-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-45-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-45-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-45-2/#enterprise-linux-7","text":"igtf-ca-certs-1.112-1.osg35.el7 osg-ca-certs-1.98-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-45-2/#enterprise-linux-8","text":"igtf-ca-certs-1.112-1.osg35.el8 osg-ca-certs-1.98-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-45-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-45-2/#enterprise-linux-7_1","text":"igtf-ca-certs-1.112-1.osg35.el7 osg-ca-certs-1.98-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-45-2/#enterprise-linux-8_1","text":"igtf-ca-certs-1.112-1.osg35.el8 osg-ca-certs-1.98-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-45/","text":"OSG Software Release 3.5.45 \u00b6 Release Date: 2021-08-12 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Gratia probes 1.24.0 Fix a problem that caused a traceback message in the condor_meter Fix a traceback caused by missing LogLevel in ProbeConfig Ensure that Gratia accounts for SciTokens-based pilots Upcoming XRootD 5.3.1 : Bug fix release Fix occasional crash under heavy load when using asynchronous I/O Note If you turned off asynchronous I/O to work around the problem with XRootD. Please turn asynchronous I/O back on again. These JIRA tickets were addressed in this release. Containers \u00b6 The OSG Docker images have been updated to contain the new software. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . Enterprise Linux 7 \u00b6 gratia-probe-1.24.0-1.osg35.el7 Enterprise Linux 8 \u00b6 gratia-probe-1.24.0-1.osg35.el8 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-osg-pilot-container gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 gratia-probe-1.24.0-1.osg35.el7 gratia-probe-common-1.24.0-1.osg35.el7 gratia-probe-condor-1.24.0-1.osg35.el7 gratia-probe-condor-events-1.24.0-1.osg35.el7 gratia-probe-dcache-storage-1.24.0-1.osg35.el7 gratia-probe-dcache-storagegroup-1.24.0-1.osg35.el7 gratia-probe-dcache-transfer-1.24.0-1.osg35.el7 gratia-probe-enstore-storage-1.24.0-1.osg35.el7 gratia-probe-enstore-tapedrive-1.24.0-1.osg35.el7 gratia-probe-enstore-transfer-1.24.0-1.osg35.el7 gratia-probe-glideinwms-1.24.0-1.osg35.el7 gratia-probe-gridftp-transfer-1.24.0-1.osg35.el7 gratia-probe-hadoop-storage-1.24.0-1.osg35.el7 gratia-probe-htcondor-ce-1.24.0-1.osg35.el7 gratia-probe-lsf-1.24.0-1.osg35.el7 gratia-probe-metric-1.24.0-1.osg35.el7 gratia-probe-onevm-1.24.0-1.osg35.el7 gratia-probe-osg-pilot-container-1.24.0-1.osg35.el7 gratia-probe-pbs-lsf-1.24.0-1.osg35.el7 gratia-probe-services-1.24.0-1.osg35.el7 gratia-probe-sge-1.24.0-1.osg35.el7 gratia-probe-slurm-1.24.0-1.osg35.el7 gratia-probe-xrootd-storage-1.24.0-1.osg35.el7 gratia-probe-xrootd-transfer-1.24.0-1.osg35.el7 Enterprise Linux 8 \u00b6 gratia-probe-1.24.0-1.osg35.el8 gratia-probe-common-1.24.0-1.osg35.el8 gratia-probe-condor-1.24.0-1.osg35.el8 gratia-probe-condor-events-1.24.0-1.osg35.el8 gratia-probe-dcache-storage-1.24.0-1.osg35.el8 gratia-probe-dcache-storagegroup-1.24.0-1.osg35.el8 gratia-probe-dcache-transfer-1.24.0-1.osg35.el8 gratia-probe-enstore-storage-1.24.0-1.osg35.el8 gratia-probe-enstore-tapedrive-1.24.0-1.osg35.el8 gratia-probe-enstore-transfer-1.24.0-1.osg35.el8 gratia-probe-glideinwms-1.24.0-1.osg35.el8 gratia-probe-gridftp-transfer-1.24.0-1.osg35.el8 gratia-probe-hadoop-storage-1.24.0-1.osg35.el8 gratia-probe-htcondor-ce-1.24.0-1.osg35.el8 gratia-probe-lsf-1.24.0-1.osg35.el8 gratia-probe-metric-1.24.0-1.osg35.el8 gratia-probe-onevm-1.24.0-1.osg35.el8 gratia-probe-osg-pilot-container-1.24.0-1.osg35.el8 gratia-probe-pbs-lsf-1.24.0-1.osg35.el8 gratia-probe-services-1.24.0-1.osg35.el8 gratia-probe-sge-1.24.0-1.osg35.el8 gratia-probe-slurm-1.24.0-1.osg35.el8 gratia-probe-xrootd-storage-1.24.0-1.osg35.el8 gratia-probe-xrootd-transfer-1.24.0-1.osg35.el8 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. Enterprise Linux 7 \u00b6 xrootd-5.3.1-1.1.osg35up.el7 Enterprise Linux 8 \u00b6 xrootd-5.3.1-1.1.osg35up.el8 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: python2-xrootd python36-xrootd xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-scitokens xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is: Enterprise Linux 7 \u00b6 python2-xrootd-5.3.1-1.1.osg35up.el7 python36-xrootd-5.3.1-1.1.osg35up.el7 xrootd-5.3.1-1.1.osg35up.el7 xrootd-client-5.3.1-1.1.osg35up.el7 xrootd-client-compat-5.3.1-1.1.osg35up.el7 xrootd-client-devel-5.3.1-1.1.osg35up.el7 xrootd-client-libs-5.3.1-1.1.osg35up.el7 xrootd-debuginfo-5.3.1-1.1.osg35up.el7 xrootd-devel-5.3.1-1.1.osg35up.el7 xrootd-doc-5.3.1-1.1.osg35up.el7 xrootd-fuse-5.3.1-1.1.osg35up.el7 xrootd-libs-5.3.1-1.1.osg35up.el7 xrootd-private-devel-5.3.1-1.1.osg35up.el7 xrootd-scitokens-5.3.1-1.1.osg35up.el7 xrootd-selinux-5.3.1-1.1.osg35up.el7 xrootd-server-5.3.1-1.1.osg35up.el7 xrootd-server-compat-5.3.1-1.1.osg35up.el7 xrootd-server-devel-5.3.1-1.1.osg35up.el7 xrootd-server-libs-5.3.1-1.1.osg35up.el7 xrootd-voms-5.3.1-1.1.osg35up.el7 Enterprise Linux 8 \u00b6 python3-xrootd-5.3.1-1.1.osg35up.el8 python3-xrootd-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-5.3.1-1.1.osg35up.el8 xrootd-client-5.3.1-1.1.osg35up.el8 xrootd-client-compat-5.3.1-1.1.osg35up.el8 xrootd-client-compat-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-client-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-client-devel-5.3.1-1.1.osg35up.el8 xrootd-client-devel-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-client-libs-5.3.1-1.1.osg35up.el8 xrootd-client-libs-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-debugsource-5.3.1-1.1.osg35up.el8 xrootd-devel-5.3.1-1.1.osg35up.el8 xrootd-doc-5.3.1-1.1.osg35up.el8 xrootd-fuse-5.3.1-1.1.osg35up.el8 xrootd-fuse-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-libs-5.3.1-1.1.osg35up.el8 xrootd-libs-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-private-devel-5.3.1-1.1.osg35up.el8 xrootd-scitokens-5.3.1-1.1.osg35up.el8 xrootd-scitokens-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-selinux-5.3.1-1.1.osg35up.el8 xrootd-server-5.3.1-1.1.osg35up.el8 xrootd-server-compat-5.3.1-1.1.osg35up.el8 xrootd-server-compat-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-server-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-server-devel-5.3.1-1.1.osg35up.el8 xrootd-server-libs-5.3.1-1.1.osg35up.el8 xrootd-server-libs-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-voms-5.3.1-1.1.osg35up.el8 xrootd-voms-debuginfo-5.3.1-1.1.osg35up.el8","title":"Release 3 5 45"},{"location":"release/3.5/release-3-5-45/#osg-software-release-3545","text":"Release Date: 2021-08-12 Supported OS Versions: EL7, EL8 Want faster access to production-ready software? OSG 3.5 offers a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.45"},{"location":"release/3.5/release-3-5-45/#summary-of-changes","text":"This release contains: Gratia probes 1.24.0 Fix a problem that caused a traceback message in the condor_meter Fix a traceback caused by missing LogLevel in ProbeConfig Ensure that Gratia accounts for SciTokens-based pilots Upcoming XRootD 5.3.1 : Bug fix release Fix occasional crash under heavy load when using asynchronous I/O Note If you turned off asynchronous I/O to work around the problem with XRootD. Please turn asynchronous I/O back on again. These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-45/#containers","text":"The OSG Docker images have been updated to contain the new software.","title":"Containers"},{"location":"release/3.5/release-3-5-45/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-45/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-45/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-45/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below .","title":"Packages"},{"location":"release/3.5/release-3-5-45/#enterprise-linux-7","text":"gratia-probe-1.24.0-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-45/#enterprise-linux-8","text":"gratia-probe-1.24.0-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-45/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: gratia-probe gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-osg-pilot-container gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer If you wish to only update the RPMs that changed, the set of RPMs is:","title":"RPMs"},{"location":"release/3.5/release-3-5-45/#enterprise-linux-7_1","text":"gratia-probe-1.24.0-1.osg35.el7 gratia-probe-common-1.24.0-1.osg35.el7 gratia-probe-condor-1.24.0-1.osg35.el7 gratia-probe-condor-events-1.24.0-1.osg35.el7 gratia-probe-dcache-storage-1.24.0-1.osg35.el7 gratia-probe-dcache-storagegroup-1.24.0-1.osg35.el7 gratia-probe-dcache-transfer-1.24.0-1.osg35.el7 gratia-probe-enstore-storage-1.24.0-1.osg35.el7 gratia-probe-enstore-tapedrive-1.24.0-1.osg35.el7 gratia-probe-enstore-transfer-1.24.0-1.osg35.el7 gratia-probe-glideinwms-1.24.0-1.osg35.el7 gratia-probe-gridftp-transfer-1.24.0-1.osg35.el7 gratia-probe-hadoop-storage-1.24.0-1.osg35.el7 gratia-probe-htcondor-ce-1.24.0-1.osg35.el7 gratia-probe-lsf-1.24.0-1.osg35.el7 gratia-probe-metric-1.24.0-1.osg35.el7 gratia-probe-onevm-1.24.0-1.osg35.el7 gratia-probe-osg-pilot-container-1.24.0-1.osg35.el7 gratia-probe-pbs-lsf-1.24.0-1.osg35.el7 gratia-probe-services-1.24.0-1.osg35.el7 gratia-probe-sge-1.24.0-1.osg35.el7 gratia-probe-slurm-1.24.0-1.osg35.el7 gratia-probe-xrootd-storage-1.24.0-1.osg35.el7 gratia-probe-xrootd-transfer-1.24.0-1.osg35.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-45/#enterprise-linux-8_1","text":"gratia-probe-1.24.0-1.osg35.el8 gratia-probe-common-1.24.0-1.osg35.el8 gratia-probe-condor-1.24.0-1.osg35.el8 gratia-probe-condor-events-1.24.0-1.osg35.el8 gratia-probe-dcache-storage-1.24.0-1.osg35.el8 gratia-probe-dcache-storagegroup-1.24.0-1.osg35.el8 gratia-probe-dcache-transfer-1.24.0-1.osg35.el8 gratia-probe-enstore-storage-1.24.0-1.osg35.el8 gratia-probe-enstore-tapedrive-1.24.0-1.osg35.el8 gratia-probe-enstore-transfer-1.24.0-1.osg35.el8 gratia-probe-glideinwms-1.24.0-1.osg35.el8 gratia-probe-gridftp-transfer-1.24.0-1.osg35.el8 gratia-probe-hadoop-storage-1.24.0-1.osg35.el8 gratia-probe-htcondor-ce-1.24.0-1.osg35.el8 gratia-probe-lsf-1.24.0-1.osg35.el8 gratia-probe-metric-1.24.0-1.osg35.el8 gratia-probe-onevm-1.24.0-1.osg35.el8 gratia-probe-osg-pilot-container-1.24.0-1.osg35.el8 gratia-probe-pbs-lsf-1.24.0-1.osg35.el8 gratia-probe-services-1.24.0-1.osg35.el8 gratia-probe-sge-1.24.0-1.osg35.el8 gratia-probe-slurm-1.24.0-1.osg35.el8 gratia-probe-xrootd-storage-1.24.0-1.osg35.el8 gratia-probe-xrootd-transfer-1.24.0-1.osg35.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-45/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG Yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-45/#enterprise-linux-7_2","text":"xrootd-5.3.1-1.1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-45/#enterprise-linux-8_2","text":"xrootd-5.3.1-1.1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-45/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: python2-xrootd python36-xrootd xrootd xrootd-client xrootd-client-compat xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-scitokens xrootd-selinux xrootd-server xrootd-server-compat xrootd-server-devel xrootd-server-libs xrootd-voms If you wish to only update the RPMs that changed, the set of RPMs is:","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-45/#enterprise-linux-7_3","text":"python2-xrootd-5.3.1-1.1.osg35up.el7 python36-xrootd-5.3.1-1.1.osg35up.el7 xrootd-5.3.1-1.1.osg35up.el7 xrootd-client-5.3.1-1.1.osg35up.el7 xrootd-client-compat-5.3.1-1.1.osg35up.el7 xrootd-client-devel-5.3.1-1.1.osg35up.el7 xrootd-client-libs-5.3.1-1.1.osg35up.el7 xrootd-debuginfo-5.3.1-1.1.osg35up.el7 xrootd-devel-5.3.1-1.1.osg35up.el7 xrootd-doc-5.3.1-1.1.osg35up.el7 xrootd-fuse-5.3.1-1.1.osg35up.el7 xrootd-libs-5.3.1-1.1.osg35up.el7 xrootd-private-devel-5.3.1-1.1.osg35up.el7 xrootd-scitokens-5.3.1-1.1.osg35up.el7 xrootd-selinux-5.3.1-1.1.osg35up.el7 xrootd-server-5.3.1-1.1.osg35up.el7 xrootd-server-compat-5.3.1-1.1.osg35up.el7 xrootd-server-devel-5.3.1-1.1.osg35up.el7 xrootd-server-libs-5.3.1-1.1.osg35up.el7 xrootd-voms-5.3.1-1.1.osg35up.el7","title":"Enterprise Linux 7"},{"location":"release/3.5/release-3-5-45/#enterprise-linux-8_3","text":"python3-xrootd-5.3.1-1.1.osg35up.el8 python3-xrootd-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-5.3.1-1.1.osg35up.el8 xrootd-client-5.3.1-1.1.osg35up.el8 xrootd-client-compat-5.3.1-1.1.osg35up.el8 xrootd-client-compat-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-client-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-client-devel-5.3.1-1.1.osg35up.el8 xrootd-client-devel-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-client-libs-5.3.1-1.1.osg35up.el8 xrootd-client-libs-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-debugsource-5.3.1-1.1.osg35up.el8 xrootd-devel-5.3.1-1.1.osg35up.el8 xrootd-doc-5.3.1-1.1.osg35up.el8 xrootd-fuse-5.3.1-1.1.osg35up.el8 xrootd-fuse-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-libs-5.3.1-1.1.osg35up.el8 xrootd-libs-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-private-devel-5.3.1-1.1.osg35up.el8 xrootd-scitokens-5.3.1-1.1.osg35up.el8 xrootd-scitokens-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-selinux-5.3.1-1.1.osg35up.el8 xrootd-server-5.3.1-1.1.osg35up.el8 xrootd-server-compat-5.3.1-1.1.osg35up.el8 xrootd-server-compat-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-server-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-server-devel-5.3.1-1.1.osg35up.el8 xrootd-server-libs-5.3.1-1.1.osg35up.el8 xrootd-server-libs-debuginfo-5.3.1-1.1.osg35up.el8 xrootd-voms-5.3.1-1.1.osg35up.el8 xrootd-voms-debuginfo-5.3.1-1.1.osg35up.el8","title":"Enterprise Linux 8"},{"location":"release/3.5/release-3-5-5/","text":"OSG Software Release 3.5.5 \u00b6 Release Date: 2019-11-14 Supported OS Versions: EL7 Summary of Changes \u00b6 This release contains: Frontier-Squid 4.9-2.1 : Fixed security vulnerabilities Heap Overflow issue in URN processing Multiple issues in URI processing XRootD 4.11.0 : New feature release BLAHP 1.18.45 Add SystemCpu accounting when Slurm jobs complete Add support for decimal seconds fields from Slurm Move local submit attributes scripts from spec to sources Pull in various updates to Slurm scripts from HTCondor's BLAHP Add BLAHP options for project name (-A, BatchProject) and runtime limit (-t, BatchRuntime). The BatchProject HTCondor job attribute is a string that's used with the project or allocation option of the batch system's submission interface. The BatchRuntime HTCondor job attribute is an integer that limits the job's wall-clock execution time, expressed in seconds. Detect and handle Slurm jobs in the TIMEOUT state Explicitly specify /bin/bash for shell scripts rather than /bin/sh Minor python bug fixes for comparing variables against None Support multiple Slurm clusters. The batch queue job parameter to the BLAHP now accepts an optional @<CLUSTER NAME> suffix. The job id generated for the BLAHP will include the cluster name to allow subsequent actions to target the specified cluster. scitokens-credmon 0.4.2: Provide configuration files in examples directory scitokens-cpp 0.3.5: Fixed Elliptic-curve public key handling These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Known Issues \u00b6 OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . blahp-1.18.45-1.osg35.el7 frontier-squid-4.9-2.1.osg35.el7 scitokens-cpp-0.3.5-1.osg35.el7 scitokens-credmon-0.4-2.osg35.el7 xrootd-4.11.0-1.osg35.el7 xrootd-hdfs-2.1.7-2.osg35.el7 xrootd-lcmaps-1.7.4-4.osg35.el7 xrootd-multiuser-0.4.2-5.osg35.el7 xrootd-scitokens-1.0.0-1.2.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo frontier-squid frontier-squid-debuginfo python2-scitokens-credmon python2-xrootd scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.45-1.osg35.el7 blahp-debuginfo-1.18.45-1.osg35.el7 frontier-squid-4.9-2.1.osg35.el7 frontier-squid-debuginfo-4.9-2.1.osg35.el7 python2-scitokens-credmon-0.4-2.osg35.el7 python2-xrootd-4.11.0-1.osg35.el7 scitokens-cpp-0.3.5-1.osg35.el7 scitokens-cpp-debuginfo-0.3.5-1.osg35.el7 scitokens-cpp-devel-0.3.5-1.osg35.el7 scitokens-credmon-0.4-2.osg35.el7 xrootd-4.11.0-1.osg35.el7 xrootd-client-4.11.0-1.osg35.el7 xrootd-client-devel-4.11.0-1.osg35.el7 xrootd-client-libs-4.11.0-1.osg35.el7 xrootd-debuginfo-4.11.0-1.osg35.el7 xrootd-devel-4.11.0-1.osg35.el7 xrootd-doc-4.11.0-1.osg35.el7 xrootd-fuse-4.11.0-1.osg35.el7 xrootd-hdfs-2.1.7-2.osg35.el7 xrootd-hdfs-debuginfo-2.1.7-2.osg35.el7 xrootd-hdfs-devel-2.1.7-2.osg35.el7 xrootd-lcmaps-1.7.4-4.osg35.el7 xrootd-lcmaps-debuginfo-1.7.4-4.osg35.el7 xrootd-libs-4.11.0-1.osg35.el7 xrootd-multiuser-0.4.2-5.osg35.el7 xrootd-multiuser-debuginfo-0.4.2-5.osg35.el7 xrootd-private-devel-4.11.0-1.osg35.el7 xrootd-scitokens-1.0.0-1.2.osg35.el7 xrootd-scitokens-debuginfo-1.0.0-1.2.osg35.el7 xrootd-selinux-4.11.0-1.osg35.el7 xrootd-server-4.11.0-1.osg35.el7 xrootd-server-devel-4.11.0-1.osg35.el7 xrootd-server-libs-4.11.0-1.osg35.el7 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.45-1.osgup.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.45-1.osgup.el7 blahp-debuginfo-1.18.45-1.osgup.el7","title":"Release 3 5 5"},{"location":"release/3.5/release-3-5-5/#osg-software-release-355","text":"Release Date: 2019-11-14 Supported OS Versions: EL7","title":"OSG Software Release 3.5.5"},{"location":"release/3.5/release-3-5-5/#summary-of-changes","text":"This release contains: Frontier-Squid 4.9-2.1 : Fixed security vulnerabilities Heap Overflow issue in URN processing Multiple issues in URI processing XRootD 4.11.0 : New feature release BLAHP 1.18.45 Add SystemCpu accounting when Slurm jobs complete Add support for decimal seconds fields from Slurm Move local submit attributes scripts from spec to sources Pull in various updates to Slurm scripts from HTCondor's BLAHP Add BLAHP options for project name (-A, BatchProject) and runtime limit (-t, BatchRuntime). The BatchProject HTCondor job attribute is a string that's used with the project or allocation option of the batch system's submission interface. The BatchRuntime HTCondor job attribute is an integer that limits the job's wall-clock execution time, expressed in seconds. Detect and handle Slurm jobs in the TIMEOUT state Explicitly specify /bin/bash for shell scripts rather than /bin/sh Minor python bug fixes for comparing variables against None Support multiple Slurm clusters. The batch queue job parameter to the BLAHP now accepts an optional @<CLUSTER NAME> suffix. The job id generated for the BLAHP will include the cluster name to allow subsequent actions to target the specified cluster. scitokens-credmon 0.4.2: Provide configuration files in examples directory scitokens-cpp 0.3.5: Fixed Elliptic-curve public key handling These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-5/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-5/#known-issues","text":"OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-5/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-5/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-5/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-5/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . blahp-1.18.45-1.osg35.el7 frontier-squid-4.9-2.1.osg35.el7 scitokens-cpp-0.3.5-1.osg35.el7 scitokens-credmon-0.4-2.osg35.el7 xrootd-4.11.0-1.osg35.el7 xrootd-hdfs-2.1.7-2.osg35.el7 xrootd-lcmaps-1.7.4-4.osg35.el7 xrootd-multiuser-0.4.2-5.osg35.el7 xrootd-scitokens-1.0.0-1.2.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-5/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo frontier-squid frontier-squid-debuginfo python2-scitokens-credmon python2-xrootd scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-multiuser xrootd-multiuser-debuginfo xrootd-private-devel xrootd-scitokens xrootd-scitokens-debuginfo xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.45-1.osg35.el7 blahp-debuginfo-1.18.45-1.osg35.el7 frontier-squid-4.9-2.1.osg35.el7 frontier-squid-debuginfo-4.9-2.1.osg35.el7 python2-scitokens-credmon-0.4-2.osg35.el7 python2-xrootd-4.11.0-1.osg35.el7 scitokens-cpp-0.3.5-1.osg35.el7 scitokens-cpp-debuginfo-0.3.5-1.osg35.el7 scitokens-cpp-devel-0.3.5-1.osg35.el7 scitokens-credmon-0.4-2.osg35.el7 xrootd-4.11.0-1.osg35.el7 xrootd-client-4.11.0-1.osg35.el7 xrootd-client-devel-4.11.0-1.osg35.el7 xrootd-client-libs-4.11.0-1.osg35.el7 xrootd-debuginfo-4.11.0-1.osg35.el7 xrootd-devel-4.11.0-1.osg35.el7 xrootd-doc-4.11.0-1.osg35.el7 xrootd-fuse-4.11.0-1.osg35.el7 xrootd-hdfs-2.1.7-2.osg35.el7 xrootd-hdfs-debuginfo-2.1.7-2.osg35.el7 xrootd-hdfs-devel-2.1.7-2.osg35.el7 xrootd-lcmaps-1.7.4-4.osg35.el7 xrootd-lcmaps-debuginfo-1.7.4-4.osg35.el7 xrootd-libs-4.11.0-1.osg35.el7 xrootd-multiuser-0.4.2-5.osg35.el7 xrootd-multiuser-debuginfo-0.4.2-5.osg35.el7 xrootd-private-devel-4.11.0-1.osg35.el7 xrootd-scitokens-1.0.0-1.2.osg35.el7 xrootd-scitokens-debuginfo-1.0.0-1.2.osg35.el7 xrootd-selinux-4.11.0-1.osg35.el7 xrootd-server-4.11.0-1.osg35.el7 xrootd-server-devel-4.11.0-1.osg35.el7 xrootd-server-libs-4.11.0-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-5/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.45-1.osgup.el7","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-5/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.45-1.osgup.el7 blahp-debuginfo-1.18.45-1.osgup.el7","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-6/","text":"OSG Software Release 3.5.6 \u00b6 Release Date: 2019-11-26 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: XCache 1.2: Added support for multi-node caches Fixed problem with StashCache working with authenticated origins CCTools 7.0.21 : Minor bug fix release osg-release 3.5-3: Updated development server host name to koji.opensciencegrid.org These JIRA tickets were addressed in this release. Containers \u00b6 The Stash Cache container has been tagged as stable in accordance with our Container Release Policy The Worker node containers have been updated to this release. Known Issues \u00b6 OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.0.21-1.osg35.el7 osg-release-3.5-3.osg35.el7 xcache-1.2.0-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cctools cctools-debuginfo cctools-devel cms-xcache osg-release stash-cache stash-origin xcache xcache-redirector If you wish to only update the RPMs that changed, the set of RPMs is: atlas-xcache-1.2.0-1.osg35.el7 cctools-7.0.21-1.osg35.el7 cctools-debuginfo-7.0.21-1.osg35.el7 cctools-devel-7.0.21-1.osg35.el7 cms-xcache-1.2.0-1.osg35.el7 osg-release-3.5-3.osg35.el7 stash-cache-1.2.0-1.osg35.el7 stash-origin-1.2.0-1.osg35.el7 xcache-1.2.0-1.osg35.el7 xcache-redirector-1.2.0-1.osg35.el7","title":"Release 3 5 6"},{"location":"release/3.5/release-3-5-6/#osg-software-release-356","text":"Release Date: 2019-11-26 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.6"},{"location":"release/3.5/release-3-5-6/#summary-of-changes","text":"This release contains: XCache 1.2: Added support for multi-node caches Fixed problem with StashCache working with authenticated origins CCTools 7.0.21 : Minor bug fix release osg-release 3.5-3: Updated development server host name to koji.opensciencegrid.org These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-6/#containers","text":"The Stash Cache container has been tagged as stable in accordance with our Container Release Policy The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-6/#known-issues","text":"OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-6/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-6/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-6/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-6/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . cctools-7.0.21-1.osg35.el7 osg-release-3.5-3.osg35.el7 xcache-1.2.0-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-6/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: atlas-xcache cctools cctools-debuginfo cctools-devel cms-xcache osg-release stash-cache stash-origin xcache xcache-redirector If you wish to only update the RPMs that changed, the set of RPMs is: atlas-xcache-1.2.0-1.osg35.el7 cctools-7.0.21-1.osg35.el7 cctools-debuginfo-7.0.21-1.osg35.el7 cctools-devel-7.0.21-1.osg35.el7 cms-xcache-1.2.0-1.osg35.el7 osg-release-3.5-3.osg35.el7 stash-cache-1.2.0-1.osg35.el7 stash-origin-1.2.0-1.osg35.el7 xcache-1.2.0-1.osg35.el7 xcache-redirector-1.2.0-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-7/","text":"OSG Software Release 3.5.7 \u00b6 Release Date: 2019-12-19 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: HTCondor-CE 4.1.0 : Bug fix release Fix an issue where condor_ce_q required authentication Re-enable the ability for local users to submit jobs to the CE queue Fix an issue where some jobs were capped at 72 minutes instead of 72 hours CVMFS 2.7.0 : New feature release Fuse 3 Support Pre-mounted Repository POSIX ACLs Client Performance Instrumentation GlideinWMS 3.6.1 : Improved Singularity support Simplify initial setup for stand-alone XRootD server HTCondor 8.8.6 : Bug fix release HTCondor 8.9.4 in the upcoming repository Amazon S3 file transfers using pre-signed URLs Further reductions in DAGMan memory usage These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Known Issues \u00b6 OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.6-1.1.osg35.el7 cvmfs-2.7.0-1.osg35.el7 glideinwms-3.6.1-1.osg35.el7 htcondor-ce-4.1.0-1.osg35.el7 osg-oasis-16-1.osg35.el7 osg-xrootd-3.5-4.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view minicondor osg-oasis osg-xrootd osg-xrootd-standalone python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.6-1.1.osg35.el7 condor-all-8.8.6-1.1.osg35.el7 condor-annex-ec2-8.8.6-1.1.osg35.el7 condor-bosco-8.8.6-1.1.osg35.el7 condor-classads-8.8.6-1.1.osg35.el7 condor-classads-devel-8.8.6-1.1.osg35.el7 condor-debuginfo-8.8.6-1.1.osg35.el7 condor-kbdd-8.8.6-1.1.osg35.el7 condor-procd-8.8.6-1.1.osg35.el7 condor-test-8.8.6-1.1.osg35.el7 condor-vm-gahp-8.8.6-1.1.osg35.el7 cvmfs-2.7.0-1.osg35.el7 cvmfs-devel-2.7.0-1.osg35.el7 cvmfs-ducc-2.7.0-1.osg35.el7 cvmfs-fuse3-2.7.0-1.osg35.el7 cvmfs-server-2.7.0-1.osg35.el7 cvmfs-shrinkwrap-2.7.0-1.osg35.el7 cvmfs-unittests-2.7.0-1.osg35.el7 glideinwms-3.6.1-1.osg35.el7 glideinwms-common-tools-3.6.1-1.osg35.el7 glideinwms-condor-common-config-3.6.1-1.osg35.el7 glideinwms-factory-3.6.1-1.osg35.el7 glideinwms-factory-condor-3.6.1-1.osg35.el7 glideinwms-glidecondor-tools-3.6.1-1.osg35.el7 glideinwms-libs-3.6.1-1.osg35.el7 glideinwms-minimal-condor-3.6.1-1.osg35.el7 glideinwms-usercollector-3.6.1-1.osg35.el7 glideinwms-userschedd-3.6.1-1.osg35.el7 glideinwms-vofrontend-3.6.1-1.osg35.el7 glideinwms-vofrontend-standalone-3.6.1-1.osg35.el7 htcondor-ce-4.1.0-1.osg35.el7 htcondor-ce-bosco-4.1.0-1.osg35.el7 htcondor-ce-client-4.1.0-1.osg35.el7 htcondor-ce-collector-4.1.0-1.osg35.el7 htcondor-ce-condor-4.1.0-1.osg35.el7 htcondor-ce-lsf-4.1.0-1.osg35.el7 htcondor-ce-pbs-4.1.0-1.osg35.el7 htcondor-ce-sge-4.1.0-1.osg35.el7 htcondor-ce-slurm-4.1.0-1.osg35.el7 htcondor-ce-view-4.1.0-1.osg35.el7 minicondor-8.8.6-1.1.osg35.el7 osg-oasis-16-1.osg35.el7 osg-xrootd-3.5-4.osg35.el7 osg-xrootd-standalone-3.5-4.osg35.el7 python2-condor-8.8.6-1.1.osg35.el7 python3-condor-8.8.6-1.1.osg35.el7 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. condor-8.9.4-1.osgup.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.9.4-1.osgup.el7 condor-all-8.9.4-1.osgup.el7 condor-annex-ec2-8.9.4-1.osgup.el7 condor-bosco-8.9.4-1.osgup.el7 condor-classads-8.9.4-1.osgup.el7 condor-classads-devel-8.9.4-1.osgup.el7 condor-debuginfo-8.9.4-1.osgup.el7 condor-kbdd-8.9.4-1.osgup.el7 condor-procd-8.9.4-1.osgup.el7 condor-test-8.9.4-1.osgup.el7 condor-vm-gahp-8.9.4-1.osgup.el7 minicondor-8.9.4-1.osgup.el7 python2-condor-8.9.4-1.osgup.el7 python3-condor-8.9.4-1.osgup.el7","title":"Release 3 5 7"},{"location":"release/3.5/release-3-5-7/#osg-software-release-357","text":"Release Date: 2019-12-19 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.7"},{"location":"release/3.5/release-3-5-7/#summary-of-changes","text":"This release contains: HTCondor-CE 4.1.0 : Bug fix release Fix an issue where condor_ce_q required authentication Re-enable the ability for local users to submit jobs to the CE queue Fix an issue where some jobs were capped at 72 minutes instead of 72 hours CVMFS 2.7.0 : New feature release Fuse 3 Support Pre-mounted Repository POSIX ACLs Client Performance Instrumentation GlideinWMS 3.6.1 : Improved Singularity support Simplify initial setup for stand-alone XRootD server HTCondor 8.8.6 : Bug fix release HTCondor 8.9.4 in the upcoming repository Amazon S3 file transfers using pre-signed URLs Further reductions in DAGMan memory usage These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-7/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-7/#known-issues","text":"OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-7/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-7/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-7/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-7/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.6-1.1.osg35.el7 cvmfs-2.7.0-1.osg35.el7 glideinwms-3.6.1-1.osg35.el7 htcondor-ce-4.1.0-1.osg35.el7 osg-oasis-16-1.osg35.el7 osg-xrootd-3.5-4.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-7/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-ducc cvmfs-fuse3 cvmfs-server cvmfs-shrinkwrap cvmfs-unittests glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view minicondor osg-oasis osg-xrootd osg-xrootd-standalone python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.6-1.1.osg35.el7 condor-all-8.8.6-1.1.osg35.el7 condor-annex-ec2-8.8.6-1.1.osg35.el7 condor-bosco-8.8.6-1.1.osg35.el7 condor-classads-8.8.6-1.1.osg35.el7 condor-classads-devel-8.8.6-1.1.osg35.el7 condor-debuginfo-8.8.6-1.1.osg35.el7 condor-kbdd-8.8.6-1.1.osg35.el7 condor-procd-8.8.6-1.1.osg35.el7 condor-test-8.8.6-1.1.osg35.el7 condor-vm-gahp-8.8.6-1.1.osg35.el7 cvmfs-2.7.0-1.osg35.el7 cvmfs-devel-2.7.0-1.osg35.el7 cvmfs-ducc-2.7.0-1.osg35.el7 cvmfs-fuse3-2.7.0-1.osg35.el7 cvmfs-server-2.7.0-1.osg35.el7 cvmfs-shrinkwrap-2.7.0-1.osg35.el7 cvmfs-unittests-2.7.0-1.osg35.el7 glideinwms-3.6.1-1.osg35.el7 glideinwms-common-tools-3.6.1-1.osg35.el7 glideinwms-condor-common-config-3.6.1-1.osg35.el7 glideinwms-factory-3.6.1-1.osg35.el7 glideinwms-factory-condor-3.6.1-1.osg35.el7 glideinwms-glidecondor-tools-3.6.1-1.osg35.el7 glideinwms-libs-3.6.1-1.osg35.el7 glideinwms-minimal-condor-3.6.1-1.osg35.el7 glideinwms-usercollector-3.6.1-1.osg35.el7 glideinwms-userschedd-3.6.1-1.osg35.el7 glideinwms-vofrontend-3.6.1-1.osg35.el7 glideinwms-vofrontend-standalone-3.6.1-1.osg35.el7 htcondor-ce-4.1.0-1.osg35.el7 htcondor-ce-bosco-4.1.0-1.osg35.el7 htcondor-ce-client-4.1.0-1.osg35.el7 htcondor-ce-collector-4.1.0-1.osg35.el7 htcondor-ce-condor-4.1.0-1.osg35.el7 htcondor-ce-lsf-4.1.0-1.osg35.el7 htcondor-ce-pbs-4.1.0-1.osg35.el7 htcondor-ce-sge-4.1.0-1.osg35.el7 htcondor-ce-slurm-4.1.0-1.osg35.el7 htcondor-ce-view-4.1.0-1.osg35.el7 minicondor-8.8.6-1.1.osg35.el7 osg-oasis-16-1.osg35.el7 osg-xrootd-3.5-4.osg35.el7 osg-xrootd-standalone-3.5-4.osg35.el7 python2-condor-8.8.6-1.1.osg35.el7 python3-condor-8.8.6-1.1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-7/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. condor-8.9.4-1.osgup.el7","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-7/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.9.4-1.osgup.el7 condor-all-8.9.4-1.osgup.el7 condor-annex-ec2-8.9.4-1.osgup.el7 condor-bosco-8.9.4-1.osgup.el7 condor-classads-8.9.4-1.osgup.el7 condor-classads-devel-8.9.4-1.osgup.el7 condor-debuginfo-8.9.4-1.osgup.el7 condor-kbdd-8.9.4-1.osgup.el7 condor-procd-8.9.4-1.osgup.el7 condor-test-8.9.4-1.osgup.el7 condor-vm-gahp-8.9.4-1.osgup.el7 minicondor-8.9.4-1.osgup.el7 python2-condor-8.9.4-1.osgup.el7 python3-condor-8.9.4-1.osgup.el7","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-8-2/","text":"OSG Data Release 3.5.8-2 \u00b6 Release Date: 2020-01-28 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: CA Certificates based on IGTF 1.103 Updated contact addresses for DigiCert (US) Regrafted InCommon IGTF Server CA onto self-signed Comodo RSA CA (US) Discontinued superfluous AddTrust External CA Root (US) Discontinued AustrianGrid CA (AT) These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . igtf-ca-certs-1.103-1.osg35.el7 osg-ca-certs-1.85-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: igtf-ca-certs-1.103-1.osg35.el7 osg-ca-certs-1.85-1.osg35.el7","title":"Release 3 5 8 2"},{"location":"release/3.5/release-3-5-8-2/#osg-data-release-358-2","text":"Release Date: 2020-01-28 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.8-2"},{"location":"release/3.5/release-3-5-8-2/#summary-of-changes","text":"This release contains: CA Certificates based on IGTF 1.103 Updated contact addresses for DigiCert (US) Regrafted InCommon IGTF Server CA onto self-signed Comodo RSA CA (US) Discontinued superfluous AddTrust External CA Root (US) Discontinued AustrianGrid CA (AT) These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-8-2/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-8-2/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-8-2/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-8-2/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-8-2/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . igtf-ca-certs-1.103-1.osg35.el7 osg-ca-certs-1.85-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-8-2/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: igtf-ca-certs-1.103-1.osg35.el7 osg-ca-certs-1.85-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-8-3/","text":"OSG Data Release 3.5.8-3 \u00b6 Release Date: 2020-01-29 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: CA Certificates based on IGTF 1.104 pre-release Reinstated AddTrust External CA Root in parallel to Comodo RSA CA to ease transition period for sites using InCommon IGTF certificates (US) These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . osg-ca-certs-1.86-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: osg-ca-certs-1.86-1.osg35.el7","title":"Release 3 5 8 3"},{"location":"release/3.5/release-3-5-8-3/#osg-data-release-358-3","text":"Release Date: 2020-01-29 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.8-3"},{"location":"release/3.5/release-3-5-8-3/#summary-of-changes","text":"This release contains: CA Certificates based on IGTF 1.104 pre-release Reinstated AddTrust External CA Root in parallel to Comodo RSA CA to ease transition period for sites using InCommon IGTF certificates (US) These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-8-3/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-8-3/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-8-3/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-8-3/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-8-3/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . osg-ca-certs-1.86-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-8-3/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: osg-ca-certs-1.86-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-8-4/","text":"OSG Data Release 3.5.8-4 \u00b6 Release Date: 2020-01-30 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release of IGTF CA Certificate bundle matches the changes made in the OSG CA Certificate bundle yesterday. This release contains: CA Certificates based on IGTF 1.104 Reinstated AddTrust External CA Root in parallel with Comodo RSA CA. This fixes failures that occur when RHEL6 clients with the IGTF 1.103 bundle attempt to authenticate with servers using InCommon IGTF host certs. Affected clients and servers are encouraged to upgrade. These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . igtf-ca-certs-1.104-2.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: igtf-ca-certs-1.104-2.osg35.el7","title":"Release 3 5 8 4"},{"location":"release/3.5/release-3-5-8-4/#osg-data-release-358-4","text":"Release Date: 2020-01-30 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Data Release 3.5.8-4"},{"location":"release/3.5/release-3-5-8-4/#summary-of-changes","text":"This release of IGTF CA Certificate bundle matches the changes made in the OSG CA Certificate bundle yesterday. This release contains: CA Certificates based on IGTF 1.104 Reinstated AddTrust External CA Root in parallel with Comodo RSA CA. This fixes failures that occur when RHEL6 clients with the IGTF 1.103 bundle attempt to authenticate with servers using InCommon IGTF host certs. Affected clients and servers are encouraged to upgrade. These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-8-4/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-8-4/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-8-4/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-8-4/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-8-4/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . igtf-ca-certs-1.104-2.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-8-4/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: igtf-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: igtf-ca-certs-1.104-2.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-8/","text":"OSG Software Release 3.5.8 \u00b6 Release Date: 2020-01-16 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: XRootD 4.11.1 : Bug fix release VOMS 2.0.14-15: Disable TLS <1.2 and insecure ciphers in VOMS server HTCondor 8.8.7 : Bug fix release gratia-probe 1.20.12: Fix silent failure when malformed ClassAd exists osg-xrootd: Enable third party copy and macaroons by default host-ce-tools 0.5-2: Ensure that sudo is installed scitokens-cpp 0.4.0: Support for the WLCG token profile worker-node tarball: Contains 'wget' osg-ce: Minor improvements Moved OSG configuration for gsi-openssh into OSG meta-package Moved OSG configuration for globus-gridftp-server into OSG meta-package Upcoming Repository: HTCondor 8.9.5 feature release These JIRA tickets were addressed in this release. Containers \u00b6 The Worker node containers have been updated to this release. Known Issues \u00b6 OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.7-1.osg35.el7 globus-gridftp-server-13.20-1.osg35.el7 gratia-probe-1.20.12-1.osg35.el7 gsi-openssh-7.4p1-4.5.osg35.el7 hosted-ce-tools-0.5-2.osg35.el7 osg-ce-3.5-4.osg35.el7 osg-gridftp-3.5-4.osg35.el7 osg-gsi-openssh-addons-1.0.0-3.osg35.el7 osg-xrootd-3.5-10.osg35.el7 scitokens-cpp-0.4.0-1.osg35.el7 voms-2.0.14-1.5.osg35.el7 xrootd-4.11.1-1.osg35.el7 xrootd-lcmaps-1.7.5-1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server hosted-ce-tools minicondor osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd osg-gsi-openssh-addons osg-xrootd osg-xrootd-standalone python2-condor python2-xrootd python3-condor scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.7-1.osg35.el7 condor-all-8.8.7-1.osg35.el7 condor-annex-ec2-8.8.7-1.osg35.el7 condor-bosco-8.8.7-1.osg35.el7 condor-classads-8.8.7-1.osg35.el7 condor-classads-devel-8.8.7-1.osg35.el7 condor-debuginfo-8.8.7-1.osg35.el7 condor-kbdd-8.8.7-1.osg35.el7 condor-procd-8.8.7-1.osg35.el7 condor-test-8.8.7-1.osg35.el7 condor-vm-gahp-8.8.7-1.osg35.el7 globus-gridftp-server-13.20-1.osg35.el7 globus-gridftp-server-debuginfo-13.20-1.osg35.el7 globus-gridftp-server-devel-13.20-1.osg35.el7 globus-gridftp-server-progs-13.20-1.osg35.el7 gratia-probe-1.20.12-1.osg35.el7 gratia-probe-common-1.20.12-1.osg35.el7 gratia-probe-condor-1.20.12-1.osg35.el7 gratia-probe-condor-events-1.20.12-1.osg35.el7 gratia-probe-dcache-storage-1.20.12-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.12-1.osg35.el7 gratia-probe-dcache-transfer-1.20.12-1.osg35.el7 gratia-probe-debuginfo-1.20.12-1.osg35.el7 gratia-probe-enstore-storage-1.20.12-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.12-1.osg35.el7 gratia-probe-enstore-transfer-1.20.12-1.osg35.el7 gratia-probe-glideinwms-1.20.12-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.12-1.osg35.el7 gratia-probe-hadoop-storage-1.20.12-1.osg35.el7 gratia-probe-htcondor-ce-1.20.12-1.osg35.el7 gratia-probe-lsf-1.20.12-1.osg35.el7 gratia-probe-metric-1.20.12-1.osg35.el7 gratia-probe-onevm-1.20.12-1.osg35.el7 gratia-probe-pbs-lsf-1.20.12-1.osg35.el7 gratia-probe-services-1.20.12-1.osg35.el7 gratia-probe-sge-1.20.12-1.osg35.el7 gratia-probe-slurm-1.20.12-1.osg35.el7 gratia-probe-xrootd-storage-1.20.12-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.12-1.osg35.el7 gsi-openssh-7.4p1-4.5.osg35.el7 gsi-openssh-clients-7.4p1-4.5.osg35.el7 gsi-openssh-debuginfo-7.4p1-4.5.osg35.el7 gsi-openssh-server-7.4p1-4.5.osg35.el7 hosted-ce-tools-0.5-2.osg35.el7 minicondor-8.8.7-1.osg35.el7 osg-ce-3.5-4.osg35.el7 osg-ce-bosco-3.5-4.osg35.el7 osg-ce-condor-3.5-4.osg35.el7 osg-ce-lsf-3.5-4.osg35.el7 osg-ce-pbs-3.5-4.osg35.el7 osg-ce-sge-3.5-4.osg35.el7 osg-ce-slurm-3.5-4.osg35.el7 osg-gridftp-3.5-4.osg35.el7 osg-gridftp-hdfs-3.5-4.osg35.el7 osg-gridftp-xrootd-3.5-4.osg35.el7 osg-gsi-openssh-addons-1.0.0-3.osg35.el7 osg-xrootd-3.5-10.osg35.el7 osg-xrootd-standalone-3.5-10.osg35.el7 python2-condor-8.8.7-1.osg35.el7 python2-xrootd-4.11.1-1.osg35.el7 python3-condor-8.8.7-1.osg35.el7 scitokens-cpp-0.4.0-1.osg35.el7 scitokens-cpp-debuginfo-0.4.0-1.osg35.el7 scitokens-cpp-devel-0.4.0-1.osg35.el7 voms-2.0.14-1.5.osg35.el7 voms-clients-cpp-2.0.14-1.5.osg35.el7 voms-debuginfo-2.0.14-1.5.osg35.el7 voms-devel-2.0.14-1.5.osg35.el7 voms-doc-2.0.14-1.5.osg35.el7 voms-server-2.0.14-1.5.osg35.el7 xrootd-4.11.1-1.osg35.el7 xrootd-client-4.11.1-1.osg35.el7 xrootd-client-devel-4.11.1-1.osg35.el7 xrootd-client-libs-4.11.1-1.osg35.el7 xrootd-debuginfo-4.11.1-1.osg35.el7 xrootd-devel-4.11.1-1.osg35.el7 xrootd-doc-4.11.1-1.osg35.el7 xrootd-fuse-4.11.1-1.osg35.el7 xrootd-lcmaps-1.7.5-1.osg35.el7 xrootd-lcmaps-debuginfo-1.7.5-1.osg35.el7 xrootd-libs-4.11.1-1.osg35.el7 xrootd-private-devel-4.11.1-1.osg35.el7 xrootd-selinux-4.11.1-1.osg35.el7 xrootd-server-4.11.1-1.osg35.el7 xrootd-server-devel-4.11.1-1.osg35.el7 xrootd-server-libs-4.11.1-1.osg35.el7 Upcoming Packages \u00b6 We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.45-1.1.osgup.el7 condor-8.9.5-1.osgup.el7 Upcoming RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.45-1.1.osgup.el7 blahp-debuginfo-1.18.45-1.1.osgup.el7 condor-8.9.5-1.osgup.el7 condor-all-8.9.5-1.osgup.el7 condor-annex-ec2-8.9.5-1.osgup.el7 condor-bosco-8.9.5-1.osgup.el7 condor-classads-8.9.5-1.osgup.el7 condor-classads-devel-8.9.5-1.osgup.el7 condor-debuginfo-8.9.5-1.osgup.el7 condor-kbdd-8.9.5-1.osgup.el7 condor-procd-8.9.5-1.osgup.el7 condor-test-8.9.5-1.osgup.el7 condor-vm-gahp-8.9.5-1.osgup.el7 minicondor-8.9.5-1.osgup.el7 python2-condor-8.9.5-1.osgup.el7 python3-condor-8.9.5-1.osgup.el7","title":"Release 3 5 8"},{"location":"release/3.5/release-3-5-8/#osg-software-release-358","text":"Release Date: 2020-01-16 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.8"},{"location":"release/3.5/release-3-5-8/#summary-of-changes","text":"This release contains: XRootD 4.11.1 : Bug fix release VOMS 2.0.14-15: Disable TLS <1.2 and insecure ciphers in VOMS server HTCondor 8.8.7 : Bug fix release gratia-probe 1.20.12: Fix silent failure when malformed ClassAd exists osg-xrootd: Enable third party copy and macaroons by default host-ce-tools 0.5-2: Ensure that sudo is installed scitokens-cpp 0.4.0: Support for the WLCG token profile worker-node tarball: Contains 'wget' osg-ce: Minor improvements Moved OSG configuration for gsi-openssh into OSG meta-package Moved OSG configuration for globus-gridftp-server into OSG meta-package Upcoming Repository: HTCondor 8.9.5 feature release These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-8/#containers","text":"The Worker node containers have been updated to this release.","title":"Containers"},{"location":"release/3.5/release-3-5-8/#known-issues","text":"OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-8/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-8/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-8/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-8/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . condor-8.8.7-1.osg35.el7 globus-gridftp-server-13.20-1.osg35.el7 gratia-probe-1.20.12-1.osg35.el7 gsi-openssh-7.4p1-4.5.osg35.el7 hosted-ce-tools-0.5-2.osg35.el7 osg-ce-3.5-4.osg35.el7 osg-gridftp-3.5-4.osg35.el7 osg-gsi-openssh-addons-1.0.0-3.osg35.el7 osg-xrootd-3.5-10.osg35.el7 scitokens-cpp-0.4.0-1.osg35.el7 voms-2.0.14-1.5.osg35.el7 xrootd-4.11.1-1.osg35.el7 xrootd-lcmaps-1.7.5-1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-8/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glideinwms gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server hosted-ce-tools minicondor osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd osg-gsi-openssh-addons osg-xrootd osg-xrootd-standalone python2-condor python2-xrootd python3-condor scitokens-cpp scitokens-cpp-debuginfo scitokens-cpp-devel voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-private-devel xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs If you wish to only update the RPMs that changed, the set of RPMs is: condor-8.8.7-1.osg35.el7 condor-all-8.8.7-1.osg35.el7 condor-annex-ec2-8.8.7-1.osg35.el7 condor-bosco-8.8.7-1.osg35.el7 condor-classads-8.8.7-1.osg35.el7 condor-classads-devel-8.8.7-1.osg35.el7 condor-debuginfo-8.8.7-1.osg35.el7 condor-kbdd-8.8.7-1.osg35.el7 condor-procd-8.8.7-1.osg35.el7 condor-test-8.8.7-1.osg35.el7 condor-vm-gahp-8.8.7-1.osg35.el7 globus-gridftp-server-13.20-1.osg35.el7 globus-gridftp-server-debuginfo-13.20-1.osg35.el7 globus-gridftp-server-devel-13.20-1.osg35.el7 globus-gridftp-server-progs-13.20-1.osg35.el7 gratia-probe-1.20.12-1.osg35.el7 gratia-probe-common-1.20.12-1.osg35.el7 gratia-probe-condor-1.20.12-1.osg35.el7 gratia-probe-condor-events-1.20.12-1.osg35.el7 gratia-probe-dcache-storage-1.20.12-1.osg35.el7 gratia-probe-dcache-storagegroup-1.20.12-1.osg35.el7 gratia-probe-dcache-transfer-1.20.12-1.osg35.el7 gratia-probe-debuginfo-1.20.12-1.osg35.el7 gratia-probe-enstore-storage-1.20.12-1.osg35.el7 gratia-probe-enstore-tapedrive-1.20.12-1.osg35.el7 gratia-probe-enstore-transfer-1.20.12-1.osg35.el7 gratia-probe-glideinwms-1.20.12-1.osg35.el7 gratia-probe-gridftp-transfer-1.20.12-1.osg35.el7 gratia-probe-hadoop-storage-1.20.12-1.osg35.el7 gratia-probe-htcondor-ce-1.20.12-1.osg35.el7 gratia-probe-lsf-1.20.12-1.osg35.el7 gratia-probe-metric-1.20.12-1.osg35.el7 gratia-probe-onevm-1.20.12-1.osg35.el7 gratia-probe-pbs-lsf-1.20.12-1.osg35.el7 gratia-probe-services-1.20.12-1.osg35.el7 gratia-probe-sge-1.20.12-1.osg35.el7 gratia-probe-slurm-1.20.12-1.osg35.el7 gratia-probe-xrootd-storage-1.20.12-1.osg35.el7 gratia-probe-xrootd-transfer-1.20.12-1.osg35.el7 gsi-openssh-7.4p1-4.5.osg35.el7 gsi-openssh-clients-7.4p1-4.5.osg35.el7 gsi-openssh-debuginfo-7.4p1-4.5.osg35.el7 gsi-openssh-server-7.4p1-4.5.osg35.el7 hosted-ce-tools-0.5-2.osg35.el7 minicondor-8.8.7-1.osg35.el7 osg-ce-3.5-4.osg35.el7 osg-ce-bosco-3.5-4.osg35.el7 osg-ce-condor-3.5-4.osg35.el7 osg-ce-lsf-3.5-4.osg35.el7 osg-ce-pbs-3.5-4.osg35.el7 osg-ce-sge-3.5-4.osg35.el7 osg-ce-slurm-3.5-4.osg35.el7 osg-gridftp-3.5-4.osg35.el7 osg-gridftp-hdfs-3.5-4.osg35.el7 osg-gridftp-xrootd-3.5-4.osg35.el7 osg-gsi-openssh-addons-1.0.0-3.osg35.el7 osg-xrootd-3.5-10.osg35.el7 osg-xrootd-standalone-3.5-10.osg35.el7 python2-condor-8.8.7-1.osg35.el7 python2-xrootd-4.11.1-1.osg35.el7 python3-condor-8.8.7-1.osg35.el7 scitokens-cpp-0.4.0-1.osg35.el7 scitokens-cpp-debuginfo-0.4.0-1.osg35.el7 scitokens-cpp-devel-0.4.0-1.osg35.el7 voms-2.0.14-1.5.osg35.el7 voms-clients-cpp-2.0.14-1.5.osg35.el7 voms-debuginfo-2.0.14-1.5.osg35.el7 voms-devel-2.0.14-1.5.osg35.el7 voms-doc-2.0.14-1.5.osg35.el7 voms-server-2.0.14-1.5.osg35.el7 xrootd-4.11.1-1.osg35.el7 xrootd-client-4.11.1-1.osg35.el7 xrootd-client-devel-4.11.1-1.osg35.el7 xrootd-client-libs-4.11.1-1.osg35.el7 xrootd-debuginfo-4.11.1-1.osg35.el7 xrootd-devel-4.11.1-1.osg35.el7 xrootd-doc-4.11.1-1.osg35.el7 xrootd-fuse-4.11.1-1.osg35.el7 xrootd-lcmaps-1.7.5-1.osg35.el7 xrootd-lcmaps-debuginfo-1.7.5-1.osg35.el7 xrootd-libs-4.11.1-1.osg35.el7 xrootd-private-devel-4.11.1-1.osg35.el7 xrootd-selinux-4.11.1-1.osg35.el7 xrootd-server-4.11.1-1.osg35.el7 xrootd-server-devel-4.11.1-1.osg35.el7 xrootd-server-libs-4.11.1-1.osg35.el7","title":"RPMs"},{"location":"release/3.5/release-3-5-8/#upcoming-packages","text":"We added or updated the following packages to the upcoming OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below. blahp-1.18.45-1.1.osgup.el7 condor-8.9.5-1.osgup.el7","title":"Upcoming Packages"},{"location":"release/3.5/release-3-5-8/#upcoming-rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-debuginfo condor-kbdd condor-procd condor-test condor-vm-gahp minicondor python2-condor python3-condor If you wish to only update the RPMs that changed, the set of RPMs is: blahp-1.18.45-1.1.osgup.el7 blahp-debuginfo-1.18.45-1.1.osgup.el7 condor-8.9.5-1.osgup.el7 condor-all-8.9.5-1.osgup.el7 condor-annex-ec2-8.9.5-1.osgup.el7 condor-bosco-8.9.5-1.osgup.el7 condor-classads-8.9.5-1.osgup.el7 condor-classads-devel-8.9.5-1.osgup.el7 condor-debuginfo-8.9.5-1.osgup.el7 condor-kbdd-8.9.5-1.osgup.el7 condor-procd-8.9.5-1.osgup.el7 condor-test-8.9.5-1.osgup.el7 condor-vm-gahp-8.9.5-1.osgup.el7 minicondor-8.9.5-1.osgup.el7 python2-condor-8.9.5-1.osgup.el7 python3-condor-8.9.5-1.osgup.el7","title":"Upcoming RPMs"},{"location":"release/3.5/release-3-5-9/","text":"OSG Software Release 3.5.9 \u00b6 Release Date: 2020-02-06 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling Summary of Changes \u00b6 This release contains: Frontier Squid 4.10 OSG Release Notes Upstream Security and Release Information This release fixes important security vulnerabilities. Please update as soon as possible. These JIRA tickets were addressed in this release. Containers \u00b6 The Frontier Squid container is available and has been tagged as stable in accordance with our Container Release Policy Frontier Squid Known Issues \u00b6 OSG System Profiler verifies all installed packages, which may result in excessively long run times . Updating to the New Release \u00b6 To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/ Need Help? \u00b6 Do you need help with this release? Contact us for help . Detailed Changes in This Release \u00b6 Packages \u00b6 We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . frontier-squid-4.10-1.1.osg35.el7 RPMs \u00b6 If you wish to manually update your system, you can run yum update against the following packages: frontier-squid frontier-squid-debuginfo igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: frontier-squid-4.10-1.1.osg35.el7 frontier-squid-debuginfo-4.10-1.1.osg35.el7","title":"Release 3 5 9"},{"location":"release/3.5/release-3-5-9/#osg-software-release-359","text":"Release Date: 2020-02-06 Supported OS Versions: EL7 Want faster access to production-ready software? OSG 3.5 and 3.4 offer a rolling release repository where packages are added as soon as they pass acceptance testing. To install packages from this repository, enable [osg-rolling] in /etc/yum.repos.d/osg-rolling.repo : [osg-rolling] ... enabled=1 Or for one-time installations, append the following to your yum command: --enablerepo=osg-rolling","title":"OSG Software Release 3.5.9"},{"location":"release/3.5/release-3-5-9/#summary-of-changes","text":"This release contains: Frontier Squid 4.10 OSG Release Notes Upstream Security and Release Information This release fixes important security vulnerabilities. Please update as soon as possible. These JIRA tickets were addressed in this release.","title":"Summary of Changes"},{"location":"release/3.5/release-3-5-9/#containers","text":"The Frontier Squid container is available and has been tagged as stable in accordance with our Container Release Policy Frontier Squid","title":"Containers"},{"location":"release/3.5/release-3-5-9/#known-issues","text":"OSG System Profiler verifies all installed packages, which may result in excessively long run times .","title":"Known Issues"},{"location":"release/3.5/release-3-5-9/#updating-to-the-new-release","text":"To update to the OSG 3.5 series, please consult the page on updating between release series . For sites using non-RPM worker node client installations, new tarballs and container images are available: Tarball: https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz Container Images: https://hub.docker.com/r/opensciencegrid/osg-wn/","title":"Updating to the New Release"},{"location":"release/3.5/release-3-5-9/#need-help","text":"Do you need help with this release? Contact us for help .","title":"Need Help?"},{"location":"release/3.5/release-3-5-9/#detailed-changes-in-this-release","text":"","title":"Detailed Changes in This Release"},{"location":"release/3.5/release-3-5-9/#packages","text":"We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below . frontier-squid-4.10-1.1.osg35.el7","title":"Packages"},{"location":"release/3.5/release-3-5-9/#rpms","text":"If you wish to manually update your system, you can run yum update against the following packages: frontier-squid frontier-squid-debuginfo igtf-ca-certs osg-ca-certs If you wish to only update the RPMs that changed, the set of RPMs is: frontier-squid-4.10-1.1.osg35.el7 frontier-squid-debuginfo-4.10-1.1.osg35.el7","title":"RPMs"},{"location":"resource-sharing/os-backfill-containers/","text":"Open Science Pool Containers \u00b6 In order to share compute resources with the Open Science pool, sites can launch pilot jobs directly by starting an OSG-provided Docker container. The container includes a simple worker node environment and an embedded pilot; when combined with an OSG-provided authentication token (not included in the container), the pilot can connect to the Open Science pool and start executing jobs. This technique is useful to implement backfill at a site: contributing computing resources when they would otherwise be idle. It does not allow the site to share resources between multiple pools and, if there are no matching idle jobs in the Open Science pool, the pilots may remain idle. Before Starting \u00b6 In order to configure the container, you will need: A registered resource in OSG Topology; resource registration allows OSG to do proper usage accounting and maintain contacts in case of security incidents. An authentication token from the OSG. Please contact OSG support to request a token for your site. An HTTP caching proxy (\"squid server\") at or near your site. Running the Container with Docker \u00b6 The Docker image is kept in DockerHub . In order to successfully start payload jobs: Configure authentication: Open Science Pool (OSP) administrators can provide the token, which you can then pass to the container by volume mounting it as a file under /etc/condor/tokens-orig.d/ . If you are using Docker to launch the container, this is done with the command line flag -v /path/to/token:/etc/condor/tokens-orig.d/flock.opensciencegrid.org . Replace /path/to/token with the full path to the token you obtained from the OSP administrators. Set GLIDEIN_Site and GLIDEIN_ResourceName to match the site name and resource name that you registered in Topology, respectively. Set the OSG_SQUID_LOCATION environment variable to the HTTP address of your preferred Squid instance. Recommended: Enable CVMFS via one of the mechanisms described below. Strongly recommended: If you want job I/O to be done in a separate directory outside of the container, volume mount the desired directory on the host to /pilot inside the container. Without this, user jobs may compete for disk space with other containers on your system. If you are using Docker to launch the container, this is done with the command line flag -v /worker-temp-dir:/pilot . Replace /worker-temp-dir with a directory you created for jobs to write into. Make sure the user you run your container as has write access to this directory. Optional: add an expression with the GLIDEIN_Start_Extra environment variable to append to the HTCondor START expression; this limits the pilot to only run certain jobs. Here is an example invocation using docker run by hand: docker run -it --rm --user osg \\ --privileged \\ -v /path/to/token:/etc/condor/tokens-orig.d/flock.opensciencegrid.org \\ -v /worker-temp-dir:/pilot \\ -e GLIDEIN_Site=\"...\" \\ -e GLIDEIN_ResourceName=\"...\" \\ -e GLIDEIN_Start_Extra=\"True\" \\ -e OSG_SQUID_LOCATION=\"...\" \\ -e CVMFSEXEC_REPOS=\" \\ oasis.opensciencegrid.org \\ singularity.opensciencegrid.org\" \\ opensciencegrid/osgvo-docker-pilot:release Replace /path/to/token with the location you saved the token obtained from the OSP administrators. Privileged mode ( --privileged ) requested in the above docker run allows the container to mount CVMFS using cvmfsexec and invoke singularity for user jobs. Singularity allows OSP users to use their own container for their job (e.g., a common use case for GPU jobs). CVMFS \u00b6 CernVM-FS (CVMFS) is a read-only remote filesystem that many OSG jobs depend on for software and data. Supporting CVMFS inside your container will greatly increase the types of OSG jobs you can run. There are two methods for making CVMFS available in your container: enabling cvmfsexec , or bind-mounting CVMFS from the host . Bind-mounting CVMFS will require CVMFS to be installed on the host first, but the container will need fewer privileges. Adding CVMFS using cvmfsexec \u00b6 cvmfsexec is a tool that can be used to mount CVMFS inside the container without requiring CVMFS to be installed on the host. To enable cvmfsexec, specify a space-separated list of repos in the CVMFSEXEC_REPOS environment variable. We recommend the following repos: - oasis.opensciencegrid.org - singularity.opensciencegrid.org cvmfsexec has the following system requirements: On EL7, you must have kernel version >= 3.10.0-1127 (run uname -vr to check), and user namespaces enabled. See step 1 in the Singularity Install document for details. On EL8, you must have kernel version >= 4.18 (run uname -vr to check). See the cvmfsexec README details. Note that cvmfsexec will not be run if CVMFS repos are already available in /cvmfs via bind-mount, regardless of the value of CVMFSEXEC_REPOS . Using cvmfsexec takes place in the entrypoint, which means it will still happen even if you specify a different command to run, such as bash . You can bypass the entrypoint by passing --entrypoint <cmd> where <cmd> is some different command to run, e.g. --entrypoint bash . Setting the entrypoint this way clears the command. Additional cvmfsexec configuration \u00b6 There are several environment variables you can set for cvmfsexec: CVMFSEXEC_REPOS - this is a space-separated list of CVMFS repos to mount. Leave this blank to disable cvmfsexec. OSG jobs frequently use the OASIS repo ( oasis.opensciencegrid.org ) and the singularity repo ( singularity.opensciencegrid.org ). CVMFS_HTTP_PROXY - this sets the proxy to use for CVMFS; if left blank it will find the best one via WLCG Web Proxy Auto Discovery. CVMFS_QUOTA_LIMIT - the quota limit in MB for CVMFS; leave this blank to use the system default (4 GB) You can add other CVMFS options by bind-mounting a config file over /cvmfsexec/default.local ; note that options in environment variables take precedence over options in /cvmfsexec/default.local . You can store the cache outside of the container by volume mounting a directory to /cvmfs-cache . You can store the logs outside of the container by volume mounting a directory to /cvmfs-logs . Adding CVMFS via bind-mount \u00b6 As an alternative to using cvmfsexec, you may install CVMFS on the host, and volume mount it into the container. This will let you avoid running the container in privileged mode. However, supporting Singularity jobs inside the container will require extra privileges, namely the capabilities DAC_OVERRIDE , DAC_READ_SEARCH , SETGID , SETUID , SYS_ADMIN , SYS_CHROOT , and SYS_PTRACE . Follow the installing CVMFS document to install CVMFS on the host. Once you have CVMFS installed and mounted on your host, add -v /cvmfs:/cvmfs:shared to your docker run invocation. This is the example at the top of the page , modified to volume mount CVMFS instead of using cvmfsexec, and using reduced privileges: docker run -it --rm --user osg \\ --cap-add DAC_OVERRIDE \\ --cap-add DAC_READ_SEARCH \\ --cap-add SETUID \\ --cap-add SETGID \\ --cap-add SYS_ADMIN \\ --cap-add SYS_CHROOT \\ --cap-add SYS_PTRACE \\ -v /cvmfs:/cvmfs:shared \\ -v /path/to/token:/etc/condor/tokens-orig.d/flock.opensciencegrid.org \\ -v /worker-temp-dir:/pilot \\ -e GLIDEIN_Site=\"...\" \\ -e GLIDEIN_ResourceName=\"...\" \\ -e GLIDEIN_Start_Extra=\"True\" \\ -e OSG_SQUID_LOCATION=\"...\" \\ opensciencegrid/osgvo-docker-pilot:release Fill in the values for /path/to/token , /worker-temp-dir , GLIDEIN_Site , GLIDEIN_ResourceName , and OSG_SQUID_LOCATION as above .","title":"Site Backfill"},{"location":"resource-sharing/os-backfill-containers/#open-science-pool-containers","text":"In order to share compute resources with the Open Science pool, sites can launch pilot jobs directly by starting an OSG-provided Docker container. The container includes a simple worker node environment and an embedded pilot; when combined with an OSG-provided authentication token (not included in the container), the pilot can connect to the Open Science pool and start executing jobs. This technique is useful to implement backfill at a site: contributing computing resources when they would otherwise be idle. It does not allow the site to share resources between multiple pools and, if there are no matching idle jobs in the Open Science pool, the pilots may remain idle.","title":"Open Science Pool Containers"},{"location":"resource-sharing/os-backfill-containers/#before-starting","text":"In order to configure the container, you will need: A registered resource in OSG Topology; resource registration allows OSG to do proper usage accounting and maintain contacts in case of security incidents. An authentication token from the OSG. Please contact OSG support to request a token for your site. An HTTP caching proxy (\"squid server\") at or near your site.","title":"Before Starting"},{"location":"resource-sharing/os-backfill-containers/#running-the-container-with-docker","text":"The Docker image is kept in DockerHub . In order to successfully start payload jobs: Configure authentication: Open Science Pool (OSP) administrators can provide the token, which you can then pass to the container by volume mounting it as a file under /etc/condor/tokens-orig.d/ . If you are using Docker to launch the container, this is done with the command line flag -v /path/to/token:/etc/condor/tokens-orig.d/flock.opensciencegrid.org . Replace /path/to/token with the full path to the token you obtained from the OSP administrators. Set GLIDEIN_Site and GLIDEIN_ResourceName to match the site name and resource name that you registered in Topology, respectively. Set the OSG_SQUID_LOCATION environment variable to the HTTP address of your preferred Squid instance. Recommended: Enable CVMFS via one of the mechanisms described below. Strongly recommended: If you want job I/O to be done in a separate directory outside of the container, volume mount the desired directory on the host to /pilot inside the container. Without this, user jobs may compete for disk space with other containers on your system. If you are using Docker to launch the container, this is done with the command line flag -v /worker-temp-dir:/pilot . Replace /worker-temp-dir with a directory you created for jobs to write into. Make sure the user you run your container as has write access to this directory. Optional: add an expression with the GLIDEIN_Start_Extra environment variable to append to the HTCondor START expression; this limits the pilot to only run certain jobs. Here is an example invocation using docker run by hand: docker run -it --rm --user osg \\ --privileged \\ -v /path/to/token:/etc/condor/tokens-orig.d/flock.opensciencegrid.org \\ -v /worker-temp-dir:/pilot \\ -e GLIDEIN_Site=\"...\" \\ -e GLIDEIN_ResourceName=\"...\" \\ -e GLIDEIN_Start_Extra=\"True\" \\ -e OSG_SQUID_LOCATION=\"...\" \\ -e CVMFSEXEC_REPOS=\" \\ oasis.opensciencegrid.org \\ singularity.opensciencegrid.org\" \\ opensciencegrid/osgvo-docker-pilot:release Replace /path/to/token with the location you saved the token obtained from the OSP administrators. Privileged mode ( --privileged ) requested in the above docker run allows the container to mount CVMFS using cvmfsexec and invoke singularity for user jobs. Singularity allows OSP users to use their own container for their job (e.g., a common use case for GPU jobs).","title":"Running the Container with Docker"},{"location":"resource-sharing/os-backfill-containers/#cvmfs","text":"CernVM-FS (CVMFS) is a read-only remote filesystem that many OSG jobs depend on for software and data. Supporting CVMFS inside your container will greatly increase the types of OSG jobs you can run. There are two methods for making CVMFS available in your container: enabling cvmfsexec , or bind-mounting CVMFS from the host . Bind-mounting CVMFS will require CVMFS to be installed on the host first, but the container will need fewer privileges.","title":"CVMFS"},{"location":"resource-sharing/os-backfill-containers/#adding-cvmfs-using-cvmfsexec","text":"cvmfsexec is a tool that can be used to mount CVMFS inside the container without requiring CVMFS to be installed on the host. To enable cvmfsexec, specify a space-separated list of repos in the CVMFSEXEC_REPOS environment variable. We recommend the following repos: - oasis.opensciencegrid.org - singularity.opensciencegrid.org cvmfsexec has the following system requirements: On EL7, you must have kernel version >= 3.10.0-1127 (run uname -vr to check), and user namespaces enabled. See step 1 in the Singularity Install document for details. On EL8, you must have kernel version >= 4.18 (run uname -vr to check). See the cvmfsexec README details. Note that cvmfsexec will not be run if CVMFS repos are already available in /cvmfs via bind-mount, regardless of the value of CVMFSEXEC_REPOS . Using cvmfsexec takes place in the entrypoint, which means it will still happen even if you specify a different command to run, such as bash . You can bypass the entrypoint by passing --entrypoint <cmd> where <cmd> is some different command to run, e.g. --entrypoint bash . Setting the entrypoint this way clears the command.","title":"Adding CVMFS using cvmfsexec"},{"location":"resource-sharing/os-backfill-containers/#additional-cvmfsexec-configuration","text":"There are several environment variables you can set for cvmfsexec: CVMFSEXEC_REPOS - this is a space-separated list of CVMFS repos to mount. Leave this blank to disable cvmfsexec. OSG jobs frequently use the OASIS repo ( oasis.opensciencegrid.org ) and the singularity repo ( singularity.opensciencegrid.org ). CVMFS_HTTP_PROXY - this sets the proxy to use for CVMFS; if left blank it will find the best one via WLCG Web Proxy Auto Discovery. CVMFS_QUOTA_LIMIT - the quota limit in MB for CVMFS; leave this blank to use the system default (4 GB) You can add other CVMFS options by bind-mounting a config file over /cvmfsexec/default.local ; note that options in environment variables take precedence over options in /cvmfsexec/default.local . You can store the cache outside of the container by volume mounting a directory to /cvmfs-cache . You can store the logs outside of the container by volume mounting a directory to /cvmfs-logs .","title":"Additional cvmfsexec configuration"},{"location":"resource-sharing/os-backfill-containers/#adding-cvmfs-via-bind-mount","text":"As an alternative to using cvmfsexec, you may install CVMFS on the host, and volume mount it into the container. This will let you avoid running the container in privileged mode. However, supporting Singularity jobs inside the container will require extra privileges, namely the capabilities DAC_OVERRIDE , DAC_READ_SEARCH , SETGID , SETUID , SYS_ADMIN , SYS_CHROOT , and SYS_PTRACE . Follow the installing CVMFS document to install CVMFS on the host. Once you have CVMFS installed and mounted on your host, add -v /cvmfs:/cvmfs:shared to your docker run invocation. This is the example at the top of the page , modified to volume mount CVMFS instead of using cvmfsexec, and using reduced privileges: docker run -it --rm --user osg \\ --cap-add DAC_OVERRIDE \\ --cap-add DAC_READ_SEARCH \\ --cap-add SETUID \\ --cap-add SETGID \\ --cap-add SYS_ADMIN \\ --cap-add SYS_CHROOT \\ --cap-add SYS_PTRACE \\ -v /cvmfs:/cvmfs:shared \\ -v /path/to/token:/etc/condor/tokens-orig.d/flock.opensciencegrid.org \\ -v /worker-temp-dir:/pilot \\ -e GLIDEIN_Site=\"...\" \\ -e GLIDEIN_ResourceName=\"...\" \\ -e GLIDEIN_Start_Extra=\"True\" \\ -e OSG_SQUID_LOCATION=\"...\" \\ opensciencegrid/osgvo-docker-pilot:release Fill in the values for /path/to/token , /worker-temp-dir , GLIDEIN_Site , GLIDEIN_ResourceName , and OSG_SQUID_LOCATION as above .","title":"Adding CVMFS via bind-mount"},{"location":"resource-sharing/overview/","text":"Compute Resource Sharing Overview \u00b6 OSG uses a resource-overlay (\"pilot\") model to share resources from your local cluster: compute resources are added to a large central resource pool in the OSG through the use of a bootstrap process, often called a pilot or a glidein . These pilots, in turn, download and execute user jobs from an OSG community (also known as \"payloads\") from the resource pool to run within the pilots. On OSG, there are several resource pools, one for each large community (such as ATLAS or CMS) and the special-purpose Open Science Pool . The latter focuses on aggregating resources together for small researcher-driven groups and is operated by the OSG itself. There are several ways pilots can join a resource pool: Submitted to your local batch system by a compute entrypoint (CE). These jobs are created by an external entity, a pilot factory based on observed demand in the pool. The CE is the most common way to receive pilot jobs since they integrate with automated processes that are responsive to existing demand. Sites can launch pilot containers when they have local resources they would like to contribute directly to a specific OSG pool. The site-launched pilot container method is useful for backfilling resources without the need for a batch system; however, at times these pilots may stay idle because there is insufficient demand within the resource pool. Users can launch personal pilot containers within a site's batch system so they can use an existing share or allocation at a site through the open science pool.","title":"Overview"},{"location":"resource-sharing/overview/#compute-resource-sharing-overview","text":"OSG uses a resource-overlay (\"pilot\") model to share resources from your local cluster: compute resources are added to a large central resource pool in the OSG through the use of a bootstrap process, often called a pilot or a glidein . These pilots, in turn, download and execute user jobs from an OSG community (also known as \"payloads\") from the resource pool to run within the pilots. On OSG, there are several resource pools, one for each large community (such as ATLAS or CMS) and the special-purpose Open Science Pool . The latter focuses on aggregating resources together for small researcher-driven groups and is operated by the OSG itself. There are several ways pilots can join a resource pool: Submitted to your local batch system by a compute entrypoint (CE). These jobs are created by an external entity, a pilot factory based on observed demand in the pool. The CE is the most common way to receive pilot jobs since they integrate with automated processes that are responsive to existing demand. Sites can launch pilot containers when they have local resources they would like to contribute directly to a specific OSG pool. The site-launched pilot container method is useful for backfilling resources without the need for a batch system; however, at times these pilots may stay idle because there is insufficient demand within the resource pool. Users can launch personal pilot containers within a site's batch system so they can use an existing share or allocation at a site through the open science pool.","title":"Compute Resource Sharing Overview"},{"location":"resource-sharing/user-containers/","text":"User-launched Containers with Singularity \u00b6 The OSG pilot container can be launched by users in order to run jobs on resources they have access to. The most common use case, documented here, is to start the pilot container inside a Slurm batch job that is launched by the user. This is a great way to add personal resources to the Open Science Pool to increase throughput for a specific workflow. Before Starting \u00b6 In order to configure the container, you will need: A registered resource in OSG Topology; resource registration allows OSG to do proper usage accounting and maintain contacts in case of security incidents. An authentication token from the OSG. Please contact OSG support to request a token for your user. Launching Inside Slurm \u00b6 To launch inside Slurm, one needs to write a small job control script; the details will vary from site-to-site and the followingg is given as an example for running on compute hosts with 24 cores: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/bin/bash #SBATCH --job-name=osg-glidein #SBATCH -p compute #SBATCH -N 1 #SBATCH -n 24 #SBATCH -t 48:00:00 #SBATCH --output=osg-glidein-%j.log export TOKEN = \"put_your_provided_token_here\" # Set this so that the OSG accouting knows where the jobs ran export GLIDEIN_Site = \"SDSC\" export GLIDEIN_ResourceName = \"Comet\" # This is an important setting limiting what jobs your glideins will accept. # At the minimum, the expression should limit the \"Owner\" of the jobs to # whatever your username is on the OSG _submit_ side export GLIDEIN_Start_Extra = \"Owner == \\\"my_osgconnect_username\\\"\" module load singularity singularity run --contain --bind /cvmfs --scratch /pilot docker://opensciencegrid/osgvo-docker-pilot:release The above example rebuilds the Docker container on each host. If you plan to run large numbers of these jobs, you can download the Docker container once and create a local Singularity image: $ singularity build osgvo-pilot.sif docker://opensciencegrid/osgvo-docker-pilot:release In this case, the singularity run command should be changed to: singularity run --contain --bind /cvmfs --scratch /pilot osgvo-pilot.sif Note, unlike the site-launched container , the Singularity container above cannot run payloads inside a separate image.","title":"User Allocations"},{"location":"resource-sharing/user-containers/#user-launched-containers-with-singularity","text":"The OSG pilot container can be launched by users in order to run jobs on resources they have access to. The most common use case, documented here, is to start the pilot container inside a Slurm batch job that is launched by the user. This is a great way to add personal resources to the Open Science Pool to increase throughput for a specific workflow.","title":"User-launched Containers with Singularity"},{"location":"resource-sharing/user-containers/#before-starting","text":"In order to configure the container, you will need: A registered resource in OSG Topology; resource registration allows OSG to do proper usage accounting and maintain contacts in case of security incidents. An authentication token from the OSG. Please contact OSG support to request a token for your user.","title":"Before Starting"},{"location":"resource-sharing/user-containers/#launching-inside-slurm","text":"To launch inside Slurm, one needs to write a small job control script; the details will vary from site-to-site and the followingg is given as an example for running on compute hosts with 24 cores: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #!/bin/bash #SBATCH --job-name=osg-glidein #SBATCH -p compute #SBATCH -N 1 #SBATCH -n 24 #SBATCH -t 48:00:00 #SBATCH --output=osg-glidein-%j.log export TOKEN = \"put_your_provided_token_here\" # Set this so that the OSG accouting knows where the jobs ran export GLIDEIN_Site = \"SDSC\" export GLIDEIN_ResourceName = \"Comet\" # This is an important setting limiting what jobs your glideins will accept. # At the minimum, the expression should limit the \"Owner\" of the jobs to # whatever your username is on the OSG _submit_ side export GLIDEIN_Start_Extra = \"Owner == \\\"my_osgconnect_username\\\"\" module load singularity singularity run --contain --bind /cvmfs --scratch /pilot docker://opensciencegrid/osgvo-docker-pilot:release The above example rebuilds the Docker container on each host. If you plan to run large numbers of these jobs, you can download the Docker container once and create a local Singularity image: $ singularity build osgvo-pilot.sif docker://opensciencegrid/osgvo-docker-pilot:release In this case, the singularity run command should be changed to: singularity run --contain --bind /cvmfs --scratch /pilot osgvo-pilot.sif Note, unlike the site-launched container , the Singularity container above cannot run payloads inside a separate image.","title":"Launching Inside Slurm"},{"location":"security/certificate-management/","text":"Managing Certificates \u00b6 The OSG provides several tools to assist in the management of host and CA certificates. This page serves as a reference guide for several of these tools: osg-pki-tools : command line tools for requesting and managing user and host certificates. osg-ca-certs-updater : A package for auto-updating CAs on a server host. osg-ca-manage : A tool for detailed management of CA directories outside RPMs. Note This is a reference document and not introduction on how to install CA certificates or request host / user certificates. Most users will want the CA overview , host certificate overview , or user certificate overview documents. OSG PKI Command Line Clients \u00b6 Overview \u00b6 The OSG PKI Command Line Clients provide a command-line interface for creating certificate signing requests (CSRs). Prerequisites \u00b6 If you have not already done so, you need to configure the OSG software repositories . Installation \u00b6 The command-line scripts have been packaged as an RPM and are available from the OSG repositories. To install the RPM, run: root@host # yum install osg-pki-tools Usage \u00b6 Documentation for usage of the osg-pki-tools can be found here OSG CA Certificates Updater \u00b6 This section explains the installation and use of osg-ca-certs-updater , a package that provides automatic updates of CA certificates. Requirements \u00b6 As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates Install instructions \u00b6 Run the following command to install the latest version of the updater. root@host# yum install osg-ca-certs-updater Services \u00b6 Starting and Enabling Services \u00b6 Run the following to enable the updater. This will persist until the machine is rebooted. root@host# service osg-ca-certs-updater-cron start Run the following to enable the updater when the machine is rebooted. root@host# chkconfig osg-ca-certs-updater-cron on Run both commands if you wish for the service to activate immediately and remain active throughout reboots. Stopping and Disabling Services \u00b6 Enter the following to disable the updater. This will persist until the machine is rebooted. root@host# service osg-ca-certs-updater-cron stop Enter the following to disable the updater when the machine is rebooted. root@host# chkconfig osg-ca-certs-updater-cron off Run both commands if you wish for the service to deactivate immediately and not get reactivated during reboots. Configuration \u00b6 While there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the cron entry of the service. This entry is located in the file /etc/cron.d/osg-ca-certs-updater . Please see the Unix manual page for crontab in section 5 for an explanation of the format. The manual page can be accessed by the command man 5 crontab . The valid command-line arguments can be listed by running osg-ca-certs-updater --help . Reasonable defaults have been provided, namely: Attempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day. Run the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day. Delay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories. Do not warn the administrator about update failures that have happened less than 72 hours since the last successful update. Log errors only. Troubleshooting \u00b6 Useful configuration and log files \u00b6 Configuration file \u00b6 Package File Description Location Comment osg-ca-certs-updater Cron entry for periodically launching the updater /etc/cron.d/osg-ca-certs-updater Command-line arguments to the updater can be specified here osg-release Repo definition files for production OSG repositories /etc/yum.repos.d/osg.repo Make sure these repositories are enabled and reachable from the host you are trying to update Log files \u00b6 Logging is performed to the console by default. Please see the manual for your cron daemon to find out how it handles console output. A logfile can be specified via the -l / --logfile command-line option. If logging to syslog via the -s / --log-to-syslog option, the updater will write to the user section of the syslog. The file /etc/syslog.conf determines where syslog messages are saved. References \u00b6 Some guides on X.509 certificates: Useful commands: http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html Install GSI authentication on a server: http://security.ncsa.illinois.edu/research/wssec/gsihttps/ Certificates how-to: http://www.nordugrid.org/documents/certificate_howto.html See this page for examples of verifying certificates. Managing CAs \u00b6 The osg-ca-manage tool provides a unified interface to manage the CA Certificate installations. This page provides the instructions on using this command. It provides status commands that allows you to list the CAs and the validity of the CAs and CRLs included in the installation. The manage commands allow you to fetch CAs and CRLs, change the distribution URL, as well as add and remove CAs from your local installation. Usage examples \u00b6 Documentation for usage of the osg-ca-manage tool can be found here Note These commands will not work if of the osg-ca-certs (or igtf-ca-certs) RPM packages are installed. Install a certificate authority package \u00b6 Before you proceed to install a Certificate Authority Package you should decide which of the available packages to install. osg , the package recommended to be used by production resources on the OSG. It is based on the CA distribution from the IGTF, but it may differ slightly as decided by the Security Team . igtf , the package is a redistribution of the unchanged CA distribution from the IGTF url a package provided at a given URL Note If in doubt, please consult the policies of your home institution and get in contact with the Security Team . Next decide at what location to install the Certificate Authority Package: on the root file system in a system directory /etc/grid-security/certificates in a custom directory that can also be shared Setup the CA certificates \u00b6 The Certificate Authority Package is preferably be used by grid users without root privileges or if the CA certificates will not be shared by other installations on the same host. root@host # osg-ca-manage setupca --location root --url osg Setting CA Certificates for at '/etc/grid-security/certificates' Setup completed successfully. After a successful installation the certificates will be installed in ( /etc/grid-security/certificates in this example). Typically to write into this default location you will need root privileges. If you need to need to install it with out root privileges use user@host $ osg-ca-manage setupca --location $HOME /certificates --url osg Setting CA Certificates for at '$HOME/certificates' Setup completed successfully. Adding a directory of local CAs \u00b6 root@host # osg-ca-manage add --cadir /etc/grid-security/localca NOTE: You did not specify the --auto-refresh flag. So the changes made to the configuration will not be reflected till the next time when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron. Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately. Here is the resulting file after add ##cat /etc/osg/osg-update-certs.conf # Configuration file for osg-update-certs # This file has been regenerated by osg-ca-manage, which removes most # comments. You can still manually modify it, any manual change will # be preserved if osg-ca-manage is used again. ## The parent location certificates will be installed at. install_dir = /etc/grid-security ## cacerts_url is the URL of your certificate distribution cacerts_url = https://repo.opensciencegrid.org/cadist/ca-certs-version-igtf-new ## log specifies where logging output will go log = /var/log/osg-update-certs.log ## include specifies files (full pathnames) that should be copied ## into the certificates installation after an update has occured. include=/etc/grid-security/localca/* ## exclude_ca specifies a CA (not full pathnames, but just the hash ## of the CA you want to exclude) that should be removed from the ## certificates installation after an update has occured. debug = 0 Removing a directory of local CAs \u00b6 root@host # osg-ca-manage remove --cadir /etc/grid-security/localca NOTE: You did not specify the --auto-refresh flag. So the changes made to the configuration will not be reflected till the next time when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron. Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately. Removing a particular CA included in OSG CA package \u00b6 root@host # osg-ca-manage remove --caname ce33db76 Symlink detected for hash: We have determided that the hash value you entered belong to the CA 'IRAN-GRID.pem'. If you wish to add this CA back you will have to use this name is the parameter. NOTE: You did not specify the --auto-refresh flag. So the changes made to the configuration will not be reflected till the next time when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron. Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately. The resulting config file after the remove is as follows ##cat /etc/osg/osg-update-certs.conf # Configuration file for osg-update-certs # This file has been regenerated by osg-ca-manage, which removes most # comments. You can still manually modify it, any manual change will # be preserved if osg-ca-manage is used again. ## The parent location certificates will be installed at. install_dir = /etc/grid-security ## cacerts_url is the URL of your certificate distribution cacerts_url = https://repo.opensciencegrid.org/cadist/ca-certs-version-igtf-new ## log specifies where logging output will go log = /var/log/osg-update-certs.log ## include specifies files (full pathnames) that should be copied ## into the certificates installation after an update has occured. ## exclude_ca specifies a CA (not full pathnames, but just the hash ## of the CA you want to exclude) that should be removed from the ## certificates installation after an update has occured. exclude_ca = IRAN-GRID debug = 0 Adding a CA from the OSG CA package \u00b6 root@host # osg-ca-manage add --caname IRAN-GRID NOTE: You did not specify the --auto-refresh flag. So the changes made to the configuration will not be reflected till the next time when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron. Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately. Inspect installed CA certificates \u00b6 You can inspect the list of CA Certificates that have been installed: user@host $ osg-ca-manage listCA Hash=09ff08b7; Subject= /C=FR/O=CNRS/CN=CNRS2-Projets; Issuer= /C=FR/O=CNRS/CN=CNRS2; Accreditation=Unknown; Status=https://repo.opensciencegrid.org/cadist/ca-certs-version-new Hash=0a12b607; Subject= /DC=org/DC=ugrid/CN=UGRID CA; Issuer= /DC=org/DC=ugrid/CN=UGRID CA; Accreditation=Unknown; Status=https://repo.opensciencegrid.org/cadist/ca-certs-version-new [...] Any certificate issued by any of the Certificate Authorities listed will be trusted. If in doubt please contact the OSG Security Team and review the policies of your home institution. Troubleshooting \u00b6 Useful configuration and log files \u00b6 Logs and configuration: File Description Location Comment Configuration File for osg-update-certs /etc/osg/osg-update-certs.conf This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters. Log file of osg-update-certs /var/log/osg-update-certs.log Stdout of osg-update-certs /var/log/osg-ca-certs-status.system.out Stdout of osg-ca-manage /var/log/osg-ca-manage.system.out Stdout of initial CA setup /var/log/osg-setup-ca-certificates.system.out References \u00b6 Installing the Certificate Authorities Certificates and the related RPMs","title":"Certificate Management Reference"},{"location":"security/certificate-management/#managing-certificates","text":"The OSG provides several tools to assist in the management of host and CA certificates. This page serves as a reference guide for several of these tools: osg-pki-tools : command line tools for requesting and managing user and host certificates. osg-ca-certs-updater : A package for auto-updating CAs on a server host. osg-ca-manage : A tool for detailed management of CA directories outside RPMs. Note This is a reference document and not introduction on how to install CA certificates or request host / user certificates. Most users will want the CA overview , host certificate overview , or user certificate overview documents.","title":"Managing Certificates"},{"location":"security/certificate-management/#osg-pki-command-line-clients","text":"","title":"OSG PKI Command Line Clients"},{"location":"security/certificate-management/#overview","text":"The OSG PKI Command Line Clients provide a command-line interface for creating certificate signing requests (CSRs).","title":"Overview"},{"location":"security/certificate-management/#prerequisites","text":"If you have not already done so, you need to configure the OSG software repositories .","title":"Prerequisites"},{"location":"security/certificate-management/#installation","text":"The command-line scripts have been packaged as an RPM and are available from the OSG repositories. To install the RPM, run: root@host # yum install osg-pki-tools","title":"Installation"},{"location":"security/certificate-management/#usage","text":"Documentation for usage of the osg-pki-tools can be found here","title":"Usage"},{"location":"security/certificate-management/#osg-ca-certificates-updater","text":"This section explains the installation and use of osg-ca-certs-updater , a package that provides automatic updates of CA certificates.","title":"OSG CA Certificates Updater"},{"location":"security/certificate-management/#requirements","text":"As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Requirements"},{"location":"security/certificate-management/#install-instructions","text":"Run the following command to install the latest version of the updater. root@host# yum install osg-ca-certs-updater","title":"Install instructions"},{"location":"security/certificate-management/#services","text":"","title":"Services"},{"location":"security/certificate-management/#starting-and-enabling-services","text":"Run the following to enable the updater. This will persist until the machine is rebooted. root@host# service osg-ca-certs-updater-cron start Run the following to enable the updater when the machine is rebooted. root@host# chkconfig osg-ca-certs-updater-cron on Run both commands if you wish for the service to activate immediately and remain active throughout reboots.","title":"Starting and Enabling Services"},{"location":"security/certificate-management/#stopping-and-disabling-services","text":"Enter the following to disable the updater. This will persist until the machine is rebooted. root@host# service osg-ca-certs-updater-cron stop Enter the following to disable the updater when the machine is rebooted. root@host# chkconfig osg-ca-certs-updater-cron off Run both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.","title":"Stopping and Disabling Services"},{"location":"security/certificate-management/#configuration","text":"While there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the cron entry of the service. This entry is located in the file /etc/cron.d/osg-ca-certs-updater . Please see the Unix manual page for crontab in section 5 for an explanation of the format. The manual page can be accessed by the command man 5 crontab . The valid command-line arguments can be listed by running osg-ca-certs-updater --help . Reasonable defaults have been provided, namely: Attempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day. Run the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day. Delay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories. Do not warn the administrator about update failures that have happened less than 72 hours since the last successful update. Log errors only.","title":"Configuration"},{"location":"security/certificate-management/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"security/certificate-management/#useful-configuration-and-log-files","text":"","title":"Useful configuration and log files"},{"location":"security/certificate-management/#configuration-file","text":"Package File Description Location Comment osg-ca-certs-updater Cron entry for periodically launching the updater /etc/cron.d/osg-ca-certs-updater Command-line arguments to the updater can be specified here osg-release Repo definition files for production OSG repositories /etc/yum.repos.d/osg.repo Make sure these repositories are enabled and reachable from the host you are trying to update","title":"Configuration file"},{"location":"security/certificate-management/#log-files","text":"Logging is performed to the console by default. Please see the manual for your cron daemon to find out how it handles console output. A logfile can be specified via the -l / --logfile command-line option. If logging to syslog via the -s / --log-to-syslog option, the updater will write to the user section of the syslog. The file /etc/syslog.conf determines where syslog messages are saved.","title":"Log files"},{"location":"security/certificate-management/#references","text":"Some guides on X.509 certificates: Useful commands: http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html Install GSI authentication on a server: http://security.ncsa.illinois.edu/research/wssec/gsihttps/ Certificates how-to: http://www.nordugrid.org/documents/certificate_howto.html See this page for examples of verifying certificates.","title":"References"},{"location":"security/certificate-management/#managing-cas","text":"The osg-ca-manage tool provides a unified interface to manage the CA Certificate installations. This page provides the instructions on using this command. It provides status commands that allows you to list the CAs and the validity of the CAs and CRLs included in the installation. The manage commands allow you to fetch CAs and CRLs, change the distribution URL, as well as add and remove CAs from your local installation.","title":"Managing CAs"},{"location":"security/certificate-management/#usage-examples","text":"Documentation for usage of the osg-ca-manage tool can be found here Note These commands will not work if of the osg-ca-certs (or igtf-ca-certs) RPM packages are installed.","title":"Usage examples"},{"location":"security/certificate-management/#install-a-certificate-authority-package","text":"Before you proceed to install a Certificate Authority Package you should decide which of the available packages to install. osg , the package recommended to be used by production resources on the OSG. It is based on the CA distribution from the IGTF, but it may differ slightly as decided by the Security Team . igtf , the package is a redistribution of the unchanged CA distribution from the IGTF url a package provided at a given URL Note If in doubt, please consult the policies of your home institution and get in contact with the Security Team . Next decide at what location to install the Certificate Authority Package: on the root file system in a system directory /etc/grid-security/certificates in a custom directory that can also be shared","title":"Install a certificate authority package"},{"location":"security/certificate-management/#setup-the-ca-certificates","text":"The Certificate Authority Package is preferably be used by grid users without root privileges or if the CA certificates will not be shared by other installations on the same host. root@host # osg-ca-manage setupca --location root --url osg Setting CA Certificates for at '/etc/grid-security/certificates' Setup completed successfully. After a successful installation the certificates will be installed in ( /etc/grid-security/certificates in this example). Typically to write into this default location you will need root privileges. If you need to need to install it with out root privileges use user@host $ osg-ca-manage setupca --location $HOME /certificates --url osg Setting CA Certificates for at '$HOME/certificates' Setup completed successfully.","title":"Setup the CA certificates"},{"location":"security/certificate-management/#adding-a-directory-of-local-cas","text":"root@host # osg-ca-manage add --cadir /etc/grid-security/localca NOTE: You did not specify the --auto-refresh flag. So the changes made to the configuration will not be reflected till the next time when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron. Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately. Here is the resulting file after add ##cat /etc/osg/osg-update-certs.conf # Configuration file for osg-update-certs # This file has been regenerated by osg-ca-manage, which removes most # comments. You can still manually modify it, any manual change will # be preserved if osg-ca-manage is used again. ## The parent location certificates will be installed at. install_dir = /etc/grid-security ## cacerts_url is the URL of your certificate distribution cacerts_url = https://repo.opensciencegrid.org/cadist/ca-certs-version-igtf-new ## log specifies where logging output will go log = /var/log/osg-update-certs.log ## include specifies files (full pathnames) that should be copied ## into the certificates installation after an update has occured. include=/etc/grid-security/localca/* ## exclude_ca specifies a CA (not full pathnames, but just the hash ## of the CA you want to exclude) that should be removed from the ## certificates installation after an update has occured. debug = 0","title":"Adding a directory of local CAs"},{"location":"security/certificate-management/#removing-a-directory-of-local-cas","text":"root@host # osg-ca-manage remove --cadir /etc/grid-security/localca NOTE: You did not specify the --auto-refresh flag. So the changes made to the configuration will not be reflected till the next time when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron. Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.","title":"Removing a directory of local CAs"},{"location":"security/certificate-management/#removing-a-particular-ca-included-in-osg-ca-package","text":"root@host # osg-ca-manage remove --caname ce33db76 Symlink detected for hash: We have determided that the hash value you entered belong to the CA 'IRAN-GRID.pem'. If you wish to add this CA back you will have to use this name is the parameter. NOTE: You did not specify the --auto-refresh flag. So the changes made to the configuration will not be reflected till the next time when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron. Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately. The resulting config file after the remove is as follows ##cat /etc/osg/osg-update-certs.conf # Configuration file for osg-update-certs # This file has been regenerated by osg-ca-manage, which removes most # comments. You can still manually modify it, any manual change will # be preserved if osg-ca-manage is used again. ## The parent location certificates will be installed at. install_dir = /etc/grid-security ## cacerts_url is the URL of your certificate distribution cacerts_url = https://repo.opensciencegrid.org/cadist/ca-certs-version-igtf-new ## log specifies where logging output will go log = /var/log/osg-update-certs.log ## include specifies files (full pathnames) that should be copied ## into the certificates installation after an update has occured. ## exclude_ca specifies a CA (not full pathnames, but just the hash ## of the CA you want to exclude) that should be removed from the ## certificates installation after an update has occured. exclude_ca = IRAN-GRID debug = 0","title":"Removing a particular CA included in OSG CA package"},{"location":"security/certificate-management/#adding-a-ca-from-the-osg-ca-package","text":"root@host # osg-ca-manage add --caname IRAN-GRID NOTE: You did not specify the --auto-refresh flag. So the changes made to the configuration will not be reflected till the next time when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron. Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.","title":"Adding a CA from the OSG CA package"},{"location":"security/certificate-management/#inspect-installed-ca-certificates","text":"You can inspect the list of CA Certificates that have been installed: user@host $ osg-ca-manage listCA Hash=09ff08b7; Subject= /C=FR/O=CNRS/CN=CNRS2-Projets; Issuer= /C=FR/O=CNRS/CN=CNRS2; Accreditation=Unknown; Status=https://repo.opensciencegrid.org/cadist/ca-certs-version-new Hash=0a12b607; Subject= /DC=org/DC=ugrid/CN=UGRID CA; Issuer= /DC=org/DC=ugrid/CN=UGRID CA; Accreditation=Unknown; Status=https://repo.opensciencegrid.org/cadist/ca-certs-version-new [...] Any certificate issued by any of the Certificate Authorities listed will be trusted. If in doubt please contact the OSG Security Team and review the policies of your home institution.","title":"Inspect installed CA certificates"},{"location":"security/certificate-management/#troubleshooting_1","text":"","title":"Troubleshooting"},{"location":"security/certificate-management/#useful-configuration-and-log-files_1","text":"Logs and configuration: File Description Location Comment Configuration File for osg-update-certs /etc/osg/osg-update-certs.conf This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters. Log file of osg-update-certs /var/log/osg-update-certs.log Stdout of osg-update-certs /var/log/osg-ca-certs-status.system.out Stdout of osg-ca-manage /var/log/osg-ca-manage.system.out Stdout of initial CA setup /var/log/osg-setup-ca-certificates.system.out","title":"Useful configuration and log files"},{"location":"security/certificate-management/#references_1","text":"Installing the Certificate Authorities Certificates and the related RPMs","title":"References"},{"location":"security/host-certs/","text":"Page moved to Host Certificates .","title":"Host certs"},{"location":"security/lcmaps-voms-authentication/","text":"Installing and Maintaining the LCMAPS VOMS Plugin \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. LCMAPS is a software library used on HTCondor-CE , GridFTP , and XRootD hosts for mapping grid certificates of incoming connections to specific Unix accounts. The LCMAPS VOMS plugin enables LCMAPS to make mapping decisions based on the VOMS attributes of grid certificates, e.g. /cms/Role=production/Capability=NULL . The OSG provides a default set of mappings from VOMS attributes to Unix accounts. By configuring LCMAPS, you can override these mappings, including changing the Unix account that a VO is mapped to; adding custom mappings for specific users and VOMS attributes; and/or banning specific users and VOMS attributes. Use this page to learn how to install and configure the LCMAPS VOMS plugin to authenticate users to access your resources on a per-VO basis. Installing the LCMAPS VOMS Plugin \u00b6 To install the LCMAPS VOMS plugin, make sure that your host is up to date before installing the required packages: Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install lcmaps , the default mapfile, and the configuration tools: root@host # yum install lcmaps vo-client-lcmaps-voms osg-configure-misc Configuring the LCMAPS VOMS Plugin \u00b6 The following section describes the steps required to configure the LCMAPS VOMS plugin for authentication. Additionally, there are optional configuration instructions if you need to make changes to the default mappings. Supporting mapped VOs and users \u00b6 Ensure Unix accounts exist for each VO, VO role, VO group, or user you choose to support in the mapfiles : Consult the default VO mappings in /usr/share/osg/voms-mapfile-default to determine the mapped Unix account names. Each of the mapfiles has the following format: \"<VO, VO role, VO group or user>\" <Unix account> Create Unix accounts for each VO, VO role, VO group, and user that you wish to support. The full list of VOs is located in the OSG topology . You are not expected to support all the VOs. If you would like to support opportunistic usage, we recommend creating the following Unix accounts: VO name Unix account(s) GLOW glow OSG osg ATLAS usatlas3 CMS cmsuser Fermilab fnalgrid HCC hcc Gluex gluex Edit /etc/osg/config.d/30-gip.ini and specify the supported VOs per Subcluster or ResourceEntry section : allowed_vos = \"VO1,VO2...\" Applying configuration settings \u00b6 Making changes to the OSG configuration files in the /etc/osg/config.d directory does not apply those settings to software automatically. For the OSG settings, use the osg-configure tool to validate (to a limited extent) and apply the settings to the relevant software components. If instead you wish to manage the LCMAPS VOMS plugin configuration yourself, skip to the manual configuration section . Make all changes to .ini files in the /etc/osg/config.d directory. Note This document only describes the critical settings for the LCMAPS VOMS plugin and related software. You may need to configure other software that is installed on your host, too. Validate the configuration settings: root@host # osg-configure -v Once the validation command succeeds without errors, apply the configuration settings: root@host # osg-configure -c Optional configuration \u00b6 The following subsections contain information on mapping or banning users by their certificates' Distinguished Names (DNs) or by their proxies' VOMS attributes. Any optional configuration is to be performed after the installation and configuration sections above. For a table of the configuration files and their order of evaluation, consult the reference section . Mapping VOs Mapping users Banning VOs Banning users Mapping using all FQANs Mapping VOs \u00b6 To map VOs, VO roles, or VO groups to Unix accounts based on their VOMS attributes, create /etc/grid-security/voms-mapfile . An example of the format of a voms-mapfile follows: # map GLOW jobs in the chtc group to the 'glow1' Unix account. \"/GLOW/chtc/*\" glow1 # map GLOW jobs with the htpc role to the 'glow2' Unix account. \"/GLOW/Role=htpc/*\" glow2 # map other GLOW jobs to the 'glow' Unix account. \"/GLOW/*\" glow Each non-commented line is a shell-style pattern which is compared against the user's VOMS attributes, and a Unix account that the user will be mapped to if the pattern matches. The patterns are compared in the order they are listed in. Therefore, more general patterns should be placed later in the file. Note The Unix account must exist for the user to be mapped. If a VO's Unix account is missing, that VO will not be able to access your resources. Additionally, if you map VOMS attributes to a non-existent user in /etc/grid-security/voms-mapfile , /usr/share/osg/voms-mapfile-default will be considered next to find a mapping. The best way to ban a VO is edit /etc/grid-security/ban-voms-mapfile as described in Banning VOs below. Do not edit voms-mapfile-default as your changes will be overwritten upon updates. Mapping users \u00b6 To map specific users to Unix accounts based on their certificates' DNs, create /etc/grid-security/grid-mapfile . An example of the format of a grid-mapfile follows: # map Matyas's FNAL DN to the 'matyas' Unix account \"/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\" matyas Note The Unix account must exist for the user to be mapped. If a user's Unix account is missing, that user will not be able to access your resources. Banning VOs \u00b6 /etc/grid-security/ban-voms-mapfile is used to ban an entire VO or a role withing a VO from accessing resources on your machine. An example of the format of a ban-voms-mapfile follows: # ban CMS production jobs \"/cms/Role=production/*\" Each non-commented line is a shell-style pattern which is compared against a user's VOMS attributes. If the pattern matches, that user will be unable to access your resources. Danger When banning VOs, you must restart the services using LCMAPS VOMS authentication (e.g. condor-ce , globus-gridftp-server , xrootd , etc.) to clear any authentication caches. In the case of XRootD when the service is not restarted the change could take up to 12hrs to take effect. This can be modified by defining the authzto option in the sec.protocol configuration attribute, e.g.: sec.protocol /usr/lib64 gsi \\ -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrd/xrdcert.pem \\ ... -authzto:3600 The units of -authzto are in seconds which means that the above will set the LCMAPS cache lifetime to 1hr. Warning /etc/grid-security/ban-voms-mapfile must exist, even if you are not banning any VOs. In that case, the file should not contain any entries. If the file does not exist, LCMAPS will ban every user. Banning users \u00b6 /etc/grid-security/ban-mapfile is used to ban specific users from accessing your resources based on their certificates' DNs. An example of the format of a ban-mapfile follows: # ban Matyas's FNAL DN \"/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\" Danger When banning users, you must restart the services using LCMAPS VOMS authentication (e.g. condor-ce , globus-gridftp-server , xrootd , etc.) to clear any authentication caches. In the case of XRootD when the service is not restarted the change could take up to 12hrs to take effect. This can be modified by defining the authzto option in the sec.protocol configuration attribute, e.g.: sec.protocol /usr/lib64 gsi \\ -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrd/xrdcert.pem \\ ... -authzto:3600 The units of -authzto are in seconds which means that the above will set the LCMAPS cache lifetime to 1hr. Warning /etc/grid-security/ban-mapfile must exist, even if you are not banning any users. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user. Mapping using all FQANs \u00b6 By default, the LCMAPS VOMS plugin only considers the first FQAN of a VOMS proxy for mapping. If you want to consider all FQANs, you must set the appropriate option. If you are using osg-configure, set all_fqans = True in 10-misc.ini , then run osg-configure -c If you are configuring lcmaps.db manually (see manual configuration below), add \"-all-fqans\" to the module definitions for vomsmapfile and defaultmapfile Using the LCMAPS VOMS Plugin \u00b6 LCMAPS is a software library that is called for authentication; therefore, there are no running services and it does not have to be invoked manually. Validating the LCMAPS VOMS Plugin VO Mappings \u00b6 To validate the LCMAPS VOMS plugin by itself, use the following procedure to test mapping your own cert to a user: Verify your DN is not in /etc/grid-security/grid-mapfile , or else it will generate a false positive Verify your DN is not in /etc/grid-security/ban-mapfile , or else it will generate a false negative Install the llrun and voms-clients packages: root@host # yum install llrun voms-clients As an unprivileged user, create a VOMS proxy (filling in <YOUR_VO> with a VO you are a member of): user@host $ voms-proxy-init -voms <YOUR_VO> Verify that your credentials are mapped as expected: user@host $ llrun -s -l mode = pem,policy = authorize_only,db = /etc/lcmaps.db \\ -p/tmp/x509up_u`id -u` If you did not get correctly mapped, check your proxy's FQAN by running: user@host $ voms-proxy-info -fqan and make sure it matches one of the patterns in /etc/grid-security/voms-mapfile or /usr/share/osg/voms-mapfile-default , and does not match any patterns in /etc/grid-security/ban-voms-mapfile . Troubleshooting the LCMAPS VOMS Plugin \u00b6 LCMAPS logs to journalctl and the verbosity of the logging can be increased by modifying the appropriate configuration and restarting the relevant service. This section outlines the configuration necessary to raise the debug level for the different hosts that can use LCMAPS VOMS authentication as well as common LCMAPS VOMS authentication issues. HTCondor-CE hosts \u00b6 If you are troubleshooting an HTCondor-CE host, follow these instructions to raise the LCMAPS debug level: Add the following text to /etc/sysconfig/condor-ce : export LCMAPS_DEBUG_LEVEL = 5 # optional (uncomment the following line to output log messages to a file): # export LCMAPS_LOG_FILE=/tmp/lcmaps.log Disable HTCondor-CE authentication caches by creating /etc/condor-ce/config.d/99-disablegsicache.conf with the following contents: GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION = 0 Restart the condor-ce service Tip After you've completed troubleshooting, remember to revert the changes above and restart services! XRootD hosts \u00b6 If you are troubleshooting an XRootD host, follow these instructions to raise the LCMAPS debug level: Choose the configuration file to edit based on the following table: If you are running XRootD in... Then modify the following file... Standalone mode /etc/xrootd/xrootd-standalone.cfg Clustered mode /etc/xrootd/xrootd-clustered.cfg Set loglevel=5 under the -authzfunparms of the sec.protocol /usr/lib64 gsi line. For example: sec.protocol /usr/lib64 gsi -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrootd/xrootdcert.pem \\ -key:/etc/grid-security/xrootd/xrootdkey.pem \\ -crl:1 \\ -authzfun:libXrdLcmaps.so \\ -authzfunparms:lcmapscfg=/etc/xrootd/lcmaps.cfg,loglevel=5,policy=authorize_only \\ -gmapopt:10 -gmapto:0 Restart the xrootd service Tip After you've completed troubleshooting, remember to revert the changes above and restart services! GridFTP hosts \u00b6 If you are troubleshooting a GridFTP host, follow these instructions to raise the LCMAPS debug level: Add the following text to /etc/sysconfig/globus-gridftp-server : export LCMAPS_DEBUG_LEVEL = 5 # optional (uncomment the following line to output log messages to a file): # export LCMAPS_LOG_FILE=/tmp/lcmaps.log Restart the globus-gridftp-server service. Tip After you've completed troubleshooting, remember to revert the changes above and restart services! Common issues \u00b6 A user/VO still has access to my XRootD server after adding them to the ban files \u00b6 The best way to ensure that a user/VO is immediately banned is to restart the XRootD server after adding the DN or VOMS attributes to the corresponding ban file. If the above is not possible, the the lifetime of the LCMAPS cache for XRootD can be controlled by setting the parameter authzto within the sec.protocol configuration attribute, e.g.: sec.protocol /usr/lib64 gsi \\ -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrd/xrdcert.pem \\ ... -authzto:3600 The units of -authzto are in seconds which means that the above will set the LCMAPS cache lifetime to 1hr. The default value for this parameter is 12hrs. Wrong version of GridFTP \u00b6 If you have the EPEL version of the GridFTP server, you may see error messages in journalctl or the location specified by LCMAPS_LOG_FILE . Symptoms Apr 11 13:51:41 atlas-hub globus-gridftp-server: You are still root after the LCMAPS execution. The implicit root-mapping safety is enabled. See documentation for details Next actions If the versions of the globus-gridftp-server-* packages do not end in osgXX.elY , continue with these instructions. To check the version of your globus-gridftp-server-* , run the following command: user@host $ rpm -qa 'globus-gridftp*' Verify that the priority of the OSG repositories are set properly Clean your yum cache root@host # yum clean all --enablerepo = * Reinstall globus-gridftp-server : root@host # yum update globus-gridftp-server Getting Help \u00b6 To get assistance, please use the this page . Reference \u00b6 Configuration Files \u00b6 The files are evaluated in the following order, with earlier files taking precedence over later ones: File Provider Purpose /etc/grid-security/ban-mapfile Admin Ban DNs /etc/grid-security/ban-voms-mapfile Admin Ban VOs /etc/grid-security/grid-mapfile Admin Map DNs /etc/grid-security/voms-mapfile Admin Map VOs /usr/share/osg/voms-mapfile-default OSG Map VOs (default) Warning /usr/share/osg/voms-mapfile-default is not meant to be edited and will be overwritten on upgrades. All VO mappings can be overridden by editing the above files in /etc/grid-security . Manual Configuration \u00b6 This section is intended for use as reference if you choose to forego configuring the LCMAPS VOMS plugin via osg-configure (i.e., if you prefer a configuration management system like Ansible or Puppet ). Therefore, the following instructions serve as a replacement for this section above. LCMAPS is configured in /etc/lcmaps.db and since the VOMS plugin is a newer component, configuration for it may not be present in your existing /etc/lcmaps.db file. Ensure the following lines are present in the \"Module definitions\" section (the top section, before authorize_only ) of /etc/lcmaps.db : gridmapfile = \"lcmaps_localaccount.mod\" \"-gridmap /etc/grid-security/grid-mapfile\" banfile = \"lcmaps_ban_dn.mod\" \"-banmapfile /etc/grid-security/ban-mapfile\" banvomsfile = \"lcmaps_ban_fqan.mod\" \"-banmapfile /etc/grid-security/ban-voms-mapfile\" vomsmapfile = \"lcmaps_voms_localaccount.mod\" \"-gridmap /etc/grid-security/voms-mapfile\" defaultmapfile = \"lcmaps_voms_localaccount2.mod\" \"-gridmap /usr/share/osg/voms-mapfile-default\" verifyproxynokey = \"lcmaps_verify_proxy2.mod\" \"--allow-limited-proxy\" \"--discard_private_key_absence\" \" -certdir /etc/grid-security/certificates\" Edit the authorize_only section so that it contains only the following uncommented lines: verifyproxynokey -> banfile banfile -> banvomsfile | bad banvomsfile -> gridmapfile | bad gridmapfile -> good | vomsmapfile vomsmapfile -> good | defaultmapfile defaultmapfile -> good | bad Edit /etc/grid-security/gsi-authz.conf and ensure that it contains the following line with a newline at the end: globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout","title":"LCMAPS VOMS authentication"},{"location":"security/lcmaps-voms-authentication/#installing-and-maintaining-the-lcmaps-voms-plugin","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. LCMAPS is a software library used on HTCondor-CE , GridFTP , and XRootD hosts for mapping grid certificates of incoming connections to specific Unix accounts. The LCMAPS VOMS plugin enables LCMAPS to make mapping decisions based on the VOMS attributes of grid certificates, e.g. /cms/Role=production/Capability=NULL . The OSG provides a default set of mappings from VOMS attributes to Unix accounts. By configuring LCMAPS, you can override these mappings, including changing the Unix account that a VO is mapped to; adding custom mappings for specific users and VOMS attributes; and/or banning specific users and VOMS attributes. Use this page to learn how to install and configure the LCMAPS VOMS plugin to authenticate users to access your resources on a per-VO basis.","title":"Installing and Maintaining the LCMAPS VOMS Plugin"},{"location":"security/lcmaps-voms-authentication/#installing-the-lcmaps-voms-plugin","text":"To install the LCMAPS VOMS plugin, make sure that your host is up to date before installing the required packages: Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install lcmaps , the default mapfile, and the configuration tools: root@host # yum install lcmaps vo-client-lcmaps-voms osg-configure-misc","title":"Installing the LCMAPS VOMS Plugin"},{"location":"security/lcmaps-voms-authentication/#configuring-the-lcmaps-voms-plugin","text":"The following section describes the steps required to configure the LCMAPS VOMS plugin for authentication. Additionally, there are optional configuration instructions if you need to make changes to the default mappings.","title":"Configuring the LCMAPS VOMS Plugin"},{"location":"security/lcmaps-voms-authentication/#supporting-mapped-vos-and-users","text":"Ensure Unix accounts exist for each VO, VO role, VO group, or user you choose to support in the mapfiles : Consult the default VO mappings in /usr/share/osg/voms-mapfile-default to determine the mapped Unix account names. Each of the mapfiles has the following format: \"<VO, VO role, VO group or user>\" <Unix account> Create Unix accounts for each VO, VO role, VO group, and user that you wish to support. The full list of VOs is located in the OSG topology . You are not expected to support all the VOs. If you would like to support opportunistic usage, we recommend creating the following Unix accounts: VO name Unix account(s) GLOW glow OSG osg ATLAS usatlas3 CMS cmsuser Fermilab fnalgrid HCC hcc Gluex gluex Edit /etc/osg/config.d/30-gip.ini and specify the supported VOs per Subcluster or ResourceEntry section : allowed_vos = \"VO1,VO2...\"","title":"Supporting mapped VOs and users"},{"location":"security/lcmaps-voms-authentication/#applying-configuration-settings","text":"Making changes to the OSG configuration files in the /etc/osg/config.d directory does not apply those settings to software automatically. For the OSG settings, use the osg-configure tool to validate (to a limited extent) and apply the settings to the relevant software components. If instead you wish to manage the LCMAPS VOMS plugin configuration yourself, skip to the manual configuration section . Make all changes to .ini files in the /etc/osg/config.d directory. Note This document only describes the critical settings for the LCMAPS VOMS plugin and related software. You may need to configure other software that is installed on your host, too. Validate the configuration settings: root@host # osg-configure -v Once the validation command succeeds without errors, apply the configuration settings: root@host # osg-configure -c","title":"Applying configuration settings"},{"location":"security/lcmaps-voms-authentication/#optional-configuration","text":"The following subsections contain information on mapping or banning users by their certificates' Distinguished Names (DNs) or by their proxies' VOMS attributes. Any optional configuration is to be performed after the installation and configuration sections above. For a table of the configuration files and their order of evaluation, consult the reference section . Mapping VOs Mapping users Banning VOs Banning users Mapping using all FQANs","title":"Optional configuration"},{"location":"security/lcmaps-voms-authentication/#mapping-vos","text":"To map VOs, VO roles, or VO groups to Unix accounts based on their VOMS attributes, create /etc/grid-security/voms-mapfile . An example of the format of a voms-mapfile follows: # map GLOW jobs in the chtc group to the 'glow1' Unix account. \"/GLOW/chtc/*\" glow1 # map GLOW jobs with the htpc role to the 'glow2' Unix account. \"/GLOW/Role=htpc/*\" glow2 # map other GLOW jobs to the 'glow' Unix account. \"/GLOW/*\" glow Each non-commented line is a shell-style pattern which is compared against the user's VOMS attributes, and a Unix account that the user will be mapped to if the pattern matches. The patterns are compared in the order they are listed in. Therefore, more general patterns should be placed later in the file. Note The Unix account must exist for the user to be mapped. If a VO's Unix account is missing, that VO will not be able to access your resources. Additionally, if you map VOMS attributes to a non-existent user in /etc/grid-security/voms-mapfile , /usr/share/osg/voms-mapfile-default will be considered next to find a mapping. The best way to ban a VO is edit /etc/grid-security/ban-voms-mapfile as described in Banning VOs below. Do not edit voms-mapfile-default as your changes will be overwritten upon updates.","title":"Mapping VOs"},{"location":"security/lcmaps-voms-authentication/#mapping-users","text":"To map specific users to Unix accounts based on their certificates' DNs, create /etc/grid-security/grid-mapfile . An example of the format of a grid-mapfile follows: # map Matyas's FNAL DN to the 'matyas' Unix account \"/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\" matyas Note The Unix account must exist for the user to be mapped. If a user's Unix account is missing, that user will not be able to access your resources.","title":"Mapping users"},{"location":"security/lcmaps-voms-authentication/#banning-vos","text":"/etc/grid-security/ban-voms-mapfile is used to ban an entire VO or a role withing a VO from accessing resources on your machine. An example of the format of a ban-voms-mapfile follows: # ban CMS production jobs \"/cms/Role=production/*\" Each non-commented line is a shell-style pattern which is compared against a user's VOMS attributes. If the pattern matches, that user will be unable to access your resources. Danger When banning VOs, you must restart the services using LCMAPS VOMS authentication (e.g. condor-ce , globus-gridftp-server , xrootd , etc.) to clear any authentication caches. In the case of XRootD when the service is not restarted the change could take up to 12hrs to take effect. This can be modified by defining the authzto option in the sec.protocol configuration attribute, e.g.: sec.protocol /usr/lib64 gsi \\ -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrd/xrdcert.pem \\ ... -authzto:3600 The units of -authzto are in seconds which means that the above will set the LCMAPS cache lifetime to 1hr. Warning /etc/grid-security/ban-voms-mapfile must exist, even if you are not banning any VOs. In that case, the file should not contain any entries. If the file does not exist, LCMAPS will ban every user.","title":"Banning VOs"},{"location":"security/lcmaps-voms-authentication/#banning-users","text":"/etc/grid-security/ban-mapfile is used to ban specific users from accessing your resources based on their certificates' DNs. An example of the format of a ban-mapfile follows: # ban Matyas's FNAL DN \"/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\" Danger When banning users, you must restart the services using LCMAPS VOMS authentication (e.g. condor-ce , globus-gridftp-server , xrootd , etc.) to clear any authentication caches. In the case of XRootD when the service is not restarted the change could take up to 12hrs to take effect. This can be modified by defining the authzto option in the sec.protocol configuration attribute, e.g.: sec.protocol /usr/lib64 gsi \\ -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrd/xrdcert.pem \\ ... -authzto:3600 The units of -authzto are in seconds which means that the above will set the LCMAPS cache lifetime to 1hr. Warning /etc/grid-security/ban-mapfile must exist, even if you are not banning any users. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.","title":"Banning users"},{"location":"security/lcmaps-voms-authentication/#mapping-using-all-fqans","text":"By default, the LCMAPS VOMS plugin only considers the first FQAN of a VOMS proxy for mapping. If you want to consider all FQANs, you must set the appropriate option. If you are using osg-configure, set all_fqans = True in 10-misc.ini , then run osg-configure -c If you are configuring lcmaps.db manually (see manual configuration below), add \"-all-fqans\" to the module definitions for vomsmapfile and defaultmapfile","title":"Mapping using all FQANs"},{"location":"security/lcmaps-voms-authentication/#using-the-lcmaps-voms-plugin","text":"LCMAPS is a software library that is called for authentication; therefore, there are no running services and it does not have to be invoked manually.","title":"Using the LCMAPS VOMS Plugin"},{"location":"security/lcmaps-voms-authentication/#validating-the-lcmaps-voms-plugin-vo-mappings","text":"To validate the LCMAPS VOMS plugin by itself, use the following procedure to test mapping your own cert to a user: Verify your DN is not in /etc/grid-security/grid-mapfile , or else it will generate a false positive Verify your DN is not in /etc/grid-security/ban-mapfile , or else it will generate a false negative Install the llrun and voms-clients packages: root@host # yum install llrun voms-clients As an unprivileged user, create a VOMS proxy (filling in <YOUR_VO> with a VO you are a member of): user@host $ voms-proxy-init -voms <YOUR_VO> Verify that your credentials are mapped as expected: user@host $ llrun -s -l mode = pem,policy = authorize_only,db = /etc/lcmaps.db \\ -p/tmp/x509up_u`id -u` If you did not get correctly mapped, check your proxy's FQAN by running: user@host $ voms-proxy-info -fqan and make sure it matches one of the patterns in /etc/grid-security/voms-mapfile or /usr/share/osg/voms-mapfile-default , and does not match any patterns in /etc/grid-security/ban-voms-mapfile .","title":"Validating the LCMAPS VOMS Plugin VO Mappings"},{"location":"security/lcmaps-voms-authentication/#troubleshooting-the-lcmaps-voms-plugin","text":"LCMAPS logs to journalctl and the verbosity of the logging can be increased by modifying the appropriate configuration and restarting the relevant service. This section outlines the configuration necessary to raise the debug level for the different hosts that can use LCMAPS VOMS authentication as well as common LCMAPS VOMS authentication issues.","title":"Troubleshooting the LCMAPS VOMS Plugin"},{"location":"security/lcmaps-voms-authentication/#htcondor-ce-hosts","text":"If you are troubleshooting an HTCondor-CE host, follow these instructions to raise the LCMAPS debug level: Add the following text to /etc/sysconfig/condor-ce : export LCMAPS_DEBUG_LEVEL = 5 # optional (uncomment the following line to output log messages to a file): # export LCMAPS_LOG_FILE=/tmp/lcmaps.log Disable HTCondor-CE authentication caches by creating /etc/condor-ce/config.d/99-disablegsicache.conf with the following contents: GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION = 0 Restart the condor-ce service Tip After you've completed troubleshooting, remember to revert the changes above and restart services!","title":"HTCondor-CE hosts"},{"location":"security/lcmaps-voms-authentication/#xrootd-hosts","text":"If you are troubleshooting an XRootD host, follow these instructions to raise the LCMAPS debug level: Choose the configuration file to edit based on the following table: If you are running XRootD in... Then modify the following file... Standalone mode /etc/xrootd/xrootd-standalone.cfg Clustered mode /etc/xrootd/xrootd-clustered.cfg Set loglevel=5 under the -authzfunparms of the sec.protocol /usr/lib64 gsi line. For example: sec.protocol /usr/lib64 gsi -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrootd/xrootdcert.pem \\ -key:/etc/grid-security/xrootd/xrootdkey.pem \\ -crl:1 \\ -authzfun:libXrdLcmaps.so \\ -authzfunparms:lcmapscfg=/etc/xrootd/lcmaps.cfg,loglevel=5,policy=authorize_only \\ -gmapopt:10 -gmapto:0 Restart the xrootd service Tip After you've completed troubleshooting, remember to revert the changes above and restart services!","title":"XRootD hosts"},{"location":"security/lcmaps-voms-authentication/#gridftp-hosts","text":"If you are troubleshooting a GridFTP host, follow these instructions to raise the LCMAPS debug level: Add the following text to /etc/sysconfig/globus-gridftp-server : export LCMAPS_DEBUG_LEVEL = 5 # optional (uncomment the following line to output log messages to a file): # export LCMAPS_LOG_FILE=/tmp/lcmaps.log Restart the globus-gridftp-server service. Tip After you've completed troubleshooting, remember to revert the changes above and restart services!","title":"GridFTP hosts"},{"location":"security/lcmaps-voms-authentication/#common-issues","text":"","title":"Common issues"},{"location":"security/lcmaps-voms-authentication/#a-uservo-still-has-access-to-my-xrootd-server-after-adding-them-to-the-ban-files","text":"The best way to ensure that a user/VO is immediately banned is to restart the XRootD server after adding the DN or VOMS attributes to the corresponding ban file. If the above is not possible, the the lifetime of the LCMAPS cache for XRootD can be controlled by setting the parameter authzto within the sec.protocol configuration attribute, e.g.: sec.protocol /usr/lib64 gsi \\ -certdir:/etc/grid-security/certificates \\ -cert:/etc/grid-security/xrd/xrdcert.pem \\ ... -authzto:3600 The units of -authzto are in seconds which means that the above will set the LCMAPS cache lifetime to 1hr. The default value for this parameter is 12hrs.","title":"A user/VO still has access to my XRootD server after adding them to the ban files"},{"location":"security/lcmaps-voms-authentication/#wrong-version-of-gridftp","text":"If you have the EPEL version of the GridFTP server, you may see error messages in journalctl or the location specified by LCMAPS_LOG_FILE . Symptoms Apr 11 13:51:41 atlas-hub globus-gridftp-server: You are still root after the LCMAPS execution. The implicit root-mapping safety is enabled. See documentation for details Next actions If the versions of the globus-gridftp-server-* packages do not end in osgXX.elY , continue with these instructions. To check the version of your globus-gridftp-server-* , run the following command: user@host $ rpm -qa 'globus-gridftp*' Verify that the priority of the OSG repositories are set properly Clean your yum cache root@host # yum clean all --enablerepo = * Reinstall globus-gridftp-server : root@host # yum update globus-gridftp-server","title":"Wrong version of GridFTP"},{"location":"security/lcmaps-voms-authentication/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"security/lcmaps-voms-authentication/#reference","text":"","title":"Reference"},{"location":"security/lcmaps-voms-authentication/#configuration-files","text":"The files are evaluated in the following order, with earlier files taking precedence over later ones: File Provider Purpose /etc/grid-security/ban-mapfile Admin Ban DNs /etc/grid-security/ban-voms-mapfile Admin Ban VOs /etc/grid-security/grid-mapfile Admin Map DNs /etc/grid-security/voms-mapfile Admin Map VOs /usr/share/osg/voms-mapfile-default OSG Map VOs (default) Warning /usr/share/osg/voms-mapfile-default is not meant to be edited and will be overwritten on upgrades. All VO mappings can be overridden by editing the above files in /etc/grid-security .","title":"Configuration Files"},{"location":"security/lcmaps-voms-authentication/#manual-configuration","text":"This section is intended for use as reference if you choose to forego configuring the LCMAPS VOMS plugin via osg-configure (i.e., if you prefer a configuration management system like Ansible or Puppet ). Therefore, the following instructions serve as a replacement for this section above. LCMAPS is configured in /etc/lcmaps.db and since the VOMS plugin is a newer component, configuration for it may not be present in your existing /etc/lcmaps.db file. Ensure the following lines are present in the \"Module definitions\" section (the top section, before authorize_only ) of /etc/lcmaps.db : gridmapfile = \"lcmaps_localaccount.mod\" \"-gridmap /etc/grid-security/grid-mapfile\" banfile = \"lcmaps_ban_dn.mod\" \"-banmapfile /etc/grid-security/ban-mapfile\" banvomsfile = \"lcmaps_ban_fqan.mod\" \"-banmapfile /etc/grid-security/ban-voms-mapfile\" vomsmapfile = \"lcmaps_voms_localaccount.mod\" \"-gridmap /etc/grid-security/voms-mapfile\" defaultmapfile = \"lcmaps_voms_localaccount2.mod\" \"-gridmap /usr/share/osg/voms-mapfile-default\" verifyproxynokey = \"lcmaps_verify_proxy2.mod\" \"--allow-limited-proxy\" \"--discard_private_key_absence\" \" -certdir /etc/grid-security/certificates\" Edit the authorize_only section so that it contains only the following uncommented lines: verifyproxynokey -> banfile banfile -> banvomsfile | bad banvomsfile -> gridmapfile | bad gridmapfile -> good | vomsmapfile vomsmapfile -> good | defaultmapfile defaultmapfile -> good | bad Edit /etc/grid-security/gsi-authz.conf and ensure that it contains the following line with a newline at the end: globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout","title":"Manual Configuration"},{"location":"security/user-certs/","text":"User Certificates \u00b6 Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. Note This document describes how to get and set up a personal certificate (also called a grid user certificate). For instructions on how to get host certificates, see the Host Certificates document . Getting a User Certificate \u00b6 This section describes how to get and set up a personal certificate to use on OSG. You need a user certificate if you are going to interact directly with OSG resources or infrastructure, including activities such as: Managing OASIS Directly running jobs on OSG resources Directly interacting with OSG storage elements Obtaining private contact information from OSG systems Currently, you can get a user certificate from CILogon. You may also be able to use other CAs to get a certificate; if your virtual organization (VO) requires that you get a certificate from a different CA, contact your VO Support Center for instructions. Know your responsibilities \u00b6 If your account or user certificate is compromised, you must notify the issuer of your certificate. In addition, you should update your certificate and revoke the old certificate if any of the information in the certificate (such as name or email address) change. For the CILogon RA send email to ca@cilogon.org . Additional responsibilities required by the CILogon CA are given on their page . Getting a certificate from CILogon \u00b6 You will have to obtain your user certificate using the CILogon web UI . Follow the steps below to get an user certificate: Open the CILogon page, https://cilogon.org , in your browser of choice First, either search for your institution and select it or scroll through list and do the same. . Warning Do not use Google, GitHub, or ORCID as providers since they are not widely supported in the OSG. If your institution is not on the list, please contact your institution's IT support to see if they can support CILogon. Click the Log On button and enter your institutional credentials if prompted. After successfully entering your credentials, click on the \"Create Password-Protected Certificate\" link Enter a password that is at least 12 characters long and then click on the Get New Certificate button. Click the Download Your Certificate button to download your certificate in .p12 format. The certificate will be protected using the password you entered in the previous step. Certificate formats \u00b6 Your user certificate can be stored in a few different formats. The two most common formats used in OSG are the PKCS12 and PEM formats. In the PEM format, your user certificate is stored in two separate files: one for the certificate and another for the private key. The PKCS12 format stores the certificate and private key in a single file along with an optional certificate chain. Most OSG user tools will work with both but will try to use PEM files first. To convert a PKCS12 file to PEM files, do the following. First, extract your user certificate from your PKCS12 file by running the following command. You'll be prompted for the password you used to create the certificate. The invocation assumes that the PKCS12 file is called usercred.p12 . After running, the PEM certificate will be written to usercert.pem . user@host $ openssl pkcs12 -in usercred.p12 -out usercert.pem -nodes -clcerts -nokeys Enter Import Password: MAC verified OK Second, extract the private key by running the following command. You'll be prompted for two different passwords. The first prompt will be for the password that you used to create the certificate. The second prompt will be for the password that will encrypt the PEM certificate that will be created. As before, the invocation assumes that your PKCS12 certificate is located in usercred.p12 . After running, the PEM certificate with your private key will be written to userkey.pem . user@host $ openssl pkcs12 -in usercred.p12 -out userkey.pem -nocerts Enter Import Password: MAC verified OK Enter PEM pass phrase: Verifying - Enter PEM pass phrase: Using Your User Certificate \u00b6 The standard location to place user certificates is in the users certificate home directory in the .globus directory: user@host $ cp mkdir ~/.globus user@host $ cp userkey.pem ~/.globus/ user@host $ cp usercert.pem ~/.globus/ user@host $ cp usercred.p12 ~/.globus/ To generate a proxy use the command voms-proxy-init . user@host $ voms-proxy-init (Optional) If user certificates are not in the .globus then the path has to be passed to voms-proxy-init user@host $ voms-proxy-init --cert /<PATH TO>/usercert.pem --key /<PATH TO>/userkey.pem In order to find the Distinguished Name (DN), issuer and lifetime of a certificate: user@host $ openssl x509 -in /<PATH TO>/usercert.pem -noout -subject -issuer -enddate Note For admins trying to validate a service add your user DN to the grid-map file of the service. Revoking Your User Certificate \u00b6 If the security of your certificate or private key has been compromised, you have a responsibility to revoke the certificate. In addition, if your name or email address changes, you must revoke your certificate and get a new one with the correct information. If you have a CILogon issued certificate, contact ca@cilogon.org in order revoke your certificate. If you received a certificate from another CA, please contact the CA to initiate a certificate revocation. Getting a Certificate from a Service Provider with cigetcert \u00b6 You may also get a user certificate from a SAML 2.0 Service Provider such as your home institution or XSEDE. This kind of certificate is short-lived, typically valid only for a week. Therefore it is not suitable for using in your browser. However, it is useful for command-line access to site services such as compute or storage. You will need to use the cigetcert tool to get a certificate this way. Use yum to install the cigetcert package from the OSG repositories. This is a new way of getting a certificate and does not work with all institutions. To get a list of institutions supported by cigetcert , run: user@host $ cigetcert --listinstitutions Clemson University Fermi National Accelerator Laboratory LIGO Scientific Collaboration LTER Network ... To get a certificate, run user@host $ cigetcert -i \"<INSTITUTION>\" Authorizing ...... authorized Fetching certificate ..... fetched Storing certificate in /tmp/x509up_u46142 Your certificate is valid until: Fri Apr 13 17:03:13 2018 Authentication is controlled by the institution; depending on the institution, you may need a valid Kerberos token, or will be prompted for a password. If all goes well, you should see output similar to what's above. The certificate is created in /tmp/x509up_u<YOUR UID> , which is the same place proxies are created by grid-proxy-init . You may specify default arguments in the CIGETCERTOPTS environment variable. This can save you from having to type in the entire institution name every time you want a cert. For example, to always use FNAL as the institution, put this in your .bashrc : export CIGETCERTOPTS = \"-i 'Fermi National Accelerator Laboratory\" Your VO may also provide specific instructions for how to best use this tool. Contact your VO support center for details. Finally, cigetcert has advanced features, such as the ability to load configuration from a server, or store the cert on a MyProxy server. See the manual page for cigetcert for more information. Using cigetcert with XSEDE credentials \u00b6 cigetcert also works with XSEDE as the service provider. To use XSEDE credentials, you will first need an account at https://portal.xsede.org . In addition, you need to set up two-factor authentication with XSEDE; see their MFA documentation for details. Push notifications using the Duo Mobile app are required. Once you have set all those up, run cigetcert as follows: user@host $ cigetcert -u <USERNAME> -i XSEDE <USERNAME> is your username at portal.xsede.org. You will get prompted to \"Enter XSEDE Kerberos Password.\" Enter the password for your account at portal.xsede.org. You should then get a 2FA authentication request with Duo Mobile; once you accept this, cigetcert will issue the certificate. Getting Help \u00b6 To get assistance, please use the this page . References \u00b6 Useful Documentation.OpenSSL commands (from NCSA) - e.g. how to convert the format of your certificate. Manual page for cigetcert","title":"User Certificates"},{"location":"security/user-certs/#user-certificates","text":"Warning This document is for software that will no longer be supported after the OSG 3.5 retirement (February 2022). See the Release Series Support Policy for details. Note This document describes how to get and set up a personal certificate (also called a grid user certificate). For instructions on how to get host certificates, see the Host Certificates document .","title":"User Certificates"},{"location":"security/user-certs/#getting-a-user-certificate","text":"This section describes how to get and set up a personal certificate to use on OSG. You need a user certificate if you are going to interact directly with OSG resources or infrastructure, including activities such as: Managing OASIS Directly running jobs on OSG resources Directly interacting with OSG storage elements Obtaining private contact information from OSG systems Currently, you can get a user certificate from CILogon. You may also be able to use other CAs to get a certificate; if your virtual organization (VO) requires that you get a certificate from a different CA, contact your VO Support Center for instructions.","title":"Getting a User Certificate"},{"location":"security/user-certs/#know-your-responsibilities","text":"If your account or user certificate is compromised, you must notify the issuer of your certificate. In addition, you should update your certificate and revoke the old certificate if any of the information in the certificate (such as name or email address) change. For the CILogon RA send email to ca@cilogon.org . Additional responsibilities required by the CILogon CA are given on their page .","title":"Know your responsibilities"},{"location":"security/user-certs/#getting-a-certificate-from-cilogon","text":"You will have to obtain your user certificate using the CILogon web UI . Follow the steps below to get an user certificate: Open the CILogon page, https://cilogon.org , in your browser of choice First, either search for your institution and select it or scroll through list and do the same. . Warning Do not use Google, GitHub, or ORCID as providers since they are not widely supported in the OSG. If your institution is not on the list, please contact your institution's IT support to see if they can support CILogon. Click the Log On button and enter your institutional credentials if prompted. After successfully entering your credentials, click on the \"Create Password-Protected Certificate\" link Enter a password that is at least 12 characters long and then click on the Get New Certificate button. Click the Download Your Certificate button to download your certificate in .p12 format. The certificate will be protected using the password you entered in the previous step.","title":"Getting a certificate from CILogon"},{"location":"security/user-certs/#certificate-formats","text":"Your user certificate can be stored in a few different formats. The two most common formats used in OSG are the PKCS12 and PEM formats. In the PEM format, your user certificate is stored in two separate files: one for the certificate and another for the private key. The PKCS12 format stores the certificate and private key in a single file along with an optional certificate chain. Most OSG user tools will work with both but will try to use PEM files first. To convert a PKCS12 file to PEM files, do the following. First, extract your user certificate from your PKCS12 file by running the following command. You'll be prompted for the password you used to create the certificate. The invocation assumes that the PKCS12 file is called usercred.p12 . After running, the PEM certificate will be written to usercert.pem . user@host $ openssl pkcs12 -in usercred.p12 -out usercert.pem -nodes -clcerts -nokeys Enter Import Password: MAC verified OK Second, extract the private key by running the following command. You'll be prompted for two different passwords. The first prompt will be for the password that you used to create the certificate. The second prompt will be for the password that will encrypt the PEM certificate that will be created. As before, the invocation assumes that your PKCS12 certificate is located in usercred.p12 . After running, the PEM certificate with your private key will be written to userkey.pem . user@host $ openssl pkcs12 -in usercred.p12 -out userkey.pem -nocerts Enter Import Password: MAC verified OK Enter PEM pass phrase: Verifying - Enter PEM pass phrase:","title":"Certificate formats"},{"location":"security/user-certs/#using-your-user-certificate","text":"The standard location to place user certificates is in the users certificate home directory in the .globus directory: user@host $ cp mkdir ~/.globus user@host $ cp userkey.pem ~/.globus/ user@host $ cp usercert.pem ~/.globus/ user@host $ cp usercred.p12 ~/.globus/ To generate a proxy use the command voms-proxy-init . user@host $ voms-proxy-init (Optional) If user certificates are not in the .globus then the path has to be passed to voms-proxy-init user@host $ voms-proxy-init --cert /<PATH TO>/usercert.pem --key /<PATH TO>/userkey.pem In order to find the Distinguished Name (DN), issuer and lifetime of a certificate: user@host $ openssl x509 -in /<PATH TO>/usercert.pem -noout -subject -issuer -enddate Note For admins trying to validate a service add your user DN to the grid-map file of the service.","title":"Using Your User Certificate"},{"location":"security/user-certs/#revoking-your-user-certificate","text":"If the security of your certificate or private key has been compromised, you have a responsibility to revoke the certificate. In addition, if your name or email address changes, you must revoke your certificate and get a new one with the correct information. If you have a CILogon issued certificate, contact ca@cilogon.org in order revoke your certificate. If you received a certificate from another CA, please contact the CA to initiate a certificate revocation.","title":"Revoking Your User Certificate"},{"location":"security/user-certs/#getting-a-certificate-from-a-service-provider-with-cigetcert","text":"You may also get a user certificate from a SAML 2.0 Service Provider such as your home institution or XSEDE. This kind of certificate is short-lived, typically valid only for a week. Therefore it is not suitable for using in your browser. However, it is useful for command-line access to site services such as compute or storage. You will need to use the cigetcert tool to get a certificate this way. Use yum to install the cigetcert package from the OSG repositories. This is a new way of getting a certificate and does not work with all institutions. To get a list of institutions supported by cigetcert , run: user@host $ cigetcert --listinstitutions Clemson University Fermi National Accelerator Laboratory LIGO Scientific Collaboration LTER Network ... To get a certificate, run user@host $ cigetcert -i \"<INSTITUTION>\" Authorizing ...... authorized Fetching certificate ..... fetched Storing certificate in /tmp/x509up_u46142 Your certificate is valid until: Fri Apr 13 17:03:13 2018 Authentication is controlled by the institution; depending on the institution, you may need a valid Kerberos token, or will be prompted for a password. If all goes well, you should see output similar to what's above. The certificate is created in /tmp/x509up_u<YOUR UID> , which is the same place proxies are created by grid-proxy-init . You may specify default arguments in the CIGETCERTOPTS environment variable. This can save you from having to type in the entire institution name every time you want a cert. For example, to always use FNAL as the institution, put this in your .bashrc : export CIGETCERTOPTS = \"-i 'Fermi National Accelerator Laboratory\" Your VO may also provide specific instructions for how to best use this tool. Contact your VO support center for details. Finally, cigetcert has advanced features, such as the ability to load configuration from a server, or store the cert on a MyProxy server. See the manual page for cigetcert for more information.","title":"Getting a Certificate from a Service Provider with cigetcert"},{"location":"security/user-certs/#using-cigetcert-with-xsede-credentials","text":"cigetcert also works with XSEDE as the service provider. To use XSEDE credentials, you will first need an account at https://portal.xsede.org . In addition, you need to set up two-factor authentication with XSEDE; see their MFA documentation for details. Push notifications using the Duo Mobile app are required. Once you have set all those up, run cigetcert as follows: user@host $ cigetcert -u <USERNAME> -i XSEDE <USERNAME> is your username at portal.xsede.org. You will get prompted to \"Enter XSEDE Kerberos Password.\" Enter the password for your account at portal.xsede.org. You should then get a 2FA authentication request with Duo Mobile; once you accept this, cigetcert will issue the certificate.","title":"Using cigetcert with XSEDE credentials"},{"location":"security/user-certs/#getting-help","text":"To get assistance, please use the this page .","title":"Getting Help"},{"location":"security/user-certs/#references","text":"Useful Documentation.OpenSSL commands (from NCSA) - e.g. how to convert the format of your certificate. Manual page for cigetcert","title":"References"},{"location":"security/host-certs/digicert/","text":"DigiCert IGTF Host Certificates \u00b6 DigiCert no longer issues IGTF certificates DigiCert stopped issuing IGTF certificates so the instructions in this document are no longer applicable. Note This document describes how to get host certificates. For instructions on how to get user certificates, see the User Certificates document . This document describes how to purchase individual IGTF-accredited host certificates from DigiCert . Before purchasing individual certificates, consider the following alternatives: Request a Let's Encrypt certificate if you don't support any VOs that require IGTF-accredited certificates (e.g. ATLAS or CMS ). Request an InCommon certificate if your institution has an InCommon subscription. Before Starting \u00b6 Before requesting a new host certificate, use openssl to check if your host already has a valid certificate, i.e. the present is between notBefore and notAfter dates and times. If so, you may safely skip this document: user@host $ openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org issuer=/DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1 notBefore=Jan 4 21:08:09 2010 GMT notAfter=Jan 4 21:08:09 2011 GMT If you do not have a valid certificate, the OSG offers a command-line tool to generate certificate signing requests (CSR) to assist in acquiring a grid host certificate. As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Create a DigiCert Account \u00b6 Before requesting a certificate, you must create a DigiCert account with permission to request grid certificates: Navigate to the DigiCert sign up page and create an account Attention It is very important to use your institution's main address and other contact information. Departmental addresses will not pass DigiCert's verification process. Submit a support request to allow you to order grid host certificates, referencing support ticket #01336672 After your request has been approved, verify that the Grid Host SSL option is available to you from your account's order page: Requesting DigiCert IGTF Host Certificates \u00b6 Install the osg-pki-tools : root@host # yum install osg-pki-tools Generate a CSR and private key using the osg-cert-request tool: user@host $ osg-cert-request --hostname <HOSTNAME> \\ --country <COUNTRY> \\ --state <STATE> \\ --locality <LOCALITY> \\ --organization <ORGANIZATION> You may also add DNS Subject Alternative Names (SAN) to the request by specifying any number of --altname <SAN> . For example, the following generates a CSR for test.opensciencegrid.org with foo.opensciencegrid.org and bar.opensciencegrid.org as SANs: user@host $ osg-cert-request --hostname test.opensciencegrid.org \\ --country US \\ --state Wisconsin \\ --locality Madison \\ --organization 'University of Wisconsin-Madison' \\ --altname foo.opensciencegrid.org \\ --altname bar.opensciencegrid.org If successful, the CSR will be named <HOSTNAME>.req and the private key will be named <HOSTNAME>-key.pem . Additional options and descriptions can be found here . Submit an order using DigiCert's grid certificate order page : Login to your DigiCert account Attach the CSR that you generated above Pay for your certificate and await approval After the certificate has been issued by DigiCert, download the host certificate only (not the full chain) to its intended host and copy over the key you generated above. Verify that the issuer CN field is DigiCert Grid Trust CA G2 : $ openssl x509 -in <PATH TO CERTIFICATE> -noout -issuer issuer= /C=US/O=DigiCert Grid/OU=www.digicert.com/CN=DigiCert Grid Trust CA G2 Where <PATH TO CERTIFICATE> is the file you downloaded in the previous step Install the host certificate and key: root@host # cp <PATH TO CERTIFICATE> /etc/grid-security/hostcert.pem root@host # chmod 444 /etc/grid-security/hostcert.pem root@host # cp <PATH TO KEY> /etc/grid-security/hostkey.pem root@host # chmod 400 /etc/grid-security/hostkey.pem Where <PATH TO KEY> is the \".key\" file you created in the first step","title":"Digicert"},{"location":"security/host-certs/digicert/#digicert-igtf-host-certificates","text":"DigiCert no longer issues IGTF certificates DigiCert stopped issuing IGTF certificates so the instructions in this document are no longer applicable. Note This document describes how to get host certificates. For instructions on how to get user certificates, see the User Certificates document . This document describes how to purchase individual IGTF-accredited host certificates from DigiCert . Before purchasing individual certificates, consider the following alternatives: Request a Let's Encrypt certificate if you don't support any VOs that require IGTF-accredited certificates (e.g. ATLAS or CMS ). Request an InCommon certificate if your institution has an InCommon subscription.","title":"DigiCert IGTF Host Certificates"},{"location":"security/host-certs/digicert/#before-starting","text":"Before requesting a new host certificate, use openssl to check if your host already has a valid certificate, i.e. the present is between notBefore and notAfter dates and times. If so, you may safely skip this document: user@host $ openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org issuer=/DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1 notBefore=Jan 4 21:08:09 2010 GMT notAfter=Jan 4 21:08:09 2011 GMT If you do not have a valid certificate, the OSG offers a command-line tool to generate certificate signing requests (CSR) to assist in acquiring a grid host certificate. As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories","title":"Before Starting"},{"location":"security/host-certs/digicert/#create-a-digicert-account","text":"Before requesting a certificate, you must create a DigiCert account with permission to request grid certificates: Navigate to the DigiCert sign up page and create an account Attention It is very important to use your institution's main address and other contact information. Departmental addresses will not pass DigiCert's verification process. Submit a support request to allow you to order grid host certificates, referencing support ticket #01336672 After your request has been approved, verify that the Grid Host SSL option is available to you from your account's order page:","title":"Create a DigiCert Account"},{"location":"security/host-certs/digicert/#requesting-digicert-igtf-host-certificates","text":"Install the osg-pki-tools : root@host # yum install osg-pki-tools Generate a CSR and private key using the osg-cert-request tool: user@host $ osg-cert-request --hostname <HOSTNAME> \\ --country <COUNTRY> \\ --state <STATE> \\ --locality <LOCALITY> \\ --organization <ORGANIZATION> You may also add DNS Subject Alternative Names (SAN) to the request by specifying any number of --altname <SAN> . For example, the following generates a CSR for test.opensciencegrid.org with foo.opensciencegrid.org and bar.opensciencegrid.org as SANs: user@host $ osg-cert-request --hostname test.opensciencegrid.org \\ --country US \\ --state Wisconsin \\ --locality Madison \\ --organization 'University of Wisconsin-Madison' \\ --altname foo.opensciencegrid.org \\ --altname bar.opensciencegrid.org If successful, the CSR will be named <HOSTNAME>.req and the private key will be named <HOSTNAME>-key.pem . Additional options and descriptions can be found here . Submit an order using DigiCert's grid certificate order page : Login to your DigiCert account Attach the CSR that you generated above Pay for your certificate and await approval After the certificate has been issued by DigiCert, download the host certificate only (not the full chain) to its intended host and copy over the key you generated above. Verify that the issuer CN field is DigiCert Grid Trust CA G2 : $ openssl x509 -in <PATH TO CERTIFICATE> -noout -issuer issuer= /C=US/O=DigiCert Grid/OU=www.digicert.com/CN=DigiCert Grid Trust CA G2 Where <PATH TO CERTIFICATE> is the file you downloaded in the previous step Install the host certificate and key: root@host # cp <PATH TO CERTIFICATE> /etc/grid-security/hostcert.pem root@host # chmod 444 /etc/grid-security/hostcert.pem root@host # cp <PATH TO KEY> /etc/grid-security/hostkey.pem root@host # chmod 400 /etc/grid-security/hostkey.pem Where <PATH TO KEY> is the \".key\" file you created in the first step","title":"Requesting DigiCert IGTF Host Certificates"},{"location":"security/host-certs/incommon/","text":"Requesting InCommon IGTF Host Certificates \u00b6 Many institutions in the United States already subscribe to InCommon and offer IGTF certificate services. If your institution is in the list of InCommon subscribers , continue with the instructions below. If your institution is not in the list, Let's Encrypt certificates do not meet your needs, and you do not have access to another IGTF CA subscription, please contact us . As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories From a host that meets the above requirements, there are two options to get InCommon IGTF-accredited host certificates: Requesting certificates from a Registration Authority (RA) : This requires a Certificate Signing Request (CSR), which can be generated with the osg-cert-request tool. Requesting certificates as an RA : As an RA, you can request, approve, and retrieve certificates yourself through the InCommon REST API using the osg-incommon-cert-request tool . Install the osg-pki-tools where both command line tools are available: root@host # yum install osg-pki-tools Requesting certificates from a registration authority \u00b6 Generate a Certificate Signing Request (CSR) and private key using the osg-cert-request tool: user@host $ osg-cert-request --hostname <HOSTNAME> \\ --country <COUNTRY> \\ --state <STATE> \\ --locality <LOCALITY> \\ --organization <ORGANIZATION> You may also add DNS Subject Alternative Names (SAN) to the request by specifying any number of --altname <SAN> . For example, the following generates a CSR for test.opensciencegrid.org with foo.opensciencegrid.org and bar.opensciencegrid.org as SANs: user@host $ osg-cert-request --hostname test.opensciencegrid.org \\ --country US \\ --state Wisconsin \\ --locality Madison \\ --organization 'University of Wisconsin-Madison' \\ --altname foo.opensciencegrid.org \\ --altname bar.opensciencegrid.org If successful, the CSR will be named <HOSTNAME>.req and the private key will be named <HOSTNAME>-key.pem . Additional options and descriptions can be found here . Find your institution-specific InCommon contact and submit the CSR that you generated above. Make sure to request a 1-year IGTF Server Certificate for OTHER server software. After the certificate has been issued by your institution, download the host certificate only (not the full chain) to its intended host and copy over the key you generated above. Verify that the issuer CN field is InCommon IGTF Server CA : $ openssl x509 -in <PATH TO CERTIFICATE> -noout -issuer issuer= /C=US/O=Internet2/OU=InCommon/CN=InCommon IGTF Server CA Where <PATH TO CERTIFICATE> is the file you downloaded in the previous step Install the host certificate and key: root@host # cp <PATH TO CERTIFICATE> /etc/grid-security/hostcert.pem root@host # chmod 444 /etc/grid-security/hostcert.pem root@host # cp <PATH TO KEY> /etc/grid-security/hostkey.pem root@host # chmod 400 /etc/grid-security/hostkey.pem Where <PATH TO KEY> is the \".key\" file you created in the first step Requesting certificates as a registration authority \u00b6 If you are a Registration Authority for your institution, skip ahead to this section . If you are not already a Registration Authority (RA) for your institution, you must request to be made one: Find your institution-specific InCommon contact (e.g. campus central IT) Request a Department Registration Authority user with SSL auto-approve enabled and a client certificate: If they do not grant your request, you will not be able to request, approve, and retrieve certificates yourself. Instead, you must request certificates from your RA . If they grant your request, you will receive an email with instructions for requesting your client certificate; download the .p12 file. Find your institution-specific organization and department codes at the InCommon Cert Manager (https://cert-manager.com/customer/InCommon). These are numeric codes that should be specified through the command line using the -O/--orgcode ORG,DEPT option: Organization code is shown as OrgID under Settings > Organizations > Edit Department code is shown as OrgID under Settings > Organizations > Departments > Edit Once you have RA privileges, you may request, approve, and retrieve host certificates using osg-incommon-cert-request : In order to request a certificate, you will need your InCommon client certificate as two separate files, incommon_user_key.pem for the key, and incommon_user_cert.pem for the cert. If you don't already have them, perform the following steps: Download the .p12 file with your client certificate and save this as incommon_file.p12 . You should have received instructions for how to obtain this file in an email when you became an RA. Extract the certificate and key: user@host $ openssl pkcs12 -in incommon_file.p12 \\ -nocerts -out ~/path_to_dir/incommon_user_key.pem user@host $ openssl pkcs12 -in incommon_file.p12 \\ -nokeys -out ~/path_to_dir/incommon_user_cert.pem Requesting a certificate with a single hostname <HOSTNAME> : user@host $ osg-incommon-cert-request --username <INCOMMON_LOGIN> \\ --cert ~/path_to_dir/incommon_user_cert.pem \\ --pkey ~/path_to_dir/incommon_user_key.pem \\ --hostname <HOSTNAME> [--orgcode <ORG,DEPT>] Requesting a certificate with Subject Alternative Names (SANs): user@host $ osg-incommon-cert-request --username <INCOMMON_LOGIN> \\ --cert ~/path_to_dir/incommon_user_cert.pem \\ --pkey ~/path_to_dir/incommon_user_key.pem \\ --hostname <HOSTNAME> \\ --altname <ALTNAME> \\ --altname <ALTNAME2> [--orgcode <ORG,DEPT>] Requesting certificates in bulk using a hostfile name: user@host $ osg-incommon-cert-request --username <INCOMMON_LOGIN> \\ --cert ~/path_to_dir/incommon_user_cert.pem \\ --pkey ~/path_to_dir/incommon_user_key.pem \\ --hostfile ~/path_to_file/hostfile.txt \\ [--orgcode <ORG,DEPT>] Where the contents of hostfile.txt contain one hostname and any number of SANs per line: hostname01.yourdomain hostname02.yourdomain hostnamealias.yourdomain hostname03.yourdomain hostname04.yourdomain hostname05.yourdomain References \u00b6 CILogon documentation for requesting InCommon certificates Useful OpenSSL commands (from NCSA) - e.g. how to convert the format of your certificate.","title":"Using InCommon"},{"location":"security/host-certs/incommon/#requesting-incommon-igtf-host-certificates","text":"Many institutions in the United States already subscribe to InCommon and offer IGTF certificate services. If your institution is in the list of InCommon subscribers , continue with the instructions below. If your institution is not in the list, Let's Encrypt certificates do not meet your needs, and you do not have access to another IGTF CA subscription, please contact us . As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories From a host that meets the above requirements, there are two options to get InCommon IGTF-accredited host certificates: Requesting certificates from a Registration Authority (RA) : This requires a Certificate Signing Request (CSR), which can be generated with the osg-cert-request tool. Requesting certificates as an RA : As an RA, you can request, approve, and retrieve certificates yourself through the InCommon REST API using the osg-incommon-cert-request tool . Install the osg-pki-tools where both command line tools are available: root@host # yum install osg-pki-tools","title":"Requesting InCommon IGTF Host Certificates"},{"location":"security/host-certs/incommon/#requesting-certificates-from-a-registration-authority","text":"Generate a Certificate Signing Request (CSR) and private key using the osg-cert-request tool: user@host $ osg-cert-request --hostname <HOSTNAME> \\ --country <COUNTRY> \\ --state <STATE> \\ --locality <LOCALITY> \\ --organization <ORGANIZATION> You may also add DNS Subject Alternative Names (SAN) to the request by specifying any number of --altname <SAN> . For example, the following generates a CSR for test.opensciencegrid.org with foo.opensciencegrid.org and bar.opensciencegrid.org as SANs: user@host $ osg-cert-request --hostname test.opensciencegrid.org \\ --country US \\ --state Wisconsin \\ --locality Madison \\ --organization 'University of Wisconsin-Madison' \\ --altname foo.opensciencegrid.org \\ --altname bar.opensciencegrid.org If successful, the CSR will be named <HOSTNAME>.req and the private key will be named <HOSTNAME>-key.pem . Additional options and descriptions can be found here . Find your institution-specific InCommon contact and submit the CSR that you generated above. Make sure to request a 1-year IGTF Server Certificate for OTHER server software. After the certificate has been issued by your institution, download the host certificate only (not the full chain) to its intended host and copy over the key you generated above. Verify that the issuer CN field is InCommon IGTF Server CA : $ openssl x509 -in <PATH TO CERTIFICATE> -noout -issuer issuer= /C=US/O=Internet2/OU=InCommon/CN=InCommon IGTF Server CA Where <PATH TO CERTIFICATE> is the file you downloaded in the previous step Install the host certificate and key: root@host # cp <PATH TO CERTIFICATE> /etc/grid-security/hostcert.pem root@host # chmod 444 /etc/grid-security/hostcert.pem root@host # cp <PATH TO KEY> /etc/grid-security/hostkey.pem root@host # chmod 400 /etc/grid-security/hostkey.pem Where <PATH TO KEY> is the \".key\" file you created in the first step","title":"Requesting certificates from a registration authority"},{"location":"security/host-certs/incommon/#requesting-certificates-as-a-registration-authority","text":"If you are a Registration Authority for your institution, skip ahead to this section . If you are not already a Registration Authority (RA) for your institution, you must request to be made one: Find your institution-specific InCommon contact (e.g. campus central IT) Request a Department Registration Authority user with SSL auto-approve enabled and a client certificate: If they do not grant your request, you will not be able to request, approve, and retrieve certificates yourself. Instead, you must request certificates from your RA . If they grant your request, you will receive an email with instructions for requesting your client certificate; download the .p12 file. Find your institution-specific organization and department codes at the InCommon Cert Manager (https://cert-manager.com/customer/InCommon). These are numeric codes that should be specified through the command line using the -O/--orgcode ORG,DEPT option: Organization code is shown as OrgID under Settings > Organizations > Edit Department code is shown as OrgID under Settings > Organizations > Departments > Edit Once you have RA privileges, you may request, approve, and retrieve host certificates using osg-incommon-cert-request : In order to request a certificate, you will need your InCommon client certificate as two separate files, incommon_user_key.pem for the key, and incommon_user_cert.pem for the cert. If you don't already have them, perform the following steps: Download the .p12 file with your client certificate and save this as incommon_file.p12 . You should have received instructions for how to obtain this file in an email when you became an RA. Extract the certificate and key: user@host $ openssl pkcs12 -in incommon_file.p12 \\ -nocerts -out ~/path_to_dir/incommon_user_key.pem user@host $ openssl pkcs12 -in incommon_file.p12 \\ -nokeys -out ~/path_to_dir/incommon_user_cert.pem Requesting a certificate with a single hostname <HOSTNAME> : user@host $ osg-incommon-cert-request --username <INCOMMON_LOGIN> \\ --cert ~/path_to_dir/incommon_user_cert.pem \\ --pkey ~/path_to_dir/incommon_user_key.pem \\ --hostname <HOSTNAME> [--orgcode <ORG,DEPT>] Requesting a certificate with Subject Alternative Names (SANs): user@host $ osg-incommon-cert-request --username <INCOMMON_LOGIN> \\ --cert ~/path_to_dir/incommon_user_cert.pem \\ --pkey ~/path_to_dir/incommon_user_key.pem \\ --hostname <HOSTNAME> \\ --altname <ALTNAME> \\ --altname <ALTNAME2> [--orgcode <ORG,DEPT>] Requesting certificates in bulk using a hostfile name: user@host $ osg-incommon-cert-request --username <INCOMMON_LOGIN> \\ --cert ~/path_to_dir/incommon_user_cert.pem \\ --pkey ~/path_to_dir/incommon_user_key.pem \\ --hostfile ~/path_to_file/hostfile.txt \\ [--orgcode <ORG,DEPT>] Where the contents of hostfile.txt contain one hostname and any number of SANs per line: hostname01.yourdomain hostname02.yourdomain hostnamealias.yourdomain hostname03.yourdomain hostname04.yourdomain hostname05.yourdomain","title":"Requesting certificates as a registration authority"},{"location":"security/host-certs/incommon/#references","text":"CILogon documentation for requesting InCommon certificates Useful OpenSSL commands (from NCSA) - e.g. how to convert the format of your certificate.","title":"References"},{"location":"security/host-certs/lets-encrypt/","text":"Requesting Host Certificates Using Let's Encrypt \u00b6 Let's Encrypt is a free, automated, and open CA frequently used for web services; see the security team's position on Let's Encrypt for more details. Let's Encrypt can be used to obtain host certificates as an alternative to InCommon if your institution does not have an InCommon subscription. Let's Encrypt uses an automated script named certbot for requesting and renewing host certs. certbot binds to port 80 when running, so services running on port 80 (such as HTCondor-CE View service ) must be temporarily stopped before running certbot . In addition, port 80 must be open to the world while certbot is running. If this does not work for your host, see the alternate renewal methods section below. Let's Encrypt host certs expire every three months so it is important to set up automated renewal. Installation and Obtaining the Initial Certificate \u00b6 Install the certbot package (available from the EPEL 7 repository): root@host # yum install certbot Stop services running on port 80 if there are any. Run the following command to obtain the host certificate with Let's Encrypt: root@host # certbot certonly --standalone --email <ADMIN_EMAIL> -d <HOST> Set up hostcert/hostkey links: root@host # ln -sf /etc/letsencrypt/live/*/cert.pem /etc/grid-security/hostcert.pem root@host # ln -sf /etc/letsencrypt/live/*/privkey.pem /etc/grid-security/hostkey.pem root@host # chmod 0600 /etc/letsencrypt/archive/*/privkey*.pem Restart services running on port 80 if there were any. Renewing Let's Encrypt host certificates \u00b6 You can manually renew your certificate with the following command: root@host # certbot renew The certificate will be renewed if it is close to expiring. Disable services listening on port 80 Just like with obtaining a new certificate, renewing a certificate requires you to temporarily disable services running on port 80 so that certbot can verify the host. Automating renewals using systemd timers \u00b6 To automate renewal using systemd, you'll need to create two files: The first is a service file that tells systemd how to invoke certbot. The second is to generate a timer file that tells systemd how often to run the service. The steps to setup the timer are as follows: Create a service file called /etc/systemd/system/certbot.service with the following contents [Unit] Description=Let's Encrypt renewal [Service] Type=oneshot ExecStart=/usr/bin/certbot renew --quiet --agree-tos Once the certbot service is working correctly, you will need to create the timer file. Create the timer file at /etc/systemd/system/certbot.timer ) with the following contents: [Unit] Description=Let's Encrypt renewal timer [Timer] OnCalendar=0/12:00:00 RandomizedDelaySec=1h Persistent=true [Install] WantedBy=timers.target Update the systemd manager configuration: root@host # systemctl daemon-reload Start and enable the certbot timer: root@host # systemctl enable --now certbot.timer You can verify that the timer is active by running systemctl list-timers . Note Verify that the service has started correctly by running systemctl status certbot.service . The timer may fail without warnings if the service does not run correctly. Pre- and post-renewal hooks \u00b6 certbot provides the ability to run scripts before and/or after certificate renewal via command hooks. Common uses for these hooks include: Copying the renewed certificate so that it can be used for a separate service (such as XRootD) Shutting down and restarting a service running on port 80 Temporarily opening up the firewall To do this, call certbot with --pre-hook <COMMAND> for a command or script to run before renewal, and --post-hook <COMMAND> for a command or script to run after renewal. The command(s) will only be run if the certificate is actually renewed. Example \u00b6 This example is for a host running CEView and XRootD standalone; CEView needs to be stopped so it doesn't block port 80, and XRootD needs its certificate in a separate location. Create the following scripts: /root/bin/certbot-pre.sh #!/bin/bash condor_ce_off -daemon CEVIEW /root/bin/certbot-post.sh #!/bin/bash cd /etc/grid-security cp -f hostcert.pem xrd/xrdcert.pem cp -f hostkey.pem xrd/xrdkey.pem chown -R xrootd:xrootd xrd condor_ce_on -daemon CEVIEW systemctl restart xrootd@standalone Then call certbot as follows: root@host # certbot renew --pre-hook /root/bin/certbot-pre.sh \\ --post-hook /root/bin/certbot-post.sh For automated renewal, edit the certbot.service file that you created above and add the --pre-hook <COMMAND> and --post-hook <COMMAND> arguments to the ExecStart line: ExecStart = /usr/bin/certbot renew --quiet --agree-tos \\ --pre-hook /root/bin/certbot-pre.sh \\ --post-hook /root/bin/certbot-post.sh Alternate renewal methods \u00b6 There are some cases in which you might need an alternative to running certbot or certbot-auto as above. For example: You have a web server running on port 80 that you do not want to shut down during renewal You cannot open port 80 during renewal You want a wildcard certificate You want to run the renewal on a different machine than where the certificate will be used Certbot plugins may help in these cases. The Apache, Nginx, and Webroot plugins integrate with an already running web server to allow renewal without shutting the webserver down. One of the DNS plugins can be used to avoid using port 80, run on a different machine, or obtain a wildcard cert. If all else fails, the manual plugin can be used for manual renewal. References \u00b6 Useful OpenSSL commands (from NCSA) - e.g. how to convert the format of your certificate. Official Let's Encrypt setup guide . Another Let's Encrypt setup reference . Under Getting your host certificate, we follow the first \"Setting up\" section.","title":"Using Let's Encrypt"},{"location":"security/host-certs/lets-encrypt/#requesting-host-certificates-using-lets-encrypt","text":"Let's Encrypt is a free, automated, and open CA frequently used for web services; see the security team's position on Let's Encrypt for more details. Let's Encrypt can be used to obtain host certificates as an alternative to InCommon if your institution does not have an InCommon subscription. Let's Encrypt uses an automated script named certbot for requesting and renewing host certs. certbot binds to port 80 when running, so services running on port 80 (such as HTCondor-CE View service ) must be temporarily stopped before running certbot . In addition, port 80 must be open to the world while certbot is running. If this does not work for your host, see the alternate renewal methods section below. Let's Encrypt host certs expire every three months so it is important to set up automated renewal.","title":"Requesting Host Certificates Using Let's Encrypt"},{"location":"security/host-certs/lets-encrypt/#installation-and-obtaining-the-initial-certificate","text":"Install the certbot package (available from the EPEL 7 repository): root@host # yum install certbot Stop services running on port 80 if there are any. Run the following command to obtain the host certificate with Let's Encrypt: root@host # certbot certonly --standalone --email <ADMIN_EMAIL> -d <HOST> Set up hostcert/hostkey links: root@host # ln -sf /etc/letsencrypt/live/*/cert.pem /etc/grid-security/hostcert.pem root@host # ln -sf /etc/letsencrypt/live/*/privkey.pem /etc/grid-security/hostkey.pem root@host # chmod 0600 /etc/letsencrypt/archive/*/privkey*.pem Restart services running on port 80 if there were any.","title":"Installation and Obtaining the Initial Certificate"},{"location":"security/host-certs/lets-encrypt/#renewing-lets-encrypt-host-certificates","text":"You can manually renew your certificate with the following command: root@host # certbot renew The certificate will be renewed if it is close to expiring. Disable services listening on port 80 Just like with obtaining a new certificate, renewing a certificate requires you to temporarily disable services running on port 80 so that certbot can verify the host.","title":"Renewing Let's Encrypt host certificates"},{"location":"security/host-certs/lets-encrypt/#automating-renewals-using-systemd-timers","text":"To automate renewal using systemd, you'll need to create two files: The first is a service file that tells systemd how to invoke certbot. The second is to generate a timer file that tells systemd how often to run the service. The steps to setup the timer are as follows: Create a service file called /etc/systemd/system/certbot.service with the following contents [Unit] Description=Let's Encrypt renewal [Service] Type=oneshot ExecStart=/usr/bin/certbot renew --quiet --agree-tos Once the certbot service is working correctly, you will need to create the timer file. Create the timer file at /etc/systemd/system/certbot.timer ) with the following contents: [Unit] Description=Let's Encrypt renewal timer [Timer] OnCalendar=0/12:00:00 RandomizedDelaySec=1h Persistent=true [Install] WantedBy=timers.target Update the systemd manager configuration: root@host # systemctl daemon-reload Start and enable the certbot timer: root@host # systemctl enable --now certbot.timer You can verify that the timer is active by running systemctl list-timers . Note Verify that the service has started correctly by running systemctl status certbot.service . The timer may fail without warnings if the service does not run correctly.","title":"Automating renewals using systemd timers"},{"location":"security/host-certs/lets-encrypt/#pre-and-post-renewal-hooks","text":"certbot provides the ability to run scripts before and/or after certificate renewal via command hooks. Common uses for these hooks include: Copying the renewed certificate so that it can be used for a separate service (such as XRootD) Shutting down and restarting a service running on port 80 Temporarily opening up the firewall To do this, call certbot with --pre-hook <COMMAND> for a command or script to run before renewal, and --post-hook <COMMAND> for a command or script to run after renewal. The command(s) will only be run if the certificate is actually renewed.","title":"Pre- and post-renewal hooks"},{"location":"security/host-certs/lets-encrypt/#example","text":"This example is for a host running CEView and XRootD standalone; CEView needs to be stopped so it doesn't block port 80, and XRootD needs its certificate in a separate location. Create the following scripts: /root/bin/certbot-pre.sh #!/bin/bash condor_ce_off -daemon CEVIEW /root/bin/certbot-post.sh #!/bin/bash cd /etc/grid-security cp -f hostcert.pem xrd/xrdcert.pem cp -f hostkey.pem xrd/xrdkey.pem chown -R xrootd:xrootd xrd condor_ce_on -daemon CEVIEW systemctl restart xrootd@standalone Then call certbot as follows: root@host # certbot renew --pre-hook /root/bin/certbot-pre.sh \\ --post-hook /root/bin/certbot-post.sh For automated renewal, edit the certbot.service file that you created above and add the --pre-hook <COMMAND> and --post-hook <COMMAND> arguments to the ExecStart line: ExecStart = /usr/bin/certbot renew --quiet --agree-tos \\ --pre-hook /root/bin/certbot-pre.sh \\ --post-hook /root/bin/certbot-post.sh","title":"Example"},{"location":"security/host-certs/lets-encrypt/#alternate-renewal-methods","text":"There are some cases in which you might need an alternative to running certbot or certbot-auto as above. For example: You have a web server running on port 80 that you do not want to shut down during renewal You cannot open port 80 during renewal You want a wildcard certificate You want to run the renewal on a different machine than where the certificate will be used Certbot plugins may help in these cases. The Apache, Nginx, and Webroot plugins integrate with an already running web server to allow renewal without shutting the webserver down. One of the DNS plugins can be used to avoid using port 80, run on a different machine, or obtain a wildcard cert. If all else fails, the manual plugin can be used for manual renewal.","title":"Alternate renewal methods"},{"location":"security/host-certs/lets-encrypt/#references","text":"Useful OpenSSL commands (from NCSA) - e.g. how to convert the format of your certificate. Official Let's Encrypt setup guide . Another Let's Encrypt setup reference . Under Getting your host certificate, we follow the first \"Setting up\" section.","title":"References"},{"location":"security/host-certs/overview/","text":"Host Certificates \u00b6 Note This document describes how to get host certificates. For instructions on how to get user certificates, see the User Certificates document . Host certificates are X.509 certificates that are used to securely identify servers and to establish encrypted connections between services and clients. In the OSG, some grid resources (e.g., HTCondor-CE, XRootD, GridFTP) require host certificates. If you are unsure if your host needs a host certificate, please consult the installation instructions for the software you are interested in installing. To acquire a host certificate, you must submit a request to a Certificate Authority (CA). We recommend requesting host certificates from one of the following CAs: InCommon IGTF : an IGTF-accredited CA for services that interact with the WLCG; requires a subscription, generally held by an institution Important For integration with the OSG, InCommon host certificates must be issued by the IGTF CA and not the InCommon RSA CA. Let's Encrypt : a free, automated, and open CA frequently used for web services; see the security team's position on Let's Encrypt for more details. Let's Encrypt is not IGTF-accredited so their certificates are not suitable for WLCG services. DigiCert IGTF: an IGTF-accredited CA for services that interact with the WLCG; certificates are no longer issued by this CA . If neither of the above options work for your site, the OSG also accepts all IGTF-accredited CAs . Before Starting \u00b6 Before requesting a new host certificate, use openssl to check if your host already has a valid certificate, i.e. the present is between notBefore and notAfter dates and times. If so, you may safely skip this document: user@host $ openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org issuer=/DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1 notBefore=Jan 4 21:08:09 2010 GMT notAfter=Jan 4 21:08:09 2011 GMT If you are using OpenSSL 1.1, you may notice minor formatting differences. Requesting Service Certificates \u00b6 Previously, the OSG recommended using separate X.509 certificates, called \"service certificates\", for each grid service on a host. This practice has become less popular as sites have separated SSL-requiring services to their own hosts. In the case where your host is only running a single service that requires a service certificate, we recommend using your host certificate as your service certificate. Ensure that the ownership of the host certificate and key are appropriate for the service you are running. If you are running multiple services that require host certificates, we recommend requesting a certificate whose CommonName is <service>-hostname and has the hostname in the list of subject alternative names. Frequently Asked Questions \u00b6 Can I use any host to request a certificate for a different host? \u00b6 YES, you can use any host to create a certificate signing request as long as the hostname for the certificate is a fully qualified domain name. How do I renew a host certificate? \u00b6 For Let's Encrypt certificates, see this section For other certificates, there is no separate renewal procedure. Instead, request a new certificate using one of the methods above. How can I check if I have a host certificate installed already? \u00b6 By default the host certificate key pair will be installed in /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem . You can use openssl to access basic information about the certificate: root@host # openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org issuer= /DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1 notBefore=Apr 8 00:00:00 2013 GMT notAfter=May 17 12:00:00 2014 GMT How can I check the expiration time of my installed host certificate? \u00b6 Use the following openssl command to find the dates that your host certificate is valid: root@host # openssl x509 -in /etc/grid-security/hostcert.pem -dates -noout notBefore=Jan 4 21:08:41 2010 GMT notAfter=Jan 4 21:08:41 2011 GMT References \u00b6 Useful OpenSSL commands (from NCSA) - e.g. how to convert the format of your certificate.","title":"Overview"},{"location":"security/host-certs/overview/#host-certificates","text":"Note This document describes how to get host certificates. For instructions on how to get user certificates, see the User Certificates document . Host certificates are X.509 certificates that are used to securely identify servers and to establish encrypted connections between services and clients. In the OSG, some grid resources (e.g., HTCondor-CE, XRootD, GridFTP) require host certificates. If you are unsure if your host needs a host certificate, please consult the installation instructions for the software you are interested in installing. To acquire a host certificate, you must submit a request to a Certificate Authority (CA). We recommend requesting host certificates from one of the following CAs: InCommon IGTF : an IGTF-accredited CA for services that interact with the WLCG; requires a subscription, generally held by an institution Important For integration with the OSG, InCommon host certificates must be issued by the IGTF CA and not the InCommon RSA CA. Let's Encrypt : a free, automated, and open CA frequently used for web services; see the security team's position on Let's Encrypt for more details. Let's Encrypt is not IGTF-accredited so their certificates are not suitable for WLCG services. DigiCert IGTF: an IGTF-accredited CA for services that interact with the WLCG; certificates are no longer issued by this CA . If neither of the above options work for your site, the OSG also accepts all IGTF-accredited CAs .","title":"Host Certificates"},{"location":"security/host-certs/overview/#before-starting","text":"Before requesting a new host certificate, use openssl to check if your host already has a valid certificate, i.e. the present is between notBefore and notAfter dates and times. If so, you may safely skip this document: user@host $ openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org issuer=/DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1 notBefore=Jan 4 21:08:09 2010 GMT notAfter=Jan 4 21:08:09 2011 GMT If you are using OpenSSL 1.1, you may notice minor formatting differences.","title":"Before Starting"},{"location":"security/host-certs/overview/#requesting-service-certificates","text":"Previously, the OSG recommended using separate X.509 certificates, called \"service certificates\", for each grid service on a host. This practice has become less popular as sites have separated SSL-requiring services to their own hosts. In the case where your host is only running a single service that requires a service certificate, we recommend using your host certificate as your service certificate. Ensure that the ownership of the host certificate and key are appropriate for the service you are running. If you are running multiple services that require host certificates, we recommend requesting a certificate whose CommonName is <service>-hostname and has the hostname in the list of subject alternative names.","title":"Requesting Service Certificates"},{"location":"security/host-certs/overview/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"security/host-certs/overview/#can-i-use-any-host-to-request-a-certificate-for-a-different-host","text":"YES, you can use any host to create a certificate signing request as long as the hostname for the certificate is a fully qualified domain name.","title":"Can I use any host to request a certificate for a different host?"},{"location":"security/host-certs/overview/#how-do-i-renew-a-host-certificate","text":"For Let's Encrypt certificates, see this section For other certificates, there is no separate renewal procedure. Instead, request a new certificate using one of the methods above.","title":"How do I renew a host certificate?"},{"location":"security/host-certs/overview/#how-can-i-check-if-i-have-a-host-certificate-installed-already","text":"By default the host certificate key pair will be installed in /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem . You can use openssl to access basic information about the certificate: root@host # openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org issuer= /DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1 notBefore=Apr 8 00:00:00 2013 GMT notAfter=May 17 12:00:00 2014 GMT","title":"How can I check if I have a host certificate installed already?"},{"location":"security/host-certs/overview/#how-can-i-check-the-expiration-time-of-my-installed-host-certificate","text":"Use the following openssl command to find the dates that your host certificate is valid: root@host # openssl x509 -in /etc/grid-security/hostcert.pem -dates -noout notBefore=Jan 4 21:08:41 2010 GMT notAfter=Jan 4 21:08:41 2011 GMT","title":"How can I check the expiration time of my installed host certificate?"},{"location":"security/host-certs/overview/#references","text":"Useful OpenSSL commands (from NCSA) - e.g. how to convert the format of your certificate.","title":"References"},{"location":"submit/osg-flock/","text":"Installing an Open Science Pool Access Point \u00b6 This document explains how to add a path for user jobs to flow from your local site out to the OSG, which in most cases means that the jobs will have far more resources available to run on than locally. If your local batch system frequently has many jobs waiting to run for a long time, you do not have a local batch system, or if you simply want to provide a local entry point for OSG-bound jobs, adding a path to OSG may result in less waiting for your users. Note that if you do not have a local batch system, consider having your users use OSG Connect , which will require less infrastructure work at your site. Note Flocking to the OSG requires some modification to user workflows. After installation, see the usage section for instructions on what your users will need to do. Background \u00b6 Every batch computing system has one or more entry points that users log on to and use to hand over their computing work to the batch system for completion. For the HTCondor batch system, we say that users log on to a access point (i.e., submit node, submit host) to submit their jobs to HTCondor, where the jobs wait (\"are queued\") until computing resources are available to run them. In a purely local HTCondor system, there are one to a few access points and many computing resources. An HTCondor access point can also be configured to forward excess jobs to an OSG-managed pool. This process is called flocking . If you already have an HTCondor pool, we recommend that you install this software on top of one of your existing HTCondor access points. This approach allows a user to submit locally and have their jobs run locally or, if the user chooses and if local resources are unavailable, have their jobs automatically flock to OSG. If you do not have an HTCondor batch system, following these instructions will install the HTCondor submit service and configure it only to forward jobs to the OSG. In other words, you do not need a whole HTCondor batch system just to have a local OSG access point. System Requirements \u00b6 The hardware requirement for an OSG access point depends on several factors such as number of users, number of jobs and for example how I/O intensity of those jobs. Our minimum recommended configuration is 6 cores, 12 GB RAM and 1 TB of local disk. The hardware can be bare metal or virtual machine, but we do not recommend containers as these submit host are running many system services which can be difficult to configure in a container. Also consider the following configuration requirements: Operating system: A RHEL 7 compatible operating system. User IDs: If it does not exist already, the installation will create the Linux user ID condor . Network: Inbound TCP port 9618 must be open. The access point must have a public IP address with both forward and reverse DNS configured. Scheduling a Planning Consultation \u00b6 Before participating in the OSG, either as a computational resource contributor or consumer, we ask that you contact us to set up a consultation. During this consultation, OSG staff will introduce you and your team to the OSG and develop a plan to meet your resource contribution and/or research goals. Initial Steps \u00b6 Register your access point in OSG Topology \u00b6 To be part of OSG, your access point should be registered with the OSG. You will need information like the hostname, and the administrative and security contacts. Follow the general registration instructions . For historical reasons, the service type is Submit Node . Request to be allowed to flock to OSG \u00b6 OSG staff will need to add your access point to the list of hosts that flocked jobs are accepted from, and provide a HTCondor IDTOKEN so that your new access point can authenticate with OSG. Send email to help@opensciencegrid.org with the hostname of your access point and request to be added to the list. Installing Required Software \u00b6 Flocking requires HTCondor software as well as software for reporting to the OSG accounting system. Start by setting up the OSG YUM repositories following the Installing Yum Repositories . Note that you have to use OSG 3.6 . Earlier versions will not work. Once the YUM repositories are setup, install the osg-flock convenience RPM that installs all required packages: root@host # yum install osg-flock Configuring Reporting via Gratia \u00b6 Reporting to the OSG accounting system is done using the Gratia service, which consists of multiple probes . HTCondor uses the \"condor\" probe, which is configured in /etc/gratia/condor/ProbeConfig ; we provide a recommended configuration for flocking. Copy over the recommended probe configuration: root@host # cp /etc/gratia/condor/ProbeConfig-flocking /etc/gratia/condor/ProbeConfig Fill in the value for ProbeName with the hostname of your access point, with the following format: ProbeName=\"condor: <HOSTNAME> \" Fill in the value for SiteName with the Resource Name you registered in Topology (see instructions above ). For example: SiteName=\"OSG_US_EXAMPLE_SUBMIT\" Enable the Gratia Probe. EnableProbe=1 Under the section Title2 make sure to set the following (if not already there): MapUnknownToGroup=\"1\" MapGroupToRole=\"1\" VOOverride=\"OSG\" Configuring Authentication \u00b6 Create a file named /etc/condor/tokens.d/flock.opensciencegrid.org with the IDTOKEN you received earlier. Ensure that there aren't any line breaks in this file (i.e., the entire token should only take up one line). Change the ownership to condor:condor and the permissions to 0600 . Verify this with ls -l /etc/condor/tokens.d/flock.opensciencegrid.org : # ls -l /etc/condor/tokens.d/flock.opensciencegrid.org -rw------- 1 condor condor 288 Nov 11 09:03 /etc/condor/tokens.d/flock.opensciencegrid.org Managing Services \u00b6 In addition to the HTCondor service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Notes Gratia gratia-probes-cron Accounting software HTcondor condor The following table gives the commands needed to start, stop, enable, and disable a service: To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> Usage \u00b6 Running jobs in OSG \u00b6 If your users are accustomed to running jobs locally, they may encounter some significant differences when running jobs in OSG. Users should be aware that OSG jobs are distributed across multiple institutions across a large geographical area. Each institution will have its own policy about the kinds of jobs that are allowed to run, and data transfer may be more complicated. The OSG Helpdesk Solutions page has information about what users should know; the Choosing Resources for Jobs and Data Management sections are particularly relevant. Specifying a project \u00b6 OSG will only run jobs that have a registered project associated with them. Users must follow the instructions for starting a project in OSG-Connect to register a project. A project is associated with a job by adding a ProjectName line to the user's submit file. For example: +ProjectName = \"My_Project\" The double quotes are necessary. Get Help \u00b6 If you need help with setup or troubleshooting, see our help procedure .","title":"Install an OS Pool Access Point"},{"location":"submit/osg-flock/#installing-an-open-science-pool-access-point","text":"This document explains how to add a path for user jobs to flow from your local site out to the OSG, which in most cases means that the jobs will have far more resources available to run on than locally. If your local batch system frequently has many jobs waiting to run for a long time, you do not have a local batch system, or if you simply want to provide a local entry point for OSG-bound jobs, adding a path to OSG may result in less waiting for your users. Note that if you do not have a local batch system, consider having your users use OSG Connect , which will require less infrastructure work at your site. Note Flocking to the OSG requires some modification to user workflows. After installation, see the usage section for instructions on what your users will need to do.","title":"Installing an Open Science Pool Access Point"},{"location":"submit/osg-flock/#background","text":"Every batch computing system has one or more entry points that users log on to and use to hand over their computing work to the batch system for completion. For the HTCondor batch system, we say that users log on to a access point (i.e., submit node, submit host) to submit their jobs to HTCondor, where the jobs wait (\"are queued\") until computing resources are available to run them. In a purely local HTCondor system, there are one to a few access points and many computing resources. An HTCondor access point can also be configured to forward excess jobs to an OSG-managed pool. This process is called flocking . If you already have an HTCondor pool, we recommend that you install this software on top of one of your existing HTCondor access points. This approach allows a user to submit locally and have their jobs run locally or, if the user chooses and if local resources are unavailable, have their jobs automatically flock to OSG. If you do not have an HTCondor batch system, following these instructions will install the HTCondor submit service and configure it only to forward jobs to the OSG. In other words, you do not need a whole HTCondor batch system just to have a local OSG access point.","title":"Background"},{"location":"submit/osg-flock/#system-requirements","text":"The hardware requirement for an OSG access point depends on several factors such as number of users, number of jobs and for example how I/O intensity of those jobs. Our minimum recommended configuration is 6 cores, 12 GB RAM and 1 TB of local disk. The hardware can be bare metal or virtual machine, but we do not recommend containers as these submit host are running many system services which can be difficult to configure in a container. Also consider the following configuration requirements: Operating system: A RHEL 7 compatible operating system. User IDs: If it does not exist already, the installation will create the Linux user ID condor . Network: Inbound TCP port 9618 must be open. The access point must have a public IP address with both forward and reverse DNS configured.","title":"System Requirements"},{"location":"submit/osg-flock/#scheduling-a-planning-consultation","text":"Before participating in the OSG, either as a computational resource contributor or consumer, we ask that you contact us to set up a consultation. During this consultation, OSG staff will introduce you and your team to the OSG and develop a plan to meet your resource contribution and/or research goals.","title":"Scheduling a Planning Consultation"},{"location":"submit/osg-flock/#initial-steps","text":"","title":"Initial Steps"},{"location":"submit/osg-flock/#register-your-access-point-in-osg-topology","text":"To be part of OSG, your access point should be registered with the OSG. You will need information like the hostname, and the administrative and security contacts. Follow the general registration instructions . For historical reasons, the service type is Submit Node .","title":"Register your access point in OSG Topology"},{"location":"submit/osg-flock/#request-to-be-allowed-to-flock-to-osg","text":"OSG staff will need to add your access point to the list of hosts that flocked jobs are accepted from, and provide a HTCondor IDTOKEN so that your new access point can authenticate with OSG. Send email to help@opensciencegrid.org with the hostname of your access point and request to be added to the list.","title":"Request to be allowed to flock to OSG"},{"location":"submit/osg-flock/#installing-required-software","text":"Flocking requires HTCondor software as well as software for reporting to the OSG accounting system. Start by setting up the OSG YUM repositories following the Installing Yum Repositories . Note that you have to use OSG 3.6 . Earlier versions will not work. Once the YUM repositories are setup, install the osg-flock convenience RPM that installs all required packages: root@host # yum install osg-flock","title":"Installing Required Software"},{"location":"submit/osg-flock/#configuring-reporting-via-gratia","text":"Reporting to the OSG accounting system is done using the Gratia service, which consists of multiple probes . HTCondor uses the \"condor\" probe, which is configured in /etc/gratia/condor/ProbeConfig ; we provide a recommended configuration for flocking. Copy over the recommended probe configuration: root@host # cp /etc/gratia/condor/ProbeConfig-flocking /etc/gratia/condor/ProbeConfig Fill in the value for ProbeName with the hostname of your access point, with the following format: ProbeName=\"condor: <HOSTNAME> \" Fill in the value for SiteName with the Resource Name you registered in Topology (see instructions above ). For example: SiteName=\"OSG_US_EXAMPLE_SUBMIT\" Enable the Gratia Probe. EnableProbe=1 Under the section Title2 make sure to set the following (if not already there): MapUnknownToGroup=\"1\" MapGroupToRole=\"1\" VOOverride=\"OSG\"","title":"Configuring Reporting via Gratia"},{"location":"submit/osg-flock/#configuring-authentication","text":"Create a file named /etc/condor/tokens.d/flock.opensciencegrid.org with the IDTOKEN you received earlier. Ensure that there aren't any line breaks in this file (i.e., the entire token should only take up one line). Change the ownership to condor:condor and the permissions to 0600 . Verify this with ls -l /etc/condor/tokens.d/flock.opensciencegrid.org : # ls -l /etc/condor/tokens.d/flock.opensciencegrid.org -rw------- 1 condor condor 288 Nov 11 09:03 /etc/condor/tokens.d/flock.opensciencegrid.org","title":"Configuring Authentication"},{"location":"submit/osg-flock/#managing-services","text":"In addition to the HTCondor service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Notes Gratia gratia-probes-cron Accounting software HTcondor condor The following table gives the commands needed to start, stop, enable, and disable a service: To... Run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME>","title":"Managing Services"},{"location":"submit/osg-flock/#usage","text":"","title":"Usage"},{"location":"submit/osg-flock/#running-jobs-in-osg","text":"If your users are accustomed to running jobs locally, they may encounter some significant differences when running jobs in OSG. Users should be aware that OSG jobs are distributed across multiple institutions across a large geographical area. Each institution will have its own policy about the kinds of jobs that are allowed to run, and data transfer may be more complicated. The OSG Helpdesk Solutions page has information about what users should know; the Choosing Resources for Jobs and Data Management sections are particularly relevant.","title":"Running jobs in OSG"},{"location":"submit/osg-flock/#specifying-a-project","text":"OSG will only run jobs that have a registered project associated with them. Users must follow the instructions for starting a project in OSG-Connect to register a project. A project is associated with a job by adding a ProjectName line to the user's submit file. For example: +ProjectName = \"My_Project\" The double quotes are necessary.","title":"Specifying a project"},{"location":"submit/osg-flock/#get-help","text":"If you need help with setup or troubleshooting, see our help procedure .","title":"Get Help"},{"location":"worker-node/install-cvmfs/","text":"Installing and Maintaining the CernVM File System Client \u00b6 EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an old version of the selinux-policy package The CernVM File System ( CVMFS ) is an HTTP-based file distribution service used to provide data and software for jobs. By installing CVMFS, you have access to an alternative installation method for required worker node software and your site you will be able to support a wider range of user jobs. For example, CVMFS provides easy access to the following: The worker node client CA and VO security data Software used by VOs Data stored in StashCache . Use this page to learn how to install, configure, run, test, and troubleshoot the CVMFS client from the OSG software repositories. Applicable versions The applicable software versions for this document are OSG Version >= 3.4.3. The version of CVMFS installed should be >= 2.4.1 Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If it does not exist already, the installation will create the cvmfs Linux user Group IDs: If they do not exist already, the installation will create the Linux groups cvmfs and fuse Network ports: You will need network access to a local squid server such as the squid distributed by OSG . The squid will need out-bound access to cvmfs stratum 1 servers. Host choice: - Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details below ) FUSE : CVMFS requires FUSE As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Installing CVMFS \u00b6 The following will install CVMFS from the OSG yum repository. It will also install fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution which is called OASIS. To simplify installation, OSG provides convenience RPMs that install all required software with a single command. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install CVMFS software: root@host # yum install osg-oasis Automount setup \u00b6 If automount is not yet in use on the system, do the following: root@host # systemctl enable autofs root@host # systemctl start autofs Put the following in /etc/auto.master.d/cvmfs.autofs : /cvmfs /etc/auto.cvmfs Restart autofs to make the change take effect: root@host # systemctl restart autofs Configuring CVMFS \u00b6 Create or edit /etc/cvmfs/default.local , a file that controls the CVMFS configuration. Below is a sample configuration, but please note that you will need to edit the parts in angle brackets . In particular, the CVMFS_HTTP_PROXY line below must be edited for your site. CVMFS_REPOSITORIES=\"$((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u|paste -sd ,)\" CVMFS_QUOTA_LIMIT=<QUOTA LIMIT> CVMFS_HTTP_PROXY=\"<SQUID URL>:<SQUID PORT>\" CVMFS by default allows any repository to be mounted, no matter what the setting of CVMFS_REPOSITORIES is; that variable is used by support tools such as cvmfs_config and cvmfs_talk when they need to know a list of repositories. The recommended CVMFS_REPOSITORIES setting above is so that those tools will use two common repositories plus any additional that have been mounted. You may want to choose a different set of always-known repositories. Set up a list of CVMFS HTTP proxies to retrieve from in CVMFS_HTTP_PROXY . If you do not have any squid at your site follow the instructions to install squid from OSG . Vertical bars separating proxies means to load balance between them and try them all before continuing. A semicolon between proxies means to try that one only after the previous ones have failed. For example: CVMFS_HTTP_PROXY=\"http://squid1.example.com:3128|http://squid2.example.com:3128;http://backup-squid.example.com:3128\" If no squid is available, it is acceptable for very small sites and laptops to set CVMFS_HTTP_PROXY=\"DIRECT\" . In that case, the OSG configuration sets the servers to be contacted through globally distributed caches . This is strongly discouraged for large sites because of the performance impact and because of the potential impact on the global caches. When there is at least one local proxy defined, the OSG configuration instead adds fallback proxies at Fermilab and CERN. Those fallback proxies are monitored by a WLCG team that will contact your site when your local proxy is failing and help you fix it. Set up the cache limit in CVMFS_QUOTA_LIMIT (in Megabytes). The recommended value for most applications is 20000 MB. This is the combined limit for all but the osgstorage.org repositories. This cache will be stored in /var/lib/cvmfs by default; to override the location, set CVMFS_CACHE_BASE in /etc/cvmfs/default.local . Note that an additional 1000 MB is allocated for a separate osgstorage.org repositories cache in $CVMFS_CACHE_BASE/osgstorage . To be safe, make sure that at least 20% more than $CVMFS_QUOTA_LIMIT + 1000 MB of space stays available for CVMFS in that filesystem. This is very important, since if that space is not available it can cause many I/O errors and application crashes. Many system administrators choose to put the cache space in a separate filesystem, which is a good way to manage it. Warning If you use SELinux and change CVMFS_CACHE_BASE , then the new cache directory must be labeled with SELinux type cvmfs_cache_t . This can be done by executing the following command: user@host $ chcon -R -t cvmfs_cache_t <CVMFS_CACHE_BASE> Validating CVMFS \u00b6 After CVMFS is installed, you should be able to see the /cvmfs directory. But note that it will initially appear to be empty: user@host $ ls /cvmfs user@host $ Directories within /cvmfs will not be mounted until you examine them. For instance: user@host $ ls /cvmfs user@host $ ls -l /cvmfs/atlas.cern.ch total 1 drwxr-xr-x 8 cvmfs cvmfs 3 Apr 13 14:50 repo user@host $ ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft total 1 lrwxrwxrwx 1 cvmfs cvmfs 18 May 13 2015 cms -> /cvmfs/cms.cern.ch user@host $ ls -l /cvmfs/glast.egi.eu total 5 drwxr-xr-x 9 cvmfs cvmfs 4096 Feb 7 2014 glast user@host $ ls -l /cvmfs/nova.osgstorage.org total 6 lrwxrwxrwx 1 cvmfs cvmfs 43 Jun 14 2016 analysis -> pnfs/fnal.gov/usr/nova/persistent/analysis/ lrwxrwxrwx 1 cvmfs cvmfs 32 Jan 19 11:40 flux -> pnfs/fnal.gov/usr/nova/data/flux drwxr-xr-x 3 cvmfs cvmfs 4096 Jan 19 11:39 pnfs user@host $ ls /cvmfs atlas.cern.ch glast.egi.eu oasis.opensciencegrid.org config-osg.opensciencegrid.org nova.osgstorage.org Troubleshooting problems \u00b6 If no directories exist under /cvmfs/ , you can try the following steps to debug: Mount it manually with mkdir -p /mnt/cvmfs and then mount -t cvmfs REPOSITORYNAME /mnt/cvmfs where REPOSITORYNAME is the repository, for example config-osg.opensciencegrid.org (which is the best one to try first because other repositories require it to be mounted). If this works, then CVMFS is working, but there is a problem with automount. If that doesn't work and doesn't give any explanatory errors, try cvmfs_config chksetup or cvmfs_config showconfig REPOSITORYNAME to verify your setup. If chksetup reports access problems to proxies, it may be caused by access control settings in the squids. If you have changed settings in /etc/cvmfs/default.local , and they do not seem to be taking effect, note that there are other configuration files that can override the settings. See the comments at the beginning of /etc/cvmfs/default.conf regarding the order in which configuration files are evaluated and look for old files that may have been left from a previous installation. More things to try are in the upstream documentation . Starting and Stopping services \u00b6 Once it is set up, CVMFS is always automatically started when one of the repositories are accessed; there are no system services to start. CVMFS can be stopped via: root@host # cvmfs_config umount Unmounting /cvmfs/config-osg.opensciencegrid.org: OK Unmounting /cvmfs/atlas.cern.ch: OK Unmounting /cvmfs/oasis.opensciencegrid.org: OK Unmounting /cvmfs/glast.egi.eu: OK Unmounting /cvmfs/nova.osgstorage.org: OK How to get Help? \u00b6 If you cannot resolve the problem, there are several ways to receive help: For bug reporting and OSG-specific issues, see our help procedure For community support and best-effort software team support contact osg-cvmfs@opensciencegrid.org . For general CernVM File System support contact cernvm.support@cern.ch . References \u00b6 http://cernvm.cern.ch/portal/filesystem/techinformation https://cvmfs.readthedocs.io/en/latest/ Users and Groups \u00b6 This installation will create one user unless it already exists User Comment cvmfs CernVM-FS service account The installation will also create a cvmfs group and default the cvmfs user to that group. In addition, if the fuse RPM is not already installed, installing cvmfs will also install fuse and that will create another group: Group Comment Group members cvmfs CernVM-FS service account none fuse FUSE service account cvmfs","title":"Install CVMFS"},{"location":"worker-node/install-cvmfs/#installing-and-maintaining-the-cernvm-file-system-client","text":"EL7 version compatibility There is an incompatibility with EL7 < 7.5 due to an old version of the selinux-policy package The CernVM File System ( CVMFS ) is an HTTP-based file distribution service used to provide data and software for jobs. By installing CVMFS, you have access to an alternative installation method for required worker node software and your site you will be able to support a wider range of user jobs. For example, CVMFS provides easy access to the following: The worker node client CA and VO security data Software used by VOs Data stored in StashCache . Use this page to learn how to install, configure, run, test, and troubleshoot the CVMFS client from the OSG software repositories. Applicable versions The applicable software versions for this document are OSG Version >= 3.4.3. The version of CVMFS installed should be >= 2.4.1","title":"Installing and Maintaining the CernVM File System Client"},{"location":"worker-node/install-cvmfs/#before-starting","text":"Before starting the installation process, consider the following points (consulting the Reference section below as needed): User IDs: If it does not exist already, the installation will create the cvmfs Linux user Group IDs: If they do not exist already, the installation will create the Linux groups cvmfs and fuse Network ports: You will need network access to a local squid server such as the squid distributed by OSG . The squid will need out-bound access to cvmfs stratum 1 servers. Host choice: - Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details below ) FUSE : CVMFS requires FUSE As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories","title":"Before Starting"},{"location":"worker-node/install-cvmfs/#installing-cvmfs","text":"The following will install CVMFS from the OSG yum repository. It will also install fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution which is called OASIS. To simplify installation, OSG provides convenience RPMs that install all required software with a single command. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install CVMFS software: root@host # yum install osg-oasis","title":"Installing CVMFS"},{"location":"worker-node/install-cvmfs/#automount-setup","text":"If automount is not yet in use on the system, do the following: root@host # systemctl enable autofs root@host # systemctl start autofs Put the following in /etc/auto.master.d/cvmfs.autofs : /cvmfs /etc/auto.cvmfs Restart autofs to make the change take effect: root@host # systemctl restart autofs","title":"Automount setup"},{"location":"worker-node/install-cvmfs/#configuring-cvmfs","text":"Create or edit /etc/cvmfs/default.local , a file that controls the CVMFS configuration. Below is a sample configuration, but please note that you will need to edit the parts in angle brackets . In particular, the CVMFS_HTTP_PROXY line below must be edited for your site. CVMFS_REPOSITORIES=\"$((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u|paste -sd ,)\" CVMFS_QUOTA_LIMIT=<QUOTA LIMIT> CVMFS_HTTP_PROXY=\"<SQUID URL>:<SQUID PORT>\" CVMFS by default allows any repository to be mounted, no matter what the setting of CVMFS_REPOSITORIES is; that variable is used by support tools such as cvmfs_config and cvmfs_talk when they need to know a list of repositories. The recommended CVMFS_REPOSITORIES setting above is so that those tools will use two common repositories plus any additional that have been mounted. You may want to choose a different set of always-known repositories. Set up a list of CVMFS HTTP proxies to retrieve from in CVMFS_HTTP_PROXY . If you do not have any squid at your site follow the instructions to install squid from OSG . Vertical bars separating proxies means to load balance between them and try them all before continuing. A semicolon between proxies means to try that one only after the previous ones have failed. For example: CVMFS_HTTP_PROXY=\"http://squid1.example.com:3128|http://squid2.example.com:3128;http://backup-squid.example.com:3128\" If no squid is available, it is acceptable for very small sites and laptops to set CVMFS_HTTP_PROXY=\"DIRECT\" . In that case, the OSG configuration sets the servers to be contacted through globally distributed caches . This is strongly discouraged for large sites because of the performance impact and because of the potential impact on the global caches. When there is at least one local proxy defined, the OSG configuration instead adds fallback proxies at Fermilab and CERN. Those fallback proxies are monitored by a WLCG team that will contact your site when your local proxy is failing and help you fix it. Set up the cache limit in CVMFS_QUOTA_LIMIT (in Megabytes). The recommended value for most applications is 20000 MB. This is the combined limit for all but the osgstorage.org repositories. This cache will be stored in /var/lib/cvmfs by default; to override the location, set CVMFS_CACHE_BASE in /etc/cvmfs/default.local . Note that an additional 1000 MB is allocated for a separate osgstorage.org repositories cache in $CVMFS_CACHE_BASE/osgstorage . To be safe, make sure that at least 20% more than $CVMFS_QUOTA_LIMIT + 1000 MB of space stays available for CVMFS in that filesystem. This is very important, since if that space is not available it can cause many I/O errors and application crashes. Many system administrators choose to put the cache space in a separate filesystem, which is a good way to manage it. Warning If you use SELinux and change CVMFS_CACHE_BASE , then the new cache directory must be labeled with SELinux type cvmfs_cache_t . This can be done by executing the following command: user@host $ chcon -R -t cvmfs_cache_t <CVMFS_CACHE_BASE>","title":"Configuring CVMFS"},{"location":"worker-node/install-cvmfs/#validating-cvmfs","text":"After CVMFS is installed, you should be able to see the /cvmfs directory. But note that it will initially appear to be empty: user@host $ ls /cvmfs user@host $ Directories within /cvmfs will not be mounted until you examine them. For instance: user@host $ ls /cvmfs user@host $ ls -l /cvmfs/atlas.cern.ch total 1 drwxr-xr-x 8 cvmfs cvmfs 3 Apr 13 14:50 repo user@host $ ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft total 1 lrwxrwxrwx 1 cvmfs cvmfs 18 May 13 2015 cms -> /cvmfs/cms.cern.ch user@host $ ls -l /cvmfs/glast.egi.eu total 5 drwxr-xr-x 9 cvmfs cvmfs 4096 Feb 7 2014 glast user@host $ ls -l /cvmfs/nova.osgstorage.org total 6 lrwxrwxrwx 1 cvmfs cvmfs 43 Jun 14 2016 analysis -> pnfs/fnal.gov/usr/nova/persistent/analysis/ lrwxrwxrwx 1 cvmfs cvmfs 32 Jan 19 11:40 flux -> pnfs/fnal.gov/usr/nova/data/flux drwxr-xr-x 3 cvmfs cvmfs 4096 Jan 19 11:39 pnfs user@host $ ls /cvmfs atlas.cern.ch glast.egi.eu oasis.opensciencegrid.org config-osg.opensciencegrid.org nova.osgstorage.org","title":"Validating CVMFS"},{"location":"worker-node/install-cvmfs/#troubleshooting-problems","text":"If no directories exist under /cvmfs/ , you can try the following steps to debug: Mount it manually with mkdir -p /mnt/cvmfs and then mount -t cvmfs REPOSITORYNAME /mnt/cvmfs where REPOSITORYNAME is the repository, for example config-osg.opensciencegrid.org (which is the best one to try first because other repositories require it to be mounted). If this works, then CVMFS is working, but there is a problem with automount. If that doesn't work and doesn't give any explanatory errors, try cvmfs_config chksetup or cvmfs_config showconfig REPOSITORYNAME to verify your setup. If chksetup reports access problems to proxies, it may be caused by access control settings in the squids. If you have changed settings in /etc/cvmfs/default.local , and they do not seem to be taking effect, note that there are other configuration files that can override the settings. See the comments at the beginning of /etc/cvmfs/default.conf regarding the order in which configuration files are evaluated and look for old files that may have been left from a previous installation. More things to try are in the upstream documentation .","title":"Troubleshooting problems"},{"location":"worker-node/install-cvmfs/#starting-and-stopping-services","text":"Once it is set up, CVMFS is always automatically started when one of the repositories are accessed; there are no system services to start. CVMFS can be stopped via: root@host # cvmfs_config umount Unmounting /cvmfs/config-osg.opensciencegrid.org: OK Unmounting /cvmfs/atlas.cern.ch: OK Unmounting /cvmfs/oasis.opensciencegrid.org: OK Unmounting /cvmfs/glast.egi.eu: OK Unmounting /cvmfs/nova.osgstorage.org: OK","title":"Starting and Stopping services"},{"location":"worker-node/install-cvmfs/#how-to-get-help","text":"If you cannot resolve the problem, there are several ways to receive help: For bug reporting and OSG-specific issues, see our help procedure For community support and best-effort software team support contact osg-cvmfs@opensciencegrid.org . For general CernVM File System support contact cernvm.support@cern.ch .","title":"How to get Help?"},{"location":"worker-node/install-cvmfs/#references","text":"http://cernvm.cern.ch/portal/filesystem/techinformation https://cvmfs.readthedocs.io/en/latest/","title":"References"},{"location":"worker-node/install-cvmfs/#users-and-groups","text":"This installation will create one user unless it already exists User Comment cvmfs CernVM-FS service account The installation will also create a cvmfs group and default the cvmfs user to that group. In addition, if the fuse RPM is not already installed, installing cvmfs will also install fuse and that will create another group: Group Comment Group members cvmfs CernVM-FS service account none fuse FUSE service account cvmfs","title":"Users and Groups"},{"location":"worker-node/install-singularity/","text":"Install Singularity \u00b6 Singularity is a tool that creates docker-like process containers but without giving extra privileges to unprivileged users. It is used by grid pilot jobs (which are submitted by per-VO grid workload management systems) to isolate user jobs from the pilot's files and processes and from other users' files and processes. It also supplies a chroot environment in order to run user jobs in different operating system images under one Linux kernel. Kernels with a version 3.10.0-957 or newer include a feature called unprivileged user namespaces that allows singularity to run completely unprivileged. This kernel version is the default for RHEL/CentOS/Scientific Linux 7.6 and is available for previous 7.x releases. Although the feature is available, it needs to be enabled to be usable (instructions below) on RHEL 7. The feature is enabled by default on RHEL 8. Without unprivileged user namespaces, singularity must be installed and run with setuid-root executables. Singularity keeps the privileged code to a minimum in order to reduce the potential for vulnerabilities. The OSG has installed singularity in OASIS , so most sites will not need to install singularity locally if they enable it to run unprivileged. An RPM installation can be configured to be unprivileged or privileged. Kernel vs. Userspace Security Enabling unprivileged user namespaces increases the risk to the kernel. However, the kernel is much more widely reviewed than singularity and the additional capability given to users is more limited. OSG Security considers the non-setuid, kernel-based method to have a lower security risk. This document is intended for system administrators that wish to enable, install, and/or configure singularity. Before Starting \u00b6 As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories . Note that with OSG 3.5 and later, the singularity RPM comes from the EPEL yum repository. OSG validates that distribution, and detailed instructions are still here. With unprivileged singularity, no yum repository is needed. In addition, this is highly recommended for image distribution: Install CVMFS Choosing Unprivileged vs Privileged Singularity \u00b6 There are two sets of instructions on this page: Enabling Unprivileged Singularity Singularity via RPM OSG VOs all support running singularity directly from CVMFS when unprivileged singularity is enabled. Unprivileged singularity is enabled by default on RHEL 8, and OSG recommends that system administrators enable it on RHEL 7 worker nodes. When unprivileged singularity is enabled, OSG recommends that sites not install the singularity RPM unless they have non-OSG users that require it. Sites that do install the RPM may choose to configure their RHEL 7 or later RPM installations to run unprivileged. In addition to improved security, unprivileged singularity enables condor_ssh_to_job to enter a container namespace without itself needing privileges. On the other hand, there are some rare use cases that require singularity to run privileged: Using single-file container images. Some systems, especially High Performance Computing (HPC) systems, deal poorly with collections of small files. In this case, container images stored in a single file (as opposed to an unpacked directory) may be needed. However, OSG VO container images are usually directory-based in CVMFS, and when possible we recommend disabling this feature on privileged installations in order to avoid potential kernel exploits. The overlay feature. The \"overlay\" feature of singularity uses the kernel overlayfs module to add bind mounts where mount points don't exist in the underlying image. However, singularity has an \"underlay\" feature that is equivalent which does not require privileges, and the overlay feature has been a source of security vulnerabilities in the past. For these reasons, we recommend replacing overlay with underlay even on privileged installations. overlayfs is also used to make the appearance of writable images when building containers, so it may be needed on some systems for that purpose. Enabling Unprivileged Singularity \u00b6 The instructions in this section are for enabling singularity to run unprivileged. Enable user namespaces via sysctl on EL 7: If the operating system is an EL 7 variant and has been updated to the EL 7.6 kernel or later, enable unprivileged singularity with the following steps. This step is not needed on EL 8 because it is enabled by default. root@host # echo \"user.max_user_namespaces = 15000\" \\ > /etc/sysctl.d/90-max_user_namespaces.conf root@host # sysctl -p /etc/sysctl.d/90-max_user_namespaces.conf (Optional) Disable network namespaces: root@host # echo \"user.max_net_namespaces = 0\" \\ > /etc/sysctl.d/90-max_net_namespaces.conf root@host # sysctl -p /etc/sysctl.d/90-max_net_namespaces.conf OSG VOs do not need network namespaces with singularity, and disabling them reduces the risk profile of enabling user namespaces. Network namespaces are, however, utilized by other software, such as Docker. Disabling network namespaces may break other software, or limit its capabilities (such as requiring the --net=host option in Docker). Disabling network namespaces blocks the systemd PrivateNetwork feature, which is a feature that is used by some EL 8 services. It is also configured for some EL 7 services but they are all disabled by default. To check them all, look for PrivateNetwork in /lib/systemd/system/*.service and see which of those services are enabled but failed to start. The only default such service on EL 8 is systemd-hostnamed, and a popular non-default such service is mlocate-updatedb. The PrivateNetwork feature can be turned off for a service without modifying an RPM-installed file through a <service>.d/*.conf file, for example for systemd-hostnamed: root@host # cd /etc/systemd/system root@host # mkdir -p systemd-hostnamed.service.d root@host # ( echo \"[Service]\" ; echo \"PrivateNetwork=no\" ) \\ >systemd-hostnamed.service.d/no-private-network.conf root@host # systemctl daemon-reload root@host # systemctl start systemd-hostnamed root@host # systemctl status systemd-hostnamed If docker is being used to run jobs, the following options are recommended to allow unprivileged singularity to run (it does not need --privileged or any added capabilities): --security-opt seccomp=unconfined --security-opt systempaths=unconfined --security-opt seccomp=unconfined enables unshare to be called (which is needed to create namespaces), and --security-opt systempaths=unconfined allows /proc to be mounted in an unprivileged process namespace (as done by singularity exec -p). --security-opt systempaths=unconfined requires Docker 19.03 or later. The options are secure as long as the system administrator controls the images and does not allow user code to run as root, and are generally more secure than adding capabilities. If at this point no setuid programs needs to be run within the container, adding the following option will add security by preventing any privilege escalation (singularity uses the same feature on its containers): --security-opt no-new-privileges In addition, the following option is recommended for allowing unprivileged fuse mounts on kernels that support that (RHEL >= 7.8): --device=/dev/fuse Configuring Unprivileged Singularity \u00b6 When unprivileged singularity is enabled and VOs run singularity from CVMFS, the singularity configuration file also comes from CVMFS so local sites have no control over changing the configuration. However, the most common local configuration change to the singularity RPM is to add additional local \"bind path\" options to map extra local file paths into containers. This can instead be accomplished by setting the SINGULARITY_BINDPATH variable in the environment of jobs, for example through configuration on your compute entrypoint. This is a comma-separated list of paths to bind, following the syntax of the singularity exec --bind option. There are also other environment variables that can affect singularity operation; see the singularity documentation for details. Validating Unprivileged Singularity \u00b6 If you haven't yet installed CVMFS , please do so. Alternatively, use the cvmfsexec package configured for osg as an unprivileged user and mount the oasis.opensciencegrid.org and singularity.opensciencegrid.org repositories. As an unprivileged user verify that singularity in OASIS works with this command: user@host $ /cvmfs/oasis.opensciencegrid.org/mis/singularity/bin/singularity \\ exec --contain --ipc --pid --bind /cvmfs \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest \\ ps -ef UID PID PPID C STIME TTY TIME CMD user 1 0 2 21:27 ? 00:00:00 shim-init user 2 1 0 21:27 ? 00:00:00 ps -ef Singularity via RPM \u00b6 The instructions in this section are for the singularity RPM, which includes setuid-root executables. The setuid-root executables can however be disabled by configuration, details below. Installing Singularity via RPM \u00b6 To install the singularity RPM, make sure that your host is up to date before installing the required packages: Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install Singularity root@host # yum install singularity Configuring Singularity RPM \u00b6 singularity includes an option called underlay that enables using bind mount points that do not exist in the container image. By default it is enabled, but only if the similar overlay option cannot be used, such as when running in unprivileged mode. It is recommended to completely disable overlay , because it is more vulnerable to security problems than underlay . Set this option in /etc/singularity/singularity.conf : enable overlay = no Warning If you modify /etc/singularity/singularity.conf , be careful with your upgrade procedures. RPM will not automatically merge your changes with new upstream configuration keys, which may cause a broken install or inadvertently change the site configuration. Singularity changes its default configuration file more frequently than typical OSG software. Look for singularity.conf.rpmnew after upgrades and merge in any changes to the defaults. Configuring the RPM to be Unprivileged \u00b6 If you choose to run the RPM unprivileged, after enabling unprivileged singularity , change the line in /etc/singularity/singularity.conf that says allow setuid = yes to allow setuid = no Note that the setuid-root executables stay installed, but they will exit very early if invoked when the configuration file disallows setuid, so the risk is very low. There are non-setuid equivalent executables that are used instead when setuid is disallowed. Limiting Image Types \u00b6 A side effect of disabling privileged singularity is that loopback mounts are disallowed. If the installation is privileged, also consider the following. Images based on loopback devices carry an inherently higher exposure to unknown kernel exploits compared to directory-based images distributed via CVMFS. See this article for further discussion. The loopback-based images are the default image type produced by singularity users and are common at sites with direct user logins. However (as of April 2019) we are only aware of directory-based images being used by OSG VOs. Hence, it is reasonable to disable the loopback-based images by setting the following option in /etc/singularity/singularity.conf : max loop devices = 0 While reasonable for some sites, this is not required as there are currently no public kernel exploits for this issue; any exploits are patched by Red Hat when they are discovered. Validating Singularity RPM \u00b6 After singularity is installed, as an ordinary user run the following command to verify it: user@host $ singularity exec --contain --ipc --pid --bind /cvmfs \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest \\ ps -ef WARNING: Container does not have an exec helper script, calling 'ps' directly UID PID PPID C STIME TTY TIME CMD user 1 0 1 21:41 ? 00:00:00 shim-init user 2 1 0 21:41 ? 00:00:00 ps -ef Starting and Stopping Services \u00b6 singularity has no services to start or stop. References \u00b6 Singularity Documentation Singularity Support Additional guidance for CMS sites","title":"Install Singularity"},{"location":"worker-node/install-singularity/#install-singularity","text":"Singularity is a tool that creates docker-like process containers but without giving extra privileges to unprivileged users. It is used by grid pilot jobs (which are submitted by per-VO grid workload management systems) to isolate user jobs from the pilot's files and processes and from other users' files and processes. It also supplies a chroot environment in order to run user jobs in different operating system images under one Linux kernel. Kernels with a version 3.10.0-957 or newer include a feature called unprivileged user namespaces that allows singularity to run completely unprivileged. This kernel version is the default for RHEL/CentOS/Scientific Linux 7.6 and is available for previous 7.x releases. Although the feature is available, it needs to be enabled to be usable (instructions below) on RHEL 7. The feature is enabled by default on RHEL 8. Without unprivileged user namespaces, singularity must be installed and run with setuid-root executables. Singularity keeps the privileged code to a minimum in order to reduce the potential for vulnerabilities. The OSG has installed singularity in OASIS , so most sites will not need to install singularity locally if they enable it to run unprivileged. An RPM installation can be configured to be unprivileged or privileged. Kernel vs. Userspace Security Enabling unprivileged user namespaces increases the risk to the kernel. However, the kernel is much more widely reviewed than singularity and the additional capability given to users is more limited. OSG Security considers the non-setuid, kernel-based method to have a lower security risk. This document is intended for system administrators that wish to enable, install, and/or configure singularity.","title":"Install Singularity"},{"location":"worker-node/install-singularity/#before-starting","text":"As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories . Note that with OSG 3.5 and later, the singularity RPM comes from the EPEL yum repository. OSG validates that distribution, and detailed instructions are still here. With unprivileged singularity, no yum repository is needed. In addition, this is highly recommended for image distribution: Install CVMFS","title":"Before Starting"},{"location":"worker-node/install-singularity/#choosing-unprivileged-vs-privileged-singularity","text":"There are two sets of instructions on this page: Enabling Unprivileged Singularity Singularity via RPM OSG VOs all support running singularity directly from CVMFS when unprivileged singularity is enabled. Unprivileged singularity is enabled by default on RHEL 8, and OSG recommends that system administrators enable it on RHEL 7 worker nodes. When unprivileged singularity is enabled, OSG recommends that sites not install the singularity RPM unless they have non-OSG users that require it. Sites that do install the RPM may choose to configure their RHEL 7 or later RPM installations to run unprivileged. In addition to improved security, unprivileged singularity enables condor_ssh_to_job to enter a container namespace without itself needing privileges. On the other hand, there are some rare use cases that require singularity to run privileged: Using single-file container images. Some systems, especially High Performance Computing (HPC) systems, deal poorly with collections of small files. In this case, container images stored in a single file (as opposed to an unpacked directory) may be needed. However, OSG VO container images are usually directory-based in CVMFS, and when possible we recommend disabling this feature on privileged installations in order to avoid potential kernel exploits. The overlay feature. The \"overlay\" feature of singularity uses the kernel overlayfs module to add bind mounts where mount points don't exist in the underlying image. However, singularity has an \"underlay\" feature that is equivalent which does not require privileges, and the overlay feature has been a source of security vulnerabilities in the past. For these reasons, we recommend replacing overlay with underlay even on privileged installations. overlayfs is also used to make the appearance of writable images when building containers, so it may be needed on some systems for that purpose.","title":"Choosing Unprivileged vs Privileged Singularity"},{"location":"worker-node/install-singularity/#enabling-unprivileged-singularity","text":"The instructions in this section are for enabling singularity to run unprivileged. Enable user namespaces via sysctl on EL 7: If the operating system is an EL 7 variant and has been updated to the EL 7.6 kernel or later, enable unprivileged singularity with the following steps. This step is not needed on EL 8 because it is enabled by default. root@host # echo \"user.max_user_namespaces = 15000\" \\ > /etc/sysctl.d/90-max_user_namespaces.conf root@host # sysctl -p /etc/sysctl.d/90-max_user_namespaces.conf (Optional) Disable network namespaces: root@host # echo \"user.max_net_namespaces = 0\" \\ > /etc/sysctl.d/90-max_net_namespaces.conf root@host # sysctl -p /etc/sysctl.d/90-max_net_namespaces.conf OSG VOs do not need network namespaces with singularity, and disabling them reduces the risk profile of enabling user namespaces. Network namespaces are, however, utilized by other software, such as Docker. Disabling network namespaces may break other software, or limit its capabilities (such as requiring the --net=host option in Docker). Disabling network namespaces blocks the systemd PrivateNetwork feature, which is a feature that is used by some EL 8 services. It is also configured for some EL 7 services but they are all disabled by default. To check them all, look for PrivateNetwork in /lib/systemd/system/*.service and see which of those services are enabled but failed to start. The only default such service on EL 8 is systemd-hostnamed, and a popular non-default such service is mlocate-updatedb. The PrivateNetwork feature can be turned off for a service without modifying an RPM-installed file through a <service>.d/*.conf file, for example for systemd-hostnamed: root@host # cd /etc/systemd/system root@host # mkdir -p systemd-hostnamed.service.d root@host # ( echo \"[Service]\" ; echo \"PrivateNetwork=no\" ) \\ >systemd-hostnamed.service.d/no-private-network.conf root@host # systemctl daemon-reload root@host # systemctl start systemd-hostnamed root@host # systemctl status systemd-hostnamed If docker is being used to run jobs, the following options are recommended to allow unprivileged singularity to run (it does not need --privileged or any added capabilities): --security-opt seccomp=unconfined --security-opt systempaths=unconfined --security-opt seccomp=unconfined enables unshare to be called (which is needed to create namespaces), and --security-opt systempaths=unconfined allows /proc to be mounted in an unprivileged process namespace (as done by singularity exec -p). --security-opt systempaths=unconfined requires Docker 19.03 or later. The options are secure as long as the system administrator controls the images and does not allow user code to run as root, and are generally more secure than adding capabilities. If at this point no setuid programs needs to be run within the container, adding the following option will add security by preventing any privilege escalation (singularity uses the same feature on its containers): --security-opt no-new-privileges In addition, the following option is recommended for allowing unprivileged fuse mounts on kernels that support that (RHEL >= 7.8): --device=/dev/fuse","title":"Enabling Unprivileged Singularity"},{"location":"worker-node/install-singularity/#configuring-unprivileged-singularity","text":"When unprivileged singularity is enabled and VOs run singularity from CVMFS, the singularity configuration file also comes from CVMFS so local sites have no control over changing the configuration. However, the most common local configuration change to the singularity RPM is to add additional local \"bind path\" options to map extra local file paths into containers. This can instead be accomplished by setting the SINGULARITY_BINDPATH variable in the environment of jobs, for example through configuration on your compute entrypoint. This is a comma-separated list of paths to bind, following the syntax of the singularity exec --bind option. There are also other environment variables that can affect singularity operation; see the singularity documentation for details.","title":"Configuring Unprivileged Singularity"},{"location":"worker-node/install-singularity/#validating-unprivileged-singularity","text":"If you haven't yet installed CVMFS , please do so. Alternatively, use the cvmfsexec package configured for osg as an unprivileged user and mount the oasis.opensciencegrid.org and singularity.opensciencegrid.org repositories. As an unprivileged user verify that singularity in OASIS works with this command: user@host $ /cvmfs/oasis.opensciencegrid.org/mis/singularity/bin/singularity \\ exec --contain --ipc --pid --bind /cvmfs \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest \\ ps -ef UID PID PPID C STIME TTY TIME CMD user 1 0 2 21:27 ? 00:00:00 shim-init user 2 1 0 21:27 ? 00:00:00 ps -ef","title":"Validating Unprivileged Singularity"},{"location":"worker-node/install-singularity/#singularity-via-rpm","text":"The instructions in this section are for the singularity RPM, which includes setuid-root executables. The setuid-root executables can however be disabled by configuration, details below.","title":"Singularity via RPM"},{"location":"worker-node/install-singularity/#installing-singularity-via-rpm","text":"To install the singularity RPM, make sure that your host is up to date before installing the required packages: Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install Singularity root@host # yum install singularity","title":"Installing Singularity via RPM"},{"location":"worker-node/install-singularity/#configuring-singularity-rpm","text":"singularity includes an option called underlay that enables using bind mount points that do not exist in the container image. By default it is enabled, but only if the similar overlay option cannot be used, such as when running in unprivileged mode. It is recommended to completely disable overlay , because it is more vulnerable to security problems than underlay . Set this option in /etc/singularity/singularity.conf : enable overlay = no Warning If you modify /etc/singularity/singularity.conf , be careful with your upgrade procedures. RPM will not automatically merge your changes with new upstream configuration keys, which may cause a broken install or inadvertently change the site configuration. Singularity changes its default configuration file more frequently than typical OSG software. Look for singularity.conf.rpmnew after upgrades and merge in any changes to the defaults.","title":"Configuring Singularity RPM"},{"location":"worker-node/install-singularity/#configuring-the-rpm-to-be-unprivileged","text":"If you choose to run the RPM unprivileged, after enabling unprivileged singularity , change the line in /etc/singularity/singularity.conf that says allow setuid = yes to allow setuid = no Note that the setuid-root executables stay installed, but they will exit very early if invoked when the configuration file disallows setuid, so the risk is very low. There are non-setuid equivalent executables that are used instead when setuid is disallowed.","title":"Configuring the RPM to be Unprivileged"},{"location":"worker-node/install-singularity/#limiting-image-types","text":"A side effect of disabling privileged singularity is that loopback mounts are disallowed. If the installation is privileged, also consider the following. Images based on loopback devices carry an inherently higher exposure to unknown kernel exploits compared to directory-based images distributed via CVMFS. See this article for further discussion. The loopback-based images are the default image type produced by singularity users and are common at sites with direct user logins. However (as of April 2019) we are only aware of directory-based images being used by OSG VOs. Hence, it is reasonable to disable the loopback-based images by setting the following option in /etc/singularity/singularity.conf : max loop devices = 0 While reasonable for some sites, this is not required as there are currently no public kernel exploits for this issue; any exploits are patched by Red Hat when they are discovered.","title":"Limiting Image Types"},{"location":"worker-node/install-singularity/#validating-singularity-rpm","text":"After singularity is installed, as an ordinary user run the following command to verify it: user@host $ singularity exec --contain --ipc --pid --bind /cvmfs \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest \\ ps -ef WARNING: Container does not have an exec helper script, calling 'ps' directly UID PID PPID C STIME TTY TIME CMD user 1 0 1 21:41 ? 00:00:00 shim-init user 2 1 0 21:41 ? 00:00:00 ps -ef","title":"Validating Singularity RPM"},{"location":"worker-node/install-singularity/#starting-and-stopping-services","text":"singularity has no services to start or stop.","title":"Starting and Stopping Services"},{"location":"worker-node/install-singularity/#references","text":"Singularity Documentation Singularity Support Additional guidance for CMS sites","title":"References"},{"location":"worker-node/install-wn-oasis/","text":"Installing the Worker Node Client via OASIS \u00b6 The OSG Worker Node Client is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. Contents of the worker node client can be found here . Note It is possible to install the Worker Node Client software in a variety of ways, depending on your local site: Use from OASIS (this guide) - useful when CVMFS is already mounted on your worker nodes Install using a tarball - useful when installing onto a shared filesystem for distribution to worker nodes Install using RPMs and Yum - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS. Before Starting \u00b6 As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system On every worker node, install and configure CVMFS Once configured to use OASIS, grid jobs will download the worker-node software on demand (into the local disk cache). This may result in extra network activity, especially on first use of the client tools. Configure the CE \u00b6 Determine the OASIS path to the Worker Node Client software for your worker nodes: Worker Node OS Use\u2026 EL 7 (64-bit) /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el7-x86_64 EL 8 (64-bit) /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el8-x86_64 On the CE, in the /etc/osg/config.d/10-storage.ini file, set the grid_dir configuration setting to the path from the previous step. Once you finish making changes to configuration files on your CE, validate, fix, and apply the configuration: root@host # osg-configure -v root@host # osg-configure -c For more information, see the OSG worker node environment documentation and the CE configuration instructions . Validating the Worker Node Client \u00b6 To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output. Submit a job that executes the env command (e.g. Run condor_ce_trace with the -d flag from your HTCondor CE) Verify that the value of OSG_GRID is set to the directory of your WN Client installation Manually Using the Worker Node Client From OASIS \u00b6 If you must log onto a worker node and use the Worker Node Client software directly during your login session, consult the following table for the command to set up your environment: Worker Node OS Run the following command\u2026 EL 7 (64-bit) source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el7-x86_64/setup.sh EL 8 (64-bit) source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el8-x86_64/setup.sh Getting Help \u00b6 To get assistance, please use this page .","title":"Install from OASIS"},{"location":"worker-node/install-wn-oasis/#installing-the-worker-node-client-via-oasis","text":"The OSG Worker Node Client is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. Contents of the worker node client can be found here . Note It is possible to install the Worker Node Client software in a variety of ways, depending on your local site: Use from OASIS (this guide) - useful when CVMFS is already mounted on your worker nodes Install using a tarball - useful when installing onto a shared filesystem for distribution to worker nodes Install using RPMs and Yum - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.","title":"Installing the Worker Node Client via OASIS"},{"location":"worker-node/install-wn-oasis/#before-starting","text":"As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system On every worker node, install and configure CVMFS Once configured to use OASIS, grid jobs will download the worker-node software on demand (into the local disk cache). This may result in extra network activity, especially on first use of the client tools.","title":"Before Starting"},{"location":"worker-node/install-wn-oasis/#configure-the-ce","text":"Determine the OASIS path to the Worker Node Client software for your worker nodes: Worker Node OS Use\u2026 EL 7 (64-bit) /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el7-x86_64 EL 8 (64-bit) /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el8-x86_64 On the CE, in the /etc/osg/config.d/10-storage.ini file, set the grid_dir configuration setting to the path from the previous step. Once you finish making changes to configuration files on your CE, validate, fix, and apply the configuration: root@host # osg-configure -v root@host # osg-configure -c For more information, see the OSG worker node environment documentation and the CE configuration instructions .","title":"Configure the CE"},{"location":"worker-node/install-wn-oasis/#validating-the-worker-node-client","text":"To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output. Submit a job that executes the env command (e.g. Run condor_ce_trace with the -d flag from your HTCondor CE) Verify that the value of OSG_GRID is set to the directory of your WN Client installation","title":"Validating the Worker Node Client"},{"location":"worker-node/install-wn-oasis/#manually-using-the-worker-node-client-from-oasis","text":"If you must log onto a worker node and use the Worker Node Client software directly during your login session, consult the following table for the command to set up your environment: Worker Node OS Run the following command\u2026 EL 7 (64-bit) source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el7-x86_64/setup.sh EL 8 (64-bit) source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el8-x86_64/setup.sh","title":"Manually Using the Worker Node Client From OASIS"},{"location":"worker-node/install-wn-oasis/#getting-help","text":"To get assistance, please use this page .","title":"Getting Help"},{"location":"worker-node/install-wn-tarball/","text":"Installing the Worker Node Client via Tarball \u00b6 The OSG Worker Node Client is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. Contents of the worker node client can be found here . Note It is possible to install the Worker Node Client software in a variety of ways, depending on your local site: Install using a tarball (this guide) - useful when installing onto a shared filesystem for distribution to worker nodes Use from OASIS - useful when worker nodes already mount CVMFS Install using RPMs and Yum - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs This document is intended to guide users through the process of installing the worker node software and configuring the installed worker node software. Although this document is oriented to system administrators, any unprivileged user may install and use the client. Before starting, ensure the host has a supported operating system . Download the WN Client \u00b6 Please pick the osg-wn-client tarball that is appropriate for your distribution and architecture. You will find them in https://repo.opensciencegrid.org/tarball-install/ . For OSG 3.5: Binaries for RHEL7 Binaries for RHEL8 Install the WN Client \u00b6 Unpack the tarball. Move the directory that was created to where you want the tarball client to be. Run osg-post-install ( <PATH_TO_CLIENT>/osg/osg-post-install ) to fix the directories in the installation. Source the setup source <PATH_TO_CLIENT>/setup.sh (or setup.csh depending on the shell). Download and set up CA certificates using osg-ca-manage (See the CA management documentation for the available options). Download CRLs using fetch-crl . Note The WN client requires a Perl interpreter to be available in /usr/bin/perl . If not present, install by running yum install perl as root. Warning Once osg-post-install is run to relocate the install, it cannot be run again. You will need to unpack a fresh copy. Example installation (in /home/user/test-install , the <PATH_TO_CLIENT>/ is /home/user/test-install/osg-wn-client ): user@host $ mkdir /home/user/test-install user@host $ cd /home/user/test-install user@host $ wget https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz user@host $ tar xzf osg-wn-client-latest.el7.x86_64.tar.gz user@host $ cd osg-wn-client user@host $ ./osg/osg-post-install user@host $ source setup.sh user@host $ osg-ca-manage setupCA --url osg user@host $ fetch-crl Configure the CE \u00b6 Using the wn-client software installed from the tarball will require a few changes on the compute entrypoint so that the resource's configuration can be correctly reported. Set grid_dir in the Storage section of your OSG-Configure configs: CE configuration instructions . grid_dir is used as the $OSG_GRID environment variable in running jobs - see the worker node environment document . Pilot jobs source $OSG_GRID/setup.sh before performing any work. The value set for grid_dir must be the path of the wn-client installation directory. This is the path returned by echo $OSG_LOCATION once you source the setup file created by this installation. Services \u00b6 The WN client is a collection of client programs that do not require service startup or shutdown. The only services are osg-update-certs that keeps the CA certificates up-to-date and fetch-crl that keeps the CRLs up-to-date. Following the instructions below you'll add the services to your crontab that will take care to run them periodically until you remove them. Auto-updating certificates and CRLs \u00b6 You must create cron jobs to run fetch-crl and osg-update-certs to update your CRLs and certificates automatically. Here is what they should look like. (Note: fill in <OSG_LOCATION> with the full path of your tarball install, including osg-wn-client that is created by the tarball). # Cron job to update certs. # Runs every hour by default, though does not update certs until they're at # least 24 hours old. There is a random sleep time for up to 45 minutes (2700 # seconds) to avoid overloading cert servers. 10 * * * * ( . <OSG_LOCATION>/setup.sh && osg-update-certs --random-sleep 2700 --called-from-cron ) # Cron job to update CRLs # Runs every 6 hours at, 45 minutes +/- 3 minutes. 42 */6 * * * ( . <OSG_LOCATION>/setup.sh && fetch-crl -q -r 360 ) You might want to configure proxy settings in $OSG_LOCATION/etc/fetch-crl.conf . Enabling and Disabling Services \u00b6 To enable the CRL updates, you must edit your cron with crontab -e and add the lines above. To disable, remove the lines from the crontab . Validating the Worker Node Client \u00b6 To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output. Submit a job that executes the env command (e.g. Run condor_ce_trace with the -d flag from your HTCondor CE) Verify that the value of $OSG_GRID is set to the directory of your worker node client installation How to get Help? \u00b6 To get assistance please use this Help Procedure .","title":"Install from Tarball"},{"location":"worker-node/install-wn-tarball/#installing-the-worker-node-client-via-tarball","text":"The OSG Worker Node Client is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. Contents of the worker node client can be found here . Note It is possible to install the Worker Node Client software in a variety of ways, depending on your local site: Install using a tarball (this guide) - useful when installing onto a shared filesystem for distribution to worker nodes Use from OASIS - useful when worker nodes already mount CVMFS Install using RPMs and Yum - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs This document is intended to guide users through the process of installing the worker node software and configuring the installed worker node software. Although this document is oriented to system administrators, any unprivileged user may install and use the client. Before starting, ensure the host has a supported operating system .","title":"Installing the Worker Node Client via Tarball"},{"location":"worker-node/install-wn-tarball/#download-the-wn-client","text":"Please pick the osg-wn-client tarball that is appropriate for your distribution and architecture. You will find them in https://repo.opensciencegrid.org/tarball-install/ . For OSG 3.5: Binaries for RHEL7 Binaries for RHEL8","title":"Download the WN Client"},{"location":"worker-node/install-wn-tarball/#install-the-wn-client","text":"Unpack the tarball. Move the directory that was created to where you want the tarball client to be. Run osg-post-install ( <PATH_TO_CLIENT>/osg/osg-post-install ) to fix the directories in the installation. Source the setup source <PATH_TO_CLIENT>/setup.sh (or setup.csh depending on the shell). Download and set up CA certificates using osg-ca-manage (See the CA management documentation for the available options). Download CRLs using fetch-crl . Note The WN client requires a Perl interpreter to be available in /usr/bin/perl . If not present, install by running yum install perl as root. Warning Once osg-post-install is run to relocate the install, it cannot be run again. You will need to unpack a fresh copy. Example installation (in /home/user/test-install , the <PATH_TO_CLIENT>/ is /home/user/test-install/osg-wn-client ): user@host $ mkdir /home/user/test-install user@host $ cd /home/user/test-install user@host $ wget https://repo.opensciencegrid.org/tarball-install/3.5/osg-wn-client-latest.el7.x86_64.tar.gz user@host $ tar xzf osg-wn-client-latest.el7.x86_64.tar.gz user@host $ cd osg-wn-client user@host $ ./osg/osg-post-install user@host $ source setup.sh user@host $ osg-ca-manage setupCA --url osg user@host $ fetch-crl","title":"Install the WN Client"},{"location":"worker-node/install-wn-tarball/#configure-the-ce","text":"Using the wn-client software installed from the tarball will require a few changes on the compute entrypoint so that the resource's configuration can be correctly reported. Set grid_dir in the Storage section of your OSG-Configure configs: CE configuration instructions . grid_dir is used as the $OSG_GRID environment variable in running jobs - see the worker node environment document . Pilot jobs source $OSG_GRID/setup.sh before performing any work. The value set for grid_dir must be the path of the wn-client installation directory. This is the path returned by echo $OSG_LOCATION once you source the setup file created by this installation.","title":"Configure the CE"},{"location":"worker-node/install-wn-tarball/#services","text":"The WN client is a collection of client programs that do not require service startup or shutdown. The only services are osg-update-certs that keeps the CA certificates up-to-date and fetch-crl that keeps the CRLs up-to-date. Following the instructions below you'll add the services to your crontab that will take care to run them periodically until you remove them.","title":"Services"},{"location":"worker-node/install-wn-tarball/#auto-updating-certificates-and-crls","text":"You must create cron jobs to run fetch-crl and osg-update-certs to update your CRLs and certificates automatically. Here is what they should look like. (Note: fill in <OSG_LOCATION> with the full path of your tarball install, including osg-wn-client that is created by the tarball). # Cron job to update certs. # Runs every hour by default, though does not update certs until they're at # least 24 hours old. There is a random sleep time for up to 45 minutes (2700 # seconds) to avoid overloading cert servers. 10 * * * * ( . <OSG_LOCATION>/setup.sh && osg-update-certs --random-sleep 2700 --called-from-cron ) # Cron job to update CRLs # Runs every 6 hours at, 45 minutes +/- 3 minutes. 42 */6 * * * ( . <OSG_LOCATION>/setup.sh && fetch-crl -q -r 360 ) You might want to configure proxy settings in $OSG_LOCATION/etc/fetch-crl.conf .","title":"Auto-updating certificates and CRLs"},{"location":"worker-node/install-wn-tarball/#enabling-and-disabling-services","text":"To enable the CRL updates, you must edit your cron with crontab -e and add the lines above. To disable, remove the lines from the crontab .","title":"Enabling and Disabling Services"},{"location":"worker-node/install-wn-tarball/#validating-the-worker-node-client","text":"To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output. Submit a job that executes the env command (e.g. Run condor_ce_trace with the -d flag from your HTCondor CE) Verify that the value of $OSG_GRID is set to the directory of your worker node client installation","title":"Validating the Worker Node Client"},{"location":"worker-node/install-wn-tarball/#how-to-get-help","text":"To get assistance please use this Help Procedure .","title":"How to get Help?"},{"location":"worker-node/install-wn/","text":"Installing the Worker Node Client From RPMs \u00b6 The OSG Worker Node Client is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. Contents of the worker node client can be found here . Note It is possible to install the Worker Node Client software in a variety of ways, depending on your local site: Install using RPMs and Yum (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs Use from OASIS - useful when worker nodes already mount CVMFS Install using a tarball - useful when installing onto a shared filesystem for distribution to worker nodes This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM. Before Starting \u00b6 As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates Install the Worker Node Client \u00b6 Install the Worker Node Client RPM: root@host # yum install osg-wn-client Services \u00b6 Fetch-CRL is the only service required to support the WN Client. Software Service name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation for more info Note fetch-crl-boot will begin fetching CRLS, which can take a few minutes and fail on transient errors. You can add configuration to ignore these transient errors in /etc/fetch-crl.conf : noerrors As a reminder, here are common service commands (all run as root ): To \u2026 Run the command \u2026 Start a service service SERVICE-NAME start Stop a service service SERVICE-NAME stop Enable a service to start during boot chkconfig SERVICE-NAME on Disable a service from starting during boot chkconfig SERVICE-NAME off Validating the Worker Node Client \u00b6 To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output. Submit a job that executes the env command (e.g. Run condor_ce_trace with the -d flag from your HTCondor CE) Verify that the value of OSG_GRID is set to /etc/osg/wn-client How to get Help? \u00b6 To get assistance please use this Help Procedure . Reference \u00b6 Please see the documentation on using Yum and RPM , and using the OSG Yum repositories . Worker node contents \u00b6 The worker node may be updated from time to time. As of OSG 3.3.21 in February 2017, the OSG worker node client contains: OSG Certificates curl Fetch CRL FTS client gfal2 globus-url-copy (GridFTP client) globus-xio-udt-driver ldapsearch MyProxy osg-system-profiler UberFTP vo-client (includes /etc/vomses file) VOMS client wget xrdcp To see the currently installed version of the worker node package, run the following command: root@host # rpm -q --requires osg-wn-client Click here for more details on using RPM to see what was installed.","title":"Install from RPM"},{"location":"worker-node/install-wn/#installing-the-worker-node-client-from-rpms","text":"The OSG Worker Node Client is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. Contents of the worker node client can be found here . Note It is possible to install the Worker Node Client software in a variety of ways, depending on your local site: Install using RPMs and Yum (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs Use from OASIS - useful when worker nodes already mount CVMFS Install using a tarball - useful when installing onto a shared filesystem for distribution to worker nodes This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.","title":"Installing the Worker Node Client From RPMs"},{"location":"worker-node/install-wn/#before-starting","text":"As with all OSG software installations, there are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system Obtain root access to the host Prepare the required Yum repositories Install CA certificates","title":"Before Starting"},{"location":"worker-node/install-wn/#install-the-worker-node-client","text":"Install the Worker Node Client RPM: root@host # yum install osg-wn-client","title":"Install the Worker Node Client"},{"location":"worker-node/install-wn/#services","text":"Fetch-CRL is the only service required to support the WN Client. Software Service name Notes Fetch CRL fetch-crl-boot and fetch-crl-cron See CA documentation for more info Note fetch-crl-boot will begin fetching CRLS, which can take a few minutes and fail on transient errors. You can add configuration to ignore these transient errors in /etc/fetch-crl.conf : noerrors As a reminder, here are common service commands (all run as root ): To \u2026 Run the command \u2026 Start a service service SERVICE-NAME start Stop a service service SERVICE-NAME stop Enable a service to start during boot chkconfig SERVICE-NAME on Disable a service from starting during boot chkconfig SERVICE-NAME off","title":"Services"},{"location":"worker-node/install-wn/#validating-the-worker-node-client","text":"To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output. Submit a job that executes the env command (e.g. Run condor_ce_trace with the -d flag from your HTCondor CE) Verify that the value of OSG_GRID is set to /etc/osg/wn-client","title":"Validating the Worker Node Client"},{"location":"worker-node/install-wn/#how-to-get-help","text":"To get assistance please use this Help Procedure .","title":"How to get Help?"},{"location":"worker-node/install-wn/#reference","text":"Please see the documentation on using Yum and RPM , and using the OSG Yum repositories .","title":"Reference"},{"location":"worker-node/install-wn/#worker-node-contents","text":"The worker node may be updated from time to time. As of OSG 3.3.21 in February 2017, the OSG worker node client contains: OSG Certificates curl Fetch CRL FTS client gfal2 globus-url-copy (GridFTP client) globus-xio-udt-driver ldapsearch MyProxy osg-system-profiler UberFTP vo-client (includes /etc/vomses file) VOMS client wget xrdcp To see the currently installed version of the worker node package, run the following command: root@host # rpm -q --requires osg-wn-client Click here for more details on using RPM to see what was installed.","title":"Worker node contents"},{"location":"worker-node/using-wn-containers/","text":"Using the Worker Node Containers \u00b6 The OSG worker node containers contain the suggested base environment for worker nodes. They can be used as a base image to build containers or to perform testing. The containers are available on Docker Hub . Available Containers \u00b6 Available tags include: latest : The latest version of the OSG worker node environment on the most recent supported OS. As of August 2019, this is OSG 3.5 and RHEL7. 3.5 : The OSG 3.5 release series on top of the most recent supported OS. As of August 2019, this is RHEL7. 3.5-el7 : The OSG 3.5 release series on top of a RHEL7 environment. 3.5-el8 : The OSG 3.5 release series on top of a RHEL8 environment. Building Upon the Container \u00b6 You may base the container on the OSG worker node by including it inside your Dockerfile : FROM opensciencegrid/osg-wn:latest You can replace latest with any tag listed above. Perform Testing \u00b6 You may perform testing from within the OSG worker node envionment by running the command: root@host # docker run -ti --rm opensciencegrid/osg-wn:latest /bin/bash","title":"Using WN Containers"},{"location":"worker-node/using-wn-containers/#using-the-worker-node-containers","text":"The OSG worker node containers contain the suggested base environment for worker nodes. They can be used as a base image to build containers or to perform testing. The containers are available on Docker Hub .","title":"Using the Worker Node Containers"},{"location":"worker-node/using-wn-containers/#available-containers","text":"Available tags include: latest : The latest version of the OSG worker node environment on the most recent supported OS. As of August 2019, this is OSG 3.5 and RHEL7. 3.5 : The OSG 3.5 release series on top of the most recent supported OS. As of August 2019, this is RHEL7. 3.5-el7 : The OSG 3.5 release series on top of a RHEL7 environment. 3.5-el8 : The OSG 3.5 release series on top of a RHEL8 environment.","title":"Available Containers"},{"location":"worker-node/using-wn-containers/#building-upon-the-container","text":"You may base the container on the OSG worker node by including it inside your Dockerfile : FROM opensciencegrid/osg-wn:latest You can replace latest with any tag listed above.","title":"Building Upon the Container"},{"location":"worker-node/using-wn-containers/#perform-testing","text":"You may perform testing from within the OSG worker node envionment by running the command: root@host # docker run -ti --rm opensciencegrid/osg-wn:latest /bin/bash","title":"Perform Testing"},{"location":"worker-node/using-wn/","text":"Worker Node Overview \u00b6 The Worker Node Client is a collection of useful software components that is expected to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that can be used to locate resources. This page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node. The OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself (preferred; most portable) or utilize CVMFS. Sites are not required to provide any specific tools ( gcc , lapack , blas , etc.) beyond the ones in the OSG worker node client and the base OS. If you would like to test the minimal OS environment that jobs can expect, you can test out your scientific software in the OSG Docker image . Hardware Recommendations \u00b6 Hardware Minimum Recommended Notes Core per pilot 1 8 Depends on the supported VOs. The total core count on every node in the cluster must be divisible by core per pilot. Memory per core 1024MB 2048MB Memory per core times core per pilot needs to be less than the total memory on every node. Do not overcommit. Scratch disk per core ( OSG_WN_TMP ) 2 GB 10 GB This can be overcommitted if a mix of different VO jobs is expected. CVMFS Cache per node (optional) 10 GB 20 GB Common Software Available on Worker Nodes \u00b6 The OSG worker node environment contains the following software: The supported set of CA certificates (located in $X509_CERT_DIR after the environment is set up) Proxy management tools: Create proxies: voms-proxy-init and grid-proxy-init Show proxy info: voms-proxy-info and grid-proxy-info Destroy the current proxy: voms-proxy-destroy and grid-proxy-destroy Data transfer tools: HTTP/plain FTP protocol tools (via system dependencies): wget and curl : standard tools for downloading files with HTTP and FTP Transfer clients GFAL -based client ( gfal-copy and others). GFAL supports SRM, GridFTP, and HTTP protocols. Globus GridFTP client ( globus-url-copy ) MyProxy client tools At some sites, these tools may not be available at the pilot launch. To setup the environment, do the following: user@host $ source $OSG_GRID /setup.sh This should be done by a pilot job, not by the end-user payload. The Worker Node Environment \u00b6 The following table outlines the various important directories and information in the worker node environment. A job running on an OSG worker node can refer to each directory using the corresponding environment variable. Several of them are defined as options in your OSG-Configure .ini files in /etc/osg/config.d . Custom variables and those that aren't listed may be defined in the Local Settings section . Environment Variable OSG-Configure section/option Purpose Notes $OSG_GRID Storage / grid_dir Location of additional environment variables. Pilots should source $OSG_GRID/setup.sh in order to guarantee the environment contains the worker node binaries in $PATH . $OSG_SQUID_LOCATION , Squid / location Location of a HTTP caching proxy server Utilize this service for downloading files via HTTP for cache-friendly workflows. $OSG_WN_TMP Storage / worker_node_temp Temporary storage area workspace for pilot job(s) Local to each worker node. See this section below for details. $X509_CERT_DIR Location of the CA certificates If not defined, defaults to /etc/grid-security/certificates . $_CONDOR_SCRATCH_DIR Suggested temporary storage for glideinWMS-based payloads. Users should prefer this environment variable over $OSG_WN_TMP if running inside glideinWMS. OSG_WN_TMP \u00b6 As described above OSG_WN_TMP is a temporary storage area on each worker node for pilot jobs to use as temporary scratch space. For site administrators \u00b6 Site administrators are responsible for cleaning up the contents of $OSG_WN_TMP (see table above for size recommendations). We recommend one of the following solutions: (Recommended) Use batch-system capabilities to create directories in the job scratch directory and bind mount them for the job so that the batch system performs the clean up. For example, HTCondor has this ability through MOUNT_UNDER_SCRATCH : MOUNT_UNDER_SCRATCH = $(MOUNT_UNDER_SCRATCH), <PATH TO OSG_WN_TMP> If using this method, space set aside for OSG_WN_TMP should be reallocated to the partition containing the job scratch directories. If using HTCondor, this will be the partition containing the path defined by the HTCondor EXECUTE configuration variable. Use batch-system capabilities to create a temporary, per-job directory that is cleaned up after each job is run. For SLURM, we recommend the Lua plugin Slurm-tmpdir . It will create per job /scratch and /tmp directories which will be cleaned up after the job completes. Periodically purge the directory (e.g. tmpwatch ). For VO managers \u00b6 Note The following advice applies to VO managers or maintainers of pilot software; end-users should contact their VO for the proper locations to stage temporary work (often, this will be either $TMPDIR or $_CONDOR_SCRATCH_DIR ). Be careful with using $OSG_WN_TMP ; at some sites, this directory might be shared with other VOs. We recommend creating a new sub-directory as a precaution: mkdir -p $OSG_WN_TMP /MYVO export mydir = ` mktemp -d -t MYVO ` cd $mydir # Run the rest of your application rm -rf $mydir The pilot should utilize $TMPDIR to communicate the location of temporary storage to payloads. A significant number of sites use the batch system to make an independent directory for each user job, and change $OSG_WN_TMP on the fly to point to this directory. There is no way to know in advance how much scratch disk space any given worker node has available; recall, what disk space is available may be shared among a number of job slots.","title":"Overview"},{"location":"worker-node/using-wn/#worker-node-overview","text":"The Worker Node Client is a collection of useful software components that is expected to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that can be used to locate resources. This page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node. The OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself (preferred; most portable) or utilize CVMFS. Sites are not required to provide any specific tools ( gcc , lapack , blas , etc.) beyond the ones in the OSG worker node client and the base OS. If you would like to test the minimal OS environment that jobs can expect, you can test out your scientific software in the OSG Docker image .","title":"Worker Node Overview"},{"location":"worker-node/using-wn/#hardware-recommendations","text":"Hardware Minimum Recommended Notes Core per pilot 1 8 Depends on the supported VOs. The total core count on every node in the cluster must be divisible by core per pilot. Memory per core 1024MB 2048MB Memory per core times core per pilot needs to be less than the total memory on every node. Do not overcommit. Scratch disk per core ( OSG_WN_TMP ) 2 GB 10 GB This can be overcommitted if a mix of different VO jobs is expected. CVMFS Cache per node (optional) 10 GB 20 GB","title":"Hardware Recommendations"},{"location":"worker-node/using-wn/#common-software-available-on-worker-nodes","text":"The OSG worker node environment contains the following software: The supported set of CA certificates (located in $X509_CERT_DIR after the environment is set up) Proxy management tools: Create proxies: voms-proxy-init and grid-proxy-init Show proxy info: voms-proxy-info and grid-proxy-info Destroy the current proxy: voms-proxy-destroy and grid-proxy-destroy Data transfer tools: HTTP/plain FTP protocol tools (via system dependencies): wget and curl : standard tools for downloading files with HTTP and FTP Transfer clients GFAL -based client ( gfal-copy and others). GFAL supports SRM, GridFTP, and HTTP protocols. Globus GridFTP client ( globus-url-copy ) MyProxy client tools At some sites, these tools may not be available at the pilot launch. To setup the environment, do the following: user@host $ source $OSG_GRID /setup.sh This should be done by a pilot job, not by the end-user payload.","title":"Common Software Available on Worker Nodes"},{"location":"worker-node/using-wn/#the-worker-node-environment","text":"The following table outlines the various important directories and information in the worker node environment. A job running on an OSG worker node can refer to each directory using the corresponding environment variable. Several of them are defined as options in your OSG-Configure .ini files in /etc/osg/config.d . Custom variables and those that aren't listed may be defined in the Local Settings section . Environment Variable OSG-Configure section/option Purpose Notes $OSG_GRID Storage / grid_dir Location of additional environment variables. Pilots should source $OSG_GRID/setup.sh in order to guarantee the environment contains the worker node binaries in $PATH . $OSG_SQUID_LOCATION , Squid / location Location of a HTTP caching proxy server Utilize this service for downloading files via HTTP for cache-friendly workflows. $OSG_WN_TMP Storage / worker_node_temp Temporary storage area workspace for pilot job(s) Local to each worker node. See this section below for details. $X509_CERT_DIR Location of the CA certificates If not defined, defaults to /etc/grid-security/certificates . $_CONDOR_SCRATCH_DIR Suggested temporary storage for glideinWMS-based payloads. Users should prefer this environment variable over $OSG_WN_TMP if running inside glideinWMS.","title":"The Worker Node Environment"},{"location":"worker-node/using-wn/#osg_wn_tmp","text":"As described above OSG_WN_TMP is a temporary storage area on each worker node for pilot jobs to use as temporary scratch space.","title":"OSG_WN_TMP"},{"location":"worker-node/using-wn/#for-site-administrators","text":"Site administrators are responsible for cleaning up the contents of $OSG_WN_TMP (see table above for size recommendations). We recommend one of the following solutions: (Recommended) Use batch-system capabilities to create directories in the job scratch directory and bind mount them for the job so that the batch system performs the clean up. For example, HTCondor has this ability through MOUNT_UNDER_SCRATCH : MOUNT_UNDER_SCRATCH = $(MOUNT_UNDER_SCRATCH), <PATH TO OSG_WN_TMP> If using this method, space set aside for OSG_WN_TMP should be reallocated to the partition containing the job scratch directories. If using HTCondor, this will be the partition containing the path defined by the HTCondor EXECUTE configuration variable. Use batch-system capabilities to create a temporary, per-job directory that is cleaned up after each job is run. For SLURM, we recommend the Lua plugin Slurm-tmpdir . It will create per job /scratch and /tmp directories which will be cleaned up after the job completes. Periodically purge the directory (e.g. tmpwatch ).","title":"For site administrators"},{"location":"worker-node/using-wn/#for-vo-managers","text":"Note The following advice applies to VO managers or maintainers of pilot software; end-users should contact their VO for the proper locations to stage temporary work (often, this will be either $TMPDIR or $_CONDOR_SCRATCH_DIR ). Be careful with using $OSG_WN_TMP ; at some sites, this directory might be shared with other VOs. We recommend creating a new sub-directory as a precaution: mkdir -p $OSG_WN_TMP /MYVO export mydir = ` mktemp -d -t MYVO ` cd $mydir # Run the rest of your application rm -rf $mydir The pilot should utilize $TMPDIR to communicate the location of temporary storage to payloads. A significant number of sites use the batch system to make an independent directory for each user job, and change $OSG_WN_TMP on the fly to point to this directory. There is no way to know in advance how much scratch disk space any given worker node has available; recall, what disk space is available may be shared among a number of job slots.","title":"For VO managers"}]}